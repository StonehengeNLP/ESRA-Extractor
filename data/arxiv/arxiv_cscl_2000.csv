,id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed
1351793,2009.10290,Xinyu Zuo,"Xinyu Zuo, Yubo Chen, Kang Liu and Jun Zhao","Event Coreference Resolution via a Multi-loss Neural Network without
  Using Argument Information",Published on SCIENCE CHINA Information Sciences,"SCIENCE CHINA Information Sciences, Volume 62, Issue
  11:212101(2019)",10.1007/s11432-018-9833-1,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Event coreference resolution(ECR) is an important task in Natural Language
Processing (NLP) and nearly all the existing approaches to this task rely on
event argument information. However, these methods tend to suffer from error
propagation from the stage of event argument extraction. Besides, not every
event mention contains all arguments of an event, and argument information may
confuse the model that events have arguments to detect event coreference in
real text. Furthermore, the context information of an event is useful to infer
the coreference between events. Thus, in order to reduce the errors propagated
from event argument extraction and use context information effectively, we
propose a multi-loss neural network model that does not need any argument
information to do the within-document event coreference resolution task and
achieve a significant performance than the state-of-the-art methods.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 02:48:48 GMT'}]",2020-09-23,"[['Zuo', 'Xinyu', ''], ['Chen', 'Yubo', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]"
1345499,2009.03996,Michael S. Fiske,Michael Stephen Fiske,Combining Determinism and Indeterminism,10 pages,,,,math.LO cs.CC cs.CL math.GR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Our goal is to construct mathematical operations that combine indeterminism
measured from quantum randomness with computational determinism so that
non-mechanistic behavior is preserved in the computation. Formally, some
results about operations applied to computably enumerable (c.e.) and bi-immune
sets are proven here, where the operations preserve bi-immunity. While
developing rearrangement operations on the natural numbers, we discovered that
the bi-immune rearrangements generate an uncountable subgroup of the infinite
symmetric group on the natural numbers. We show that this new subgroup contains
the bounded symmetric group on the natural numbers, and consequently is highly
transitive. The complete structure of this new subgroup and its subgroups
generated by one or more bi-immune rearrangements is unknown.
","[{'version': 'v1', 'created': 'Wed, 2 Sep 2020 01:30:00 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Sep 2020 20:49:50 GMT'}, {'version': 'v3', 'created': 'Mon, 21 Sep 2020 20:00:10 GMT'}]",2020-09-23,"[['Fiske', 'Michael Stephen', '']]"
1351837,2009.10334,Yerbolat Khassanov,"Yerbolat Khassanov, Saida Mussakhojayeva, Almas Mirzakhmetov, Alen
  Adiyev, Mukhamet Nurpeiissov and Huseyin Atakan Varol","A Crowdsourced Open-Source Kazakh Speech Corpus and Initial Speech
  Recognition Baseline","8 pages, 2 figures, 3 tables",,,,eess.AS cs.CL cs.SD,http://creativecommons.org/licenses/by/4.0/,"  We present an open-source speech corpus for the Kazakh language. The Kazakh
speech corpus (KSC) contains around 335 hours of transcribed audio comprising
over 154,000 utterances spoken by participants from different regions, age
groups, and gender. It was carefully inspected by native Kazakh speakers to
ensure high quality. The KSC is the largest publicly available database
developed to advance various Kazakh speech and language processing
applications. In this paper, we first describe the data collection and
prepossessing procedures followed by the description of the database
specifications. We also share our experience and challenges faced during
database construction. To demonstrate the reliability of the database, we
performed the preliminary speech recognition experiments. The experimental
results imply that the quality of audio and transcripts are promising. To
enable experiment reproducibility and ease the corpus usage, we also released
the ESPnet recipe.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 05:57:15 GMT'}]",2020-09-23,"[['Khassanov', 'Yerbolat', ''], ['Mussakhojayeva', 'Saida', ''], ['Mirzakhmetov', 'Almas', ''], ['Adiyev', 'Alen', ''], ['Nurpeiissov', 'Mukhamet', ''], ['Varol', 'Huseyin Atakan', '']]"
1351071,2009.09568,Su Zhu,"Su Zhu, Ruisheng Cao, Lu Chen and Kai Yu","Vector Projection Network for Few-shot Slot Tagging in Natural Language
  Understanding",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Few-shot slot tagging becomes appealing for rapid domain transfer and
adaptation, motivated by the tremendous development of conversational dialogue
systems. In this paper, we propose a vector projection network for few-shot
slot tagging, which exploits projections of contextual word embeddings on each
target label vector as word-label similarities. Essentially, this approach is
equivalent to a normalized linear model with an adaptive bias. The contrastive
experiment demonstrates that our proposed vector projection based similarity
metric can significantly surpass other variants. Specifically, in the five-shot
setting on benchmarks SNIPS and NER, our method outperforms the strongest
few-shot learning baseline by $6.30$ and $13.79$ points on F$_1$ score,
respectively. Our code will be released at
https://github.com/sz128/few_shot_slot_tagging_and_NER.
","[{'version': 'v1', 'created': 'Mon, 21 Sep 2020 01:52:32 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Sep 2020 07:48:21 GMT'}]",2020-09-23,"[['Zhu', 'Su', ''], ['Cao', 'Ruisheng', ''], ['Chen', 'Lu', ''], ['Yu', 'Kai', '']]"
1351829,2009.10326,Zhi Chen,"Zhi Chen, Lu Chen, Xiaoyuan Liu, and Kai Yu","Distributed Structured Actor-Critic Reinforcement Learning for Universal
  Dialogue Management","12 pages, 7 figures",,10.1109/TASLP.2020.3013392,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task-oriented spoken dialogue system (SDS) aims to assist a human user in
accomplishing a specific task (e.g., hotel booking). The dialogue management is
a core part of SDS. There are two main missions in dialogue management:
dialogue belief state tracking (summarising conversation history) and dialogue
decision-making (deciding how to reply to the user). In this work, we only
focus on devising a policy that chooses which dialogue action to respond to the
user. The sequential system decision-making process can be abstracted into a
partially observable Markov decision process (POMDP). Under this framework,
reinforcement learning approaches can be used for automated policy
optimization. In the past few years, there are many deep reinforcement learning
(DRL) algorithms, which use neural networks (NN) as function approximators,
investigated for dialogue policy.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 05:39:31 GMT'}]",2020-09-23,"[['Chen', 'Zhi', ''], ['Chen', 'Lu', ''], ['Liu', 'Xiaoyuan', ''], ['Yu', 'Kai', '']]"
1351818,2009.10315,Aneesh Vartakavi,Aneesh Vartakavi and Amanmeet Garg,PodSumm -- Podcast Audio Summarization,For PodRecs: Workshop on Podcast Recommendations at RecSys 2020,,,,cs.CL cs.LG cs.MM,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The diverse nature, scale, and specificity of podcasts present a unique
challenge to content discovery systems. Listeners often rely on text
descriptions of episodes provided by the podcast creators to discover new
content. Some factors like the presentation style of the narrator and
production quality are significant indicators of subjective user preference but
are difficult to quantify and not reflected in the text descriptions provided
by the podcast creators. We propose the automated creation of podcast audio
summaries to aid in content discovery and help listeners to quickly preview
podcast content before investing time in listening to an entire episode. In
this paper, we present a method to automatically construct a podcast summary
via guidance from the text-domain. Our method performs two key steps, namely,
audio to text transcription and text summary generation. Motivated by a lack of
datasets for this task, we curate an internal dataset, find an effective scheme
for data augmentation, and design a protocol to gather summaries from
annotators. We fine-tune a PreSumm[10] model with our augmented dataset and
perform an ablation study. Our method achieves ROUGE-F(1/2/L) scores of
0.63/0.53/0.63 on our dataset. We hope these results may inspire future
research in this direction.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 04:49:33 GMT'}]",2020-09-23,"[['Vartakavi', 'Aneesh', ''], ['Garg', 'Amanmeet', '']]"
1347988,2009.06485,Jonathan Lenchner,Jonathan Lenchner,"A Finitist's Manifesto: Do we need to Reformulate the Foundations of
  Mathematics?",,,,,math.LO cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is a problem with the foundations of classical mathematics, and
potentially even with the foundations of computer science, that mathematicians
have by-and-large ignored. This essay is a call for practicing mathematicians
who have been sleep-walking in their infinitary mathematical paradise to take
heed. Much of mathematics relies upon either (i) the ""existence'"" of objects
that contain an infinite number of elements, (ii) our ability, ""in theory"", to
compute with an arbitrary level of precision, or (iii) our ability, ""in
theory"", to compute for an arbitrarily large number of time steps. All of
calculus relies on the notion of a limit. The monumental results of real and
complex analysis rely on a seamless notion of the ""continuum"" of real numbers,
which extends in the plane to the complex numbers and gives us, among other
things, ""rigorous"" definitions of continuity, the derivative, various different
integrals, as well as the fundamental theorems of calculus and of algebra --
the former of which says that the derivative and integral can be viewed as
inverse operations, and the latter of which says that every polynomial over
$\mathbb{C}$ has a complex root. This essay is an inquiry into whether there is
any way to assign meaning to the notions of ""existence"" and ""in theory'"" in (i)
to (iii) above.
","[{'version': 'v1', 'created': 'Mon, 14 Sep 2020 14:44:08 GMT'}]",2020-09-23,"[['Lenchner', 'Jonathan', '']]"
1329961,2008.02239,Aleksander Mendoza-Drosik,Aleksander Mendoza-Drosik,Glushkov's construction for functional subsequential transducers,,,,,cs.FL cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Glushkov's construction has many interesting properties and they become even
more evident when applied to transducers. This article strives to show the wast
range of possible extensions and optimisations for this algorithm. Special
flavour of regular expressions is introduced, which can be efficiently
converted to $\epsilon$-free functional subsequential weighted finite state
transducers. Produced automata are very compact, as they contain only one state
for each symbol (from input alphabet) of original expression and only one
transition for each range of symbols, no matter how large. Such compactified
ranges of transitions allow for efficient binary search lookup during automaton
evaluation. All the methods and algorithms presented here were used to
implement open-source compiler of regular expressions for multitape
transducers.
","[{'version': 'v1', 'created': 'Wed, 5 Aug 2020 17:09:58 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Aug 2020 16:09:54 GMT'}, {'version': 'v3', 'created': 'Fri, 21 Aug 2020 17:42:10 GMT'}, {'version': 'v4', 'created': 'Tue, 22 Sep 2020 13:36:57 GMT'}]",2020-09-23,"[['Mendoza-Drosik', 'Aleksander', '']]"
1351742,2009.10239,EPTCS,"Kinjal Basu, Sarat Chandra Varanasi, Farhad Shakerin, Gopal Gupta",SQuARE: Semantics-based Question Answering and Reasoning Engine,"In Proceedings ICLP 2020, arXiv:2009.09158","EPTCS 325, 2020, pp. 73-86",10.4204/EPTCS.325.13,,cs.AI cs.CL cs.LO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding the meaning of a text is a fundamental challenge of natural
language understanding (NLU) and from its early days, it has received
significant attention through question answering (QA) tasks. We introduce a
general semantics-based framework for natural language QA and also describe the
SQuARE system, an application of this framework. The framework is based on the
denotational semantics approach widely used in programming language research.
In our framework, valuation function maps syntax tree of the text to its
commonsense meaning represented using basic knowledge primitives (the semantic
algebra) coded using answer set programming (ASP). We illustrate an application
of this framework by using VerbNet primitives as our semantic algebra and a
novel algorithm based on partial tree matching that generates an answer set
program that represents the knowledge in the text. A question posed against
that text is converted into an ASP query using the same framework and executed
using the s(CASP) goal-directed ASP system. Our approach is based purely on
(commonsense) reasoning. SQuARE achieves 100% accuracy on all the five datasets
of bAbI QA tasks that we have tested. The significance of our work is that,
unlike other machine learning based approaches, ours is based on
""understanding"" the text and does not require any training. SQuARE can also
generate an explanation for an answer while maintaining high accuracy.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 00:48:18 GMT'}]",2020-09-23,"[['Basu', 'Kinjal', ''], ['Varanasi', 'Sarat Chandra', ''], ['Shakerin', 'Farhad', ''], ['Gupta', 'Gopal', '']]"
1216978,1912.05082,Caroline Schroeder,"Caroline T. Schroeder, Amir Zeldes",A Collaborative Ecosystem for Digital Coptic Studies,"9 pages; paper presented at the Stanford University CESTA Workshop
  ""Collecting, Preserving and Disseminating Endangered Cultural Heritage for
  New Understandings Through Multilingual Approaches""",,,,cs.CL cs.DL,http://creativecommons.org/licenses/by/4.0/,"  Scholarship on underresourced languages bring with them a variety of
challenges which make access to the full spectrum of source materials and their
evaluation difficult. For Coptic in particular, large scale analyses and any
kind of quantitative work become difficult due to the fragmentation of
manuscripts, the highly fusional nature of an incorporational morphology, and
the complications of dealing with influences from Hellenistic era Greek, among
other concerns. Many of these challenges, however, can be addressed using
Digital Humanities tools and standards. In this paper, we outline some of the
latest developments in Coptic Scriptorium, a DH project dedicated to bringing
Coptic resources online in uniform, machine readable, and openly available
formats. Collaborative web-based tools create online 'virtual departments' in
which scholars dispersed sparsely across the globe can collaborate, and natural
language processing tools counterbalance the scarcity of trained editors by
enabling machine processing of Coptic text to produce searchable, annotated
corpora.
","[{'version': 'v1', 'created': 'Wed, 11 Dec 2019 02:03:31 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Jun 2020 19:39:30 GMT'}, {'version': 'v3', 'created': 'Mon, 21 Sep 2020 19:22:24 GMT'}]",2020-09-23,"[['Schroeder', 'Caroline T.', ''], ['Zeldes', 'Amir', '']]"
1280577,2005.00782,Pei Zhou,"Pei Zhou, Rahul Khanna, Bill Yuchen Lin, Daniel Ho, Jay Pujara, Xiang
  Ren","RICA: Evaluating Robust Inference Capabilities Based on Commonsense
  Axioms","14 pages, 7 figures. Work in progress",,,,cs.CL cs.AI cs.LO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models (PTLM) have impressive performance on commonsense
inference benchmarks, but their ability to practically employ commonsense to
communicate with humans is fiercely debated. Prior evaluations of PTLMs have
focused on factual world knowledge or the ability to reason when the necessary
knowledge is provided explicitly. However, effective communication with humans
requires inferences based on implicit commonsense relationships, and robustness
despite paraphrasing. In the pursuit of advancing fluid human-AI communication,
we propose a new challenge, RICA, that evaluates the capabilities of making
commonsense inferences and the robustness of these inferences to language
variations. In our work, we develop a systematic procedure to probe PTLMs
across three different evaluation settings. Extensive experiments on our
generated probe sets show that PTLMs perform no better than random guessing
(even with fine-tuning), are heavily impacted by statistical biases, and are
not robust to perturbation attacks. Our framework and probe sets can help
future work improve PTLMs' inference abilities and robustness to linguistic
variations--bringing us closer to more fluid communication.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 10:36:55 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 04:11:53 GMT'}]",2020-09-24,"[['Zhou', 'Pei', ''], ['Khanna', 'Rahul', ''], ['Lin', 'Bill Yuchen', ''], ['Ho', 'Daniel', ''], ['Pujara', 'Jay', ''], ['Ren', 'Xiang', '']]"
1352763,2009.11260,Weiwei Hou,"Weiwei Hou, Hanna Suominen, Piotr Koniusz, Sabrina Caldwell and Tom
  Gedeon",A Token-wise CNN-based Method for Sentence Compression,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentence compression is a Natural Language Processing (NLP) task aimed at
shortening original sentences and preserving their key information. Its
applications can benefit many fields e.g. one can build tools for language
education. However, current methods are largely based on Recurrent Neural
Network (RNN) models which suffer from poor processing speed. To address this
issue, in this paper, we propose a token-wise Convolutional Neural Network, a
CNN-based model along with pre-trained Bidirectional Encoder Representations
from Transformers (BERT) features for deletion-based sentence compression. We
also compare our model with RNN-based models and fine-tuned BERT. Although one
of the RNN-based models outperforms marginally other models given the same
input, our CNN-based model was ten times faster than the RNN-based approach.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 17:12:06 GMT'}]",2020-09-24,"[['Hou', 'Weiwei', ''], ['Suominen', 'Hanna', ''], ['Koniusz', 'Piotr', ''], ['Caldwell', 'Sabrina', ''], ['Gedeon', 'Tom', '']]"
882734,1708.07722,Ramon Ferrer i Cancho,"Xinying Chen, Carlos G\'omez-Rodr\'iguez and Ramon Ferrer-i-Cancho",A dependency look at the reality of constituency,Final version,"Glottometrics 40, 104-106 (2018)",,,cs.CL cs.SI physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A comment on ""Neurophysiological dynamics of phrase-structure building during
sentence processing"" by Nelson et al (2017), Proceedings of the National
Academy of Sciences USA 114(18), E3669-E3678.
","[{'version': 'v1', 'created': 'Thu, 24 Aug 2017 04:59:53 GMT'}, {'version': 'v2', 'created': 'Mon, 11 Sep 2017 08:27:18 GMT'}, {'version': 'v3', 'created': 'Fri, 16 Mar 2018 08:17:18 GMT'}]",2020-09-24,"[['Chen', 'Xinying', ''], ['Gómez-Rodríguez', 'Carlos', ''], ['Ferrer-i-Cancho', 'Ramon', '']]"
794254,1611.08807,Ramon Ferrer i Cancho,"Bernardino Casas, Neus Catal\`a, Ramon Ferrer-i-Cancho, Antoni
  Hern\'andez-Fern\'andez and Jaume Baixeries",The polysemy of the words that children learn over time,"Substantially revised version based on referee comments from
  Interaction Studies","Interaction Studies 19 (3), 389-426 (2018)",10.1075/is.16036.cas,,cs.CL physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Here we study polysemy as a potential learning bias in vocabulary learning in
children. Words of low polysemy could be preferred as they reduce the
disambiguation effort for the listener. However, such preference could be a
side-effect of another bias: the preference of children for nouns in
combination with the lower polysemy of nouns with respect to other
part-of-speech categories. Our results show that mean polysemy in children
increases over time in two phases, i.e. a fast growth till the 31st month
followed by a slower tendency towards adult speech. In contrast, this evolution
is not found in adults interacting with children. This suggests that children
have a preference for non-polysemous words in their early stages of vocabulary
acquisition. Interestingly, the evolutionary pattern described above weakens
when controlling for syntactic category (noun, verb, adjective or adverb) but
it does not disappear completely, suggesting that it could result from
acombination of a standalone bias for low polysemy and a preference for nouns.
","[{'version': 'v1', 'created': 'Sun, 27 Nov 2016 08:32:19 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Mar 2019 12:57:23 GMT'}]",2020-09-24,"[['Casas', 'Bernardino', ''], ['Català', 'Neus', ''], ['Ferrer-i-Cancho', 'Ramon', ''], ['Hernández-Fernández', 'Antoni', ''], ['Baixeries', 'Jaume', '']]"
1352639,2009.11136,Felix Stahlberg,Felix Stahlberg and Shankar Kumar,Seq2Edits: Sequence Transduction Using Span-level Edit Operations,Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose Seq2Edits, an open-vocabulary approach to sequence editing for
natural language processing (NLP) tasks with a high degree of overlap between
input and output texts. In this approach, each sequence-to-sequence
transduction is represented as a sequence of edit operations, where each
operation either replaces an entire source span with target tokens or keeps it
unchanged. We evaluate our method on five NLP tasks (text normalization,
sentence fusion, sentence splitting & rephrasing, text simplification, and
grammatical error correction) and report competitive results across the board.
For grammatical error correction, our method speeds up inference by up to 5.2x
compared to full sequence models because inference time depends on the number
of edits rather than the number of target tokens. For text normalization,
sentence fusion, and grammatical error correction, our approach improves
explainability by associating each edit operation with a human-readable tag.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 13:28:38 GMT'}]",2020-09-24,"[['Stahlberg', 'Felix', ''], ['Kumar', 'Shankar', '']]"
1301872,2006.07358,Thomas Searle,"Thomas Searle, Zina Ibrahim, Richard Dobson","Comparing Natural Language Processing Techniques for Alzheimer's
  Dementia Prediction in Spontaneous Speech","Submitted to INTERSPEECH 2020: Alzheimer's Dementia Recognition
  through Spontaneous Speech The ADReSS Challenge Workshop",,,,cs.LG cs.CL cs.SD eess.AS stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Alzheimer's Dementia (AD) is an incurable, debilitating, and progressive
neurodegenerative condition that affects cognitive function. Early diagnosis is
important as therapeutics can delay progression and give those diagnosed vital
time. Developing models that analyse spontaneous speech could eventually
provide an efficient diagnostic modality for earlier diagnosis of AD. The
Alzheimer's Dementia Recognition through Spontaneous Speech task offers
acoustically pre-processed and balanced datasets for the classification and
prediction of AD and associated phenotypes through the modelling of spontaneous
speech. We exclusively analyse the supplied textual transcripts of the
spontaneous speech dataset, building and comparing performance across numerous
models for the classification of AD vs controls and the prediction of Mental
Mini State Exam scores. We rigorously train and evaluate Support Vector
Machines (SVMs), Gradient Boosting Decision Trees (GBDT), and Conditional
Random Fields (CRFs) alongside deep learning Transformer based models. We find
our top performing models to be a simple Term Frequency-Inverse Document
Frequency (TF-IDF) vectoriser as input into a SVM model and a pre-trained
Transformer based model `DistilBERT' when used as an embedding layer into
simple linear models. We demonstrate test set scores of 0.81-0.82 across
classification metrics and a RMSE of 4.58.
","[{'version': 'v1', 'created': 'Fri, 12 Jun 2020 17:51:16 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 14:21:22 GMT'}]",2020-09-24,"[['Searle', 'Thomas', ''], ['Ibrahim', 'Zina', ''], ['Dobson', 'Richard', '']]"
1351708,2009.10205,Mohammad Sadegh Rasooli,"Mohammad Sadegh Rasooli, Pegah Safari, Amirsaeid Moloodi, Alireza
  Nourian",The Persian Dependency Treebank Made Universal,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We describe an automatic method for converting the Persian Dependency
Treebank (Rasooli et al, 2013) to Universal Dependencies. This treebank
contains 29107 sentences. Our experiments along with manual linguistic analysis
show that our data is more compatible with Universal Dependencies than the
Uppsala Persian Universal Dependency Treebank (Seraji et al., 2016), and is
larger in size and more diverse in vocabulary. Our data brings in a labeled
attachment F-score of 85.2 in supervised parsing. Our delexicalized
Persian-to-English parser transfer experiments show that a parsing model
trained on our data is ~2% absolutely more accurate than that of Seraji et al.
(2016) in terms of labeled attachment score.
","[{'version': 'v1', 'created': 'Mon, 21 Sep 2020 22:34:13 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 02:44:07 GMT'}]",2020-09-24,"[['Rasooli', 'Mohammad Sadegh', ''], ['Safari', 'Pegah', ''], ['Moloodi', 'Amirsaeid', ''], ['Nourian', 'Alireza', '']]"
1270960,2004.06190,Alexios Gidiotis,Alexios Gidiotis and Grigorios Tsoumakas,A Divide-and-Conquer Approach to the Summarization of Long Documents,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a novel divide-and-conquer method for the neural summarization of
long documents. Our method exploits the discourse structure of the document and
uses sentence similarity to split the problem into an ensemble of smaller
summarization problems. In particular, we break a long document and its summary
into multiple source-target pairs, which are used for training a model that
learns to summarize each part of the document separately. These partial
summaries are then combined in order to produce a final complete summary. With
this approach we can decompose the problem of long document summarization into
smaller and simpler problems, reducing computational complexity and creating
more training examples, which at the same time contain less noise in the target
summaries compared to the standard approach. We demonstrate that this approach
paired with different summarization models, including sequence-to-sequence RNNs
and Transformers, can lead to improved summarization performance. Our best
models achieve results that are on par with the state-of-the-art in two two
publicly available datasets of academic articles.
","[{'version': 'v1', 'created': 'Mon, 13 Apr 2020 20:38:49 GMT'}, {'version': 'v2', 'created': 'Sun, 17 May 2020 08:45:45 GMT'}, {'version': 'v3', 'created': 'Wed, 23 Sep 2020 14:10:54 GMT'}]",2020-09-24,"[['Gidiotis', 'Alexios', ''], ['Tsoumakas', 'Grigorios', '']]"
1352358,2009.10855,Y-Lan Boureau,"Eric Michael Smith, Diana Gonzalez-Rico, Emily Dinan, Y-Lan Boureau",Controlling Style in Generated Dialogue,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open-domain conversation models have become good at generating
natural-sounding dialogue, using very large architectures with billions of
trainable parameters. The vast training data required to train these
architectures aggregates many different styles, tones, and qualities. Using
that data to train a single model makes it difficult to use the model as a
consistent conversational agent, e.g. with a stable set of persona traits and a
typical style of expression. Several architectures affording control mechanisms
over generation architectures have been proposed, each with different
trade-offs. However, it remains unclear whether their use in dialogue is
viable, and what the trade-offs look like with the most recent state-of-the-art
conversational architectures. In this work, we adapt three previously proposed
controllable generation architectures to open-domain dialogue generation,
controlling the style of the generation to match one among about 200 possible
styles. We compare their respective performance and tradeoffs, and show how
they can be used to provide insights into existing conversational datasets, and
generate a varied set of styled conversation replies.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 23:21:04 GMT'}]",2020-09-24,"[['Smith', 'Eric Michael', ''], ['Gonzalez-Rico', 'Diana', ''], ['Dinan', 'Emily', ''], ['Boureau', 'Y-Lan', '']]"
580464,1412.2486,Ramon Ferrer i Cancho,Ramon Ferrer-i-Cancho,Optimization models of natural communication,,"Journal of Quantitative Linguistics 25 (3), 207-237 (2018)",10.1080/09296174.2017.1366095,,physics.soc-ph cs.CL physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A family of information theoretic models of communication was introduced more
than a decade ago to explain the origins of Zipf's law for word frequencies.
The family is a based on a combination of two information theoretic principles:
maximization of mutual information between forms and meanings and minimization
of form entropy. The family also sheds light on the origins of three other
patterns: the principle of contrast, a related vocabulary learning bias and the
meaning-frequency law. Here two important components of the family, namely the
information theoretic principles and the energy function that combines them
linearly, are reviewed from the perspective of psycholinguistics, language
learning, information theory and synergetic linguistics. The minimization of
this linear function is linked to the problem of compression of standard
information theory and might be tuned by self-organization.
","[{'version': 'v1', 'created': 'Mon, 8 Dec 2014 09:05:40 GMT'}, {'version': 'v2', 'created': 'Mon, 31 Jul 2017 09:23:06 GMT'}]",2020-09-24,"[['Ferrer-i-Cancho', 'Ramon', '']]"
1347709,2009.06206,Ningyu Zhang,"Ningyu Zhang, Luoqiu Li, Shumin Deng, Haiyang Yu, Xu Cheng, Wei Zhang,
  Huajun Chen","Can Fine-tuning Pre-trained Models Lead to Perfect NLP? A Study of the
  Generalizability of Relation Extraction",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fine-tuning pre-trained models have achieved impressive performance on
standard natural language processing benchmarks. However, the resultant model
generalizability remains poorly understood. We do not know, for example, how
excellent performance can lead to the perfection of generalization models. In
this study, we analyze a fine-tuned BERT model from different perspectives
using relation extraction. We also characterize the differences in
generalization techniques according to our proposed improvements. From
empirical experimentation, we find that BERT suffers a bottleneck in terms of
robustness by way of randomizations, adversarial and counterfactual tests, and
biases (i.e., selection and semantic). These findings highlight opportunities
for future improvements. Our open-sourced testbed DiagnoseRE is available in
https://github.com/zjunlp/DiagnoseRE/.
","[{'version': 'v1', 'created': 'Mon, 14 Sep 2020 05:24:28 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 02:34:42 GMT'}]",2020-09-24,"[['Zhang', 'Ningyu', ''], ['Li', 'Luoqiu', ''], ['Deng', 'Shumin', ''], ['Yu', 'Haiyang', ''], ['Cheng', 'Xu', ''], ['Zhang', 'Wei', ''], ['Chen', 'Huajun', '']]"
1352253,2009.10750,Sahisnu Mazumder,"Bing Liu, Sahisnu Mazumder",Lifelong Learning Dialogue Systems: Chatbots that Self-Learn On the Job,,,,,cs.CL cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialogue systems, also called chatbots, are now used in a wide range of
applications. However, they still have some major weaknesses. One key weakness
is that they are typically trained from manually-labeled data and/or written
with handcrafted rules, and their knowledge bases (KBs) are also compiled by
human experts. Due to the huge amount of manual effort involved, they are
difficult to scale and also tend to produce many errors ought to their limited
ability to understand natural language and the limited knowledge in their KBs.
Thus, the level of user satisfactory is often low. In this paper, we propose to
dramatically improve this situation by endowing the system the ability to
continually learn (1) new world knowledge, (2) new language expressions to
ground them to actions, and (3) new conversational skills, during conversation
or ""on the job"" by themselves so that as the systems chat more and more with
users, they become more and more knowledgeable and are better and better able
to understand diverse natural language expressions and improve their
conversational skills. A key approach to achieving these is to exploit the
multi-user environment of such systems to self-learn through interactions with
users via verb and non-verb means. The paper discusses not only key challenges
and promising directions to learn from users during conversation but also how
to ensure the correctness of the learned knowledge.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 18:10:08 GMT'}]",2020-09-24,"[['Liu', 'Bing', ''], ['Mazumder', 'Sahisnu', '']]"
1352281,2009.10778,Danqing Zhang,"Danqing Zhang, Tao Li, Haiyang Zhang, Bing Yin",On Data Augmentation for Extreme Multi-label Classification,,,,,cs.CL cs.AI cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we focus on data augmentation for the extreme multi-label
classification (XMC) problem. One of the most challenging issues of XMC is the
long tail label distribution where even strong models suffer from insufficient
supervision. To mitigate such label bias, we propose a simple and effective
augmentation framework and a new state-of-the-art classifier. Our augmentation
framework takes advantage of the pre-trained GPT-2 model to generate
label-invariant perturbations of the input texts to augment the existing
training data. As a result, it present substantial improvements over baseline
models. Our contributions are two-factored: (1) we introduce a new
state-of-the-art classifier that uses label attention with RoBERTa and combine
it with our augmentation framework for further improvement; (2) we present a
broad study on how effective are different augmentation methods in the XMC
task.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 19:31:08 GMT'}]",2020-09-24,"[['Zhang', 'Danqing', ''], ['Li', 'Tao', ''], ['Zhang', 'Haiyang', ''], ['Yin', 'Bing', '']]"
1352622,2009.11119,Qi Qin,"Qi Qin, Wenpeng Hu, Bing Liu",Text Classification with Novelty Detection,,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper studies the problem of detecting novel or unexpected instances in
text classification. In traditional text classification, the classes appeared
in testing must have been seen in training. However, in many applications, this
is not the case because in testing, we may see unexpected instances that are
not from any of the training classes. In this paper, we propose a significantly
more effective approach that converts the original problem to a pair-wise
matching problem and then outputs how probable two instances belong to the same
class. Under this approach, we present two models. The more effective model
uses two embedding matrices of a pair of instances as two channels of a CNN.
The output probabilities from such pairs are used to judge whether a test
instance is from a seen class or is novel/unexpected. Experimental results show
that the proposed method substantially outperforms the state-of-the-art
baselines.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 12:54:34 GMT'}]",2020-09-24,"[['Qin', 'Qi', ''], ['Hu', 'Wenpeng', ''], ['Liu', 'Bing', '']]"
1352350,2009.10847,Mikhail Galkin,"Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck,
  Jens Lehmann",Message Passing for Hyper-Relational Knowledge Graphs,Accepted to EMNLP 2020,,,,cs.LG cs.AI cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating
additional key-value pairs along with the main triple to disambiguate, or
restrict the validity of a fact. In this work, we propose a message passing
based graph encoder - StarE capable of modeling such hyper-relational KGs.
Unlike existing approaches, StarE can encode an arbitrary number of additional
information (qualifiers) along with the main triple while keeping the semantic
roles of qualifiers and triples intact. We also demonstrate that existing
benchmarks for evaluating link prediction (LP) performance on hyper-relational
KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset
- WD50K. Our experiments demonstrate that StarE based LP model outperforms
existing approaches across multiple benchmarks. We also confirm that leveraging
qualifiers is vital for link prediction with gains up to 25 MRR points compared
to triple-based representations.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 22:38:54 GMT'}]",2020-09-24,"[['Galkin', 'Mikhail', ''], ['Trivedi', 'Priyansh', ''], ['Maheshwari', 'Gaurav', ''], ['Usbeck', 'Ricardo', ''], ['Lehmann', 'Jens', '']]"
458947,1309.1939,Ramon Ferrer i Cancho,Ramon Ferrer-i-Cancho,"The placement of the head that minimizes online memory: a complex
  systems approach","Minor changes (language improved; typos in Eqs. 5, 6 and 13
  corrected)","Language Dynamics and Change 5 (1), 114-137 (2015)",10.1163/22105832-00501007,,cs.CL nlin.AO physics.data-an physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is well known that the length of a syntactic dependency determines its
online memory cost. Thus, the problem of the placement of a head and its
dependents (complements or modifiers) that minimizes online memory is
equivalent to the problem of the minimum linear arrangement of a star tree.
However, how that length is translated into cognitive cost is not known. This
study shows that the online memory cost is minimized when the head is placed at
the center, regardless of the function that transforms length into cost,
provided only that this function is strictly monotonically increasing. Online
memory defines a quasi-convex adaptive landscape with a single central minimum
if the number of elements is odd and two central minima if that number is even.
We discuss various aspects of the dynamics of word order of subject (S), verb
(V) and object (O) from a complex systems perspective and suggest that word
orders tend to evolve by swapping adjacent constituents from an initial or
early SOV configuration that is attracted towards a central word order by
online memory minimization. We also suggest that the stability of SVO is due to
at least two factors, the quasi-convex shape of the adaptive landscape in the
online memory dimension and online memory adaptations that avoid regression to
SOV. Although OVS is also optimal for placing the verb at the center, its low
frequency is explained by its long distance to the seminal SOV in the
permutation space.
","[{'version': 'v1', 'created': 'Sun, 8 Sep 2013 08:31:09 GMT'}, {'version': 'v2', 'created': 'Sun, 15 Mar 2015 07:12:49 GMT'}]",2020-09-24,"[['Ferrer-i-Cancho', 'Ramon', '']]"
1347904,2009.06401,Pepa Atanasova,"Wojciech Ostrowski, Arnav Arora, Pepa Atanasova, Isabelle Augenstein",Multi-Hop Fact Checking of Political Claims,11 pages,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, novel multi-hop models and datasets have been introduced to achieve
more complex natural language reasoning with neural networks. One notable task
that requires multi-hop reasoning is fact checking, where a chain of connected
evidence pieces leads to the final verdict of a claim. However, existing
datasets do not provide annotations for the gold evidence pieces, which is a
critical aspect for improving the explainability of fact checking systems. The
only exception is the FEVER dataset, which is artificially constructed based on
Wikipedia and does not use naturally occurring political claims and evidence
pages, which is more challenging. Most claims in FEVER only have one evidence
sentence associated with them and require no reasoning to make label
predictions -- the small number of instances with two evidence sentences only
require simple reasoning. In this paper, we study how to perform more complex
claim verification on naturally occurring claims with multiple hops over
evidence chunks. We first construct a small annotated dataset, PolitiHop, of
reasoning chains for claim verification. We then compare the dataset to other
existing multi-hop datasets and study how to transfer knowledge from more
extensive in- and out-of-domain resources to PolitiHop. We find that the task
is complex, and achieve the best performance using an architecture that
specifically models reasoning over evidence chains in combination with
in-domain transfer learning.
","[{'version': 'v1', 'created': 'Thu, 10 Sep 2020 13:54:15 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 17:00:26 GMT'}]",2020-09-24,"[['Ostrowski', 'Wojciech', ''], ['Arora', 'Arnav', ''], ['Atanasova', 'Pepa', ''], ['Augenstein', 'Isabelle', '']]"
1352710,2009.11207,Tiziano Piccardi,"Tiziano Piccardi, Robert West",Crosslingual Topic Modeling with WikiPDA,"10 pages, first version",,,,cs.CL cs.DL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present Wikipedia-based Polyglot Dirichlet Allocation (WikiPDA), a
crosslingual topic model that learns to represent Wikipedia articles written in
any language as distributions over a common set of language-independent topics.
It leverages the fact that Wikipedia articles link to each other and are mapped
to concepts in the Wikidata knowledge base, such that, when represented as bags
of links, articles are inherently language-independent. WikiPDA works in two
steps, by first densifying bags of links using matrix completion and then
training a standard monolingual topic model. A human evaluation shows that
WikiPDA produces more coherent topics than monolingual text-based LDA, thus
offering crosslinguality at no cost. We demonstrate WikiPDA's utility in two
applications: a study of topical biases in 28 Wikipedia editions, and
crosslingual supervised classification. Finally, we highlight WikiPDA's
capacity for zero-shot language transfer, where a model is reused for new
languages without any fine-tuning.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 15:19:27 GMT'}]",2020-09-24,"[['Piccardi', 'Tiziano', ''], ['West', 'Robert', '']]"
1352295,2009.10792,Ehsan Doostmohammadi,"Ehsan Doostmohammadi, Hossein Sameti, Ali Saffar","Ghmerti at SemEval-2019 Task 6: A Deep Word- and Character-based
  Approach to Offensive Language Identification",,,10.18653/v1/S19-2110,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents the models submitted by Ghmerti team for subtasks A and B
of the OffensEval shared task at SemEval 2019. OffensEval addresses the problem
of identifying and categorizing offensive language in social media in three
subtasks; whether or not a content is offensive (subtask A), whether it is
targeted (subtask B) towards an individual, a group, or other entities (subtask
C). The proposed approach includes character-level Convolutional Neural
Network, word-level Recurrent Neural Network, and some preprocessing. The
performance achieved by the proposed model for subtask A is 77.93%
macro-averaged F1-score.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 20:13:48 GMT'}]",2020-09-24,"[['Doostmohammadi', 'Ehsan', ''], ['Sameti', 'Hossein', ''], ['Saffar', 'Ali', '']]"
1133731,1906.01545,Ramon Ferrer i Cancho,"Ramon Ferrer-i-Cancho, Christian Bentz and Caio Seguin",Optimal coding and the origins of Zipfian laws,"in press in the Journal of Quantitative Linguistics; definition of
  concordant pair corrected, proofs polished, references updated",,10.1080/09296174.2020.1778387,,cs.CL cs.IT math.IT physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The problem of compression in standard information theory consists of
assigning codes as short as possible to numbers. Here we consider the problem
of optimal coding -- under an arbitrary coding scheme -- and show that it
predicts Zipf's law of abbreviation, namely a tendency in natural languages for
more frequent words to be shorter. We apply this result to investigate optimal
coding also under so-called non-singular coding, a scheme where unique
segmentation is not warranted but codes stand for a distinct number. Optimal
non-singular coding predicts that the length of a word should grow
approximately as the logarithm of its frequency rank, which is again consistent
with Zipf's law of abbreviation. Optimal non-singular coding in combination
with the maximum entropy principle also predicts Zipf's rank-frequency
distribution. Furthermore, our findings on optimal non-singular coding
challenge common beliefs about random typing. It turns out that random typing
is in fact an optimal coding process, in stark contrast with the common
assumption that it is detached from cost cutting considerations. Finally, we
discuss the implications of optimal coding for the construction of a compact
theory of Zipfian laws and other linguistic laws.
","[{'version': 'v1', 'created': 'Tue, 4 Jun 2019 16:03:18 GMT'}, {'version': 'v2', 'created': 'Fri, 5 Jul 2019 06:29:57 GMT'}, {'version': 'v3', 'created': 'Thu, 19 Dec 2019 16:09:39 GMT'}, {'version': 'v4', 'created': 'Fri, 29 May 2020 16:32:11 GMT'}]",2020-09-24,"[['Ferrer-i-Cancho', 'Ramon', ''], ['Bentz', 'Christian', ''], ['Seguin', 'Caio', '']]"
1352297,2009.10794,Ehsan Doostmohammadi,"Ehsan Doostmohammadi, Minoo Nassajian","Investigating Machine Learning Methods for Language and Dialect
  Identification of Cuneiform Texts",,,10.18653/v1/W19-1420,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Identification of the languages written using cuneiform symbols is a
difficult task due to the lack of resources and the problem of tokenization.
The Cuneiform Language Identification task in VarDial 2019 addresses the
problem of identifying seven languages and dialects written in cuneiform;
Sumerian and six dialects of Akkadian language: Old Babylonian, Middle
Babylonian Peripheral, Standard Babylonian, Neo-Babylonian, Late Babylonian,
and Neo-Assyrian. This paper describes the approaches taken by SharifCL team to
this problem in VarDial 2019. The best result belongs to an ensemble of Support
Vector Machines and a naive Bayes classifier, both working on character-level
features, with macro-averaged F1-score of 72.10%.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 20:17:45 GMT'}]",2020-09-24,"[['Doostmohammadi', 'Ehsan', ''], ['Nassajian', 'Minoo', '']]"
547080,1408.1774,Ramon Ferrer i Cancho,Ramon Ferrer-i-Cancho,"Beyond description. Comment on ""Approaching human language with complex
  networks"" by Cong & Liu",,"Physics of Life Reviews 11 (4), 621-623 (2014)",10.1016/j.plrev.2014.07.014,,cs.CL cs.SI physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Comment on ""Approaching human language with complex networks"" by Cong & Liu
","[{'version': 'v1', 'created': 'Fri, 8 Aug 2014 07:39:13 GMT'}]",2020-09-24,"[['Ferrer-i-Cancho', 'Ramon', '']]"
1352632,2009.11129,Saba Nazir,"Saba Nazir. Taner Cagali, Chris Newell, Mehrnoosh Sadrzadeh",Cosine Similarity of Multimodal Content Vectors for TV Programmes,"3 pages, 1 figure, Machine Learning for Media Discovery (ML4MD)
  Workshop at ICML 2020",,,,cs.MM cs.CL cs.IR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Multimodal information originates from a variety of sources: audiovisual
files, textual descriptions, and metadata. We show how one can represent the
content encoded by each individual source using vectors, how to combine the
vectors via middle and late fusion techniques, and how to compute the semantic
similarities between the contents. Our vectorial representations are built from
spectral features and Bags of Audio Words, for audio, LSI topics and Doc2vec
embeddings for subtitles, and the categorical features, for metadata. We
implement our model on a dataset of BBC TV programmes and evaluate the fused
representations to provide recommendations. The late fused similarity matrices
significantly improve the precision and diversity of recommendations.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 13:12:30 GMT'}]",2020-09-24,"[['Cagali', 'Saba Nazir. Taner', ''], ['Newell', 'Chris', ''], ['Sadrzadeh', 'Mehrnoosh', '']]"
1315319,2007.03805,Tuan Manh Lai,"Tuan Manh Lai, Trung Bui, Nedim Lipka",ISA: An Intelligent Shopping Assistant,Accepted by AACL 2020 (Demo),,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the growth of e-commerce, brick-and-mortar stores are still the
preferred destinations for many people. In this paper, we present ISA, a
mobile-based intelligent shopping assistant that is designed to improve
shopping experience in physical stores. ISA assists users by leveraging
advanced techniques in computer vision, speech processing, and natural language
processing. An in-store user only needs to take a picture or scan the barcode
of the product of interest, and then the user can talk to the assistant about
the product. The assistant can also guide the user through the purchase process
or recommend other similar products to the user. We take a data-driven approach
in building the engines of ISA's natural language processing component, and the
engines achieve good performance.
","[{'version': 'v1', 'created': 'Tue, 7 Jul 2020 21:57:34 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 05:42:24 GMT'}]",2020-09-24,"[['Lai', 'Tuan Manh', ''], ['Bui', 'Trung', ''], ['Lipka', 'Nedim', '']]"
1352729,2009.11226,Alexander Kalinowski,Alexander Kalinowski and Yuan An,"A Comparative Study on Structural and Semantic Properties of Sentence
  Embeddings","10 pages, 3 figures",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentence embeddings encode natural language sentences as low-dimensional
dense vectors. A great deal of effort has been put into using sentence
embeddings to improve several important natural language processing tasks.
Relation extraction is such an NLP task that aims at identifying structured
relations defined in a knowledge base from unstructured text. A promising and
more efficient approach would be to embed both the text and structured
knowledge in low-dimensional spaces and discover semantic alignments or
mappings between them. Although a number of techniques have been proposed in
the literature for embedding both sentences and knowledge graphs, little is
known about the structural and semantic properties of these embedding spaces in
terms of relation extraction. In this paper, we investigate the aforementioned
properties by evaluating the extent to which sentences carrying similar senses
are embedded in close proximity sub-spaces, and if we can exploit that
structure to align sentences to a knowledge graph. We propose a set of
experiments using a widely-used large-scale data set for relation extraction
and focusing on a set of key sentence embedding methods. We additionally
provide the code for reproducing these experiments at
https://github.com/akalino/semantic-structural-sentences. These embedding
methods cover a wide variety of techniques ranging from simple word embedding
combination to transformer-based BERT-style model. Our experimental results
show that different embedding spaces have different degrees of strength for the
structural and semantic properties. These results provide useful information
for developing embedding-based relation extraction methods.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 15:45:32 GMT'}]",2020-09-24,"[['Kalinowski', 'Alexander', ''], ['An', 'Yuan', '']]"
1340815,2008.13093,Wei Li,"Wei Li, James Qin, Chung-Cheng Chiu, Ruoming Pang, Yanzhang He","Parallel Rescoring with Transformer for Streaming On-Device Speech
  Recognition","Proceedings of Interspeech, 2020",,,,eess.AS cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances of end-to-end models have outperformed conventional models
through employing a two-pass model. The two-pass model provides better
speed-quality trade-offs for on-device speech recognition, where a 1st-pass
model generates hypotheses in a streaming fashion, and a 2nd-pass model
re-scores the hypotheses with full audio sequence context. The 2nd-pass model
plays a key role in the quality improvement of the end-to-end model to surpass
the conventional model. One main challenge of the two-pass model is the
computation latency introduced by the 2nd-pass model. Specifically, the
original design of the two-pass model uses LSTMs for the 2nd-pass model, which
are subject to long latency as they are constrained by the recurrent nature and
have to run inference sequentially. In this work we explore replacing the LSTM
layers in the 2nd-pass rescorer with Transformer layers, which can process the
entire hypothesis sequences in parallel and can therefore utilize the on-device
computation resources more efficiently. Compared with an LSTM-based baseline,
our proposed Transformer rescorer achieves more than 50% latency reduction with
quality improvement.
","[{'version': 'v1', 'created': 'Sun, 30 Aug 2020 05:17:31 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Sep 2020 06:04:36 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Sep 2020 23:05:17 GMT'}]",2020-09-24,"[['Li', 'Wei', ''], ['Qin', 'James', ''], ['Chiu', 'Chung-Cheng', ''], ['Pang', 'Ruoming', ''], ['He', 'Yanzhang', '']]"
1352647,2009.11144,Bai Li,Bai Li,Evolution of Part-of-Speech in Classical Chinese,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Classical Chinese is a language notable for its word class flexibility: the
same word may often be used as a noun or a verb. Bisang (2008) claimed that
Classical Chinese is a precategorical language, where the syntactic position of
a word determines its part-of-speech category. In this paper, we apply
entropy-based metrics to evaluate these claims on historical corpora. We
further explore differences between nouns and verbs in Classical Chinese: using
psycholinguistic norms, we find a positive correlation between concreteness and
noun usage. Finally, we align character embeddings from Classical and Modern
Chinese, and find that verbs undergo more semantic change than nouns.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 13:41:27 GMT'}]",2020-09-24,"[['Li', 'Bai', '']]"
1267355,2004.02585,Tom Sherborne,"Tom Sherborne, Yumo Xu, Mirella Lapata",Bootstrapping a Crosslingual Semantic Parser,Camera Ready for EMNLP2020 Findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent progress in semantic parsing scarcely considers languages other than
English but professional translation can be prohibitively expensive. We adapt a
semantic parser trained on a single language, such as English, to new languages
and multiple domains with minimal annotation. We query if machine translation
is an adequate substitute for training data, and extend this to investigate
bootstrapping using joint training with English, paraphrasing, and multilingual
pre-trained models. We develop a Transformer-based parser combining paraphrases
by ensembling attention over multiple encoders and present new versions of ATIS
and Overnight in German and Chinese for evaluation. Experimental results
indicate that MT can approximate training data in a new language for accurate
parsing when augmented with paraphrasing through multiple MT engines.
Considering when MT is inadequate, we also find that using our approach
achieves parsing accuracy within 2% of complete translation using only 50% of
training data.
","[{'version': 'v1', 'created': 'Mon, 6 Apr 2020 12:05:02 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Apr 2020 11:04:34 GMT'}, {'version': 'v3', 'created': 'Thu, 16 Apr 2020 10:15:10 GMT'}, {'version': 'v4', 'created': 'Wed, 23 Sep 2020 14:33:47 GMT'}]",2020-09-24,"[['Sherborne', 'Tom', ''], ['Xu', 'Yumo', ''], ['Lapata', 'Mirella', '']]"
1352704,2009.11201,Xavier Garcia,"Xavier Garcia, Aditya Siddhant, Orhan Firat, Ankur P. Parikh","Harnessing Multilinguality in Unsupervised Machine Translation for Rare
  Languages",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unsupervised translation has reached impressive performance on resource-rich
language pairs such as English-French and English-German. However, early
studies have shown that in more realistic settings involving low-resource, rare
languages, unsupervised translation performs poorly, achieving less than 3.0
BLEU. In this work, we show that multilinguality is critical to making
unsupervised systems practical for low-resource settings. In particular, we
present a single model for 5 low-resource languages (Gujarati, Kazakh, Nepali,
Sinhala, and Turkish) to and from English directions, which leverages
monolingual and auxiliary parallel data from other high-resource language pairs
via a three-stage training scheme. We outperform all current state-of-the-art
unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU.
Additionally, we outperform a large collection of supervised WMT submissions
for various language pairs as well as match the performance of the current
state-of-the-art supervised model for Nepali-English. We conduct a series of
ablation studies to establish the robustness of our model under different
degrees of data quality, as well as to analyze the factors which led to the
superior performance of the proposed approach over traditional unsupervised
models.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 15:07:33 GMT'}]",2020-09-24,"[['Garcia', 'Xavier', ''], ['Siddhant', 'Aditya', ''], ['Firat', 'Orhan', ''], ['Parikh', 'Ankur P.', '']]"
1352526,2009.11023,Oana-Maria Camburu,"Oana-Maria Camburu, Eleonora Giunchiglia, Jakob Foerster, Thomas
  Lukasiewicz, Phil Blunsom","The Struggles of Feature-Based Explanations: Shapley Values vs. Minimal
  Sufficient Subsets",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  For neural models to garner widespread public trust and ensure fairness, we
must have human-intelligible explanations for their predictions. Recently, an
increasing number of works focus on explaining the predictions of neural models
in terms of the relevance of the input features. In this work, we show that
feature-based explanations pose problems even for explaining trivial models. We
show that, in certain cases, there exist at least two ground-truth
feature-based explanations, and that, sometimes, neither of them is enough to
provide a complete view of the decision-making process of the model. Moreover,
we show that two popular classes of explainers, Shapley explainers and minimal
sufficient subsets explainers, target fundamentally different types of
ground-truth explanations, despite the apparently implicit assumption that
explainers should look for one specific feature-based explanation. These
findings bring an additional dimension to consider in both developing and
choosing explainers.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 09:45:23 GMT'}]",2020-09-24,"[['Camburu', 'Oana-Maria', ''], ['Giunchiglia', 'Eleonora', ''], ['Foerster', 'Jakob', ''], ['Lukasiewicz', 'Thomas', ''], ['Blunsom', 'Phil', '']]"
1320131,2007.08617,Chris Thomas,Christopher Thomas and Adriana Kovashka,Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval,,ECCV 2020,,,cs.CV cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The abundance of multimodal data (e.g. social media posts) has inspired
interest in cross-modal retrieval methods. Popular approaches rely on a variety
of metric learning losses, which prescribe what the proximity of image and text
should be, in the learned space. However, most prior methods have focused on
the case where image and text convey redundant information; in contrast,
real-world image-text pairs convey complementary information with little
overlap. Further, images in news articles and media portray topics in a
visually diverse fashion; thus, we need to take special care to ensure a
meaningful image representation. We propose novel within-modality losses which
encourage semantic coherency in both the text and image subspaces, which does
not necessarily align with visual coherency. Our method ensures that not only
are paired images and texts close, but the expected image-image and text-text
relationships are also observed. Our approach improves the results of
cross-modal retrieval on four datasets compared to five baselines.
","[{'version': 'v1', 'created': 'Thu, 16 Jul 2020 20:32:54 GMT'}]",2020-09-24,"[['Thomas', 'Christopher', ''], ['Kovashka', 'Adriana', '']]"
1105852,1904.00812,Ramon Ferrer i Cancho,"Bernardino Casas, Antoni Hern\'andez-Fern\'andez, Neus Catal\`a, Ramon
  Ferrer-i-Cancho and Jaume Baixeries",Polysemy and brevity versus frequency in language,,"Computer Speech and Language 58, 19-50 (2019)",10.1016/j.csl.2019.03.007,,cs.CL physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The pioneering research of G. K. Zipf on the relationship between word
frequency and other word features led to the formulation of various linguistic
laws. The most popular is Zipf's law for word frequencies. Here we focus on two
laws that have been studied less intensively: the meaning-frequency law, i.e.
the tendency of more frequent words to be more polysemous, and the law of
abbreviation, i.e. the tendency of more frequent words to be shorter. In a
previous work, we tested the robustness of these Zipfian laws for English,
roughly measuring word length in number of characters and distinguishing adult
from child speech. In the present article, we extend our study to other
languages (Dutch and Spanish) and introduce two additional measures of length:
syllabic length and phonemic length. Our correlation analysis indicates that
both the meaning-frequency law and the law of abbreviation hold overall in all
the analyzed languages.
","[{'version': 'v1', 'created': 'Wed, 27 Mar 2019 14:21:57 GMT'}]",2020-09-24,"[['Casas', 'Bernardino', ''], ['Hernández-Fernández', 'Antoni', ''], ['Català', 'Neus', ''], ['Ferrer-i-Cancho', 'Ramon', ''], ['Baixeries', 'Jaume', '']]"
688234,1512.05582,Ramon Ferrer i Cancho,Ramon Ferrer-i-Cancho,Kauffman's adjacent possible in word order evolution,"Minor corrections (small errors concerning the parameters of model 1,
  language, style,...) except for the mathematical arguments at the end of
  section ""Further details about Model 2"" of the supplementary","In S.G. Roberts et al (eds.). The Evolution of Language:
  Proceedings of the 11th International Conference (EVOLANG11). New Orleans,
  USA, March 21-24 (2016)",,,cs.CL cs.IT math.IT physics.data-an physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Word order evolution has been hypothesized to be constrained by a word order
permutation ring: transitions involving orders that are closer in the
permutation ring are more likely. The hypothesis can be seen as a particular
case of Kauffman's adjacent possible in word order evolution. Here we consider
the problem of the association of the six possible orders of S, V and O to
yield a couple of primary alternating orders as a window to word order
evolution. We evaluate the suitability of various competing hypotheses to
predict one member of the couple from the other with the help of information
theoretic model selection. Our ensemble of models includes a six-way model that
is based on the word order permutation ring (Kauffman's adjacent possible) and
another model based on the dual two-way of standard typology, that reduces word
order to basic orders preferences (e.g., a preference for SV over VS and
another for SO over OS). Our analysis indicates that the permutation ring
yields the best model when favoring parsimony strongly, providing support for
Kauffman's general view and a six-way typology.
","[{'version': 'v1', 'created': 'Thu, 17 Dec 2015 14:01:14 GMT'}, {'version': 'v2', 'created': 'Mon, 18 Jan 2016 10:01:53 GMT'}]",2020-09-24,"[['Ferrer-i-Cancho', 'Ramon', '']]"
1352530,2009.11027,Zorik Gekhman,"Zorik Gekhman, Roee Aharoni, Genady Beryozkin, Markus Freitag,
  Wolfgang Macherey",KoBE: Knowledge-Based Machine Translation Evaluation,Accepted as a short paper in Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a simple and effective method for machine translation evaluation
which does not require reference translations. Our approach is based on (1)
grounding the entity mentions found in each source sentence and candidate
translation against a large-scale multilingual knowledge base, and (2)
measuring the recall of the grounded entities found in the candidate vs. those
found in the source. Our approach achieves the highest correlation with human
judgements on 9 out of the 18 language pairs from the WMT19 benchmark for
evaluation without references, which is the largest number of wins for a single
evaluation method on this task. On 4 language pairs, we also achieve higher
correlation with human judgements than BLEU. To foster further research, we
release a dataset containing 1.8 million grounded entity mentions across 18
language pairs from the WMT19 metrics track data.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 09:52:28 GMT'}]",2020-09-24,"[['Gekhman', 'Zorik', ''], ['Aharoni', 'Roee', ''], ['Beryozkin', 'Genady', ''], ['Freitag', 'Markus', ''], ['Macherey', 'Wolfgang', '']]"
1352781,2009.11278,Jaemin Cho,"Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha
  Kembhavi","X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal
  Transformers",EMNLP 2020,,,,cs.CV cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Mirroring the success of masked language models, vision-and-language
counterparts like ViLBERT, LXMERT and UNITER have achieved state of the art
performance on a variety of multimodal discriminative tasks like visual
question answering and visual grounding. Recent work has also successfully
adapted such models towards the generative task of image captioning. This begs
the question: Can these models go the other way and generate images from pieces
of text? Our analysis of a popular representative from this model family -
LXMERT - finds that it is unable to generate rich and semantically meaningful
imagery with its current training setup. We introduce X-LXMERT, an extension to
LXMERT with training refinements including: discretizing visual
representations, using uniform masking with a large range of masking ratios and
aligning the right pre-training datasets to the right objectives which enables
it to paint. X-LXMERT's image generation capabilities rival state of the art
generative models while its question answering and captioning abilities remains
comparable to LXMERT. Finally, we demonstrate the generality of these training
refinements by adding image generation capabilities into UNITER to produce
X-UNITER.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 17:45:17 GMT'}]",2020-09-24,"[['Cho', 'Jaemin', ''], ['Lu', 'Jiasen', ''], ['Schwenk', 'Dustin', ''], ['Hajishirzi', 'Hannaneh', ''], ['Kembhavi', 'Aniruddha', '']]"
572617,1411.2645,Ramon Ferrer i Cancho,Ramon Ferrer-i-Cancho,"Non-crossing dependencies: least effort, not grammar",,,10.1007/978-3-662-47238-5_10,,cs.CL cs.SI physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The use of null hypotheses (in a statistical sense) is common in hard
sciences but not in theoretical linguistics. Here the null hypothesis that the
low frequency of syntactic dependency crossings is expected by an arbitrary
ordering of words is rejected. It is shown that this would require star
dependency structures, which are both unrealistic and too restrictive. The
hypothesis of the limited resources of the human brain is revisited. Stronger
null hypotheses taking into account actual dependency lengths for the
likelihood of crossings are presented. Those hypotheses suggests that crossings
are likely to reduce when dependencies are shortened. A hypothesis based on
pressure to reduce dependency lengths is more parsimonious than a principle of
minimization of crossings or a grammatical ban that is totally dissociated from
the general and non-linguistic principle of economy.
","[{'version': 'v1', 'created': 'Mon, 10 Nov 2014 22:12:56 GMT'}]",2020-09-24,"[['Ferrer-i-Cancho', 'Ramon', '']]"
1299677,2006.05163,Vaishali Pal,"Vaishali Pal, Manish Shrivastava and Laurent Besacier",ConfNet2Seq: Full Length Answer Generation from Spoken Questions,"Accepted at Text, Speech and Dialogue, 2020","ConfNet2Seq, Text, Speech, and Dialogue - 23rd International
  Conference, {TSD}, Brno, Czech Republic, September 8-11, 2020, Proceedings,
  12284, 2020, 524-531 (2020)",10.1007/978-3-030-58323-1_56,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conversational and task-oriented dialogue systems aim to interact with the
user using natural responses through multi-modal interfaces, such as text or
speech. These desired responses are in the form of full-length natural answers
generated over facts retrieved from a knowledge source. While the task of
generating natural answers to questions from an answer span has been widely
studied, there has been little research on natural sentence generation over
spoken content. We propose a novel system to generate full length natural
language answers from spoken questions and factoid answers. The spoken sequence
is compactly represented as a confusion network extracted from a pre-trained
Automatic Speech Recognizer. This is the first attempt towards generating
full-length natural answers from a graph input(confusion network) to the best
of our knowledge. We release a large-scale dataset of 259,788 samples of spoken
questions, their factoid answers and corresponding full-length textual answers.
Following our proposed approach, we achieve comparable performance with best
ASR hypothesis.
","[{'version': 'v1', 'created': 'Tue, 9 Jun 2020 10:04:49 GMT'}, {'version': 'v2', 'created': 'Thu, 11 Jun 2020 08:39:41 GMT'}]",2020-09-24,"[['Pal', 'Vaishali', ''], ['Shrivastava', 'Manish', ''], ['Besacier', 'Laurent', '']]"
585164,1412.7186,Ramon Ferrer i Cancho,Ramon Ferrer-i-Cancho,"Reply to the commentary ""Be careful when assuming the obvious"", by P.
  Alday",Minor corrections (language improved),"Language Dynamics and Change 5 (1), 147-155 (2015)",10.1163/22105832-00501009,,cs.CL physics.data-an physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Here we respond to some comments by Alday concerning headedness in linguistic
theory and the validity of the assumptions of a mathematical model for word
order. For brevity, we focus only on two assumptions: the unit of measurement
of dependency length and the monotonicity of the cost of a dependency as a
function of its length. We also revise the implicit psychological bias in
Alday's comments. Notwithstanding, Alday is indicating the path for linguistic
research with his unusual concerns about parsimony from multiple dimensions.
","[{'version': 'v1', 'created': 'Mon, 22 Dec 2014 22:05:06 GMT'}, {'version': 'v2', 'created': 'Sun, 15 Mar 2015 06:52:04 GMT'}]",2020-09-24,"[['Ferrer-i-Cancho', 'Ramon', '']]"
1352641,2009.11138,Sheng Zhang,"Sheng Zhang, Xin Zhang, Weiming Zhang, Anders S{\o}gaard",Worst-Case-Aware Curriculum Learning for Zero and Few Shot Transfer,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-task transfer learning based on pre-trained language encoders achieves
state-of-the-art performance across a range of tasks. Standard approaches
implicitly assume the tasks, for which we have training data, are equally
representative of the tasks we are interested in, an assumption which is often
hard to justify. This paper presents a more agnostic approach to multi-task
transfer learning, which uses automated curriculum learning to minimize a new
family of worst-case-aware losses across tasks. Not only do these losses lead
to better performance on outlier tasks; they also lead to better performance in
zero-shot and few-shot transfer settings.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 13:32:39 GMT'}]",2020-09-24,"[['Zhang', 'Sheng', ''], ['Zhang', 'Xin', ''], ['Zhang', 'Weiming', ''], ['Søgaard', 'Anders', '']]"
1352441,2009.10938,Xinyi Zhang,Xinyi Zhang and Jiahao Xu and Charlie Soh and Lihui Chen,"LA-HCN: Label-based Attention for Hierarchical Multi-label
  TextClassification Neural Network",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Hierarchical multi-label text classification(HMTC) problems become popular
recently because of its practicality. Most existing algorithms for HMTC focus
on the design of classifiers, and are largely referred to as local, global, or
a combination of local/global approaches. However, a few studies have started
exploring hierarchical feature extraction based on the label hierarchy
associating with text in HMTC. In this paper, a \textbf{N}eural network-based
method called \textbf{LA-HCN} is proposed where a novel \textbf{L}abel-based
\textbf{A}ttention module is designed to hierarchically extract important
information from the text based on different labels. Besides, local and global
document embeddings are separately generated to support the respective local
and global classifications. In our experiments, LA-HCN achieves the top
performance on the four public HMTC datasets when compared with other neural
network-based state-of-the-art algorithms. The comparison between LA-HCN with
its variants also demonstrates the effectiveness of the proposed label-based
attention module as well as the use of the combination of local and global
classifications. By visualizing the learned attention(words), we find LA-HCN is
able to extract meaningful but different information from text based on
different labels which is helpful for human understanding and explanation of
classification results.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 06:18:25 GMT'}]",2020-09-24,"[['Zhang', 'Xinyi', ''], ['Xu', 'Jiahao', ''], ['Soh', 'Charlie', ''], ['Chen', 'Lihui', '']]"
1323726,2007.12212,Anurag Roy,"Anurag Roy, Vinay Kumar Verma, Kripabandhu Ghosh, Saptarshi Ghosh","ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot
  Retrieval of Images from Textual Descriptions",Accepted in CIKM-2020,,,,cs.CV cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most existing algorithms for cross-modal Information Retrieval are based on a
supervised train-test setup, where a model learns to align the mode of the
query (e.g., text) to the mode of the documents (e.g., images) from a given
training set. Such a setup assumes that the training set contains an exhaustive
representation of all possible classes of queries. In reality, a retrieval
model may need to be deployed on previously unseen classes, which implies a
zero-shot IR setup. In this paper, we propose a novel GAN-based model for
zero-shot text to image retrieval. When given a textual description as the
query, our model can retrieve relevant images in a zero-shot setup. The
proposed model is trained using an Expectation-Maximization framework.
Experiments on multiple benchmark datasets show that our proposed model
comfortably outperforms several state-of-the-art zero-shot text to image
retrieval models, as well as zero-shot classification and hashing models
suitably used for retrieval.
","[{'version': 'v1', 'created': 'Thu, 23 Jul 2020 18:50:03 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Jul 2020 11:57:35 GMT'}, {'version': 'v3', 'created': 'Wed, 23 Sep 2020 11:41:12 GMT'}]",2020-09-24,"[['Roy', 'Anurag', ''], ['Verma', 'Vinay Kumar', ''], ['Ghosh', 'Kripabandhu', ''], ['Ghosh', 'Saptarshi', '']]"
729441,1605.01326,Ramon Ferrer i Cancho,Ramon Ferrer-i-Cancho,Compression and the origins of Zipf's law for word frequencies,arguments have been improved; in press in Complexity (Wiley),"Complexity 21, 409-411 (2016)",10.1002/cplx.21820,,cs.CL physics.data-an physics.soc-ph q-bio.NC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Here we sketch a new derivation of Zipf's law for word frequencies based on
optimal coding. The structure of the derivation is reminiscent of Mandelbrot's
random typing model but it has multiple advantages over random typing: (1) it
starts from realistic cognitive pressures (2) it does not require fine tuning
of parameters and (3) it sheds light on the origins of other statistical laws
of language and thus can lead to a compact theory of linguistic laws. Our
findings suggest that the recurrence of Zipf's law in human languages could
originate from pressure for easy and fast communication.
","[{'version': 'v1', 'created': 'Wed, 4 May 2016 16:00:59 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Jul 2016 15:14:10 GMT'}]",2020-09-24,"[['Ferrer-i-Cancho', 'Ramon', '']]"
1288920,2005.09125,EPTCS,"Yong Li (State Key Laboratory of Computer Science, Institute of
  Software, Chinese Academy of Sciences), Moshe Y. Vardi (Rice University),
  Lijun Zhang (State Key Laboratory of Computer Science, Institute of Software,
  Chinese Academy of Sciences)","On the Power of Unambiguity in B\""uchi Complementation","In Proceedings GandALF 2020, arXiv:2009.09360","EPTCS 326, 2020, pp. 182-198",10.4204/EPTCS.326.12,,cs.FL cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we exploit the power of \emph{unambiguity} for the
complementation problem of B\""uchi automata by utilizing reduced run directed
acyclic graphs (DAGs) over infinite words, in which each vertex has at most one
predecessor. We then show how to use this type of reduced run DAGs as a
\emph{unified tool} to optimize \emph{both} rank-based and slice-based
complementation constructions for B\""uchi automata with a finite degree of
ambiguity. As a result, given a B\""uchi automaton with $n$ states and a finite
degree of ambiguity, the number of states in the complementary B\""uchi
automaton constructed by the classical rank-based and slice-based
complementation constructions can be improved, respectively, to $2^{O(n)}$ from
$2^{O(n\log n)}$ and to $O(4^n)$ from $O((3n)^n)$.
","[{'version': 'v1', 'created': 'Mon, 18 May 2020 22:51:34 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 01:27:07 GMT'}]",2020-09-24,"[['Li', 'Yong', '', 'State Key Laboratory of Computer Science, Institute of\n  Software, Chinese Academy of Sciences'], ['Vardi', 'Moshe Y.', '', 'Rice University'], ['Zhang', 'Lijun', '', 'State Key Laboratory of Computer Science, Institute of Software,\n  Chinese Academy of Sciences']]"
929099,1801.00168,Ramon Ferrer i Cancho,Ramon Ferrer-i-Cancho and Michael S. Vitevitch,The origins of Zipf's meaning-frequency law,,"Journal of the American Society for Information Science and
  Technology 69 (11), 1369-1379 (2018)",10.1002/asi.24057,,cs.CL cs.IT math.IT physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In his pioneering research, G. K. Zipf observed that more frequent words tend
to have more meanings, and showed that the number of meanings of a word grows
as the square root of its frequency. He derived this relationship from two
assumptions: that words follow Zipf's law for word frequencies (a power law
dependency between frequency and rank) and Zipf's law of meaning distribution
(a power law dependency between number of meanings and rank). Here we show that
a single assumption on the joint probability of a word and a meaning suffices
to infer Zipf's meaning-frequency law or relaxed versions. Interestingly, this
assumption can be justified as the outcome of a biased random walk in the
process of mental exploration.
","[{'version': 'v1', 'created': 'Sat, 30 Dec 2017 17:59:41 GMT'}]",2020-09-24,"[['Ferrer-i-Cancho', 'Ramon', ''], ['Vitevitch', 'Michael S.', '']]"
1352318,2009.10815,Ritam Dutt,"Ritam Dutt, Rishabh Joshi, Carolyn Penstein Rose","Keeping Up Appearances: Computational Modeling of Face Acts in
  Persuasion Oriented Discussions","To appear at Proceedings of the 2020 Conference on Empirical Methods
  in Natural Language Processing (EMNLP, 2020) as a full paper",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The notion of face refers to the public self-image of an individual that
emerges both from the individual's own actions as well as from the interaction
with others. Modeling face and understanding its state changes throughout a
conversation is critical to the study of maintenance of basic human needs in
and through interaction. Grounded in the politeness theory of Brown and
Levinson (1978), we propose a generalized framework for modeling face acts in
persuasion conversations, resulting in a reliable coding manual, an annotated
corpus, and computational models. The framework reveals insights about
differences in face act utilization between asymmetric roles in persuasion
conversations. Using computational models, we are able to successfully identify
face acts as well as predict a key conversational outcome (e.g. donation
success). Finally, we model a latent representation of the conversational state
to analyze the impact of predicted face acts on the probability of a positive
conversational outcome and observe several correlations that corroborate
previous findings.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 21:02:14 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Sep 2020 02:10:44 GMT'}]",2020-09-25,"[['Dutt', 'Ritam', ''], ['Joshi', 'Rishabh', ''], ['Rose', 'Carolyn Penstein', '']]"
1270846,2004.06076,Adyasha Maharana,"Adyasha Maharana, Mohit Bansal","Adversarial Augmentation Policy Search for Domain and Cross-Lingual
  Generalization in Reading Comprehension","15 pages (Findings of EMNLP, 2020)",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reading comprehension models often overfit to nuances of training datasets
and fail at adversarial evaluation. Training with adversarially augmented
dataset improves robustness against those adversarial attacks but hurts
generalization of the models. In this work, we present several effective
adversaries and automated data augmentation policy search methods with the goal
of making reading comprehension models more robust to adversarial evaluation,
but also improving generalization to the source domain as well as new domains
and languages. We first propose three new methods for generating QA
adversaries, that introduce multiple points of confusion within the context,
show dependence on insertion location of the distractor, and reveal the
compounding effect of mixing adversarial strategies with syntactic and semantic
paraphrasing methods. Next, we find that augmenting the training datasets with
uniformly sampled adversaries improves robustness to the adversarial attacks
but leads to decline in performance on the original unaugmented dataset. We
address this issue via RL and more efficient Bayesian policy search methods for
automatically learning the best augmentation policy combinations of the
transformation probability for each adversary in a large search space. Using
these learned policies, we show that adversarial training can lead to
significant improvements in in-domain, out-of-domain, and cross-lingual
(German, Russian, Turkish) generalization.
","[{'version': 'v1', 'created': 'Mon, 13 Apr 2020 17:20:08 GMT'}, {'version': 'v2', 'created': 'Fri, 1 May 2020 15:30:48 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Sep 2020 01:38:28 GMT'}]",2020-09-25,"[['Maharana', 'Adyasha', ''], ['Bansal', 'Mohit', '']]"
1279349,2004.14579,Zhiyu Chen,"Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam
  Sundaresan, William Yang Wang",Logic2Text: High-Fidelity Natural Language Generation from Logical Forms,"Findings of EMNLP 2020, 9 pages, 6 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous works on Natural Language Generation (NLG) from structured data have
primarily focused on surface-level descriptions of record sequences. However,
for complex structured data, e.g., multi-row tables, it is often desirable for
an NLG system to describe interesting facts from logical inferences across
records. If only provided with the table, it is hard for existing models to
produce controllable and high-fidelity logical generations. In this work, we
formulate logical level NLG as generation from logical forms in order to obtain
controllable, high-fidelity, and faithful generations. We present a new
large-scale dataset, \textsc{Logic2Text}, with 10,753 descriptions involving
common logic types paired with the underlying logical forms. The logical forms
show diversified graph structure of free schema, which poses great challenges
on the model's ability to understand the semantics. We experiment on (1)
Fully-supervised training with the full datasets, and (2) Few-shot setting,
provided with hundreds of paired examples; We compare several popular
generation models and analyze their performances. We hope our dataset can
encourage research towards building an advanced NLG system capable of natural,
faithful, and human-like generation. The dataset and code are available at
https://github.com/czyssrs/Logic2Text.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 04:06:06 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Sep 2020 01:29:08 GMT'}]",2020-09-25,"[['Chen', 'Zhiyu', ''], ['Chen', 'Wenhu', ''], ['Zha', 'Hanwen', ''], ['Zhou', 'Xiyou', ''], ['Zhang', 'Yunkai', ''], ['Sundaresan', 'Sairam', ''], ['Wang', 'William Yang', '']]"
1279371,2004.14601,Isabel Papadimitriou,Isabel Papadimitriou and Dan Jurafsky,"Learning Music Helps You Read: Using Transfer to Study Linguistic
  Structure in Language Models",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose transfer learning as a method for analyzing the encoding of
grammatical structure in neural language models. We train LSTMs on
non-linguistic data and evaluate their performance on natural language to
assess which kinds of data induce generalizable structural features that LSTMs
can use for natural language. We find that training on non-linguistic data with
latent structure (MIDI music or Java code) improves test performance on natural
language, despite no overlap in surface form or vocabulary. Training on
artificial languages containing recursion (hierarchical structure) also
improves performance on natural language, again with no vocabulary overlap.
Surprisingly, training on artificial languages consisting of sets of separated
pairs of words, but with no recursion, improves performance on natural language
as well as recursive languages do. Experiments on transfer between natural
languages show that zero-shot performance on a test language is highly
correlated with typological syntactic similarity to the training language,
suggesting that representations induced from natural languages correspond to
the cross-linguistic syntactic properties studied in linguistic typology. Our
results provide insights into the ways that neural models represent abstract
syntactic structure, and also about the kind of structural inductive biases
which a learner needs to model language.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 06:24:03 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 21:45:59 GMT'}]",2020-09-25,"[['Papadimitriou', 'Isabel', ''], ['Jurafsky', 'Dan', '']]"
1307472,2006.12958,Apostol Vassilev,Apostol Vassilev and Munawar Hasan,"Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network
  Framework for Sentiment Analysis","13 pages, 6 figures, 2 tables, 36 references",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When people try to understand nuanced language they typically process
multiple input sensor modalities to complete this cognitive task. It turns out
the human brain has even a specialized neuron formation, called sagittal
stratum, to help us understand sarcasm. We use this biological formation as the
inspiration for designing a neural network architecture that combines
predictions of different models on the same text to construct a robust,
accurate and computationally efficient classifier for sentiment analysis.
Experimental results on representative benchmark datasets and comparisons to
other methods1show the advantages of the new network architecture.
","[{'version': 'v1', 'created': 'Tue, 23 Jun 2020 12:55:02 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Sep 2020 15:02:36 GMT'}]",2020-09-25,"[['Vassilev', 'Apostol', ''], ['Hasan', 'Munawar', '']]"
1353256,2009.11753,Haozhe Ji,"Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Minlie Huang","Generating Commonsense Explanation by Extracting Bridge Concepts from
  Reasoning Paths",Accepted by AACL-IJCNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Commonsense explanation generation aims to empower the machine's sense-making
capability by generating plausible explanations to statements against
commonsense. While this task is easy to human, the machine still struggles to
generate reasonable and informative explanations. In this work, we propose a
method that first extracts the underlying concepts which are served as
\textit{bridges} in the reasoning chain and then integrates these concepts to
generate the final explanation. To facilitate the reasoning process, we utilize
external commonsense knowledge to build the connection between a statement and
the bridge concepts by extracting and pruning multi-hop paths to build a
subgraph. We design a bridge concept extraction model that first scores the
triples, routes the paths in the subgraph, and further selects bridge concepts
with weak supervision at both the triple level and the concept level. We
conduct experiments on the commonsense explanation generation task and our
model outperforms the state-of-the-art baselines in both automatic and human
evaluation.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 15:27:20 GMT'}]",2020-09-25,"[['Ji', 'Haozhe', ''], ['Ke', 'Pei', ''], ['Huang', 'Shaohan', ''], ['Wei', 'Furu', ''], ['Huang', 'Minlie', '']]"
1352855,2009.11352,Mohammad Aliannejadi,"Mohammad Aliannejadi and Julia Kiseleva and Aleksandr Chuklin and Jeff
  Dalton and Mikhail Burtsev","ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue
  Systems (ClariQ)",,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This document presents a detailed description of the challenge on clarifying
questions for dialogue systems (ClariQ). The challenge is organized as part of
the Conversational AI challenge series (ConvAI3) at Search Oriented
Conversational AI (SCAI) EMNLP workshop in 2020. The main aim of the
conversational systems is to return an appropriate answer in response to the
user requests. However, some user requests might be ambiguous. In IR settings
such a situation is handled mainly thought the diversification of the search
result page. It is however much more challenging in dialogue settings with
limited bandwidth. Therefore, in this challenge, we provide a common evaluation
framework to evaluate mixed-initiative conversations. Participants are asked to
rank clarifying questions in an information-seeking conversations. The
challenge is organized in two stages where in Stage 1 we evaluate the
submissions in an offline setting and single-turn conversations. Top
participants of Stage 1 get the chance to have their model tested by human
annotators.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 19:48:02 GMT'}]",2020-09-25,"[['Aliannejadi', 'Mohammad', ''], ['Kiseleva', 'Julia', ''], ['Chuklin', 'Aleksandr', ''], ['Dalton', 'Jeff', ''], ['Burtsev', 'Mikhail', '']]"
1352824,2009.11321,Ananya B Sai,"Ananya B. Sai, Akash Kumar Mohankumar, Siddhartha Arora, Mitesh M.
  Khapra","Improving Dialog Evaluation with a Multi-reference Adversarial Dataset
  and Large Scale Pretraining",Accepted for publication in TACL,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is an increasing focus on model-based dialog evaluation metrics such as
ADEM, RUBER, and the more recent BERT-based metrics. These models aim to assign
a high score to all relevant responses and a low score to all irrelevant
responses. Ideally, such models should be trained using multiple relevant and
irrelevant responses for any given context. However, no such data is publicly
available, and hence existing models are usually trained using a single
relevant response and multiple randomly selected responses from other contexts
(random negatives). To allow for better training and robust evaluation of
model-based metrics, we introduce the DailyDialog++ dataset, consisting of (i)
five relevant responses for each context and (ii) five adversarially crafted
irrelevant responses for each context. Using this dataset, we first show that
even in the presence of multiple correct references, n-gram based metrics and
embedding based metrics do not perform well at separating relevant responses
from even random negatives. While model-based metrics perform better than
n-gram and embedding based metrics on random negatives, their performance drops
substantially when evaluated on adversarial examples. To check if large scale
pretraining could help, we propose a new BERT-based evaluation metric called
DEB, which is pretrained on 727M Reddit conversations and then finetuned on our
dataset. DEB significantly outperforms existing models, showing better
correlation with human judgements and better performance on random negatives
(88.27% accuracy). However, its performance again drops substantially, when
evaluated on adversarial responses, thereby highlighting that even large-scale
pretrained evaluation models are not robust to the adversarial examples in our
dataset. The dataset and code are publicly available.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 18:06:52 GMT'}]",2020-09-25,"[['Sai', 'Ananya B.', ''], ['Mohankumar', 'Akash Kumar', ''], ['Arora', 'Siddhartha', ''], ['Khapra', 'Mitesh M.', '']]"
1201087,1911.02493,Pei Ke,"Pei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, Minlie Huang","SentiLARE: Sentiment-Aware Language Representation Learning with
  Linguistic Knowledge",Accepted by EMNLP 2020 (Main Conference),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most of the existing pre-trained language representation models neglect to
consider the linguistic knowledge of texts, which can promote language
understanding in NLP tasks. To benefit the downstream tasks in sentiment
analysis, we propose a novel language representation model called SentiLARE,
which introduces word-level linguistic knowledge including part-of-speech tag
and sentiment polarity (inferred from SentiWordNet) into pre-trained models. We
first propose a context-aware sentiment attention mechanism to acquire the
sentiment polarity of each word with its part-of-speech tag by querying
SentiWordNet. Then, we devise a new pre-training task called label-aware masked
language model to construct knowledge-aware language representation.
Experiments show that SentiLARE obtains new state-of-the-art performance on a
variety of sentiment analysis tasks.
","[{'version': 'v1', 'created': 'Wed, 6 Nov 2019 17:05:26 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Apr 2020 13:14:08 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Sep 2020 05:58:01 GMT'}]",2020-09-25,"[['Ke', 'Pei', ''], ['Ji', 'Haozhe', ''], ['Liu', 'Siyang', ''], ['Zhu', 'Xiaoyan', ''], ['Huang', 'Minlie', '']]"
1353187,2009.11684,Fenglin Li,"Feng-Lin Li, Hehong Chen, Guohai Xu, Tian Qiu, Feng Ji, Ji Zhang,
  Haiqing Chen","AliMe KG: Domain Knowledge Graph Construction and Application in
  E-commerce",,,10.1145/3340531.3412685,,cs.AI cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Pre-sales customer service is of importance to E-commerce platforms as it
contributes to optimizing customers' buying process. To better serve users, we
propose AliMe KG, a domain knowledge graph in E-commerce that captures user
problems, points of interests (POI), item information and relations thereof. It
helps to understand user needs, answer pre-sales questions and generate
explanation texts. We applied AliMe KG to several online business scenarios
such as shopping guide, question answering over properties and recommendation
reason generation, and gained positive results. In the paper, we systematically
introduce how we construct domain knowledge graph from free text, and
demonstrate its business value with several applications. Our experience shows
that mining structured knowledge from free text in vertical domain is
practicable, and can be of substantial value in industrial settings.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 13:40:18 GMT'}]",2020-09-25,"[['Li', 'Feng-Lin', ''], ['Chen', 'Hehong', ''], ['Xu', 'Guohai', ''], ['Qiu', 'Tian', ''], ['Ji', 'Feng', ''], ['Zhang', 'Ji', ''], ['Chen', 'Haiqing', '']]"
1353195,2009.11692,Haozhe Ji,"Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang","Language Generation with Multi-Hop Reasoning on Commonsense Knowledge
  Graph",accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the success of generative pre-trained language models on a series of
text generation tasks, they still suffer in cases where reasoning over
underlying commonsense knowledge is required during generation. Existing
approaches that integrate commonsense knowledge into generative pre-trained
language models simply transfer relational knowledge by post-training on
individual knowledge triples while ignoring rich connections within the
knowledge graph. We argue that exploiting both the structural and semantic
information of the knowledge graph facilitates commonsense-aware text
generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow
(GRF) that enables pre-trained models with dynamic multi-hop reasoning on
multi-relational paths extracted from the external commonsense knowledge graph.
We empirically show that our model outperforms existing baselines on three text
generation tasks that require reasoning over commonsense knowledge. We also
demonstrate the effectiveness of the dynamic multi-hop reasoning module with
reasoning paths inferred by the model that provide rationale to the generation.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 13:55:32 GMT'}]",2020-09-25,"[['Ji', 'Haozhe', ''], ['Ke', 'Pei', ''], ['Huang', 'Shaohan', ''], ['Wei', 'Furu', ''], ['Zhu', 'Xiaoyan', ''], ['Huang', 'Minlie', '']]"
1353335,2009.11832,Junade Ali,"Malgorzata Pikies, Andronicus Riyono, Junade Ali",Novel Keyword Extraction and Language Detection Approaches,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fuzzy string matching and language classification are important tools in
Natural Language Processing pipelines, this paper provides advances in both
areas. We propose a fast novel approach to string tokenisation for fuzzy
language matching and experimentally demonstrate an 83.6% decrease in
processing time with an estimated improvement in recall of 3.1% at the cost of
a 2.6% decrease in precision. This approach is able to work even where keywords
are subdivided into multiple words, without needing to scan
character-to-character. So far there has been little work considering using
metadata to enhance language classification algorithms. We provide
observational data and find the Accept-Language header is 14% more likely to
match the classification than the IP Address.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 17:28:59 GMT'}]",2020-09-25,"[['Pikies', 'Malgorzata', ''], ['Riyono', 'Andronicus', ''], ['Ali', 'Junade', '']]"
1318275,2007.06761,Alex Warstadt,"Alex Warstadt, Samuel R. Bowman",Can neural networks acquire a structural bias from raw linguistic data?,"To appear in Proceedings of 42nd Annual Meeting of the Cognitive
  Science Society",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We evaluate whether BERT, a widely used neural network for sentence
processing, acquires an inductive bias towards forming structural
generalizations through pretraining on raw data. We conduct four experiments
testing its preference for structural vs. linear generalizations in different
structure-dependent phenomena. We find that BERT makes a structural
generalization in 3 out of 4 empirical domains---subject-auxiliary inversion,
reflexive binding, and verb tense detection in embedded clauses---but makes a
linear generalization when tested on NPI licensing. We argue that these results
are the strongest evidence so far from artificial learners supporting the
proposition that a structural bias can be acquired from raw data. If this
conclusion is correct, it is tentative evidence that some linguistic universals
can be acquired by learners without innate biases. However, the precise
implications for human language acquisition are unclear, as humans learn
language from significantly less data than BERT.
","[{'version': 'v1', 'created': 'Tue, 14 Jul 2020 01:41:25 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 20:04:47 GMT'}]",2020-09-25,"[['Warstadt', 'Alex', ''], ['Bowman', 'Samuel R.', '']]"
1212478,1912.00582,Alex Warstadt,"Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng,
  Sheng-Fu Wang, Samuel R. Bowman",BLiMP: The Benchmark of Linguistic Minimal Pairs for English,To appear in TACL,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP),
a challenge set for evaluating what language models (LMs) know about major
grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each
containing 1000 minimal pairs isolating specific contrasts in syntax,
morphology, or semantics. The data is automatically generated according to
expert-crafted grammars, and aggregate human agreement with the labels is
96.4%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and
Transformer-XL) LMs. We find that state-of-the-art models identify
morphological contrasts reliably, but they struggle with semantic restrictions
on the distribution of quantifiers and negative polarity items and subtle
syntactic phenomena such as extraction islands.
","[{'version': 'v1', 'created': 'Mon, 2 Dec 2019 05:42:41 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Apr 2020 02:07:03 GMT'}, {'version': 'v3', 'created': 'Wed, 23 Sep 2020 20:08:54 GMT'}]",2020-09-25,"[['Warstadt', 'Alex', ''], ['Parrish', 'Alicia', ''], ['Liu', 'Haokun', ''], ['Mohananey', 'Anhad', ''], ['Peng', 'Wei', ''], ['Wang', 'Sheng-Fu', ''], ['Bowman', 'Samuel R.', '']]"
1351791,2009.10288,Xinyu Zuo,"Xinyu Zuo, Yubo Chen, Kang Liu and Jun Zhao",Towards Causal Explanation Detection with Pyramid Salient-Aware Network,Accepted to CCL2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Causal explanation analysis (CEA) can assist us to understand the reasons
behind daily events, which has been found very helpful for understanding the
coherence of messages. In this paper, we focus on Causal Explanation Detection,
an important subtask of causal explanation analysis, which determines whether a
causal explanation exists in one message. We design a Pyramid Salient-Aware
Network (PSAN) to detect causal explanations on messages. PSAN can assist in
causal explanation detection via capturing the salient semantics of discourses
contained in their keywords with a bottom graph-based word-level salient
network. Furthermore, PSAN can modify the dominance of discourses via a top
attention-based discourse-level salient network to enhance explanatory
semantics of messages. The experiments on the commonly used dataset of CEA
shows that the PSAN outperforms the state-of-the-art method by 1.8% F1 value on
the Causal Explanation Detection task.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 02:35:45 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Sep 2020 02:13:21 GMT'}]",2020-09-25,"[['Zuo', 'Xinyu', ''], ['Chen', 'Yubo', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]"
1352939,2009.11436,Daiki Takeuchi,"Daiki Takeuchi, Yuma Koizumi, Yasunori Ohishi, Noboru Harada, Kunio
  Kashino","Effects of Word-frequency based Pre- and Post- Processings for Audio
  Captioning",Accepted to DCASE2020 Workshop,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The system we used for Task 6 (Automated Audio Captioning)of the Detection
and Classification of Acoustic Scenes and Events(DCASE) 2020 Challenge combines
three elements, namely, dataaugmentation, multi-task learning, and
post-processing, for audiocaptioning. The system received the highest
evaluation scores, butwhich of the individual elements most fully contributed
to its perfor-mance has not yet been clarified. Here, to asses their
contributions,we first conducted an element-wise ablation study on our systemto
estimate to what extent each element is effective. We then con-ducted a
detailed module-wise ablation study to further clarify thekey processing
modules for improving accuracy. The results showthat data augmentation and
post-processing significantly improvethe score in our system. In particular,
mix-up data augmentationand beam search in post-processing improve SPIDEr by
0.8 and 1.6points, respectively.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 01:07:33 GMT'}]",2020-09-25,"[['Takeuchi', 'Daiki', ''], ['Koizumi', 'Yuma', ''], ['Ohishi', 'Yasunori', ''], ['Harada', 'Noboru', ''], ['Kashino', 'Kunio', '']]"
1352885,2009.11382,Chiori Hori Ph.D.,"Peng Gao, Chiori Hori, Shijie Geng, Takaaki Hori, Jonathan Le Roux",Multi-Pass Transformer for Machine Translation,"10 pages, 5 figures and 2 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In contrast with previous approaches where information flows only towards
deeper layers of a stack, we consider a multi-pass transformer (MPT)
architecture in which earlier layers are allowed to process information in
light of the output of later layers. To maintain a directed acyclic graph
structure, the encoder stack of a transformer is repeated along a new
multi-pass dimension, keeping the parameters tied, and information is allowed
to proceed unidirectionally both towards deeper layers within an encoder stack
and towards any layer of subsequent stacks. We consider both soft (i.e.,
continuous) and hard (i.e., discrete) connections between parallel encoder
stacks, relying on a neural architecture search to find the best connection
pattern in the hard case. We perform an extensive ablation study of the
proposed MPT architecture and compare it with other state-of-the-art
transformer architectures. Surprisingly, Base Transformer equipped with MPT can
surpass the performance of Large Transformer on the challenging machine
translation En-De and En-Fr datasets. In the hard connection case, the optimal
connection pattern found for En-De also leads to improved performance for
En-Fr.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 21:22:15 GMT'}]",2020-09-25,"[['Gao', 'Peng', ''], ['Hori', 'Chiori', ''], ['Geng', 'Shijie', ''], ['Hori', 'Takaaki', ''], ['Roux', 'Jonathan Le', '']]"
1352976,2009.11473,Huishuang Tian,"Huishuang Tian, Kexin Yang, Dayiheng Liu, Jiancheng Lv","AnchiBERT: A Pre-Trained Model for Ancient ChineseLanguage Understanding
  and Generation",10 pages with 3 figures,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ancient Chinese is the essence of Chinese culture. There are several natural
language processing tasks of ancient Chinese domain, such as ancient-modern
Chinese translation, poem generation, and couplet generation. Previous studies
usually use the supervised models which deeply rely on parallel data. However,
it is difficult to obtain large-scale parallel data of ancient Chinese. In
order to make full use of the more easily available monolingual ancient Chinese
corpora, we release AnchiBERT, a pre-trained language model based on the
architecture of BERT, which is trained on large-scale ancient Chinese corpora.
We evaluate AnchiBERT on both language understanding and generation tasks,
including poem classification, ancient-modern Chinese translation, poem
generation, and couplet generation. The experimental results show that
AnchiBERT outperforms BERT as well as the non-pretrained models and achieves
state-of-the-art results in all cases.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 03:41:13 GMT'}]",2020-09-25,"[['Tian', 'Huishuang', ''], ['Yang', 'Kexin', ''], ['Liu', 'Dayiheng', ''], ['Lv', 'Jiancheng', '']]"
1352988,2009.11485,Zehong Cao Dr.,"Xinping Liu, Zehong Cao, Son Tran","CogniFNN: A Fuzzy Neural Network Framework for Cognitive Word Embedding
  Evaluation",,,,,cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Word embeddings can reflect the semantic representations, and the embedding
qualities can be comprehensively evaluated with human natural reading-related
cognitive data sources. In this paper, we proposed the CogniFNN framework,
which is the first attempt at using fuzzy neural networks to extract non-linear
and non-stationary characteristics for evaluations of English word embeddings
against the corresponding cognitive datasets. In our experiment, we used 15
human cognitive datasets across three modalities: EEG, fMRI, and eye-tracking,
and selected the mean square error and multiple hypotheses testing as metrics
to evaluate our proposed CogniFNN framework. Compared to the recent pioneer
framework, our proposed CogniFNN showed smaller prediction errors of both
context-independent (GloVe) and context-sensitive (BERT) word embeddings, and
achieved higher significant ratios with randomly generated word embeddings. Our
findings suggested that the CogniFNN framework could provide a more accurate
and comprehensive evaluation of cognitive word embeddings. It will potentially
be beneficial to the further word embeddings evaluation on extrinsic natural
language processing tasks.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 04:39:38 GMT'}]",2020-09-25,"[['Liu', 'Xinping', ''], ['Cao', 'Zehong', ''], ['Tran', 'Son', '']]"
1353549,2009.12046,Lei Shu,"Lei Shu, Alexandros Papangelis, Yi-Chia Wang, Gokhan Tur, Hu Xu,
  Zhaleh Feizollahi, Bing Liu, Piero Molino",Controllable Text Generation with Focused Variation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This work introduces Focused-Variation Network (FVN), a novel model to
control language generation. The main problems in previous controlled language
generation models range from the difficulty of generating text according to the
given attributes, to the lack of diversity of the generated texts. FVN
addresses these issues by learning disjoint discrete latent spaces for each
attribute inside codebooks, which allows for both controllability and
diversity, while at the same time generating fluent text. We evaluate FVN on
two text generation datasets with annotated content and style, and show
state-of-the-art performance as assessed by automatic and human evaluations.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 06:31:06 GMT'}]",2020-09-28,"[['Shu', 'Lei', ''], ['Papangelis', 'Alexandros', ''], ['Wang', 'Yi-Chia', ''], ['Tur', 'Gokhan', ''], ['Xu', 'Hu', ''], ['Feizollahi', 'Zhaleh', ''], ['Liu', 'Bing', ''], ['Molino', 'Piero', '']]"
1353844,2009.12341,Yurio Windiatmoko,"Yurio Windiatmoko, Ahmad Fathan Hidayatullah, Ridho Rahmadi","Developing FB Chatbot Based on Deep Learning Using RASA Framework for
  University Enquiries","15 pages, 11 figures, prepare for ICITDA conference Batch 3",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Smart systems for Universities powered by Artificial Intelligence have been
massively developed to help humans in various tasks. The chatbot concept is not
something new in today society which is developing with recent technology.
College students or candidates of college students often need actual
information like asking for something to customer service, especially during
this pandemic, when it is difficult to have an immediate face-to-face meeting.
Chatbots are functionally helping in several things such as curriculum
information, admission for new students, schedule info for any lecture courses,
students grade information, and some adding features for Muslim worships
schedule, also weather forecast information. This Chatbot is developed by Deep
Learning models, which was adopted by an artificial intelligence model that
replicates human intelligence with some specific training schemes. This kind of
Deep Learning is based on RNN which has some specific memory savings scheme for
the Deep Learning Model, specifically this chatbot using LSTM which already
integrates by RASA framework. LSTM is also known as Long Short Term Memory
which efficiently saves some required memory but will remove some memory that
is not needed. This Chatbot uses the FB platform because of the FB users have
already reached up to 60.8% of its entire population in Indonesia. Here's the
chatbot only focuses on case studies at campus of the Magister Informatics FTI
University of Islamic Indonesia. This research is a first stage development
within fairly sufficient simulate data.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 17:01:19 GMT'}]",2020-09-28,"[['Windiatmoko', 'Yurio', ''], ['Hidayatullah', 'Ahmad Fathan', ''], ['Rahmadi', 'Ridho', '']]"
1353772,2009.12269,Ehsan Doostmohammadi,"Ehsan Doostmohammadi, Mohammad Hadi Bokaei, Hossein Sameti",PerKey: A Persian News Corpus for Keyphrase Extraction and Generation,,,10.1109/ISTEL.2018.8661095,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Keyphrases provide an extremely dense summary of a text. Such information can
be used in many Natural Language Processing tasks, such as information
retrieval and text summarization. Since previous studies on Persian keyword or
keyphrase extraction have not published their data, the field suffers from the
lack of a human extracted keyphrase dataset. In this paper, we introduce
PerKey, a corpus of 553k news articles from six Persian news websites and
agencies with relatively high quality author extracted keyphrases, which is
then filtered and cleaned to achieve higher quality keyphrases. The resulted
data was put into human assessment to ensure the quality of the keyphrases. We
also measured the performance of different supervised and unsupervised
techniques, e.g. TFIDF, MultipartiteRank, KEA, etc. on the dataset using
precision, recall, and F1-score.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 14:36:41 GMT'}]",2020-09-28,"[['Doostmohammadi', 'Ehsan', ''], ['Bokaei', 'Mohammad Hadi', ''], ['Sameti', 'Hossein', '']]"
1310508,2006.15994,Phuong Le-Hong,"Viet Bui The, Oanh Tran Thi, Phuong Le-Hong","Improving Sequence Tagging for Vietnamese Text Using Transformer-based
  Neural Models",Accepted at the Conference PACLIC 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes our study on using mutilingual BERT embeddings and some
new neural models for improving sequence tagging tasks for the Vietnamese
language. We propose new model architectures and evaluate them extensively on
two named entity recognition datasets of VLSP 2016 and VLSP 2018, and on two
part-of-speech tagging datasets of VLSP 2010 and VLSP 2013. Our proposed models
outperform existing methods and achieve new state-of-the-art results. In
particular, we have pushed the accuracy of part-of-speech tagging to 95.40% on
the VLSP 2010 corpus, to 96.77% on the VLSP 2013 corpus; and the F1 score of
named entity recognition to 94.07% on the VLSP 2016 corpus, to 90.31% on the
VLSP 2018 corpus. Our code and pre-trained models viBERT and vELECTRA are
released as open source to facilitate adoption and further research.
","[{'version': 'v1', 'created': 'Mon, 29 Jun 2020 12:39:44 GMT'}, {'version': 'v2', 'created': 'Tue, 30 Jun 2020 06:32:02 GMT'}, {'version': 'v3', 'created': 'Thu, 2 Jul 2020 11:10:51 GMT'}, {'version': 'v4', 'created': 'Fri, 25 Sep 2020 13:24:18 GMT'}]",2020-09-28,"[['The', 'Viet Bui', ''], ['Thi', 'Oanh Tran', ''], ['Le-Hong', 'Phuong', '']]"
1352060,2009.10557,Huaishao Luo,"Huaishao Luo, Lei Ji, Tianrui Li, Nan Duan, Daxin Jiang","GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based
  Sentiment Analysis",to appear in Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we focus on the imbalance issue, which is rarely studied in
aspect term extraction and aspect sentiment classification when regarding them
as sequence labeling tasks. Besides, previous works usually ignore the
interaction between aspect terms when labeling polarities. We propose a
GRadient hArmonized and CascadEd labeling model (GRACE) to solve these
problems. Specifically, a cascaded labeling module is developed to enhance the
interchange between aspect terms and improve the attention of sentiment tokens
when labeling sentiment polarities. The polarities sequence is designed to
depend on the generated aspect terms labels. To alleviate the imbalance issue,
we extend the gradient harmonized mechanism used in object detection to the
aspect-based sentiment analysis by adjusting the weight of each label
dynamically. The proposed GRACE adopts a post-pretraining BERT as its backbone.
Experimental results demonstrate that the proposed model achieves consistency
improvement on multiple benchmark datasets and generates state-of-the-art
results.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 13:55:34 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 03:19:54 GMT'}]",2020-09-28,"[['Luo', 'Huaishao', ''], ['Ji', 'Lei', ''], ['Li', 'Tianrui', ''], ['Duan', 'Nan', ''], ['Jiang', 'Daxin', '']]"
1353564,2009.12061,Yan Zhang,"Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, Lidong Bing","An Unsupervised Sentence Embedding Method byMutual Information
  Maximization",Accepted to EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  BERT is inefficient for sentence-pair tasks such as clustering or semantic
search as it needs to evaluate combinatorially many sentence pairs which is
very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by
learning semantically meaningful representations of single sentences, such that
similarity comparison can be easily accessed. However, SBERT is trained on
corpus with high-quality labeled sentence pairs, which limits its application
to tasks where labeled data is extremely scarce. In this paper, we propose a
lightweight extension on top of BERT and a novel self-supervised learning
objective based on mutual information maximization strategies to derive
meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our
method is not restricted by the availability of labeled data, such that it can
be applied on different domain-specific corpus. Experimental results show that
the proposed method significantly outperforms other unsupervised sentence
embedding baselines on common semantic textual similarity (STS) tasks and
downstream supervised tasks. It also outperforms SBERT in a setting where
in-domain labeled data is not available, and achieves performance competitive
with supervised methods on various tasks.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 07:16:51 GMT'}]",2020-09-28,"[['Zhang', 'Yan', ''], ['He', 'Ruidan', ''], ['Liu', 'Zuozhu', ''], ['Lim', 'Kwan Hui', ''], ['Bing', 'Lidong', '']]"
1353567,2009.12064,Shunsuke Kitada,Shunsuke Kitada and Hitoshi Iyatomi,"Attention Meets Perturbations: Robust and Interpretable Attention with
  Adversarial Training","10 pages, 4 figures",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, deep learning models have placed more emphasis on the
interpretability and robustness of models. The attention mechanism is an
important technique that contributes to these elements and is widely used,
especially in the natural language processing (NLP) field. Adversarial training
(AT) is a powerful regularization technique for enhancing the robustness of
neural networks and has been successful in many applications. The application
of AT to the attention mechanism is expected to be highly effective, but there
is little research on this. In this paper, we propose a new general training
technique for NLP tasks, using AT for attention (Attention AT) and more
interpretable adversarial training for attention (Attention iAT). Our proposals
improved both the prediction performance and interpretability of the model by
applying AT to the attention mechanisms. In particular, Attention iAT enhances
those advantages by introducing adversarial perturbation, which differentiates
the attention of sentences where it is unclear which words are important. We
performed various NLP tasks on ten open datasets and compared the performance
of our techniques to a recent model using attention mechanisms. Our experiments
revealed that AT for attention mechanisms, especially Attention iAT,
demonstrated (1) the best prediction performance in nine out of ten tasks and
(2) more interpretable attention (i.e., the resulting attention correlated more
strongly with gradient-based word importance) for all tasks. Additionally, our
techniques are (3) much less dependent on perturbation size in AT. Our code and
more results are available at
https://github.com/shunk031/attention-meets-perturbation
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 07:26:45 GMT'}]",2020-09-28,"[['Kitada', 'Shunsuke', ''], ['Iyatomi', 'Hitoshi', '']]"
1282672,2005.02877,Michael Heck,"Michael Heck, Carel van Niekerk, Nurul Lubis, Christian Geishauser,
  Hsien-Chin Lin, Marco Moresi, Milica Ga\v{s}i\'c","TripPy: A Triple Copy Strategy for Value Independent Neural Dialog State
  Tracking","10 pages, 6 figures, published in Proceedings of the 21st Annual
  SIGdial Meeting on Discourse and Dialogue, Code at:
  https://gitlab.cs.uni-duesseldorf.de/general/dsml/trippy-public","Proceedings of the 21th Annual Meeting of the Special Interest
  Group on Discourse and Dialogue (July 2020), Pages 35-44; Association for
  Computational Linguistics",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task-oriented dialog systems rely on dialog state tracking (DST) to monitor
the user's goal during the course of an interaction. Multi-domain and
open-vocabulary settings complicate the task considerably and demand scalable
solutions. In this paper we present a new approach to DST which makes use of
various copy mechanisms to fill slots with values. Our model has no need to
maintain a list of candidate values. Instead, all values are extracted from the
dialog context on-the-fly. A slot is filled by one of three copy mechanisms:
(1) Span prediction may extract values directly from the user input; (2) a
value may be copied from a system inform memory that keeps track of the
system's inform operations; (3) a value may be copied over from a different
slot that is already contained in the dialog state to resolve coreferences
within and across domains. Our approach combines the advantages of span-based
slot filling methods with memory methods to avoid the use of value picklists
altogether. We argue that our strategy simplifies the DST task while at the
same time achieving state of the art performance on various popular evaluation
sets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55%.
","[{'version': 'v1', 'created': 'Wed, 6 May 2020 14:52:48 GMT'}, {'version': 'v2', 'created': 'Mon, 18 May 2020 09:10:27 GMT'}, {'version': 'v3', 'created': 'Tue, 2 Jun 2020 15:14:56 GMT'}, {'version': 'v4', 'created': 'Fri, 25 Sep 2020 13:46:29 GMT'}]",2020-09-28,"[['Heck', 'Michael', ''], ['van Niekerk', 'Carel', ''], ['Lubis', 'Nurul', ''], ['Geishauser', 'Christian', ''], ['Lin', 'Hsien-Chin', ''], ['Moresi', 'Marco', ''], ['Gašić', 'Milica', '']]"
1353584,2009.12081,Robin Hirsch,"Robin Hirsch, Szabolcs Mikul\'as and Tim Stokes","The algebra of non-deterministic programs: demonic operators, orders and
  axioms",,,,,cs.LO cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Demonic composition, demonic refinement and demonic union are alternatives to
the usual ""angelic"" composition, angelic refinement (inclusion) and angelic
(usual) union defined on binary relations. We first motivate both the angelic
and demonic via an analysis of the behaviour of non-deterministic programs,
with the angelic associated with partial correctness and demonic with total
correctness, both cases emerging from a richer algebraic model of
non-deterministic programs incorporating both aspects. Zareckii has shown that
the isomorphism class of algebras of binary relations under angelic composition
and inclusion is finitely axiomatised as the class of ordered semigroups. The
proof can be used to establish that the same axiomatisation applies to binary
relations under demonic composition and refinement, and a further modification
of the proof can be used to incorporate a zero element representing the empty
relation in the angelic case and the full relation in the demonic case. For the
signature of angelic composition and union, it is known that no finite
axiomatisation exists, and we show the analogous result for demonic composition
and demonic union by showing that the same axiomatisation holds for both. We
show that the isomorphism class of algebras of binary relations with the
""mixed"" signature of demonic composition and angelic inclusion has no finite
axiomatisation. As a contrast, we show that the isomorphism class of partial
algebras of binary relations with the partial operation of constellation
product and inclusion (also a ""mixed"" signature) is finitely axiomatisable.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 08:13:07 GMT'}]",2020-09-28,"[['Hirsch', 'Robin', ''], ['Mikulás', 'Szabolcs', ''], ['Stokes', 'Tim', '']]"
1267907,2004.03137,Mingxuan Wang,"Mingxuan Wang, Hongxiao Bai, Hai Zhao, Lei Li","Cross-lingual Supervision Improves Unsupervised Neural Machine
  Translation",,,,10,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural machine translation~(NMT) is ineffective for zero-resource languages.
Recent works exploring the possibility of unsupervised neural machine
translation (UNMT) with only monolingual data can achieve promising results.
However, there are still big gaps between UNMT and NMT with parallel
supervision. In this work, we introduce a multilingual unsupervised NMT
(\method) framework to leverage weakly supervised signals from high-resource
language pairs to zero-resource translation directions. More specifically, for
unsupervised language pairs \texttt{En-De}, we can make full use of the
information from parallel dataset \texttt{En-Fr} to jointly train the
unsupervised translation directions all in one model. \method is based on
multilingual models which require no changes to the standard unsupervised NMT.
Empirical results demonstrate that \method significantly improves the
translation quality by more than 3 BLEU score on six benchmark unsupervised
translation directions.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 05:46:49 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 03:44:16 GMT'}]",2020-09-28,"[['Wang', 'Mingxuan', ''], ['Bai', 'Hongxiao', ''], ['Zhao', 'Hai', ''], ['Li', 'Lei', '']]"
1270631,2004.05861,Maram Hasanain,"Fatima Haouari, Maram Hasanain, Reem Suwaileh, Tamer Elsayed","ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation
  Networks","Updated some content, statistics and figures of the paper",,,,cs.CL cs.IR cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that
covers the period from 27th of January till 30th of April 2020. ArCOV-19 is the
first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that
includes over 1M tweets alongside the propagation networks of the most-popular
subset of them (i.e., most-retweeted and -liked). The propagation networks
include both retweets and conversational threads (i.e., threads of replies).
ArCOV-19 is designed to enable research under several domains including natural
language processing, information retrieval, and social computing, among others.
Preliminary analysis shows that ArCOV-19 captures rising discussions associated
with the first reported cases of the disease as they appeared in the Arab
world. In addition to the source tweets and the propagation networks, we also
release the search queries and the language-independent crawler used to collect
the tweets to encourage the curation of similar datasets.
","[{'version': 'v1', 'created': 'Mon, 13 Apr 2020 10:49:53 GMT'}, {'version': 'v2', 'created': 'Sat, 18 Apr 2020 07:09:53 GMT'}, {'version': 'v3', 'created': 'Fri, 25 Sep 2020 16:14:53 GMT'}]",2020-09-28,"[['Haouari', 'Fatima', ''], ['Hasanain', 'Maram', ''], ['Suwaileh', 'Reem', ''], ['Elsayed', 'Tamer', '']]"
1255526,2003.05171,Peter beim Graben,"Peter beim Graben, Markus Huber, Werner Meyer, Ronald R\""omer and
  Matthias Wolff",Vector symbolic architectures for context-free grammars,"36 pages, 3 figures",,,,cs.CL q-bio.NC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Background / introduction. Vector symbolic architectures (VSA) are a viable
approach for the hyperdimensional representation of symbolic data, such as
documents, syntactic structures, or semantic frames. Methods. We present a
rigorous mathematical framework for the representation of phrase structure
trees and parse trees of context-free grammars (CFG) in Fock space, i.e.
infinite-dimensional Hilbert space as being used in quantum field theory. We
define a novel normal form for CFG by means of term algebras. Using a recently
developed software toolbox, called FockBox, we construct Fock space
representations for the trees built up by a CFG left-corner (LC) parser.
Results. We prove a universal representation theorem for CFG term algebras in
Fock space and illustrate our findings through a low-dimensional principal
component projection of the LC parser states. Conclusions. Our approach could
leverage the development of VSA for explainable artificial intelligence (XAI)
by means of hyperdimensional deep neural computation. It could be of
significance for the improvement of cognitive user interfaces and other
applications of VSA in machine learning.
","[{'version': 'v1', 'created': 'Wed, 11 Mar 2020 09:07:02 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 08:34:46 GMT'}]",2020-09-28,"[['Graben', 'Peter beim', ''], ['Huber', 'Markus', ''], ['Meyer', 'Werner', ''], ['Römer', 'Ronald', ''], ['Wolff', 'Matthias', '']]"
1353466,2009.11963,Jonathan Enderle,Jonathan Scott Enderle,Toward a Thermodynamics of Meaning,"To be published in the proceedings of CHR 2020: Workshop on
  Computational Humanities Research, November 18-20, 2020, Amsterdam, The
  Netherlands",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  As language models such as GPT-3 become increasingly successful at generating
realistic text, questions about what purely text-based modeling can learn about
the world have become more urgent. Is text purely syntactic, as skeptics argue?
Or does it in fact contain some semantic information that a sufficiently
sophisticated language model could use to learn about the world without any
additional inputs? This paper describes a new model that suggests some
qualified answers to those questions. By theorizing the relationship between
text and the world it describes as an equilibrium relationship between a
thermodynamic system and a much larger reservoir, this paper argues that even
very simple language models do learn structural facts about the world, while
also proposing relatively precise limits on the nature and extent of those
facts. This perspective promises not only to answer questions about what
language models actually learn, but also to explain the consistent and
surprising success of cooccurrence prediction as a meaning-making strategy in
AI.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 21:56:02 GMT'}]",2020-09-28,"[['Enderle', 'Jonathan Scott', '']]"
1353401,2009.11898,Anna Glazkova,"Anna Glazkova, Yury Egorov, Maksim Glazkov",A Comparative Study of Feature Types for Age-Based Text Classification,"Accepted to AIST-2020 (The 9th International Conference on Analysis
  of Images, Social Networks and Texts)",,,,cs.CL cs.IR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The ability to automatically determine the age audience of a novel provides
many opportunities for the development of information retrieval tools. Firstly,
developers of book recommendation systems and electronic libraries may be
interested in filtering texts by the age of the most likely readers. Further,
parents may want to select literature for children. Finally, it will be useful
for writers and publishers to determine which features influence whether the
texts are suitable for children. In this article, we compare the empirical
effectiveness of various types of linguistic features for the task of age-based
classification of fiction texts. For this purpose, we collected a text corpus
of book previews labeled with one of two categories -- children's or adult. We
evaluated the following types of features: readability indices, sentiment,
lexical, grammatical and general features, and publishing attributes. The
results obtained show that the features describing the text at the document
level can significantly increase the quality of machine learning models.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 18:41:10 GMT'}]",2020-09-28,"[['Glazkova', 'Anna', ''], ['Egorov', 'Yury', ''], ['Glazkov', 'Maksim', '']]"
1353399,2009.11896,Subhajit Chaudhury,"Subhajit Chaudhury, Daiki Kimura, Kartik Talamadupula, Michiaki
  Tatsubori, Asim Munawar and Ryuki Tachibana","Bootstrapped Q-learning with Context Relevant Observation Pruning to
  Generalize in Text-based Games",Accepted to EMNLP 2020,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We show that Reinforcement Learning (RL) methods for solving Text-Based Games
(TBGs) often fail to generalize on unseen games, especially in small data
regimes. To address this issue, we propose Context Relevant Episodic State
Truncation (CREST) for irrelevant token removal in observation text for
improved generalization. Our method first trains a base model using Q-learning,
which typically overfits the training games. The base model's action token
distribution is used to perform observation pruning that removes irrelevant
tokens. A second bootstrapped model is then retrained on the pruned observation
text. Our bootstrapped agent shows improved generalization in solving unseen
TextWorld games, using 10x-20x fewer training games compared to previous
state-of-the-art methods despite requiring less number of training episodes.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 18:38:30 GMT'}]",2020-09-28,"[['Chaudhury', 'Subhajit', ''], ['Kimura', 'Daiki', ''], ['Talamadupula', 'Kartik', ''], ['Tatsubori', 'Michiaki', ''], ['Munawar', 'Asim', ''], ['Tachibana', 'Ryuki', '']]"
1353695,2009.12192,Benjamin Chamberlain,"Benjamin P. Chamberlain, Emanuele Rossi, Dan Shiebler, Suvash Sedhain,
  Michael M. Bronstein",Tuning Word2vec for Large Scale Recommendation Systems,"11 pages, 4 figures, Fourteenth ACM Conference on Recommender Systems","Fourteenth ACM Conference on Recommender Systems (RecSys '20),
  September 22--26, 2020, Virtual Event, Brazil",10.1145/3383313.3418486,,cs.IR cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Word2vec is a powerful machine learning tool that emerged from Natural
Lan-guage Processing (NLP) and is now applied in multiple domains, including
recom-mender systems, forecasting, and network analysis. As Word2vec is often
used offthe shelf, we address the question of whether the default
hyperparameters are suit-able for recommender systems. The answer is
emphatically no. In this paper, wefirst elucidate the importance of
hyperparameter optimization and show that un-constrained optimization yields an
average 221% improvement in hit rate over thedefault parameters. However,
unconstrained optimization leads to hyperparametersettings that are very
expensive and not feasible for large scale recommendationtasks. To this end, we
demonstrate 138% average improvement in hit rate with aruntime
budget-constrained hyperparameter optimization. Furthermore, to
makehyperparameter optimization applicable for large scale recommendation
problemswhere the target dataset is too large to search over, we investigate
generalizinghyperparameters settings from samples. We show that applying
constrained hy-perparameter optimization using only a 10% sample of the data
still yields a 91%average improvement in hit rate over the default parameters
when applied to thefull datasets. Finally, we apply hyperparameters learned
using our method of con-strained optimization on a sample to the Who To Follow
recommendation serviceat Twitter and are able to increase follow rates by 15%.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 10:50:19 GMT'}]",2020-09-28,"[['Chamberlain', 'Benjamin P.', ''], ['Rossi', 'Emanuele', ''], ['Shiebler', 'Dan', ''], ['Sedhain', 'Suvash', ''], ['Bronstein', 'Michael M.', '']]"
1201703,1911.03109,"Mathias M\""uller","Mathias M\""uller, Annette Rios, Rico Sennrich",Domain Robustness in Neural Machine Translation,V2: AMTA camera-ready,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Translating text that diverges from the training domain is a key challenge
for machine translation. Domain robustness---the generalization of models to
unseen test domains---is low for both statistical (SMT) and neural machine
translation (NMT). In this paper, we study the performance of SMT and NMT
models on out-of-domain test sets. We find that in unknown domains, SMT and NMT
suffer from very different problems: SMT systems are mostly adequate but not
fluent, while NMT systems are mostly fluent, but not adequate. For NMT, we
identify such hallucinations (translations that are fluent but unrelated to the
source) as a key reason for low domain robustness. To mitigate this problem, we
empirically compare methods that are reported to improve adequacy or in-domain
robustness in terms of their effectiveness at improving domain robustness. In
experiments on German to English OPUS data, and German to Romansh (a
low-resource setting) we find that several methods improve domain robustness.
While those methods do lead to higher BLEU scores overall, they only slightly
increase the adequacy of translations compared to SMT.
","[{'version': 'v1', 'created': 'Fri, 8 Nov 2019 07:57:46 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Sep 2020 07:31:33 GMT'}]",2020-09-28,"[['Müller', 'Mathias', ''], ['Rios', 'Annette', ''], ['Sennrich', 'Rico', '']]"
1284072,2005.04277,Peng Su,Peng Su and K. Vijay-Shanker,"Adversarial Learning for Supervised and Semi-supervised Relation
  Extraction in Biomedical Literature",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversarial training is a technique of improving model performance by
involving adversarial examples in the training process. In this paper, we
investigate adversarial training with multiple adversarial examples to benefit
the relation extraction task. We also apply adversarial training technique in
semi-supervised scenarios to utilize unlabeled data. The evaluation results on
protein-protein interaction and protein subcellular localization task
illustrate adversarial training provides improvement on the supervised model,
and is also effective on involving unlabeled data in the semi-supervised
training case. In addition, our method achieves state-of-the-art performance on
two benchmarking datasets.
","[{'version': 'v1', 'created': 'Fri, 8 May 2020 20:19:26 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 15:21:50 GMT'}]",2020-09-28,"[['Su', 'Peng', ''], ['Vijay-Shanker', 'K.', '']]"
1353774,2009.12271,Ehsan Doostmohammadi,"Ehsan Doostmohammadi, Mohammad Hadi Bokaei, Hossein Sameti",Persian Keyphrase Generation Using Sequence-to-Sequence Models,,,10.1109/IranianCEE.2019.8786505,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Keyphrases are a very short summary of an input text and provide the main
subjects discussed in the text. Keyphrase extraction is a useful upstream task
and can be used in various natural language processing problems, for example,
text summarization and information retrieval, to name a few. However, not all
the keyphrases are explicitly mentioned in the body of the text. In real-world
examples there are always some topics that are discussed implicitly. Extracting
such keyphrases requires a generative approach, which is adopted here. In this
paper, we try to tackle the problem of keyphrase generation and extraction from
news articles using deep sequence-to-sequence models. These models
significantly outperform the conventional methods such as Topic Rank, KPMiner,
and KEA in the task of keyphrase extraction.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 14:40:14 GMT'}]",2020-09-28,"[['Doostmohammadi', 'Ehsan', ''], ['Bokaei', 'Mohammad Hadi', ''], ['Sameti', 'Hossein', '']]"
1353743,2009.12240,Mark Riedl,Mark Riedl,Weird AI Yankovic: Generating Parody Lyrics,"9 pages, serious paper about a silly task, written accordingly",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lyrics parody swaps one set of words that accompany a melody with a new set
of words, preserving the number of syllables per line and the rhyme scheme.
Lyrics parody generation is a challenge for controllable text generation. We
show how a specialized sampling procedure, combined with backward text
generation with XLNet can produce parody lyrics that reliably meet the syllable
and rhyme scheme constraints.We introduce the Weird AI Yankovic system and
provide a case study evaluation. We conclude with societal implications of
neural lyric parody generation.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 13:56:20 GMT'}]",2020-09-28,"[['Riedl', 'Mark', '']]"
1245198,2002.07775,Jeena Kleenankandy,"Jeena Kleenankandy, K. A. Abdul Nazeer (Department of Computer Science
  and Engineering, National Institute of Technology Calicut, Kerala, India)","An enhanced Tree-LSTM architecture for sentence semantic modeling using
  typed dependencies","Accepted manuscript submitted to Journal of Information Processing
  and Management ( Elsevier ) on June 11, 2020","Information Processing & Management, Elsevier (2020)",10.1016/j.ipm.2020.102362,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Tree-based Long short term memory (LSTM) network has become state-of-the-art
for modeling the meaning of language texts as they can effectively exploit the
grammatical syntax and thereby non-linear dependencies among words of the
sentence. However, most of these models cannot recognize the difference in
meaning caused by a change in semantic roles of words or phrases because they
do not acknowledge the type of grammatical relations, also known as typed
dependencies, in sentence structure. This paper proposes an enhanced LSTM
architecture, called relation gated LSTM, which can model the relationship
between two inputs of a sequence using a control input. We also introduce a
Tree-LSTM model called Typed Dependency Tree-LSTM that uses the sentence
dependency parse structure as well as the dependency type to embed sentence
meaning into a dense vector. The proposed model outperformed its type-unaware
counterpart in two typical NLP tasks - Semantic Relatedness Scoring and
Sentiment Analysis, in a lesser number of training epochs. The results were
comparable or competitive with other state-of-the-art models. Qualitative
analysis showed that changes in the voice of sentences had little effect on the
model's predicted scores, while changes in nominal (noun) words had a more
significant impact. The model recognized subtle semantic relationships in
sentence pairs. The magnitudes of learned typed dependencies embeddings were
also in agreement with human intuitions. The research findings imply the
significance of grammatical relations in sentence modeling. The proposed models
would serve as a base for future researches in this direction.
","[{'version': 'v1', 'created': 'Tue, 18 Feb 2020 18:10:03 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 09:45:26 GMT'}]",2020-09-28,"[['Kleenankandy', 'Jeena', '', 'Department of Computer Science\n  and Engineering, National Institute of Technology Calicut, Kerala, India'], ['Nazeer', 'K. A. Abdul', '', 'Department of Computer Science\n  and Engineering, National Institute of Technology Calicut, Kerala, India']]"
1353605,2009.12102,Zhi Cui,"Zhi Cui, Yanran Li, Jiayi Zhang, Jianwei Cui, Chen Wei, Bin Wang",Focus-Constrained Attention Mechanism for CVAE-based Response Generation,To appear in findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To model diverse responses for a given post, one promising way is to
introduce a latent variable into Seq2Seq models. The latent variable is
supposed to capture the discourse-level information and encourage the
informativeness of target responses. However, such discourse-level information
is often too coarse for the decoder to be utilized. To tackle it, our idea is
to transform the coarse-grained discourse-level information into fine-grained
word-level information. Specifically, we firstly measure the semantic
concentration of corresponding target response on the post words by introducing
a fine-grained focus signal. Then, we propose a focus-constrained attention
mechanism to take full advantage of focus in well aligning the input to the
target response. The experimental results demonstrate that by exploiting the
fine-grained signal, our model can generate more diverse and informative
responses compared with several state-of-the-art models.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 09:38:59 GMT'}]",2020-09-28,"[['Cui', 'Zhi', ''], ['Li', 'Yanran', ''], ['Zhang', 'Jiayi', ''], ['Cui', 'Jianwei', ''], ['Wei', 'Chen', ''], ['Wang', 'Bin', '']]"
1280432,2005.00637,Rajarshi Bhowmik,Rajarshi Bhowmik and Gerard de Melo,Explainable Link Prediction for Emerging Entities in Knowledge Graphs,"To appear in the proceedings of International Semantic Web
  Conference, 2020 (ISWC 2020)",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Despite their large-scale coverage, cross-domain knowledge graphs invariably
suffer from inherent incompleteness and sparsity. Link prediction can alleviate
this by inferring a target entity, given a source entity and a query relation.
Recent embedding-based approaches operate in an uninterpretable latent semantic
vector space of entities and relations, while path-based approaches operate in
the symbolic space, making the inference process explainable. However, these
approaches typically consider static snapshots of the knowledge graphs,
severely restricting their applicability for evolving knowledge graphs with
newly emerging entities. To overcome this issue, we propose an inductive
representation learning framework that is able to learn representations of
previously unseen entities. Our method finds reasoning paths between source and
target entities, thereby making the link prediction for unseen entities
interpretable and providing support evidence for the inferred link.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 22:17:37 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 13:38:29 GMT'}]",2020-09-28,"[['Bhowmik', 'Rajarshi', ''], ['de Melo', 'Gerard', '']]"
1352965,2009.11462,Suchin Gururangan,"Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, Noah A.
  Smith","RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language
  Models",Findings in EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained neural language models (LMs) are prone to generating racist,
sexist, or otherwise toxic language which hinders their safe deployment. We
investigate the extent to which pretrained LMs can be prompted to generate
toxic language, and the effectiveness of controllable text generation
algorithms at preventing such toxic degeneration. We create and release
RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level
prompts derived from a large corpus of English web text, paired with toxicity
scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we
find that pretrained LMs can degenerate into toxic text even from seemingly
innocuous prompts. We empirically assess several controllable generation
methods, and find that while data- or compute-intensive methods (e.g., adaptive
pretraining on non-toxic data) are more effective at steering away from
toxicity than simpler solutions (e.g., banning ""bad"" words), no current method
is failsafe against neural toxic degeneration. To pinpoint the potential cause
of such persistent toxic degeneration, we analyze two web text corpora used to
pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a
significant amount of offensive, factually unreliable, and otherwise toxic
content. Our work provides a test bed for evaluating toxic generations by LMs
and stresses the need for better data selection processes for pretraining.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 03:17:19 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 20:22:26 GMT'}]",2020-09-29,"[['Gehman', 'Samuel', ''], ['Gururangan', 'Suchin', ''], ['Sap', 'Maarten', ''], ['Choi', 'Yejin', ''], ['Smith', 'Noah A.', '']]"
1354815,2009.13312,Zheng Zhao,"Zheng Zhao, Shay B. Cohen, Bonnie Webber",Reducing Quantity Hallucinations in Abstractive Summarization,Accepted to Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is well-known that abstractive summaries are subject to
hallucination---including material that is not supported by the original text.
While summaries can be made hallucination-free by limiting them to general
phrases, such summaries would fail to be very informative. Alternatively, one
can try to avoid hallucinations by verifying that any specific entities in the
summary appear in the original text in a similar context. This is the approach
taken by our system, Herman. The system learns to recognize and verify quantity
entities (dates, numbers, sums of money, etc.) in a beam-worth of abstractive
summaries produced by state-of-the-art models, in order to up-rank those
summaries whose quantity terms are supported by the original text. Experimental
results demonstrate that the ROUGE scores of such up-ranked summaries have a
higher Precision than summaries that have not been up-ranked, without a
comparable loss in Recall, resulting in higher F$_1$. Preliminary human
evaluation of up-ranked vs. original summaries shows people's preference for
the former.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 13:32:59 GMT'}]",2020-09-29,"[['Zhao', 'Zheng', ''], ['Cohen', 'Shay B.', ''], ['Webber', 'Bonnie', '']]"
1280353,2005.00558,Yizhe Zhang,"Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, Bill
  Dolan","POINTER: Constrained Progressive Text Generation via Insertion-based
  Generative Pre-training",EMNLP 2020 long paper,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale pre-trained language models, such as BERT and GPT-2, have
achieved excellent performance in language representation learning and
free-form text generation. However, these models cannot be directly employed to
generate text under specified lexical constraints. To address this challenge,
we present POINTER (PrOgressive INsertion-based TransformER), a simple yet
novel insertion-based approach for hard-constrained text generation. The
proposed method operates by progressively inserting new tokens between existing
tokens in a parallel manner. This procedure is recursively applied until a
sequence is completed. The resulting coarse-to-fine hierarchy makes the
generation process intuitive and interpretable. We pre-train our model with the
proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and
fine-tune it on downstream hard-constrained generation tasks.
Non-autoregressive decoding yields an empirically logarithmic time complexity
during inference time. Experimental results on both News and Yelp datasets
demonstrate that POINTER achieves state-of-the-art performance on constrained
text generation. We released the pre-trained models and the source code to
facilitate future research (https://github.com/dreasysnail/POINTER).
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 18:11:54 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Sep 2020 00:07:39 GMT'}]",2020-09-29,"[['Zhang', 'Yizhe', ''], ['Wang', 'Guoyin', ''], ['Li', 'Chunyuan', ''], ['Gan', 'Zhe', ''], ['Brockett', 'Chris', ''], ['Dolan', 'Bill', '']]"
1354787,2009.13284,Hongjin Qian,"Xiaohe Li, Hanxun Zhong, Yu Guo, Yueyuan Ma, Hongjin Qian, Zhanliang
  Liu, Zhicheng Dou, Ji-Rong Wen",Pchatbot: A Large-Scale Dataset for Personalized Chatbot,10 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language dialogue systems raise great attention recently. As many
dialogue models are data-driven, high quality datasets are essential to these
systems. In this paper, we introduce Pchatbot, a large scale dialogue dataset
which contains two subsets collected from Weibo and Judical forums
respectively. Different from existing datasets which only contain post-response
pairs, we include anonymized user IDs as well as timestamps. This enables the
development of personalized dialogue models which depend on the availability of
users' historical conversations. Furthermore, the scale of Pchatbot is
significantly larger than existing datasets, which might benefit the
data-driven models. Our preliminary experimental study shows that a
personalized chatbot model trained on Pchatbot outperforms the corresponding
ad-hoc chatbot models. We also demonstrate that using larger dataset improves
the quality of dialog models.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 12:49:07 GMT'}]",2020-09-29,"[['Li', 'Xiaohe', ''], ['Zhong', 'Hanxun', ''], ['Guo', 'Yu', ''], ['Ma', 'Yueyuan', ''], ['Qian', 'Hongjin', ''], ['Liu', 'Zhanliang', ''], ['Dou', 'Zhicheng', ''], ['Wen', 'Ji-Rong', '']]"
1354798,2009.13295,Pepa Atanasova,"Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle
  Augenstein",A Diagnostic Study of Explainability Techniques for Text Classification,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent developments in machine learning have introduced models that approach
human performance at the cost of increased architectural complexity. Efforts to
make the rationales behind the models' predictions transparent have inspired an
abundance of new explainability techniques. Provided with an already trained
model, they compute saliency scores for the words of an input instance.
However, there exists no definitive guide on (i) how to choose such a technique
given a particular application task and model architecture, and (ii) the
benefits and drawbacks of using each such technique. In this paper, we develop
a comprehensive list of diagnostic properties for evaluating existing
explainability techniques. We then employ the proposed list to compare a set of
diverse explainability techniques on downstream text classification tasks and
neural network architectures. We also compare the saliency scores assigned by
the explainability techniques with human annotations of salient input regions
to find relations between a model's performance and the agreement of its
rationales with human ones. Overall, we find that the gradient-based
explanations perform best across tasks and model architectures, and we present
further insights into the properties of the reviewed explainability techniques.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 12:01:53 GMT'}]",2020-09-29,"[['Atanasova', 'Pepa', ''], ['Simonsen', 'Jakob Grue', ''], ['Lioma', 'Christina', ''], ['Augenstein', 'Isabelle', '']]"
1280073,2005.00278,Yanpeng Zhao,Yanpeng Zhao and Ivan Titov,"Unsupervised Transfer of Semantic Role Models from Verbal to Nominal
  Domain",Our code is available at https://github.com/zhaoyanpeng/srltransfer,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Semantic role labeling (SRL) is an NLP task involving the assignment of
predicate arguments to types, called semantic roles. Though research on SRL has
primarily focused on verbal predicates and many resources available for SRL
provide annotations only for verbs, semantic relations are often triggered by
other linguistic constructions, e.g., nominalizations. In this work, we
investigate a transfer scenario where we assume role-annotated data for the
source verbal domain but only unlabeled data for the target nominal domain. Our
key assumption, enabling the transfer between the two domains, is that
selectional preferences of a role (i.e., preferences or constraints on the
admissible arguments) do not strongly depend on whether the relation is
triggered by a verb or a noun. For example, the same set of arguments can fill
the Acquirer role for the verbal predicate `acquire' and its nominal form
`acquisition'. We approach the transfer task from the variational autoencoding
perspective. The labeler serves as an encoder (predicting role labels given a
sentence), whereas selectional preferences are captured in the decoder
component (generating arguments for the predicting roles). Nominal roles are
not labeled in the training data, and the learning objective instead pushes the
labeler to assign roles predictive of the arguments. Sharing the decoder
parameters across the domains encourages consistency between labels predicted
for both domains and facilitates the transfer. The method substantially
outperforms baselines, such as unsupervised and `direct transfer' methods, on
the English CoNLL-2009 dataset.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 09:20:48 GMT'}, {'version': 'v2', 'created': 'Sat, 26 Sep 2020 12:56:08 GMT'}]",2020-09-29,"[['Zhao', 'Yanpeng', ''], ['Titov', 'Ivan', '']]"
1354870,2009.13367,Inna Vogel,"Inna Vogel, Jeong-Eun Choi, Meghana Meghana","Similarity Detection Pipeline for Crawling a Topic Related Fake News
  Corpus",,,,,cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Fake news detection is a challenging task aiming to reduce human time and
effort to check the truthfulness of news. Automated approaches to combat fake
news, however, are limited by the lack of labeled benchmark datasets,
especially in languages other than English. Moreover, many publicly available
corpora have specific limitations that make them difficult to use. To address
this problem, our contribution is threefold. First, we propose a new, publicly
available German topic related corpus for fake news detection. To the best of
our knowledge, this is the first corpus of its kind. In this regard, we
developed a pipeline for crawling similar news articles. As our third
contribution, we conduct different learning experiments to detect fake news.
The best performance was achieved using sentence level embeddings from SBERT in
combination with a Bi-LSTM (k=0.88).
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 14:35:31 GMT'}]",2020-09-29,"[['Vogel', 'Inna', ''], ['Choi', 'Jeong-Eun', ''], ['Meghana', 'Meghana', '']]"
1354795,2009.13292,Itzik Malkiel,"Itzik Malkiel, Oren Barkan, Avi Caciularu, Noam Razin, Ori Katz and
  Noam Koenigstein",RecoBERT: A Catalog Language Model for Text-Based Recommendations,,,,,cs.IR cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language models that utilize extensive self-supervised pre-training from
unlabeled text, have recently shown to significantly advance the
state-of-the-art performance in a variety of language understanding tasks.
However, it is yet unclear if and how these recent models can be harnessed for
conducting text-based recommendations. In this work, we introduce RecoBERT, a
BERT-based approach for learning catalog-specialized language models for
text-based item recommendations. We suggest novel training and inference
procedures for scoring similarities between pairs of items, that don't require
item similarity labels. Both the training and the inference techniques were
designed to utilize the unlabeled structure of textual catalogs, and minimize
the discrepancy between them. By incorporating four scores during inference,
RecoBERT can infer text-based item-to-item similarities more accurately than
other techniques. In addition, we introduce a new language understanding task
for wine recommendations using similarities based on professional wine reviews.
As an additional contribution, we publish annotated recommendations dataset
crafted by human wine experts. Finally, we evaluate RecoBERT and compare it to
various state-of-the-art NLP models on wine and fashion recommendations tasks.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 14:23:38 GMT'}]",2020-09-29,"[['Malkiel', 'Itzik', ''], ['Barkan', 'Oren', ''], ['Caciularu', 'Avi', ''], ['Razin', 'Noam', ''], ['Katz', 'Ori', ''], ['Koenigstein', 'Noam', '']]"
1353907,2009.12404,Yanpeng Zhao,Yanpeng Zhao and Ivan Titov,Visually Grounded Compound PCFGs,"Accepted to EMNLP 2020. Our code is available at
  https://github.com/zhaoyanpeng/vpcfg",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Exploiting visual groundings for language understanding has recently been
drawing much attention. In this work, we study visually grounded grammar
induction and learn a constituency parser from both unlabeled text and its
visual groundings. Existing work on this task (Shi et al., 2019) optimizes a
parser via Reinforce and derives the learning signal only from the alignment of
images and sentences. While their model is relatively accurate overall, its
error distribution is very uneven, with low performance on certain constituents
types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6%
recall on noun phrases, NPs). This is not surprising as the learning signal is
likely insufficient for deriving all aspects of phrase-structure syntax and
gradient estimates are noisy. We show that using an extension of probabilistic
context-free grammar model we can do fully-differentiable end-to-end visually
grounded learning. Additionally, this enables us to complement the image-text
alignment loss with a language modeling objective. On the MSCOCO test captions,
our model establishes a new state of the art, outperforming its non-grounded
version and, thus, confirming the effectiveness of visual groundings in
constituency grammar induction. It also substantially outperforms the previous
grounded model, with largest improvements on more `abstract' categories (e.g.,
+55.1% recall on VPs).
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 19:07:00 GMT'}]",2020-09-29,"[['Zhao', 'Yanpeng', ''], ['Titov', 'Ivan', '']]"
1354901,2009.13398,Mihael Arcan,"Daniel Torregrosa and Nivranshu Pasricha and Maraim Masoud and
  Bharathi Raja Chakravarthi and Juan Alonso and Noe Casas and Mihael Arcan","Aspects of Terminological and Named Entity Knowledge within Rule-Based
  Machine Translation Models for Under-Resourced Neural Machine Translation
  Scenarios",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Rule-based machine translation is a machine translation paradigm where
linguistic knowledge is encoded by an expert in the form of rules that
translate text from source to target language. While this approach grants
extensive control over the output of the system, the cost of formalising the
needed linguistic knowledge is much higher than training a corpus-based system,
where a machine learning approach is used to automatically learn to translate
from examples. In this paper, we describe different approaches to leverage the
information contained in rule-based machine translation systems to improve a
corpus-based one, namely, a neural machine translation model, with a focus on a
low-resource scenario. Three different kinds of information were used:
morphological information, named entities and terminology. In addition to
evaluating the general performance of the system, we systematically analysed
the performance of the proposed approaches when dealing with the targeted
phenomena. Our results suggest that the proposed models have limited ability to
learn from external information, and most approaches do not significantly alter
the results of the automatic evaluation, but our preliminary qualitative
evaluation shows that in certain cases the hypothesis generated by our system
exhibit favourable behaviour such as keeping the use of passive voice.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 15:19:23 GMT'}]",2020-09-29,"[['Torregrosa', 'Daniel', ''], ['Pasricha', 'Nivranshu', ''], ['Masoud', 'Maraim', ''], ['Chakravarthi', 'Bharathi Raja', ''], ['Alonso', 'Juan', ''], ['Casas', 'Noe', ''], ['Arcan', 'Mihael', '']]"
1354802,2009.13299,Shuqing Bian,"Shuqing Bian, Xu Chen, Wayne Xin Zhao, Kun Zhou, Yupeng Hou, Yang
  Song, Tao Zhang and Ji-Rong Wen","Learning to Match Jobs with Resumes from Sparse Interaction Data using
  Multi-View Co-Teaching Network",,CIKM 2020,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the ever-increasing growth of online recruitment data, job-resume
matching has become an important task to automatically match jobs with suitable
resumes. This task is typically casted as a supervised text matching problem.
Supervised learning is powerful when the labeled data is sufficient. However,
on online recruitment platforms, job-resume interaction data is sparse and
noisy, which affects the performance of job-resume match algorithms. To
alleviate these problems, in this paper, we propose a novel multi-view
co-teaching network from sparse interaction data for job-resume matching. Our
network consists of two major components, namely text-based matching model and
relation-based matching model. The two parts capture semantic compatibility in
two different views, and complement each other. In order to address the
challenges from sparse and noisy data, we design two specific strategies to
combine the two components. First, two components share the learned parameters
or representations, so that the original representations of each component can
be enhanced. More importantly, we adopt a co-teaching mechanism to reduce the
influence of noise in training data. The core idea is to let the two components
help each other by selecting more reliable training instances. The two
strategies focus on representation enhancement and data enhancement,
respectively. Compared with pure text-based matching models, the proposed
approach is able to learn better data representations from limited or even
sparse interaction data, which is more resistible to noise in training data.
Experiment results have demonstrated that our model is able to outperform
state-of-the-art methods for job-resume matching.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 03:09:54 GMT'}]",2020-09-29,"[['Bian', 'Shuqing', ''], ['Chen', 'Xu', ''], ['Zhao', 'Wayne Xin', ''], ['Zhou', 'Kun', ''], ['Hou', 'Yupeng', ''], ['Song', 'Yang', ''], ['Zhang', 'Tao', ''], ['Wen', 'Ji-Rong', '']]"
1354118,2009.12615,Tsolak Ghukasyan,"Arthur Malajyan, Karen Avetisyan, Tsolak Ghukasyan",ARPA: Armenian Paraphrase Detection Corpus and Models,"To be published in the proceedings of Ivannikov Memorial Workshop
  2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we employ a semi-automatic method based on back translation to
generate a sentential paraphrase corpus for the Armenian language. The initial
collection of sentences is translated from Armenian to English and back twice,
resulting in pairs of lexically distant but semantically similar sentences. The
generated paraphrases are then manually reviewed and annotated. Using the
method train and test datasets are created, containing 2360 paraphrases in
total. In addition, the datasets are used to train and evaluate BERTbased
models for detecting paraphrase in Armenian, achieving results comparable to
the state-of-the-art of other languages.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 14:56:57 GMT'}]",2020-09-29,"[['Malajyan', 'Arthur', ''], ['Avetisyan', 'Karen', ''], ['Ghukasyan', 'Tsolak', '']]"
1063854,1812.06280,Ikuya Yamada,"Ikuya Yamada, Akari Asai, Jin Sakuma, Hiroyuki Shindo, Hideaki Takeda,
  Yoshiyasu Takefuji, and Yuji Matsumoto","Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the
  Embeddings of Words and Entities from Wikipedia",EMNLP 2020 (system demonstration),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The embeddings of entities in a large knowledge base (e.g., Wikipedia) are
highly beneficial for solving various natural language tasks that involve real
world knowledge. In this paper, we present Wikipedia2Vec, a Python-based
open-source tool for learning the embeddings of words and entities from
Wikipedia. The proposed tool enables users to learn the embeddings efficiently
by issuing a single command with a Wikipedia dump file as an argument. We also
introduce a web-based demonstration of our tool that allows users to visualize
and explore the learned embeddings. In our experiments, our tool achieved a
state-of-the-art result on the KORE entity relatedness dataset, and competitive
results on various standard benchmark datasets. Furthermore, our tool has been
used as a key component in various recent studies. We publicize the source
code, demonstration, and the pretrained embeddings for 12 languages at
https://wikipedia2vec.github.io.
","[{'version': 'v1', 'created': 'Sat, 15 Dec 2018 12:51:39 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Dec 2018 14:25:27 GMT'}, {'version': 'v3', 'created': 'Thu, 30 Jan 2020 10:58:05 GMT'}, {'version': 'v4', 'created': 'Sat, 26 Sep 2020 14:28:42 GMT'}]",2020-09-29,"[['Yamada', 'Ikuya', ''], ['Asai', 'Akari', ''], ['Sakuma', 'Jin', ''], ['Shindo', 'Hiroyuki', ''], ['Takeda', 'Hideaki', ''], ['Takefuji', 'Yoshiyasu', ''], ['Matsumoto', 'Yuji', '']]"
1354268,2009.12765,Damai Dai,"Damai Dai, Hua Zheng, Fuli Luo, Pengcheng Yang, Baobao Chang, Zhifang
  Sui","Inductively Representing Out-of-Knowledge-Graph Entities by Optimal
  Estimation Under Translational Assumptions",,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conventional Knowledge Graph Completion (KGC) assumes that all test entities
appear during training. However, in real-world scenarios, Knowledge Graphs (KG)
evolve fast with out-of-knowledge-graph (OOKG) entities added frequently, and
we need to represent these entities efficiently. Most existing Knowledge Graph
Embedding (KGE) methods cannot represent OOKG entities without costly
retraining on the whole KG. To enhance efficiency, we propose a simple and
effective method that inductively represents OOKG entities by their optimal
estimation under translational assumptions. Given pretrained embeddings of the
in-knowledge-graph (IKG) entities, our method needs no additional learning.
Experimental results show that our method outperforms the state-of-the-art
methods with higher efficiency on two KGC tasks with OOKG entities.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 07:12:18 GMT'}]",2020-09-29,"[['Dai', 'Damai', ''], ['Zheng', 'Hua', ''], ['Luo', 'Fuli', ''], ['Yang', 'Pengcheng', ''], ['Chang', 'Baobao', ''], ['Sui', 'Zhifang', '']]"
1258967,2003.08612,Chenguang Zhu,"Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael
  Zeng, Xuedong Huang, Meng Jiang","Boosting Factual Correctness of Abstractive Summarization with Knowledge
  Graph","15 pages, 3 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A commonly observed problem with abstractive summarization is the distortion
or fabrication of factual information in the article. This inconsistency
between summary and original text has led to various concerns over its
applicability. In this paper, we propose a Fact-Aware Summarization model,
FASum, which extracts factual relations from the article to build a knowledge
graph and integrates it into the neural decoding process. Then, we propose a
Factual Corrector model, FC, that can modify abstractive summaries generated by
any summarization model to improve factual correctness. Empirical results show
that FASum can generate summaries with higher factual correctness compared with
state-of-the-art abstractive summarization systems. And FC improves the factual
correctness of summaries generated by various models via only modifying several
entity tokens.
","[{'version': 'v1', 'created': 'Thu, 19 Mar 2020 07:36:10 GMT'}, {'version': 'v2', 'created': 'Thu, 26 Mar 2020 04:49:16 GMT'}, {'version': 'v3', 'created': 'Sat, 4 Apr 2020 06:30:18 GMT'}, {'version': 'v4', 'created': 'Sat, 25 Apr 2020 19:55:10 GMT'}, {'version': 'v5', 'created': 'Sat, 26 Sep 2020 02:55:31 GMT'}]",2020-09-29,"[['Zhu', 'Chenguang', ''], ['Hinthorn', 'William', ''], ['Xu', 'Ruochen', ''], ['Zeng', 'Qingkai', ''], ['Zeng', 'Michael', ''], ['Huang', 'Xuedong', ''], ['Jiang', 'Meng', '']]"
1354259,2009.12756,Barlas Oguz,"Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei Du, Patrick
  Lewis, William Yang Wang, Yashar Mehdad, Wen-tau Yih, Sebastian Riedel, Douwe
  Kiela, Barlas O\u{g}uz",Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a simple and efficient multi-hop dense retrieval approach for
answering complex open-domain questions, which achieves state-of-the-art
performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER.
Contrary to previous work, our method does not require access to any
corpus-specific information, such as inter-document hyperlinks or
human-annotated entity markers, and can be applied to any unstructured text
corpus. Our system also yields a much better efficiency-accuracy trade-off,
matching the best published accuracy on HotpotQA while being 10 times faster at
inference time.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 06:12:29 GMT'}]",2020-09-29,"[['Xiong', 'Wenhan', ''], ['Li', 'Xiang Lorraine', ''], ['Iyer', 'Srini', ''], ['Du', 'Jingfei', ''], ['Lewis', 'Patrick', ''], ['Wang', 'William Yang', ''], ['Mehdad', 'Yashar', ''], ['Yih', 'Wen-tau', ''], ['Riedel', 'Sebastian', ''], ['Kiela', 'Douwe', ''], ['Oğuz', 'Barlas', '']]"
1354238,2009.12735,Hainan Zhang,"Hainan Zhang, Yanyan Lan, Liang Pang, Hongshen Chen, Zhuoye Ding and
  Dawei Yin",Modeling Topical Relevance for Multi-Turn Dialogue Generation,,"the 29th International Joint Conference on Artificial
  Intelligence(IJCAI 2020)",,,cs.CL cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Topic drift is a common phenomenon in multi-turn dialogue. Therefore, an
ideal dialogue generation models should be able to capture the topic
information of each context, detect the relevant context, and produce
appropriate responses accordingly. However, existing models usually use word or
sentence level similarities to detect the relevant contexts, which fail to well
capture the topical level relevance. In this paper, we propose a new model,
named STAR-BTM, to tackle this problem. Firstly, the Biterm Topic Model is
pre-trained on the whole training dataset. Then, the topic level attention
weights are computed based on the topic representation of each context.
Finally, the attention weights and the topic distribution are utilized in the
decoding process to generate the corresponding responses. Experimental results
on both Chinese customer services data and English Ubuntu dialogue data show
that STAR-BTM significantly outperforms several state-of-the-art methods, in
terms of both metric-based and human evaluations.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 03:33:22 GMT'}]",2020-09-29,"[['Zhang', 'Hainan', ''], ['Lan', 'Yanyan', ''], ['Pang', 'Liang', ''], ['Chen', 'Hongshen', ''], ['Ding', 'Zhuoye', ''], ['Yin', 'Dawei', '']]"
1354230,2009.12727,Shivangi Mahto,"Shivangi Mahto, Vy A. Vo, Javier S. Turek, Alexander G. Huth",Multi-timescale representation learning in LSTM Language Models,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although neural language models are effective at capturing statistics of
natural language, their representations are challenging to interpret. In
particular, it is unclear how these models retain information over multiple
timescales. In this work, we construct explicitly multi-timescale language
models by manipulating the input and forget gate biases in a long short-term
memory (LSTM) network. The distribution of timescales is selected to
approximate power law statistics of natural language through a combination of
exponentially decaying memory cells. We then empirically analyze the timescale
of information routed through each part of the model using word ablation
experiments and forget gate visualizations. These experiments show that the
multi-timescale model successfully learns representations at the desired
timescales, and that the distribution includes longer timescales than a
standard LSTM. Further, information about high-,mid-, and low-frequency words
is routed preferentially through units with the appropriate timescales. Thus we
show how to construct language models with interpretable representations of
different information timescales.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 02:13:38 GMT'}]",2020-09-29,"[['Mahto', 'Shivangi', ''], ['Vo', 'Vy A.', ''], ['Turek', 'Javier S.', ''], ['Huth', 'Alexander G.', '']]"
1354222,2009.12719,Yinhe Zheng Dr.,"Yinhe Zheng, Zikai Chen, Rongsheng Zhang, Shilei Huang, Xiaoxi Mao,
  Minlie Huang",Stylized Dialogue Response Generation Using Stylized Unpaired Texts,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating stylized responses is essential to build intelligent and engaging
dialogue systems. However, this task is far from well-explored due to the
difficulties of rendering a particular style in coherent responses, especially
when the target style is embedded only in unpaired texts that cannot be
directly used to train the dialogue model. This paper proposes a stylized
dialogue generation method that can capture stylistic features embedded in
unpaired texts. Specifically, our method can produce dialogue responses that
are both coherent to the given context and conform to the target style. In this
study, an inverse dialogue model is first introduced to predict possible posts
for the input responses, and then this inverse model is used to generate
stylized pseudo dialogue pairs based on these stylized unpaired texts. Further,
these pseudo pairs are employed to train the stylized dialogue model with a
joint training process, and a style routing approach is proposed to intensify
stylistic features in the decoder. Automatic and manual evaluations on two
datasets demonstrate that our method outperforms competitive baselines in
producing coherent and style-intensive dialogue responses.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 01:04:06 GMT'}]",2020-09-29,"[['Zheng', 'Yinhe', ''], ['Chen', 'Zikai', ''], ['Zhang', 'Rongsheng', ''], ['Huang', 'Shilei', ''], ['Mao', 'Xiaoxi', ''], ['Huang', 'Minlie', '']]"
1353485,2009.11982,Ana Valeria Gonzalez,"Ana Valeria Gonzalez, Maria Barrett, Rasmus Hvingelby, Kellie Webster,
  Anders S{\o}gaard","Type B Reflexivization as an Unambiguous Testbed for Multilingual
  Multi-Task Gender Bias",To appear in EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The one-sided focus on English in previous studies of gender bias in NLP
misses out on opportunities in other languages: English challenge datasets such
as GAP and WinoGender highlight model preferences that are ""hallucinatory"",
e.g., disambiguating gender-ambiguous occurrences of 'doctor' as male doctors.
We show that for languages with type B reflexivization, e.g., Swedish and
Russian, we can construct multi-task challenge datasets for detecting gender
bias that lead to unambiguously wrong model predictions: In these languages,
the direct translation of 'the doctor removed his mask' is not ambiguous
between a coreferential reading and a disjoint reading. Instead, the
coreferential reading requires a non-gendered pronoun, and the gendered,
possessive pronouns are anti-reflexive. We present a multilingual, multi-task
challenge dataset, which spans four languages and four NLP tasks and focuses
only on this phenomenon. We find evidence for gender bias across all
task-language combinations and correlate model bias with national labor market
statistics.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 23:47:18 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Sep 2020 05:12:48 GMT'}]",2020-09-29,"[['Gonzalez', 'Ana Valeria', ''], ['Barrett', 'Maria', ''], ['Hvingelby', 'Rasmus', ''], ['Webster', 'Kellie', ''], ['Søgaard', 'Anders', '']]"
1354214,2009.12711,Gasper Begus,Ga\v{s}per Begu\v{s},"Local and non-local dependency learning and emergence of rule-like
  representations in speech data by Deep Convolutional Generative Adversarial
  Networks",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper argues that training GANs on local and non-local dependencies in
speech data offers insights into how deep neural networks discretize continuous
data and how symbolic-like rule-based morphophonological processes emerge in a
deep convolutional architecture. Acquisition of speech has recently been
modeled as a dependency between latent space and data generated by GANs in
Begu\v{s} (arXiv:2006.03965), who models learning of a simple local allophonic
distribution. We extend this approach to test learning of local and non-local
phonological processes that include approximations of morphological processes.
We further parallel outputs of the model to results of a behavioral experiment
where human subjects are trained on the data used for training the GAN network.
Four main conclusions emerge: (i) the networks provide useful information for
computational models of language acquisition even if trained on a comparatively
small dataset of an artificial grammar learning experiment; (ii) local
processes are easier to learn than non-local processes, which matches both
behavioral data in human subjects and typology in the world's languages. This
paper also proposes (iii) how we can actively observe the network's progress in
learning and explore the effect of training steps on learning representations
by keeping latent space constant across different training steps. Finally, this
paper shows that (iv) the network learns to encode the presence of a prefix
with a single latent variable; by interpolating this variable, we can actively
observe the operation of a non-local phonological process. The proposed
technique for retrieving learning representations has general implications for
our understanding of how GANs discretize continuous speech data and suggests
that rule-like generalizations in the training data are represented as an
interaction between variables in the network's latent space.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 00:02:34 GMT'}]",2020-09-29,"[['Beguš', 'Gašper', '']]"
1354020,2009.12517,Dai Quoc Nguyen,Dai Quoc Nguyen and Thanh Vu and Tu Dinh Nguyen and Dinh Phung,QuatRE: Relation-Aware Quaternions for Knowledge Graph Embeddings,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a simple and effective embedding model, named QuatRE, to learn
quaternion embeddings for entities and relations in knowledge graphs. QuatRE
aims to enhance correlations between head and tail entities given a relation
within the Quaternion space with Hamilton product. QuatRE achieves this by
associating each relation with two quaternion vectors which are used to rotate
the quaternion embeddings of the head and tail entities, respectively. To
obtain the triple score, QuatRE rotates the rotated embedding of the head
entity using the normalized quaternion embedding of the relation, followed by a
quaternion-inner product with the rotated embedding of the tail entity.
Experimental results show that our QuatRE outperforms up-to-date embedding
models on well-known benchmark datasets for knowledge graph completion.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 04:44:25 GMT'}]",2020-09-29,"[['Nguyen', 'Dai Quoc', ''], ['Vu', 'Thanh', ''], ['Nguyen', 'Tu Dinh', ''], ['Phung', 'Dinh', '']]"
1353508,2009.12005,Zhaojiang Lin,"Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Pascale Fung",MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems,EMNLP 2020 camera ready,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify
the system design process of task-oriented dialogue systems and alleviate the
over-dependency on annotated data. MinTL is a simple yet effective transfer
learning framework, which allows us to plug-and-play pre-trained seq2seq
models, and jointly learn dialogue state tracking and dialogue response
generation. Unlike previous approaches, which use a copy mechanism to
""carryover"" the old dialogue states to the new one, we introduce Levenshtein
belief spans (Lev), that allows efficient dialogue state tracking with a
minimal generation length. We instantiate our learning framework with two
pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive
experiments demonstrate that: 1) our systems establish new state-of-the-art
results on end-to-end response generation, 2) MinTL-based systems are more
robust than baseline methods in the low resource setting, and they achieve
competitive results with only 20\% training data, and 3) Lev greatly improves
the inference efficiency.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 02:19:13 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Sep 2020 06:43:17 GMT'}]",2020-09-29,"[['Lin', 'Zhaojiang', ''], ['Madotto', 'Andrea', ''], ['Winata', 'Genta Indra', ''], ['Fung', 'Pascale', '']]"
1354205,2009.12702,Konstantinos Kogkalidis,"Konstantinos Kogkalidis, Michael Moortgat, Richard Moot",Neural Proof Nets,"14 pages, CoNLL2020",,,,cs.CL cs.LG cs.LO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Linear logic and the linear {\lambda}-calculus have a long standing tradition
in the study of natural language form and meaning. Among the proof calculi of
linear logic, proof nets are of particular interest, offering an attractive
geometric representation of derivations that is unburdened by the bureaucratic
complications of conventional prooftheoretic formats. Building on recent
advances in set-theoretic learning, we propose a neural variant of proof nets
based on Sinkhorn networks, which allows us to translate parsing as the problem
of extracting syntactic primitives and permuting them into alignment. Our
methodology induces a batch-efficient, end-to-end differentiable architecture
that actualizes a formally grounded yet highly efficient neuro-symbolic parser.
We test our approach on {\AE}Thel, a dataset of type-logical derivations for
written Dutch, where it manages to correctly transcribe raw text sentences into
proofs and terms of the linear {\lambda}-calculus with an accuracy of as high
as 70%.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 22:48:47 GMT'}]",2020-09-29,"[['Kogkalidis', 'Konstantinos', ''], ['Moortgat', 'Michael', ''], ['Moot', 'Richard', '']]"
1354198,2009.12695,Tabish Maniar,"Chejui Liao, Tabish Maniar, Sravanajyothi N and Anantha Sharma","Techniques to Improve Q&A Accuracy with Transformer-based models on
  Large Complex Documents",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper discusses the effectiveness of various text processing techniques,
their combinations, and encodings to achieve a reduction of complexity and size
in a given text corpus. The simplified text corpus is sent to BERT (or similar
transformer based models) for question and answering and can produce more
relevant responses to user queries. This paper takes a scientific approach to
determine the benefits and effectiveness of various techniques and concludes a
best-fit combination that produces a statistically significant improvement in
accuracy.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 21:56:22 GMT'}]",2020-09-29,"[['Liao', 'Chejui', ''], ['Maniar', 'Tabish', ''], ['N', 'Sravanajyothi', ''], ['Sharma', 'Anantha', '']]"
1354186,2009.12683,Hoda Eldardiry,"Chenhan Yuan, Ryan Rossi, Andrew Katz, and Hoda Eldardiry",Reinforcement Learning-based N-ary Cross-Sentence Relation Extraction,"10 pages, 3 figures, submitted to AAAI",,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The models of n-ary cross sentence relation extraction based on distant
supervision assume that consecutive sentences mentioning n entities describe
the relation of these n entities. However, on one hand, this assumption
introduces noisy labeled data and harms the models' performance. On the other
hand, some non-consecutive sentences also describe one relation and these
sentences cannot be labeled under this assumption. In this paper, we relax this
strong assumption by a weaker distant supervision assumption to address the
second issue and propose a novel sentence distribution estimator model to
address the first problem. This estimator selects correctly labeled sentences
to alleviate the effect of noisy data is a two-level agent reinforcement
learning model. In addition, a novel universal relation extractor with a hybrid
approach of attention mechanism and PCNN is proposed such that it can be
deployed in any tasks, including consecutive and nonconsecutive sentences.
Experiments demonstrate that the proposed model can reduce the impact of noisy
data and achieve better performance on general n-ary cross sentence relation
extraction task compared to baseline models.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 20:39:55 GMT'}]",2020-09-29,"[['Yuan', 'Chenhan', ''], ['Rossi', 'Ryan', ''], ['Katz', 'Andrew', ''], ['Eldardiry', 'Hoda', '']]"
1354184,2009.12681,Hoda Eldardiry,"Chenhan Yuan, Ryan Rossi, Andrew Katz, and Hoda Eldardiry",Clustering-based Unsupervised Generative Relation Extraction,"11 pages, 5 figures",,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper focuses on the problem of unsupervised relation extraction.
Existing probabilistic generative model-based relation extraction methods work
by extracting sentence features and using these features as inputs to train a
generative model. This model is then used to cluster similar relations.
However, these methods do not consider correlations between sentences with the
same entity pair during training, which can negatively impact model
performance. To address this issue, we propose a Clustering-based Unsupervised
generative Relation Extraction (CURE) framework that leverages an
""Encoder-Decoder"" architecture to perform self-supervised learning so the
encoder can extract relation information. Given multiple sentences with the
same entity pair as inputs, self-supervised learning is deployed by predicting
the shortest path between entity pairs on the dependency graph of one of the
sentences. After that, we extract the relation information using the
well-trained encoder. Then, entity pairs that share the same relation are
clustered based on their corresponding relation information. Each cluster is
labeled with a few words based on the words in the shortest paths corresponding
to the entity pairs in each cluster. These cluster labels also describe the
meaning of these relation clusters. We compare the triplets extracted by our
proposed framework (CURE) and baseline methods with a ground-truth Knowledge
Base. Experimental results show that our model performs better than
state-of-the-art models on both New York Times (NYT) and United Nations
Parallel Corpus (UNPC) standard datasets.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 20:36:40 GMT'}]",2020-09-29,"[['Yuan', 'Chenhan', ''], ['Rossi', 'Ryan', ''], ['Katz', 'Andrew', ''], ['Eldardiry', 'Hoda', '']]"
1354180,2009.12677,Ye Liu,"Ye Liu, Yao Wan, Lifang He, Hao Peng, Philip S. Yu","KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense
  Reasoning","9 pages, 7 figures",,,,cs.CL cs.SC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative commonsense reasoning which aims to empower machines to generate
sentences with the capacity of reasoning over a set of concepts is a critical
bottleneck for text generation. Even the state-of-the-art pre-trained language
generation models struggle at this task and often produce implausible and
anomalous sentences. One reason is that they rarely consider incorporating the
knowledge graph which can provide rich relational information among the
commonsense concepts. To promote the ability of commonsense reasoning for text
generation, we propose a novel knowledge graphaugmented pre-trained language
generation model KG-BART, which encompasses the complex relations of concepts
through the knowledge graph and produces more logical and natural sentences as
output. Moreover, KG-BART can leverage the graph attention to aggregate the
rich concept semantics that enhances the model generalization on unseen concept
sets. Experiments on benchmark CommonGen dataset verify the effectiveness of
our proposed approach by comparing with several strong pre-trained language
generation models, particularly KG-BART outperforms BART by 15.98%, 17.49%, in
terms of BLEU-3, 4. Moreover, we also show that the generated context by our
model can work as background scenarios to benefit downstream commonsense QA
tasks.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 19:57:49 GMT'}]",2020-09-29,"[['Liu', 'Ye', ''], ['Wan', 'Yao', ''], ['He', 'Lifang', ''], ['Peng', 'Hao', ''], ['Yu', 'Philip S.', '']]"
1354042,2009.12539,Yi Xu,"Yi Xu, Hai Zhao, Zhuosheng Zhang",Topic-Aware Multi-turn Dialogue Modeling,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the retrieval-based multi-turn dialogue modeling, it remains a challenge
to select the most appropriate response according to extracting salient
features in context utterances. As a conversation goes on, topic shift at
discourse-level naturally happens through the continuous multi-turn dialogue
context. However, all known retrieval-based systems are satisfied with
exploiting local topic words for context utterance representation but fail to
capture such essential global topic-aware clues at discourse-level. Instead of
taking topic-agnostic n-gram utterance as processing unit for matching purpose
in existing systems, this paper presents a novel topic-aware solution for
multi-turn dialogue modeling, which segments and extracts topic-aware
utterances in an unsupervised way, so that the resulted model is capable of
capturing salient topic shift at discourse-level in need and thus effectively
track topic flow during multi-turn conversation. Our topic-aware modeling is
implemented by a newly proposed unsupervised topic-aware segmentation algorithm
and Topic-Aware Dual-attention Matching (TADAM) Network, which matches each
topic segment with the response in a dual cross-attention way. Experimental
results on three public datasets show TADAM can outperform the state-of-the-art
method by a large margin, especially by 3.4% on E-commerce dataset that has an
obvious topic shift.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 08:43:06 GMT'}]",2020-09-29,"[['Xu', 'Yi', ''], ['Zhao', 'Hai', ''], ['Zhang', 'Zhuosheng', '']]"
1354068,2009.12565,Shashwat Aggarwal,"Shashwat Aggarwal, Ramesh Singh",Metaphor Detection using Deep Contextualized Word Embeddings,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Metaphors are ubiquitous in natural language, and their detection plays an
essential role in many natural language processing tasks, such as language
understanding, sentiment analysis, etc. Most existing approaches for metaphor
detection rely on complex, hand-crafted and fine-tuned feature pipelines, which
greatly limit their applicability. In this work, we present an end-to-end
method composed of deep contextualized word embeddings, bidirectional LSTMs and
multi-head attention mechanism to address the task of automatic metaphor
detection. Our method, unlike many other existing approaches, requires only the
raw text sequences as input features to detect the metaphoricity of a phrase.
We compare the performance of our method against the existing baselines on two
benchmark datasets, TroFi, and MOH-X respectively. Experimental evaluations
confirm the effectiveness of our approach.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 11:00:35 GMT'}]",2020-09-29,"[['Aggarwal', 'Shashwat', ''], ['Singh', 'Ramesh', '']]"
1354129,2009.12626,Klim Zaporojets,"Klim Zaporojets, Johannes Deleu, Chris Develder, Thomas Demeester","DWIE: an entity-centric dataset for multi-task document-level
  information extraction",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents DWIE, the 'Deutsche Welle corpus for Information
Extraction', a newly created multi-task dataset that combines four main
Information Extraction (IE) annotation sub-tasks: (i) Named Entity Recognition
(NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv)
Entity Linking. DWIE is conceived as an entity-centric dataset that describes
interactions and properties of conceptual entities on the level of the complete
document. This contrasts with currently dominant mention-driven approaches that
start from the detection and classification of named entity mentions in
individual sentences. Further, DWIE presented two main challenges when building
and evaluating IE models for it. First, the use of traditional mention-level
evaluation metrics for NER and RE tasks on entity-centric DWIE dataset can
result in measurements dominated by predictions on more frequently mentioned
entities. We tackle this issue by proposing a new entity-driven metric that
takes into account the number of mentions that compose each of the predicted
and ground truth entities. Second, the document-level multi-task annotations
require the models to transfer information between entity mentions located in
different parts of the document, as well as between different tasks, in a joint
learning setting. To realize this, we propose to use graph-based neural message
passing techniques between document-level mention spans. Our experiments show
an improvement of up to 5.5 F1 percentage points when incorporating neural
graph propagation into our joint model. This demonstrates DWIE's potential to
stimulate further research in graph neural networks for representation learning
in multi-task IE. We make DWIE publicly available at
https://github.com/klimzaporojets/DWIE.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 15:53:22 GMT'}]",2020-09-29,"[['Zaporojets', 'Klim', ''], ['Deleu', 'Johannes', ''], ['Develder', 'Chris', ''], ['Demeester', 'Thomas', '']]"
1354125,2009.12622,Maha Jarallah Althobaiti,Maha J. Althobaiti,"Automatic Arabic Dialect Identification Systems for Written Texts: A
  Survey",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Arabic dialect identification is a specific task of natural language
processing, aiming to automatically predict the Arabic dialect of a given text.
Arabic dialect identification is the first step in various natural language
processing applications such as machine translation, multilingual
text-to-speech synthesis, and cross-language text generation. Therefore, in the
last decade, interest has increased in addressing the problem of Arabic dialect
identification. In this paper, we present a comprehensive survey of Arabic
dialect identification research in written texts. We first define the problem
and its challenges. Then, the survey extensively discusses in a critical manner
many aspects related to Arabic dialect identification task. So, we review the
traditional machine learning methods, deep learning architectures, and complex
learning approaches to Arabic dialect identification. We also detail the
features and techniques for feature representations used to train the proposed
systems. Moreover, we illustrate the taxonomy of Arabic dialects studied in the
literature, the various levels of text processing at which Arabic dialect
identification are conducted (e.g., token, sentence, and document level), as
well as the available annotated resources, including evaluation benchmark
corpora. Open challenges and issues are discussed at the end of the survey.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 15:33:16 GMT'}]",2020-09-29,"[['Althobaiti', 'Maha J.', '']]"
1354273,2009.12770,Deepak Gupta,"Deepak Gupta, Swati Suman, Asif Ekbal","Hierarchical Deep Multi-modal Network for Medical Visual Question
  Answering",Accepted for publication at Expert Systems with Applications,,,,cs.CL cs.CV cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Visual Question Answering in Medical domain (VQA-Med) plays an important role
in providing medical assistance to the end-users. These users are expected to
raise either a straightforward question with a Yes/No answer or a challenging
question that requires a detailed and descriptive answer. The existing
techniques in VQA-Med fail to distinguish between the different question types
sometimes complicates the simpler problems, or over-simplifies the complicated
ones. It is certainly true that for different question types, several distinct
systems can lead to confusion and discomfort for the end-users. To address this
issue, we propose a hierarchical deep multi-modal network that analyzes and
classifies end-user questions/queries and then incorporates a query-specific
approach for answer prediction. We refer our proposed approach as Hierarchical
Question Segregation based Visual Question Answering, in short HQS-VQA. Our
contributions are three-fold, viz. firstly, we propose a question segregation
(QS) technique for VQAMed; secondly, we integrate the QS model to the
hierarchical deep multi-modal neural network to generate proper answers to the
queries related to medical images; and thirdly, we study the impact of QS in
Medical-VQA by comparing the performance of the proposed model with QS and a
model without QS. We evaluate the performance of our proposed model on two
benchmark datasets, viz. RAD and CLEF18. Experimental results show that our
proposed HQS-VQA technique outperforms the baseline models with significant
margins. We also conduct a detailed quantitative and qualitative analysis of
the obtained results and discover potential causes of errors and their
solutions.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 07:24:41 GMT'}]",2020-09-29,"[['Gupta', 'Deepak', ''], ['Suman', 'Swati', ''], ['Ekbal', 'Asif', '']]"
1353924,2009.12421,Victor Prokhorov,"Victor Prokhorov, Yingzhen Li, Ehsan Shareghi, Nigel Collier",Hierarchical Sparse Variational Autoencoder for Text Encoding,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we focus on unsupervised representation learning and propose a
novel framework, Hierarchical Sparse Variational Autoencoder (HSVAE), that
imposes sparsity on sentence representations via direct optimisation of
Evidence Lower Bound (ELBO). Our experimental results illustrate that HSVAE is
flexible and adapts nicely to the underlying characteristics of the corpus
which is reflected by the level of sparsity and its distributional patterns.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 20:08:32 GMT'}]",2020-09-29,"[['Prokhorov', 'Victor', ''], ['Li', 'Yingzhen', ''], ['Shareghi', 'Ehsan', ''], ['Collier', 'Nigel', '']]"
1354455,2009.12952,Trapit Bansal,"Vaishnavi Kommaraju, Karthick Gunasekaran, Kun Li, Trapit Bansal,
  Andrew McCallum, Ivana Williams, Ana-Maria Istrate",Unsupervised Pre-training for Biomedical Question Answering,To appear in BioASQ workshop 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We explore the suitability of unsupervised representation learning methods on
biomedical text -- BioBERT, SciBERT, and BioSentVec -- for biomedical question
answering. To further improve unsupervised representations for biomedical QA,
we introduce a new pre-training task from unlabeled data designed to reason
about biomedical entities in the context. Our pre-training method consists of
corrupting a given context by randomly replacing some mention of a biomedical
entity with a random entity mention and then querying the model with the
correct entity mention in order to locate the corrupted part of the context.
This de-noising task enables the model to learn good representations from
abundant, unlabeled biomedical text that helps QA tasks and minimizes the
train-test mismatch between the pre-training task and the downstream QA tasks
by requiring the model to predict spans. Our experiments show that pre-training
BioBERT on the proposed pre-training task significantly boosts performance and
outperforms the previous best model from the 7th BioASQ Task 7b-Phase B
challenge.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 21:07:51 GMT'}]",2020-09-29,"[['Kommaraju', 'Vaishnavi', ''], ['Gunasekaran', 'Karthick', ''], ['Li', 'Kun', ''], ['Bansal', 'Trapit', ''], ['McCallum', 'Andrew', ''], ['Williams', 'Ivana', ''], ['Istrate', 'Ana-Maria', '']]"
1354516,2009.13013,Tiancheng Zhao,"Tiancheng Zhao, Xiaopeng Lu, Kyusong Lee","SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer
  Matching Retrieval",11 pages,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce SPARTA, a novel neural retrieval method that shows great promise
in performance, generalization, and interpretability for open-domain question
answering. Unlike many neural ranking methods that use dense vector nearest
neighbor search, SPARTA learns a sparse representation that can be efficiently
implemented as an Inverted Index. The resulting representation enables scalable
neural retrieval that does not require expensive approximate vector search and
leads to better performance than its dense counterpart. We validated our
approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval
question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results
across a variety of open-domain question answering tasks in both English and
Chinese datasets, including open SQuAD, Natuarl Question, CMRC and etc.
Analysis also confirms that the proposed method creates human interpretable
representation and allows flexible control over the trade-off between
performance and efficiency.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 02:11:02 GMT'}]",2020-09-29,"[['Zhao', 'Tiancheng', ''], ['Lu', 'Xiaopeng', ''], ['Lee', 'Kyusong', '']]"
1353119,2009.11616,Libo Qin,"Wanxiang Che, Yunlong Feng, Libo Qin, Ting Liu","N-LTP: A Open-source Neural Chinese Language Technology Platform with
  Pretrained Models","Work in progress. Code is available at
  [GitHub](https://github.com/HIT-SCIR/ltp)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce N-LTP, an open-source Python Chinese natural language processing
toolkit supporting five basic tasks: Chinese word segmentation, part-of-speech
tagging, named entity recognition, dependency parsing, and semantic dependency
parsing. N-LTP adopts the multi-task framework with the pre-trained model to
capture the shared knowledge across all Chinese relevant tasks. In addition, we
propose to use knowledge distillation where single-task models teach a
multi-task model, helping the multi-task model surpass its single-task
teachers. Finally, we provide fundamental tasks API and a visualization tool to
make users easier to use and view the processing results directly. To the best
of our knowledge, this is the first toolkit to support all Chinese NLP
fundamental tasks. Source code, documentation, and pre-trained models are
available at https://github.com/HIT-SCIR/ltp.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 11:45:39 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Sep 2020 14:20:33 GMT'}]",2020-09-29,"[['Che', 'Wanxiang', ''], ['Feng', 'Yunlong', ''], ['Qin', 'Libo', ''], ['Liu', 'Ting', '']]"
1354785,2009.13282,Liang Zhao,"Liang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang, Hongxia Yang, Xu
  Sun",Graph-based Multi-hop Reasoning for Long Text Generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Long text generation is an important but challenging task.The main problem
lies in learning sentence-level semantic dependencies which traditional
generative models often suffer from. To address this problem, we propose a
Multi-hop Reasoning Generation (MRG) approach that incorporates multi-hop
reasoning over a knowledge graph to learn semantic dependencies among
sentences. MRG consists of twoparts, a graph-based multi-hop reasoning module
and a path-aware sentence realization module. The reasoning module is
responsible for searching skeleton paths from a knowledge graph to imitate the
imagination process in the human writing for semantic transfer. Based on the
inferred paths, the sentence realization module then generates a complete
sentence. Unlike previous black-box models, MRG explicitly infers the skeleton
path, which provides explanatory views tounderstand how the proposed model
works. We conduct experiments on three representative tasks, including story
generation, review generation, and product description generation. Automatic
and manual evaluation show that our proposed method can generate more
informative and coherentlong text than strong baselines, such as pre-trained
models(e.g. GPT-2) and knowledge-enhanced models.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 12:47:59 GMT'}]",2020-09-29,"[['Zhao', 'Liang', ''], ['Xu', 'Jingjing', ''], ['Lin', 'Junyang', ''], ['Zhang', 'Yichang', ''], ['Yang', 'Hongxia', ''], ['Sun', 'Xu', '']]"
1352183,2009.10680,Wei Zhu,"Wei Zhu, Xipeng Qiu, Yuan Ni and Guotong Xie","AutoRC: Improving BERT Based Relation Classification Models via
  Architecture Search",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although BERT based relation classification (RC) models have achieved
significant improvements over the traditional deep learning models, it seems
that no consensus can be reached on what is the optimal architecture. Firstly,
there are multiple alternatives for entity span identification. Second, there
are a collection of pooling operations to aggregate the representations of
entities and contexts into fixed length vectors. Third, it is difficult to
manually decide which feature vectors, including their interactions, are
beneficial for classifying the relation types. In this work, we design a
comprehensive search space for BERT based RC models and employ neural
architecture search (NAS) method to automatically discover the design choices
mentioned above. Experiments on seven benchmark RC tasks show that our method
is efficient and effective in finding better architectures than the baseline
BERT based RC model. Ablation study demonstrates the necessity of our search
space design and the effectiveness of our search method.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 16:55:49 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Sep 2020 02:37:03 GMT'}]",2020-09-29,"[['Zhu', 'Wei', ''], ['Qiu', 'Xipeng', ''], ['Ni', 'Yuan', ''], ['Xie', 'Guotong', '']]"
1354778,2009.13275,Edgar Altszyler,"Edgar Altszyler, Pablo Brusco, Nikoletta Basiou, John Byrnes and
  Dimitra Vergyri",Zero-shot Multi-Domain Dialog State Tracking Using Descriptive Rules,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we present a framework for incorporating descriptive logical
rules in state-of-the-art neural networks, enabling them to learn how to handle
unseen labels without the introduction of any new training data. The rules are
integrated into existing networks without modifying their architecture, through
an additional term in the network's loss function that penalizes states of the
network that do not obey the designed rules. As a case of study, the framework
is applied to an existing neural-based Dialog State Tracker. Our experiments
demonstrate that the inclusion of logical rules allows the prediction of unseen
labels, without deteriorating the predictive capacity of the original system.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 18:14:25 GMT'}]",2020-09-29,"[['Altszyler', 'Edgar', ''], ['Brusco', 'Pablo', ''], ['Basiou', 'Nikoletta', ''], ['Byrnes', 'John', ''], ['Vergyri', 'Dimitra', '']]"
1353934,2009.12431,Vivian Silva,"Vivian S. Silva, Andr\'e Freitas, Siegfried Handschuh",XTE: Explainable Text Entailment,"44 pages, 7 figures. Submitted to the Artificial Intelligence Journal",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text entailment, the task of determining whether a piece of text logically
follows from another piece of text, is a key component in NLP, providing input
for many semantic applications such as question answering, text summarization,
information extraction, and machine translation, among others. Entailment
scenarios can range from a simple syntactic variation to more complex semantic
relationships between pieces of text, but most approaches try a
one-size-fits-all solution that usually favors some scenario to the detriment
of another. Furthermore, for entailments requiring world knowledge, most
systems still work as a ""black box"", providing a yes/no answer that does not
explain the underlying reasoning process. In this work, we introduce XTE -
Explainable Text Entailment - a novel composite approach for recognizing text
entailment which analyzes the entailment pair to decide whether it must be
resolved syntactically or semantically. Also, if a semantic matching is
involved, we make the answer interpretable, using external knowledge bases
composed of structured lexical definitions to generate natural language
justifications that explain the semantic relationship holding between the
pieces of text. Besides outperforming well-established entailment algorithms,
our composite approach gives an important step towards Explainable AI, allowing
the inference model interpretation, making the semantic reasoning process
explicit and understandable.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 20:49:07 GMT'}]",2020-09-29,"[['Silva', 'Vivian S.', ''], ['Freitas', 'André', ''], ['Handschuh', 'Siegfried', '']]"
1354775,2009.13272,Ben Athiwaratkun,"Ben Athiwaratkun, Cicero Nogueira dos Santos, Jason Krone, Bing Xiang",Augmented Natural Language for Generative Sequence Labeling,To appear at EMNLP 2020,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a generative framework for joint sequence labeling and
sentence-level classification. Our model performs multiple sequence labeling
tasks at once using a single, shared natural language output space. Unlike
prior discriminative methods, our model naturally incorporates label semantics
and shares knowledge across tasks. Our framework is general purpose, performing
well on few-shot, low-resource, and high-resource tasks. We demonstrate these
advantages on popular named entity recognition, slot labeling, and intent
classification benchmarks. We set a new state-of-the-art for few-shot slot
labeling, improving substantially upon the previous 5-shot ($75.0\% \rightarrow
90.9\%$) and 1-shot ($70.4\% \rightarrow 81.0\%$) state-of-the-art results.
Furthermore, our model generates large improvements ($46.27\% \rightarrow
63.83\%$) in low-resource slot labeling over a BERT baseline by incorporating
label semantics. We also maintain competitive results on high-resource tasks,
performing within two points of the state-of-the-art on all tasks and setting a
new state-of-the-art on the SNIPS dataset.
","[{'version': 'v1', 'created': 'Tue, 15 Sep 2020 19:23:53 GMT'}]",2020-09-29,"[['Athiwaratkun', 'Ben', ''], ['Santos', 'Cicero Nogueira dos', ''], ['Krone', 'Jason', ''], ['Xiang', 'Bing', '']]"
1353955,2009.12452,Jean-Philippe Corbeil,Jean-Philippe Corbeil and Hadi Abdi Ghadivel,"BET: A Backtranslation Approach for Easy Data Augmentation in
  Transformer-based Paraphrase Identification Context",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Newly-introduced deep learning architectures, namely BERT, XLNet, RoBERTa and
ALBERT, have been proved to be robust on several NLP tasks. However, the
datasets trained on these architectures are fixed in terms of size and
generalizability. To relieve this issue, we apply one of the most inexpensive
solutions to update these datasets. We call this approach BET by which we
analyze the backtranslation data augmentation on the transformer-based
architectures. Using the Google Translate API with ten intermediary languages
from ten different language families, we externally evaluate the results in the
context of automatic paraphrase identification in a transformer-based
framework. Our findings suggest that BET improves the paraphrase identification
performance on the Microsoft Research Paraphrase Corpus (MRPC) to more than 3%
on both accuracy and F1 score. We also analyze the augmentation in the low-data
regime with downsampled versions of MRPC, Twitter Paraphrase Corpus (TPC) and
Quora Question Pairs. In many low-data cases, we observe a switch from a
failing model on the test set to reasonable performances. The results
demonstrate that BET is a highly promising data augmentation technique: to push
the current state-of-the-art of existing datasets and to bootstrap the
utilization of deep learning architectures in the low-data regime of a hundred
samples.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 22:06:06 GMT'}]",2020-09-29,"[['Corbeil', 'Jean-Philippe', ''], ['Ghadivel', 'Hadi Abdi', '']]"
1354755,2009.13252,Xueping Peng,"Xueping Peng, Guodong Long, Tao Shen, Sen Wang, Jing Jiang, Chengqi
  Zhang","BiteNet: Bidirectional Temporal Encoder Network to Predict Medical
  Outcomes","10 pages, 8 figures, accepted by IEEE ICDM 2020. arXiv admin note:
  substantial text overlap with arXiv:2006.10516",,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Electronic health records (EHRs) are longitudinal records of a patient's
interactions with healthcare systems. A patient's EHR data is organized as a
three-level hierarchy from top to bottom: patient journey - all the experiences
of diagnoses and treatments over a period of time; individual visit - a set of
medical codes in a particular visit; and medical code - a specific record in
the form of medical codes. As EHRs begin to amass in millions, the potential
benefits, which these data might hold for medical research and medical outcome
prediction, are staggering - including, for example, predicting future
admissions to hospitals, diagnosing illnesses or determining the efficacy of
medical treatments. Each of these analytics tasks requires a domain knowledge
extraction method to transform the hierarchical patient journey into a vector
representation for further prediction procedure. The representations should
embed a sequence of visits and a set of medical codes with a specific
timestamp, which are crucial to any downstream prediction tasks. Hence,
expressively powerful representations are appealing to boost learning
performance. To this end, we propose a novel self-attention mechanism that
captures the contextual dependency and temporal relationships within a
patient's healthcare journey. An end-to-end bidirectional temporal encoder
network (BiteNet) then learns representations of the patient's journeys, based
solely on the proposed attention mechanism. We have evaluated the effectiveness
of our methods on two supervised prediction and two unsupervised clustering
tasks with a real-world EHR dataset. The empirical results demonstrate the
proposed BiteNet model produces higher-quality representations than
state-of-the-art baseline methods.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 00:42:36 GMT'}]",2020-09-29,"[['Peng', 'Xueping', ''], ['Long', 'Guodong', ''], ['Shen', 'Tao', ''], ['Wang', 'Sen', ''], ['Jiang', 'Jing', ''], ['Zhang', 'Chengqi', '']]"
1354702,2009.13199,Zhihan Zhang,"Zhihan Zhang, Xiubo Geng, Tao Qin, Yunfang Wu, Daxin Jiang",Knowledge-Aware Procedural Text Understanding with Multi-Stage Training,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We focus on the task of procedural text understanding, which aims to track
entities' states and locations during a natural process. Although recent
approaches have achieved substantial progress, they are far behind human
performance. Two challenges, difficulty of commonsense reasoning and data
insufficiency, still remain unsolved. In this paper, we propose a novel
KnOwledge-Aware proceduraL text understAnding (KOALA) model, which leverages
external knowledge sources to solve these issues. Specifically, we retrieve
informative knowledge triples from ConceptNet and perform knowledge-aware
reasoning while tracking the entities. Besides, we employ a multi-stage
training schema which fine-tunes the BERT model over unlabeled data collected
from Wikipedia before further fine-tuning it on the final model. Experimental
results on two procedural text datasets, ProPara and Recipes, verify the
effectiveness of the proposed methods, in which our model achieves
state-of-the-art performance in comparison to various baselines.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 10:28:40 GMT'}]",2020-09-29,"[['Zhang', 'Zhihan', ''], ['Geng', 'Xiubo', ''], ['Qin', 'Tao', ''], ['Wu', 'Yunfang', ''], ['Jiang', 'Daxin', '']]"
1223869,1912.11973,Muhammad Haroon Shakeel,"Muhammad Haroon Shakeel, Turki Alghamidi, Safi Faizullah, Imdadullah
  Khan",Language Independent Sentiment Analysis,,"International Conference on Advances in the Emerging Computing
  Technologies (AECT), 2020",10.1109/AECT47998.2020.9194186,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Social media platforms and online forums generate rapid and increasing amount
of textual data. Businesses, government agencies, and media organizations seek
to perform sentiment analysis on this rich text data. The results of these
analytics are used for adapting marketing strategies, customizing products,
security and various other decision makings. Sentiment analysis has been
extensively studied and various methods have been developed for it with great
success. These methods, however apply to texts written in a specific language.
This limits applicability to a limited demographic and a specific geographic
region. In this paper we propose a general approach for sentiment analysis on
data containing texts from multiple languages. This enables all the
applications to utilize the results of sentiment analysis in a language
oblivious or language-independent fashion.
","[{'version': 'v1', 'created': 'Fri, 27 Dec 2019 03:20:48 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Jan 2020 12:55:41 GMT'}]",2020-09-29,"[['Shakeel', 'Muhammad Haroon', ''], ['Alghamidi', 'Turki', ''], ['Faizullah', 'Safi', ''], ['Khan', 'Imdadullah', '']]"
1354669,2009.13166,Qian Liu,"Qian Liu, Bei Chen, Jian-Guang Lou, Bin Zhou, Dongmei Zhang",Incomplete Utterance Rewriting as Semantic Segmentation,To appear in EMNLP 2020 (Long Paper),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years the task of incomplete utterance rewriting has raised a large
attention. Previous works usually shape it as a machine translation task and
employ sequence to sequence based architecture with copy mechanism. In this
paper, we present a novel and extensive approach, which formulates it as a
semantic segmentation task. Instead of generating from scratch, such a
formulation introduces edit operations and shapes the problem as prediction of
a word-level edit matrix. Benefiting from being able to capture both local and
global information, our approach achieves state-of-the-art performance on
several public datasets. Furthermore, our approach is four times faster than
the standard approach in inference.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 09:29:49 GMT'}]",2020-09-29,"[['Liu', 'Qian', ''], ['Chen', 'Bei', ''], ['Lou', 'Jian-Guang', ''], ['Zhou', 'Bin', ''], ['Zhang', 'Dongmei', '']]"
1354620,2009.13117,Anh Khoa Ngo Ho,"Anh Khoa Ngo Ho (LIMSI), Fran\c{c}ois Yvon",Generative latent neural models for automatic word alignment,,"The Association for Machine Translation in the Americas, Oct 2020,
  Florida, United States",,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Word alignments identify translational correspondences between words in a
parallel sentence pair and are used, for instance, to learn bilingual
dictionaries, to train statistical machine translation systems or to perform
quality estimation. Variational autoencoders have been recently used in various
of natural language processing to learn in an unsupervised way latent
representations that are useful for language generation tasks. In this paper,
we study these models for the task of word alignment and propose and assess
several evolutions of a vanilla variational autoencoders. We demonstrate that
these techniques can yield competitive results as compared to Giza++ and to a
strong neural network alignment system for two language pairs.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 07:54:09 GMT'}]",2020-09-29,"[['Ho', 'Anh Khoa Ngo', '', 'LIMSI'], ['Yvon', 'François', '']]"
1238182,2002.00759,Son T. Luu,"Son T. Luu, Hung P. Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen","Comparison Between Traditional Machine Learning Models And Neural
  Network Models For Vietnamese Hate Speech Detection","Published in The 2020 RIVF International Conference on Computing and
  Communication Technologies (RIVF)",,10.1109/RIVF48685.2020.9140745,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Hate-speech detection on social network language has become one of the main
researching fields recently due to the spreading of social networks like
Facebook and Twitter. In Vietnam, the threat of offensive and harassment cause
bad impacts for online user. The VLSP - Shared task about Hate Speech Detection
on social networks showed many proposed approaches for detecting whatever
comment is clean or not. However, this problem still needs further researching.
Consequently, we compare traditional machine learning and deep learning on a
large dataset about the user's comments on social network in Vietnamese and
find out what is the advantage and disadvantage of each model by comparing
their accuracy on F1-score, then we pick two models in which has highest
accuracy in traditional machine learning models and deep neural models
respectively. Next, we compare these two models capable of predicting the right
label by referencing their confusion matrices and considering the advantages
and disadvantages of each model. Finally, from the comparison result, we
propose our ensemble method that concentrates the abilities of traditional
methods and deep learning methods.
","[{'version': 'v1', 'created': 'Fri, 31 Jan 2020 09:28:57 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Sep 2020 01:54:32 GMT'}]",2020-09-29,"[['Luu', 'Son T.', ''], ['Nguyen', 'Hung P.', ''], ['Van Nguyen', 'Kiet', ''], ['Nguyen', 'Ngan Luu-Thuy', '']]"
1354619,2009.13116,Anh Khoa Ngo Ho,"Anh Khoa Ngo Ho (LIMSI), Fran\c{c}ois Yvon",Neural Baselines for Word Alignment,"The 16th International Workshop on Spoken Language Translation, Nov
  2019, Hong Kong, Hong Kong SAR China",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Word alignments identify translational correspondences between words in a
parallel sentence pair and is used, for instance, to learn bilingual
dictionaries, to train statistical machine translation systems , or to perform
quality estimation. In most areas of natural language processing, neural
network models nowadays constitute the preferred approach, a situation that
might also apply to word alignment models. In this work, we study and
comprehensively evaluate neural models for unsupervised word alignment for four
language pairs, contrasting several variants of neural models. We show that in
most settings, neural versions of the IBM-1 and hidden Markov models vastly
outperform their discrete counterparts. We also analyze typical alignment
errors of the baselines that our models overcome to illustrate the benefits-and
the limitations-of these new models for morphologically rich languages.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 07:51:03 GMT'}]",2020-09-29,"[['Ho', 'Anh Khoa Ngo', '', 'LIMSI'], ['Yvon', 'François', '']]"
1354584,2009.13081,Di Jin,"Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang and
  Peter Szolovits","What Disease does this Patient Have? A Large-scale Open Domain Question
  Answering Dataset from Medical Exams",Submitted to AAAI 2021,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open domain question answering (OpenQA) tasks have been recently attracting
more and more attention from the natural language processing (NLP) community.
In this work, we present the first free-form multiple-choice OpenQA dataset for
solving medical problems, MedQA, collected from the professional medical board
exams. It covers three languages: English, simplified Chinese, and traditional
Chinese, and contains 12,723, 34,251, and 14,123 questions for the three
languages, respectively. We implement both rule-based and popular neural
methods by sequentially combining a document retriever and a machine
comprehension model. Through experiments, we find that even the current best
method can only achieve 36.7\%, 42.0\%, and 70.1\% of test accuracy on the
English, traditional Chinese, and simplified Chinese questions, respectively.
We expect MedQA to present great challenges to existing OpenQA systems and hope
that it can serve as a platform to promote much stronger OpenQA models from the
NLP community in the future.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 05:07:51 GMT'}]",2020-09-29,"[['Jin', 'Di', ''], ['Pan', 'Eileen', ''], ['Oufattole', 'Nassim', ''], ['Weng', 'Wei-Hung', ''], ['Fang', 'Hanyi', ''], ['Szolovits', 'Peter', '']]"
1354583,2009.13080,Boaz Shmueli,"Boaz Shmueli, Lun-Wei Ku, Soumya Ray",Reactive Supervision: A New Method for Collecting Sarcasm Data,"7 pages, 2 figures, 8 tables. To be published in Proceedings of the
  2020 Conference on Empirical Methods in Natural Language Processing (EMNLP
  2020)",,,,cs.CL cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sarcasm detection is an important task in affective computing, requiring
large amounts of labeled data. We introduce reactive supervision, a novel data
collection method that utilizes the dynamics of online conversations to
overcome the limitations of existing data collection techniques. We use the new
method to create and release a first-of-its-kind large dataset of tweets with
sarcasm perspective labels and new contextual features. The dataset is expected
to advance sarcasm detection research. Our method can be adapted to other
affective computing domains, thus opening up new research opportunities.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 05:04:22 GMT'}]",2020-09-29,"[['Shmueli', 'Boaz', ''], ['Ku', 'Lun-Wei', ''], ['Ray', 'Soumya', '']]"
1353999,2009.12496,Qiang Liu,Qiang Liu,Modeling Dyadic Conversations for Personality Inference,,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Nowadays, automatical personality inference is drawing extensive attention
from both academia and industry. Conventional methods are mainly based on user
generated contents, e.g., profiles, likes, and texts of an individual, on
social media, which are actually not very reliable. In contrast, dyadic
conversations between individuals can not only capture how one expresses
oneself, but also reflect how one reacts to different situations. Rich
contextual information in dyadic conversation can explain an individual's
response during his or her conversation. In this paper, we propose a novel
augmented Gated Recurrent Unit (GRU) model for learning unsupervised Personal
Conversational Embeddings (PCE) based on dyadic conversations between
individuals. We adjust the formulation of each layer of a conventional GRU with
sequence to sequence learning and personal information of both sides of the
conversation. Based on the learned PCE, we can infer the personality of each
individual. We conduct experiments on the Movie Script dataset, which is
collected from conversations between characters in movie scripts. We find that
modeling dyadic conversations between individuals can significantly improve
personality inference accuracy. Experimental results illustrate the successful
performance of our proposed method.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 01:25:42 GMT'}]",2020-09-29,"[['Liu', 'Qiang', '']]"
1354562,2009.13059,Shashwat Aggarwal,"Shashwat Aggarwal, Ramesh Singh",Visual Exploration and Knowledge Discovery from Biomedical Dark Data,,,,,cs.DL cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Data visualization techniques proffer efficient means to organize and present
data in graphically appealing formats, which not only speeds up the process of
decision making and pattern recognition but also enables decision-makers to
fully understand data insights and make informed decisions. Over time, with the
rise in technological and computational resources, there has been an
exponential increase in the world's scientific knowledge. However, most of it
lacks structure and cannot be easily categorized and imported into regular
databases. This type of data is often termed as Dark Data. Data visualization
techniques provide a promising solution to explore such data by allowing quick
comprehension of information, the discovery of emerging trends, identification
of relationships and patterns, etc. In this empirical research study, we use
the rich corpus of PubMed comprising of more than 30 million citations from
biomedical literature to visually explore and understand the underlying
key-insights using various information visualization techniques. We employ a
natural language processing based pipeline to discover knowledge out of the
biomedical dark data. The pipeline comprises of different lexical analysis
techniques like Topic Modeling to extract inherent topics and major focus
areas, Network Graphs to study the relationships between various entities like
scientific documents and journals, researchers, and, keywords and terms, etc.
With this analytical research, we aim to proffer a potential solution to
overcome the problem of analyzing overwhelming amounts of information and
diminish the limitation of human cognition and perception in handling and
examining such large volumes of data.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 04:27:05 GMT'}]",2020-09-29,"[['Aggarwal', 'Shashwat', ''], ['Singh', 'Ramesh', '']]"
1354531,2009.13028,Haochen Liu,"Haochen Liu, Wentao Wang, Yiqi Wang, Hui Liu, Zitao Liu and Jiliang
  Tang","Mitigating Gender Bias for Neural Dialogue Generation with Adversarial
  Learning",Accepted by EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialogue systems play an increasingly important role in various aspects of
our daily life. It is evident from recent research that dialogue systems
trained on human conversation data are biased. In particular, they can produce
responses that reflect people's gender prejudice. Many debiasing methods have
been developed for various natural language processing tasks, such as word
embedding. However, they are not directly applicable to dialogue systems
because they are likely to force dialogue models to generate similar responses
for different genders. This greatly degrades the diversity of the generated
responses and immensely hurts the performance of the dialogue models. In this
paper, we propose a novel adversarial learning framework Debiased-Chat to train
dialogue models free from gender bias while keeping their performance.
Extensive experiments on two real-world conversation datasets show that our
framework significantly reduces gender bias in dialogue models while
maintaining the response quality.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 02:46:59 GMT'}]",2020-09-29,"[['Liu', 'Haochen', ''], ['Wang', 'Wentao', ''], ['Wang', 'Yiqi', ''], ['Liu', 'Hui', ''], ['Liu', 'Zitao', ''], ['Tang', 'Jiliang', '']]"
1354500,2009.12997,Haoding Meng Souray,"Haoding Meng, Qingcheng Zeng, Xiaoyang Fang, Zhexin Liang","Fancy Man Lauches Zippo at WNUT 2020 Shared Task-1: A Bert Case Model
  for Wet Lab Entity Extraction",EMNLP2020 WNUT,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic or semi-automatic conversion of protocols specifying steps in
performing a lab procedure into machine-readable format benefits biological
research a lot. These noisy, dense, and domain-specific lab protocols
processing draws more and more interests with the development of deep learning.
This paper presents our teamwork on WNUT 2020 shared task-1: wet lab entity
extract, that we conducted studies in several models, including a BiLSTM CRF
model and a Bert case model which can be used to complete wet lab entity
extraction. And we mainly discussed the performance differences of \textbf{Bert
case} under different situations such as \emph{transformers} versions, case
sensitivity that may don't get enough attention before.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 01:05:08 GMT'}]",2020-09-29,"[['Meng', 'Haoding', ''], ['Zeng', 'Qingcheng', ''], ['Fang', 'Xiaoyang', ''], ['Liang', 'Zhexin', '']]"
1354365,2009.12862,Rochelle Choenni,"Rochelle Choenni, Ekaterina Shutova","What does it mean to be language-agnostic? Probing multilingual sentence
  encoders for typological properties",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual sentence encoders have seen much success in cross-lingual model
transfer for downstream NLP tasks. Yet, we know relatively little about the
properties of individual languages or the general patterns of linguistic
variation that they encode. We propose methods for probing sentence
representations from state-of-the-art multilingual encoders (LASER, M-BERT, XLM
and XLM-R) with respect to a range of typological properties pertaining to
lexical, morphological and syntactic structure. In addition, we investigate how
this information is distributed across all layers of the models. Our results
show interesting differences in encoding linguistic variation associated with
different pretraining strategies.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 15:00:52 GMT'}]",2020-09-29,"[['Choenni', 'Rochelle', ''], ['Shutova', 'Ekaterina', '']]"
1354224,2009.12721,Hongming Zhang,"Hongming Zhang, Xinran Zhao, Yangqiu Song","A Brief Survey and Comparative Study of Recent Development of Pronoun
  Coreference Resolution",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pronoun Coreference Resolution (PCR) is the task of resolving pronominal
expressions to all mentions they refer to. Compared with the general
coreference resolution task, the main challenge of PCR is the coreference
relation prediction rather than the mention detection. As one important natural
language understanding (NLU) component, pronoun resolution is crucial for many
downstream tasks and still challenging for existing models, which motivates us
to survey existing approaches and think about how to do better. In this survey,
we first introduce representative datasets and models for the ordinary pronoun
coreference resolution task. Then we focus on recent progress on hard pronoun
coreference resolution problems (e.g., Winograd Schema Challenge) to analyze
how well current models can understand commonsense. We conduct extensive
experiments to show that even though current models are achieving good
performance on the standard evaluation set, they are still not ready to be used
in real applications (e.g., all SOTA models struggle on correctly resolving
pronouns to infrequent objects). All experiment codes are available at
https://github.com/HKUST-KnowComp/PCR.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 01:40:01 GMT'}]",2020-09-29,"[['Zhang', 'Hongming', ''], ['Zhao', 'Xinran', ''], ['Song', 'Yangqiu', '']]"
1354904,2009.13401,Wenhao Yu,"Xiangyu Dong, Wenhao Yu, Chenguang Zhu, Meng Jiang",Injecting Entity Types into Entity-Guided Text Generation,"Preprint; Under review as a conference paper; Code is available at:
  https://github.com/wyu97/InjType",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Recent successes in deep generative modeling have led to significant advances
in natural language generation (NLG). Incorporating entities into neural
generation models has demonstrated great improvements by assisting to infer the
summary topic and to generate coherent content. In order to enhance the role of
entity in NLG, in this paper, we aim to model the entity type in the decoding
phase to generate contextual words accurately. We develop a novel NLG model to
produce a target sequence (i.e., a news article) based on a given list of
entities. The generation quality depends significantly on whether the input
entities are logically connected and expressed in the output. Our model has a
multi-step decoder that injects the entity types into the process of entity
mention generation. It first predicts the token of being a contextual word or
an entity, then if an entity, predicts the entity mention. It effectively
embeds the entity's meaning into hidden states, making the generated words
precise. Experiments on two public datasets demonstrate type injection performs
better than type embedding concatenation baselines.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 15:19:28 GMT'}]",2020-09-29,"[['Dong', 'Xiangyu', ''], ['Yu', 'Wenhao', ''], ['Zhu', 'Chenguang', ''], ['Jiang', 'Meng', '']]"
1218110,1912.06214,Kuldeep Singh,"Isaiah Onando Mulang, Kuldeep Singh, Akhilesh Vyas, Saeedeh
  Shekarpour, Maria Esther Vidal, Jens Lehmann, Soren Auer","Encoding Knowledge Graph Entity Aliases in Attentive Neural Network for
  Wikidata Entity Linking",15 pages,"WISE 2020 (21st International Conference on Web Information
  Systems Engineering)",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The collaborative knowledge graphs such as Wikidata excessively rely on the
crowd to author the information. Since the crowd is not bound to a standard
protocol for assigning entity titles, the knowledge graph is populated by
non-standard, noisy, long or even sometimes awkward titles. The issue of long,
implicit, and nonstandard entity representations is a challenge in Entity
Linking (EL) approaches for gaining high precision and recall. Underlying KG,
in general, is the source of target entities for EL approaches, however, it
often contains other relevant information, such as aliases of entities (e.g.,
Obama and Barack Hussein Obama are aliases for the entity Barack Obama). EL
models usually ignore such readily available entity attributes. In this paper,
we examine the role of knowledge graph context on an attentive neural network
approach for entity linking on Wikidata. Our approach contributes by exploiting
the sufficient context from a KG as a source of background knowledge, which is
then fed into the neural network. This approach demonstrates merit to address
challenges associated with entity titles (multi-word, long, implicit,
case-sensitive). Our experimental study shows approx 8% improvements over the
baseline approach, and significantly outperform an end to end approach for
Wikidata entity linking.
","[{'version': 'v1', 'created': 'Thu, 12 Dec 2019 21:11:56 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Jul 2020 13:16:16 GMT'}, {'version': 'v3', 'created': 'Sat, 26 Sep 2020 16:03:18 GMT'}]",2020-09-29,"[['Mulang', 'Isaiah Onando', ''], ['Singh', 'Kuldeep', ''], ['Vyas', 'Akhilesh', ''], ['Shekarpour', 'Saeedeh', ''], ['Vidal', 'Maria Esther', ''], ['Lehmann', 'Jens', ''], ['Auer', 'Soren', '']]"
1314919,2007.03405,Jong Won Park,Jong Won Park,"Continual BERT: Continual Learning for Adaptive Extractive Summarization
  of COVID-19 Literature","6 pages, 3 figures",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The scientific community continues to publish an overwhelming amount of new
research related to COVID-19 on a daily basis, leading to much literature
without little to no attention. To aid the community in understanding the
rapidly flowing array of COVID-19 literature, we propose a novel BERT
architecture that provides a brief yet original summarization of lengthy
papers. The model continually learns on new data in online fashion while
minimizing catastrophic forgetting, thus fitting to the need of the community.
Benchmark and manual examination of its performance show that the model provide
a sound summary of new scientific literature.
","[{'version': 'v1', 'created': 'Tue, 7 Jul 2020 13:16:19 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Sep 2020 08:02:30 GMT'}]",2020-09-29,"[['Park', 'Jong Won', '']]"
1110479,1904.05439,Marco Rovera,"Marco Rovera, Federico Nanni, Simone Paolo Ponzetto",Event-based Access to Historical Italian War Memoirs,"Accepted by Journal on Computing and Cultural Heritage (JOCCH), to be
  published in 2021. 23 pages, 6 figures",,,,cs.CL cs.DL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The progressive digitization of historical archives provides new, often
domain specific, textual resources that report on facts and events which have
happened in the past; among these, memoirs are a very common type of primary
source. In this paper, we present an approach for extracting information from
Italian historical war memoirs and turning it into structured knowledge. This
is based on the semantic notions of events, participants and roles. We evaluate
quantitatively each of the key-steps of our approach and provide a graph-based
representation of the extracted knowledge, which allows to move between a Close
and a Distant Reading of the collection.
","[{'version': 'v1', 'created': 'Mon, 8 Apr 2019 18:30:36 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Sep 2020 13:19:25 GMT'}]",2020-09-29,"[['Rovera', 'Marco', ''], ['Nanni', 'Federico', ''], ['Ponzetto', 'Simone Paolo', '']]"
1039886,1810.08699,Tsolak Ghukasyan,"Tsolak Ghukasyan, Garnik Davtyan, Karen Avetisyan, Ivan Andrianov",pioNER: Datasets and Baselines for Armenian Named Entity Recognition,"Accepted paper at Ivannikov ISP RAS Open Conference 2018.
  \c{opyright} 2018 IEEE",,10.1109/ISPRAS.2018.00015.,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we tackle the problem of Armenian named entity recognition,
providing silver- and gold-standard datasets as well as establishing baseline
results on popular models. We present a 163000-token named entity corpus
automatically generated and annotated from Wikipedia, and another 53400-token
corpus of news sentences with manual annotation of people, organization and
location named entities. The corpora were used to train and evaluate several
popular named entity recognition models. Alongside the datasets, we release
50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection
of Armenian texts from Wikipedia, news, blogs, and encyclopedia.
","[{'version': 'v1', 'created': 'Fri, 19 Oct 2018 22:01:48 GMT'}]",2020-09-29,"[['Ghukasyan', 'Tsolak', ''], ['Davtyan', 'Garnik', ''], ['Avetisyan', 'Karen', ''], ['Andrianov', 'Ivan', '']]"
1350629,2009.09126,Kai Fan Dr,"Jiayi Wang, Ke Wang, Niyu Ge, Yangbing Shi, Yu Zhao, Kai Fan","Computer Assisted Translation with Neural Quality Estimation and
  Automatic Post-Editing",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the advent of neural machine translation, there has been a marked shift
towards leveraging and consuming the machine translation results. However, the
gap between machine translation systems and human translators needs to be
manually closed by post-editing. In this paper, we propose an end-to-end deep
learning framework of the quality estimation and automatic post-editing of the
machine translation output. Our goal is to provide error correction suggestions
and to further relieve the burden of human translators through an interpretable
model. To imitate the behavior of human translators, we design three efficient
delegation modules -- quality estimation, generative post-editing, and atomic
operation post-editing and construct a hierarchical model based on them. We
examine this approach with the English--German dataset from WMT 2017 APE shared
task and our experimental results can achieve the state-of-the-art performance.
We also verify that the certified translators can significantly expedite their
post-editing processing with our model in human evaluation.
","[{'version': 'v1', 'created': 'Sat, 19 Sep 2020 00:29:00 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Sep 2020 00:23:25 GMT'}]",2020-09-29,"[['Wang', 'Jiayi', ''], ['Wang', 'Ke', ''], ['Ge', 'Niyu', ''], ['Shi', 'Yangbing', ''], ['Zhao', 'Yu', ''], ['Fan', 'Kai', '']]"
1300075,2006.05561,"Gabriele Bettgenh\""auser","Gabriele Bettgenh\""auser, Michael A. Hedderich, Dietrich Klakow",Learning Functions to Study the Benefit of Multitask Learning,,,,,cs.LG cs.CL stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We study and quantify the generalization patterns of multitask learning (MTL)
models for sequence labeling tasks. MTL models are trained to optimize a set of
related tasks jointly. Although multitask learning has achieved improved
performance in some problems, there are also tasks that lose performance when
trained together. These mixed results motivate us to study the factors that
impact the performance of MTL models. We note that theoretical bounds and
convergence rates for MTL models exist, but they rely on strong assumptions
such as task relatedness and the use of balanced datasets. To remedy these
limitations, we propose the creation of a task simulator and the use of
Symbolic Regression to learn expressions relating model performance to possible
factors of influence. For MTL, we study the model performance against the
number of tasks (T), the number of samples per task (n) and the task
relatedness measured by the adjusted mutual information (AMI). In our
experiments, we could empirically find formulas relating model performance with
factors of sqrt(n), sqrt(T), which are equivalent to sound mathematical proofs
in Maurer[2016], and we went beyond by discovering that performance relates to
a factor of sqrt(AMI).
","[{'version': 'v1', 'created': 'Tue, 9 Jun 2020 23:51:32 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Sep 2020 06:19:12 GMT'}]",2020-09-29,"[['Bettgenhäuser', 'Gabriele', ''], ['Hedderich', 'Michael A.', ''], ['Klakow', 'Dietrich', '']]"
1319076,2007.07562,Vladimir Kokh,"Pavel Blinov, Manvel Avetisian, Vladimir Kokh, Dmitry Umerenkov,
  Alexander Tuzhilin","Predicting Clinical Diagnosis from Patients Electronic Health Records
  Using BERT-based Neural Networks","To be published in the proceedings of 2020 International Conference
  on Artificial Intelligence in Medicine, Minneapolis MN, USA",,10.1007/978-3-030-59137-3_11,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we study the problem of predicting clinical diagnoses from
textual Electronic Health Records (EHR) data. We show the importance of this
problem in medical community and present comprehensive historical review of the
problem and proposed methods. As the main scientific contributions we present a
modification of Bidirectional Encoder Representations from Transformers (BERT)
model for sequence classification that implements a novel way of
Fully-Connected (FC) layer composition and a BERT model pretrained only on
domain data. To empirically validate our model, we use a large-scale Russian
EHR dataset consisting of about 4 million unique patient visits. This is the
largest such study for the Russian language and one of the largest globally. We
performed a number of comparative experiments with other text representation
models on the task of multiclass classification for 265 disease subset of
ICD-10. The experiments demonstrate improved performance of our models compared
to other baselines, including a fine-tuned Russian BERT (RuBERT) variant. We
also show comparable performance of our model with a panel of experienced
medical experts. This allows us to hope that implementation of this system will
reduce misdiagnosis.
","[{'version': 'v1', 'created': 'Wed, 15 Jul 2020 09:22:55 GMT'}]",2020-09-29,"[['Blinov', 'Pavel', ''], ['Avetisian', 'Manvel', ''], ['Kokh', 'Vladimir', ''], ['Umerenkov', 'Dmitry', ''], ['Tuzhilin', 'Alexander', '']]"
1299076,2006.04562,Mirko Lenz,"Mirko Lenz, Premtim Sahitaj, Sean Kallenberg, Christopher Coors, Lorik
  Dumani, Ralf Schenkel, Ralph Bergmann","Towards an Argument Mining Pipeline Transforming Texts to Argument
  Graphs",,,10.3233/FAIA200510,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper targets the automated extraction of components of argumentative
information and their relations from natural language text. Moreover, we
address a current lack of systems to provide complete argumentative structure
from arbitrary natural language text for general usage. We present an argument
mining pipeline as a universally applicable approach for transforming German
and English language texts to graph-based argument representations. We also
introduce new methods for evaluating the results based on existing benchmark
argument structures. Our results show that the generated argument graphs can be
beneficial to detect new connections between different statements of an
argumentative text. Our pipeline implementation is publicly available on
GitHub.
","[{'version': 'v1', 'created': 'Mon, 8 Jun 2020 13:10:19 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Sep 2020 11:07:04 GMT'}]",2020-09-29,"[['Lenz', 'Mirko', ''], ['Sahitaj', 'Premtim', ''], ['Kallenberg', 'Sean', ''], ['Coors', 'Christopher', ''], ['Dumani', 'Lorik', ''], ['Schenkel', 'Ralf', ''], ['Bergmann', 'Ralph', '']]"
1344595,2009.03092,Soohwan Kim,"Soohwan Kim, Seyoung Bae, Cheolhwang Won",KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition,,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present KoSpeech, an open-source software, which is modular and extensible
end-to-end Korean automatic speech recognition (ASR) toolkit based on the deep
learning library PyTorch. Several automatic speech recognition open-source
toolkits have been released, but all of them deal with non-Korean languages,
such as English (e.g. ESPnet, Espresso). Although AI Hub opened 1,000 hours of
Korean speech corpus known as KsponSpeech, there is no established
preprocessing method and baseline model to compare model performances.
Therefore, we propose preprocessing methods for KsponSpeech corpus and a
baseline model for benchmarks. Our baseline model is based on Listen, Attend
and Spell (LAS) architecture and ables to customize various training
hyperparameters conveniently. By KoSpeech, we hope this could be a guideline
for those who research Korean speech recognition. Our baseline model achieved
10.31% character error rate (CER) at KsponSpeech corpus only with the acoustic
model. Our source code is available here.
","[{'version': 'v1', 'created': 'Mon, 7 Sep 2020 13:25:36 GMT'}, {'version': 'v2', 'created': 'Sat, 26 Sep 2020 17:25:34 GMT'}]",2020-09-29,"[['Kim', 'Soohwan', ''], ['Bae', 'Seyoung', ''], ['Won', 'Cheolhwang', '']]"
1344405,2009.02902,Zilong Wang,"Zilong Wang, Zhaohong Wan, and Xiaojun Wan","TransModality: An End2End Fusion Method with Transformer for Multimodal
  Sentiment Analysis",Proceedings of The Web Conference 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal sentiment analysis is an important research area that predicts
speaker's sentiment tendency through features extracted from textual, visual
and acoustic modalities. The central challenge is the fusion method of the
multimodal information. A variety of fusion methods have been proposed, but few
of them adopt end-to-end translation models to mine the subtle correlation
between modalities. Enlightened by recent success of Transformer in the area of
machine translation, we propose a new fusion method, TransModality, to address
the task of multimodal sentiment analysis. We assume that translation between
modalities contributes to a better joint representation of speaker's utterance.
With Transformer, the learned features embody the information both from the
source modality and the target modality. We validate our model on multiple
multimodal datasets: CMU-MOSI, MELD, IEMOCAP. The experiments show that our
proposed method achieves the state-of-the-art performance.
","[{'version': 'v1', 'created': 'Mon, 7 Sep 2020 06:11:56 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Sep 2020 04:44:54 GMT'}]",2020-09-29,"[['Wang', 'Zilong', ''], ['Wan', 'Zhaohong', ''], ['Wan', 'Xiaojun', '']]"
1207116,1911.08522,Omar U. Florez,Omar U. Florez and Erik Mueller,"Aging Memories Generate More Fluent Dialogue Responses with Memory
  Augmented Neural Networks",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Memory Networks have emerged as effective models to incorporate Knowledge
Bases (KB) into neural networks. By storing KB embeddings into a memory
component, these models can learn meaningful representations that are grounded
to external knowledge. However, as the memory unit becomes full, the oldest
memories are replaced by newer representations.
  In this paper, we question this approach and provide experimental evidence
that conventional Memory Networks store highly correlated vectors during
training. While increasing the memory size mitigates this problem, this also
leads to overfitting as the memory stores a large number of training latent
representations. To address these issues, we propose a novel regularization
mechanism named memory dropout which 1) Samples a single latent vector from the
distribution of redundant memories. 2) Ages redundant memories thus increasing
their probability of overwriting them during training. This fully
differentiable technique allows us to achieve state-of-the-art response
generation in the Stanford Multi-Turn Dialogue and Cambridge Restaurant
datasets.
","[{'version': 'v1', 'created': 'Tue, 19 Nov 2019 19:34:15 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Sep 2020 02:42:46 GMT'}]",2020-09-29,"[['Florez', 'Omar U.', ''], ['Mueller', 'Erik', '']]"
1342551,2009.01048,Thai Le,"Thai Le, Suhang Wang, Dongwon Lee","MALCOM: Generating Malicious Comments to Attack Neural Fake News
  Detection Models","Accepted at the 20th IEEE International Conference on Data Mining
  (ICDM 2020)",,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, the proliferation of so-called ""fake news"" has caused much
disruptions in society and weakened the news ecosystem. Therefore, to mitigate
such problems, researchers have developed state-of-the-art models to
auto-detect fake news on social media using sophisticated data science and
machine learning techniques. In this work, then, we ask ""what if adversaries
attempt to attack such detection models?"" and investigate related issues by (i)
proposing a novel threat model against fake news detectors, in which
adversaries can post malicious comments toward news articles to mislead fake
news detectors, and (ii) developing MALCOM, an end-to-end adversarial comment
generation framework to achieve such an attack. Through a comprehensive
evaluation, we demonstrate that about 94% and 93.5% of the time on average
MALCOM can successfully mislead five of the latest neural detection models to
always output targeted real and fake news labels. Furthermore, MALCOM can also
fool black box fake news detectors to always output real news labels 90% of the
time on average. We also compare our attack model with four baselines across
two real-world datasets, not only on attack performance but also on generated
quality, coherency, transferability, and robustness.
","[{'version': 'v1', 'created': 'Tue, 1 Sep 2020 01:26:01 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Sep 2020 10:15:06 GMT'}]",2020-09-29,"[['Le', 'Thai', ''], ['Wang', 'Suhang', ''], ['Lee', 'Dongwon', '']]"
1342099,2009.00596,Salvatore Giorgi,"Salvatore Giorgi, Sharath Chandra Guntuku, Muhammad Rahman, McKenzie
  Himelein-Wachowiak, Amy Kwarteng, Brenda Curtis","Twitter Corpus of the #BlackLivesMatter Movement And Counter Protests:
  2013 to 2020",,,,,cs.SI cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Black Lives Matter (BLM) is a grassroots movement protesting violence towards
Black individuals and communities with a focus on police brutality. The
movement has gained significant media and political attention following the
killings of Ahmaud Arbery, Breonna Taylor, and George Floyd and the shooting of
Jacob Blake in 2020. Due to its decentralized nature, the #BlackLivesMatter
social media hashtag has come to both represent the movement and been used as a
call to action. Similar hashtags have appeared to counter the BLM movement,
such as #AllLivesMatter and #BlueLivesMatter. We introduce a data set of 41.8
million tweets from 10 million users which contain one of the following
keywords: BlackLivesMatter, AllLivesMatter and BlueLivesMatter. This data set
contains all currently available tweets from the beginning of the BLM movement
in 2013 to June 2020. We summarize the data set and show temporal trends in use
of both the BlackLivesMatter keyword and keywords associated with counter
movements. In the past, similarly themed, though much smaller in scope, BLM
data sets have been used for studying discourse in protest and counter protest
movements, predicting retweets, examining the role of social media in protest
movements and exploring narrative agency. This paper open-sources a large-scale
data set to facilitate research in the areas of computational social science,
communications, political science, natural language processing, and machine
learning.
","[{'version': 'v1', 'created': 'Tue, 1 Sep 2020 17:37:39 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Sep 2020 16:20:16 GMT'}]",2020-09-29,"[['Giorgi', 'Salvatore', ''], ['Guntuku', 'Sharath Chandra', ''], ['Rahman', 'Muhammad', ''], ['Himelein-Wachowiak', 'McKenzie', ''], ['Kwarteng', 'Amy', ''], ['Curtis', 'Brenda', '']]"
1326501,2007.14987,Patrick Jenkins,"Patrick Jenkins, Rishabh Sachdeva, Gaoussou Youssouf Kebe, Padraig
  Higgins, Kasra Darvish, Edward Raff, Don Engel, John Winder, Francis Ferraro,
  Cynthia Matuszek","Presentation and Analysis of a Multimodal Dataset for Grounded Language
  Learning","11 pages, 6 figures",,,,cs.RO cs.CL cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Grounded language acquisition -- learning how language-based interactions
refer to the world around them -- is amajor area of research in robotics, NLP,
and HCI. In practice the data used for learning consists almost entirely of
textual descriptions, which tend to be cleaner, clearer, and more grammatical
than actual human interactions. In this work, we present the Grounded Language
Dataset (GoLD), a multimodal dataset of common household objects described by
people using either spoken or written language. We analyze the differences and
present an experiment showing how the different modalities affect language
learning from human in-put. This will enable researchers studying the
intersection of robotics, NLP, and HCI to better investigate how the multiple
modalities of image, text, and speech interact, as well as show differences in
the vernacular of these modalities impact results.
","[{'version': 'v1', 'created': 'Wed, 29 Jul 2020 17:58:04 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Jul 2020 15:37:58 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Sep 2020 15:25:34 GMT'}, {'version': 'v4', 'created': 'Mon, 28 Sep 2020 16:47:50 GMT'}]",2020-09-29,"[['Jenkins', 'Patrick', ''], ['Sachdeva', 'Rishabh', ''], ['Kebe', 'Gaoussou Youssouf', ''], ['Higgins', 'Padraig', ''], ['Darvish', 'Kasra', ''], ['Raff', 'Edward', ''], ['Engel', 'Don', ''], ['Winder', 'John', ''], ['Ferraro', 'Francis', ''], ['Matuszek', 'Cynthia', '']]"
1298479,2006.03965,Gasper Begus,Ga\v{s}per Begu\v{s},"Generative Adversarial Phonology: Modeling unsupervised phonetic and
  phonological learning with neural networks",Provisionally accepted in Frontiers in Artificial Intelligence,Frontiers in Artificial Intelligence 2020,10.3389/frai.2020.00044,,cs.CL cs.LG q-bio.NC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Training deep neural networks on well-understood dependencies in speech data
can provide new insights into how they learn internal representations. This
paper argues that acquisition of speech can be modeled as a dependency between
random space and generated speech data in the Generative Adversarial Network
architecture and proposes a methodology to uncover the network's internal
representations that correspond to phonetic and phonological properties. The
Generative Adversarial architecture is uniquely appropriate for modeling
phonetic and phonological learning because the network is trained on
unannotated raw acoustic data and learning is unsupervised without any
language-specific assumptions or pre-assumed levels of abstraction. A
Generative Adversarial Network was trained on an allophonic distribution in
English. The network successfully learns the allophonic alternation: the
network's generated speech signal contains the conditional distribution of
aspiration duration. The paper proposes a technique for establishing the
network's internal representations that identifies latent variables that
correspond to, for example, presence of [s] and its spectral properties. By
manipulating these variables, we actively control the presence of [s] and its
frication amplitude in the generated outputs. This suggests that the network
learns to use latent variables as an approximation of phonetic and phonological
representations. Crucially, we observe that the dependencies learned in
training extend beyond the training interval, which allows for additional
exploration of learning representations. The paper also discusses how the
network's architecture and innovative outputs resemble and differ from
linguistic behavior in language acquisition, speech disorders, and speech
errors, and how well-understood dependencies in speech data can help us
interpret how neural networks learn their representations.
","[{'version': 'v1', 'created': 'Sat, 6 Jun 2020 20:31:23 GMT'}]",2020-09-29,"[['Beguš', 'Gašper', '']]"
1351800,2009.10297,Shuo Ren,"Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel
  Sundaresan, Ming Zhou, Ambrosio Blanco, Shuai Ma",CodeBLEU: a Method for Automatic Evaluation of Code Synthesis,"8 pages, 6 figures",,,,cs.SE cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Evaluation metrics play a vital role in the growth of an area as it defines
the standard of distinguishing between good and bad models. In the area of code
synthesis, the commonly used evaluation metric is BLEU or perfect accuracy, but
they are not suitable enough to evaluate codes, because BLEU is originally
designed to evaluate the natural language, neglecting important syntactic and
semantic features of codes, and perfect accuracy is too strict thus it
underestimates different outputs with the same semantic logic. To remedy this,
we introduce a new automatic evaluation metric, dubbed CodeBLEU. It absorbs the
strength of BLEU in the n-gram match and further injects code syntax via
abstract syntax trees (AST) and code semantics via data-flow. We conduct
experiments by evaluating the correlation coefficient between CodeBLEU and
quality scores assigned by the programmers on three code synthesis tasks, i.e.,
text-to-code, code translation, and code refinement. Experimental results show
that our proposed CodeBLEU can achieve a better correlation with programmer
assigned scores compared with BLEU and accuracy.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 03:10:49 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Sep 2020 04:07:11 GMT'}]",2020-09-29,"[['Ren', 'Shuo', ''], ['Guo', 'Daya', ''], ['Lu', 'Shuai', ''], ['Zhou', 'Long', ''], ['Liu', 'Shujie', ''], ['Tang', 'Duyu', ''], ['Sundaresan', 'Neel', ''], ['Zhou', 'Ming', ''], ['Blanco', 'Ambrosio', ''], ['Ma', 'Shuai', '']]"
1249428,2002.12005,Zhenisbek Assylbekov,Zhenisbek Assylbekov and Alibi Jangeldin,"Squashed Shifted PMI Matrix: Bridging Word Embeddings and Hyperbolic
  Spaces",AJCAI 2020,,,,cs.CL stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We show that removing sigmoid transformation in the skip-gram with negative
sampling (SGNS) objective does not harm the quality of word vectors
significantly and at the same time is related to factorizing a squashed shifted
PMI matrix which, in turn, can be treated as a connection probabilities matrix
of a random graph. Empirically, such graph is a complex network, i.e. it has
strong clustering and scale-free degree distribution, and is tightly connected
with hyperbolic spaces. In short, we show the connection between static word
embeddings and hyperbolic spaces through the squashed shifted PMI matrix using
analytical and empirical methods.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2020 09:50:41 GMT'}, {'version': 'v2', 'created': 'Sat, 26 Sep 2020 15:06:00 GMT'}]",2020-09-29,"[['Assylbekov', 'Zhenisbek', ''], ['Jangeldin', 'Alibi', '']]"
1354934,2009.13431,Peilin Zhou,"Peilin Zhou, Zhiqi Huang, Fenglin Liu, Yuexian Zou","PIN: A Novel Parallel Interactive Network for Spoken Language
  Understanding",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Spoken Language Understanding (SLU) is an essential part of the spoken
dialogue system, which typically consists of intent detection (ID) and slot
filling (SF) tasks. Recently, recurrent neural networks (RNNs) based methods
achieved the state-of-the-art for SLU. It is noted that, in the existing
RNN-based approaches, ID and SF tasks are often jointly modeled to utilize the
correlation information between them. However, we noted that, so far, the
efforts to obtain better performance by supporting bidirectional and explicit
information exchange between ID and SF are not well studied.In addition, few
studies attempt to capture the local context information to enhance the
performance of SF. Motivated by these findings, in this paper, Parallel
Interactive Network (PIN) is proposed to model the mutual guidance between ID
and SF. Specifically, given an utterance, a Gaussian self-attentive encoder is
introduced to generate the context-aware feature embedding of the utterance
which is able to capture local context information. Taking the feature
embedding of the utterance, Slot2Intent module and Intent2Slot module are
developed to capture the bidirectional information flow for ID and SF tasks.
Finally, a cooperation mechanism is constructed to fuse the information
obtained from Slot2Intent and Intent2Slot modules to further reduce the
prediction bias.The experiments on two benchmark datasets, i.e., SNIPS and
ATIS, demonstrate the effectiveness of our approach, which achieves a
competitive result with state-of-the-art models. More encouragingly, by using
the feature embedding of the utterance generated by the pre-trained language
model BERT, our method achieves the state-of-the-art among all comparison
approaches.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 15:59:31 GMT'}]",2020-09-29,"[['Zhou', 'Peilin', ''], ['Huang', 'Zhiqi', ''], ['Liu', 'Fenglin', ''], ['Zou', 'Yuexian', '']]"
1355185,2009.13682,Xiaowei Hu,"Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao,
  Zicheng Liu","VIVO: Surpassing Human Performance in Novel Object Captioning with
  Visual Vocabulary Pre-Training",,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is highly desirable yet challenging to generate image captions that can
describe novel objects which are unseen in caption-labeled training data, a
capability that is evaluated in the novel object captioning challenge (nocaps).
In this challenge, no additional image-caption training data, other than COCO
Captions, is allowed for model training. Thus, conventional Vision-Language
Pre-training (VLP) methods cannot be applied. This paper presents VIsual
VOcabulary pre-training (VIVO) that performs pre-training in the absence of
caption annotations. By breaking the dependency of paired image-caption
training data in VLP, VIVO can leverage large amounts of paired image-tag data
to learn a visual vocabulary. This is done by pre-training a multi-layer
Transformer model that learns to align image-level tags with their
corresponding image region features. To address the unordered nature of image
tags, VIVO uses a Hungarian matching loss with masked tag prediction to conduct
pre-training.
  We validate the effectiveness of VIVO by fine-tuning the pre-trained model
for image captioning. In addition, we perform an analysis of the visual-text
alignment inferred by our model. The results show that our model can not only
generate fluent image captions that describe novel objects, but also identify
the locations of these objects. Our single model has achieved new
state-of-the-art results on nocaps and surpassed the human CIDEr score.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 23:20:02 GMT'}]",2020-09-30,"[['Hu', 'Xiaowei', ''], ['Yin', 'Xi', ''], ['Lin', 'Kevin', ''], ['Wang', 'Lijuan', ''], ['Zhang', 'Lei', ''], ['Gao', 'Jianfeng', ''], ['Liu', 'Zicheng', '']]"
1354878,2009.13375,Antonios Maronikolakis,"Antonis Maronikolakis, Mark Stevenson, Hinrich Schutze",Transformers Are Better Than Humans at Identifying Generated Text,,,,,cs.CL cs.CY cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Fake information spread via the internet and social media influences public
opinion and user activity. Generative models enable fake content to be
generated faster and more cheaply than had previously been possible. This paper
examines the problem of identifying fake content generated by lightweight deep
learning models. A dataset containing human and machine-generated headlines was
created and a user study indicated that humans were only able to identify the
fake headlines in 45.3% of the cases. However, the most accurate automatic
approach, transformers, achieved an accuracy of 94%, indicating that content
generated from language models can be filtered out accurately.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 14:48:27 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 11:52:01 GMT'}]",2020-09-30,"[['Maronikolakis', 'Antonis', ''], ['Stevenson', 'Mark', ''], ['Schutze', 'Hinrich', '']]"
1318391,2007.06877,Jiuniu Wang,"Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B. Chan","Compare and Reweight: Distinctive Image Captioning Using Similar Images
  Sets",,,,Accepted at ECCV 2020 (oral),cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A wide range of image captioning models has been developed, achieving
significant improvement based on popular metrics, such as BLEU, CIDEr, and
SPICE. However, although the generated captions can accurately describe the
image, they are generic for similar images and lack distinctiveness, i.e.,
cannot properly describe the uniqueness of each image. In this paper, we aim to
improve the distinctiveness of image captions through training with sets of
similar images. First, we propose a distinctiveness metric -- between-set CIDEr
(CIDErBtw) to evaluate the distinctiveness of a caption with respect to those
of similar images. Our metric shows that the human annotations of each image
are not equivalent based on distinctiveness. Thus we propose several new
training strategies to encourage the distinctiveness of the generated caption
for each image, which are based on using CIDErBtw in a weighted loss function
or as a reinforcement learning reward. Finally, extensive experiments are
conducted, showing that our proposed approach significantly improves both
distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy
(e.g., as measured by CIDEr) for a wide variety of image captioning baselines.
These results are further confirmed through a user study.
","[{'version': 'v1', 'created': 'Tue, 14 Jul 2020 07:40:39 GMT'}]",2020-09-30,"[['Wang', 'Jiuniu', ''], ['Xu', 'Wenjia', ''], ['Wang', 'Qingzhong', ''], ['Chan', 'Antoni B.', '']]"
1041834,1810.10647,Revanth Reddy Gangi Reddy,"Revanth Reddy, Danish Contractor, Dinesh Raghu, Sachindra Joshi",Multi-level Memory for Task Oriented Dialogs,Accepted as full paper at NAACL 2019,,10.18653/v1/N19-1375,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent end-to-end task oriented dialog systems use memory architectures to
incorporate external knowledge in their dialogs. Current work makes simplifying
assumptions about the structure of the knowledge base, such as the use of
triples to represent knowledge, and combines dialog utterances (context) as
well as knowledge base (KB) results as part of the same memory. This causes an
explosion in the memory size, and makes the reasoning over memory harder. In
addition, such a memory design forces hierarchical properties of the data to be
fit into a triple structure of memory. This requires the memory reader to infer
relationships across otherwise connected attributes. In this paper we relax the
strong assumptions made by existing architectures and separate memories used
for modeling dialog context and KB results. Instead of using triples to store
KB results, we introduce a novel multi-level memory architecture consisting of
cells for each query and their corresponding results. The multi-level memory
first addresses queries, followed by results and finally each key-value pair
within a result. We conduct detailed experiments on three publicly available
task oriented dialog data sets and we find that our method conclusively
outperforms current state-of-the-art models. We report a 15-25% increase in
both entity F1 and BLEU scores.
","[{'version': 'v1', 'created': 'Wed, 24 Oct 2018 22:49:32 GMT'}, {'version': 'v2', 'created': 'Sun, 12 May 2019 02:05:05 GMT'}]",2020-09-30,"[['Reddy', 'Revanth', ''], ['Contractor', 'Danish', ''], ['Raghu', 'Dinesh', ''], ['Joshi', 'Sachindra', '']]"
1219471,1912.07575,Th\'eodore Bluche,Theodore Bluche and Thibault Gisselbrecht,"Predicting detection filters for small footprint open-vocabulary keyword
  spotting",Submtted to Interspeech 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose a fully-neural approach to open-vocabulary keyword
spotting, that allows the users to include a customizable voice interface to
their device and that does not require task-specific data. We present a keyword
detection neural network weighing less than 250KB, in which the topmost layer
performing keyword detection is predicted by an auxiliary network, that may be
run offline to generate a detector for any keyword. We show that the proposed
model outperforms acoustic keyword spotting baselines by a large margin on two
tasks of detecting keywords in utterances and three tasks of detecting isolated
speech commands. We also propose a method to fine-tune the model when specific
training data is available for some keywords, which yields a performance
similar to a standard speech command neural network while keeping the ability
of the model to be applied to new keywords.
","[{'version': 'v1', 'created': 'Mon, 16 Dec 2019 18:41:49 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 09:09:15 GMT'}]",2020-09-30,"[['Bluche', 'Theodore', ''], ['Gisselbrecht', 'Thibault', '']]"
1355159,2009.13656,Andrea Madotto Mr,"Andrea Madotto, Samuel Cahyawijaya, Genta Indra Winata, Yan Xu, Zihan
  Liu, Zhaojiang Lin, Pascale Fung","Learning Knowledge Bases with Parameters for Task-Oriented Dialogue
  Systems",Accepted EMNLP findings,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task-oriented dialogue systems are either modularized with separate dialogue
state tracking (DST) and management steps or end-to-end trainable. In either
case, the knowledge base (KB) plays an essential role in fulfilling user
requests. Modularized systems rely on DST to interact with the KB, which is
expensive in terms of annotation and inference time. End-to-end systems use the
KB directly as input, but they cannot scale when the KB is larger than a few
hundred entries. In this paper, we propose a method to embed the KB, of any
size, directly into the model parameters. The resulting model does not require
any DST or template responses, nor the KB as input, and it can dynamically
update its KB via fine-tuning. We evaluate our solution in five task-oriented
dialogue datasets with small, medium, and large KB size. Our experiments show
that end-to-end models can effectively embed knowledge bases in their
parameters and achieve competitive performance in all evaluated datasets.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 22:13:54 GMT'}]",2020-09-30,"[['Madotto', 'Andrea', ''], ['Cahyawijaya', 'Samuel', ''], ['Winata', 'Genta Indra', ''], ['Xu', 'Yan', ''], ['Liu', 'Zihan', ''], ['Lin', 'Zhaojiang', ''], ['Fung', 'Pascale', '']]"
1131854,1905.13418,Konstantinos Kogkalidis,"Konstantinos Kogkalidis, Michael Moortgat, Tejaswini Deoskar",Constructive Type-Logical Supertagging with Self-Attention Networks,"REPL4NLP 4, ACL 2019","Proceedings of the 4th Workshop on Representation Learning for NLP
  (RepL4NLP-2019)",10.18653/v1/W19-4314,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a novel application of self-attention networks towards grammar
induction. We present an attention-based supertagger for a refined type-logical
grammar, trained on constructing types inductively. In addition to achieving a
high overall type accuracy, our model is able to learn the syntax of the
grammar's type system along with its denotational semantics. This lifts the
closed world assumption commonly made by lexicalized grammar supertaggers,
greatly enhancing its generalization potential. This is evidenced both by its
adequate accuracy over sparse word types and its ability to correctly construct
complex types never seen during training, which, to the best of our knowledge,
was as of yet unaccomplished.
","[{'version': 'v1', 'created': 'Fri, 31 May 2019 05:16:15 GMT'}]",2020-09-30,"[['Kogkalidis', 'Konstantinos', ''], ['Moortgat', 'Michael', ''], ['Deoskar', 'Tejaswini', '']]"
1202181,1911.03587,Fabio Petroni,"Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim
  Rockt\""aschel, Vassilis Plachouras, Fabrizio Silvestri, Sebastian Riedel",How Decoding Strategies Affect the Verifiability of Generated Text,accepted at Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent progress in pre-trained language models led to systems that are able
to generate text of an increasingly high quality. While several works have
investigated the fluency and grammatical correctness of such models, it is
still unclear to which extent the generated text is consistent with factual
world knowledge. Here, we go beyond fluency and also investigate the
verifiability of text generated by state-of-the-art pre-trained language
models. A generated sentence is verifiable if it can be corroborated or
disproved by Wikipedia, and we find that the verifiability of generated text
strongly depends on the decoding strategy. In particular, we discover a
tradeoff between factuality (i.e., the ability of generating Wikipedia
corroborated text) and repetitiveness. While decoding strategies such as top-k
and nucleus sampling lead to less repetitive generations, they also produce
less verifiable text. Based on these finding, we introduce a simple and
effective decoding strategy which, in comparison to previously used decoding
strategies, produces less repetitive and more verifiable text.
","[{'version': 'v1', 'created': 'Sat, 9 Nov 2019 00:16:03 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 07:39:38 GMT'}]",2020-09-30,"[['Massarelli', 'Luca', ''], ['Petroni', 'Fabio', ''], ['Piktus', 'Aleksandra', ''], ['Ott', 'Myle', ''], ['Rocktäschel', 'Tim', ''], ['Plachouras', 'Vassilis', ''], ['Silvestri', 'Fabrizio', ''], ['Riedel', 'Sebastian', '']]"
1181424,1909.11386,Diego Antognini,"Diego Antognini, Claudiu Musat, Boi Faltings",Multi-Dimensional Explanation of Target Variables from Documents,"Under review. 17 pages, 13 figures, 9 tables",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated predictions require explanations to be interpretable by humans.
Past work used attention and rationale mechanisms to find words that predict
the target variable of a document. Often though, they result in a tradeoff
between noisy explanations or a drop in accuracy. Furthermore, rationale
methods cannot capture the multi-faceted nature of justifications for multiple
targets, because of the non-probabilistic nature of the mask. In this paper, we
propose the Multi-Target Masker (MTM) to address these shortcomings. The
novelty lies in the soft multi-dimensional mask that models a relevance
probability distribution over the set of target variables to handle
ambiguities. Additionally, two regularizers guide MTM to induce long,
meaningful explanations. We evaluate MTM on two datasets and show, using
standard metrics and human annotations, that the resulting masks are more
accurate and coherent than those generated by the state-of-the-art methods.
Moreover, MTM is the first to also achieve the highest F1 scores for all the
target variables simultaneously.
","[{'version': 'v1', 'created': 'Wed, 25 Sep 2019 10:26:36 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Mar 2020 14:49:03 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Sep 2020 13:52:27 GMT'}]",2020-09-30,"[['Antognini', 'Diego', ''], ['Musat', 'Claudiu', ''], ['Faltings', 'Boi', '']]"
1331884,2008.04162,Philip Feldman,Philip Feldman and Antonio Bucchiarone,Navigating Human Language Models with Synthetic Agents,"8 pages, 6 figures, 2 tables, 1 algorithm",,,,cs.AI cs.CL cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern natural language models such as the GPT-2/GPT-3 contain tremendous
amounts of information about human belief in a consistently testable form. If
these models could be shown to accurately reflect the underlying beliefs of the
human beings that produced the data used to train these models, then such
models become a powerful sociological tool in ways that are distinct from
traditional methods, such as interviews and surveys. In this study, We train a
version of the GPT-2 on a corpora of historical chess games, and then ""launch""
clusters of synthetic agents into the model, using text strings to create
context and orientation. We compare the trajectories contained in the text
generated by the agents/model and compare that to the known ground truth of the
chess board, move legality, and historical patterns of play. We find that the
percentages of moves by piece using the model are substantially similar from
human patterns. We further find that the model creates an accurate latent
representation of the chessboard, and that it is possible to plot trajectories
of legal moves across the board using this knowledge.
","[{'version': 'v1', 'created': 'Mon, 10 Aug 2020 14:39:53 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Aug 2020 15:09:36 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Aug 2020 13:42:00 GMT'}, {'version': 'v4', 'created': 'Fri, 14 Aug 2020 11:12:43 GMT'}, {'version': 'v5', 'created': 'Mon, 24 Aug 2020 19:18:21 GMT'}, {'version': 'v6', 'created': 'Mon, 28 Sep 2020 15:40:41 GMT'}, {'version': 'v7', 'created': 'Tue, 29 Sep 2020 09:57:33 GMT'}]",2020-09-30,"[['Feldman', 'Philip', ''], ['Bucchiarone', 'Antonio', '']]"
1202408,1911.03814,Fabio Petroni,"Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, Luke
  Zettlemoyer",Scalable Zero-shot Entity Linking with Dense Entity Retrieval,accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces a conceptually simple, scalable, and highly effective
BERT-based entity linking model, along with an extensive evaluation of its
accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm,
where each entity is defined only by a short textual description. The first
stage does retrieval in a dense space defined by a bi-encoder that
independently embeds the mention context and the entity descriptions. Each
candidate is then re-ranked with a cross-encoder, that concatenates the mention
and entity text. Experiments demonstrate that this approach is state of the art
on recent zero-shot benchmarks (6 point absolute gains) and also on more
established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative
simplicity (e.g. no explicit entity embeddings or manually engineered mention
tables). We also show that bi-encoder linking is very fast with nearest
neighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds),
and that much of the accuracy gain from the more expensive cross-encoder can be
transferred to the bi-encoder via knowledge distillation. Our code and models
are available at https://github.com/facebookresearch/BLINK.
","[{'version': 'v1', 'created': 'Sun, 10 Nov 2019 01:01:45 GMT'}, {'version': 'v2', 'created': 'Fri, 1 May 2020 10:06:20 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Sep 2020 08:13:47 GMT'}]",2020-09-30,"[['Wu', 'Ledell', ''], ['Petroni', 'Fabio', ''], ['Josifoski', 'Martin', ''], ['Riedel', 'Sebastian', ''], ['Zettlemoyer', 'Luke', '']]"
1260188,2003.09833,Jiwei Li,"Xiaoya Li, Yuxian Meng, Mingxin Zhou, Qinghong Han, Fei Wu and Jiwei
  Li","SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive
  Connection",To appear at NeurIPS 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While the self-attention mechanism has been widely used in a wide variety of
tasks, it has the unfortunate property of a quadratic cost with respect to the
input length, which makes it difficult to deal with long inputs. In this paper,
we present a method for accelerating and structuring self-attentions: Sparse
Adaptive Connection (SAC). In SAC, we regard the input sequence as a graph and
attention operations are performed between linked nodes. In contrast with
previous self-attention models with pre-defined structures (edges), the model
learns to construct attention edges to improve task-specific performances. In
this way, the model is able to select the most salient nodes and reduce the
quadratic complexity regardless of the sequence length. Based on SAC, we show
that previous variants of self-attention models are its special cases. Through
extensive experiments on neural machine translation, language modeling, graph
representation learning and image classification, we demonstrate SAC is
competitive with state-of-the-art models while significantly reducing memory
cost.
","[{'version': 'v1', 'created': 'Sun, 22 Mar 2020 07:58:44 GMT'}, {'version': 'v2', 'created': 'Sat, 11 Apr 2020 08:23:41 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Sep 2020 08:01:23 GMT'}]",2020-09-30,"[['Li', 'Xiaoya', ''], ['Meng', 'Yuxian', ''], ['Zhou', 'Mingxin', ''], ['Han', 'Qinghong', ''], ['Wu', 'Fei', ''], ['Li', 'Jiwei', '']]"
1349869,2009.08366,Daya Guo,"Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu,
  Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao
  Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
  Ming Zhou",GraphCodeBERT: Pre-training Code Representations with Data Flow,,,,,cs.SE cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained models for programming language have achieved dramatic empirical
improvements on a variety of code-related tasks such as code search, code
completion, code summarization, etc. However, existing pre-trained models
regard a code snippet as a sequence of tokens, while ignoring the inherent
structure of code, which provides crucial code semantics and would enhance the
code understanding process. We present GraphCodeBERT, a pre-trained model for
programming language that considers the inherent structure of code. Instead of
taking syntactic-level structure of code like abstract syntax tree (AST), we
use data flow in the pre-training stage, which is a semantic-level structure of
code that encodes the relation of ""where-the-value-comes-from"" between
variables. Such a semantic-level structure is neat and does not bring an
unnecessarily deep hierarchy of AST, the property of which makes the model more
efficient. We develop GraphCodeBERT based on Transformer. In addition to using
the task of masked language modeling, we introduce two structure-aware
pre-training tasks. One is to predict code structure edges, and the other is to
align representations between source code and code structure. We implement the
model in an efficient way with a graph-guided masked attention function to
incorporate the code structure. We evaluate our model on four tasks, including
code search, clone detection, code translation, and code refinement. Results
show that code structure and newly introduced pre-training tasks can improve
GraphCodeBERT and achieves state-of-the-art performance on the four downstream
tasks. We further show that the model prefers structure-level attentions over
token-level attentions in the task of code search.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 15:25:56 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 02:37:30 GMT'}]",2020-09-30,"[['Guo', 'Daya', ''], ['Ren', 'Shuo', ''], ['Lu', 'Shuai', ''], ['Feng', 'Zhangyin', ''], ['Tang', 'Duyu', ''], ['Liu', 'Shujie', ''], ['Zhou', 'Long', ''], ['Duan', 'Nan', ''], ['Svyatkovskiy', 'Alexey', ''], ['Fu', 'Shengyu', ''], ['Tufano', 'Michele', ''], ['Deng', 'Shao Kun', ''], ['Clement', 'Colin', ''], ['Drain', 'Dawn', ''], ['Sundaresan', 'Neel', ''], ['Yin', 'Jian', ''], ['Jiang', 'Daxin', ''], ['Zhou', 'Ming', '']]"
930756,1801.01825,Danish Contractor,"Danish Contractor, Barun Patra, Mausam Singla, Parag Singla","Towards Understanding and Answering Multi-Sentence Recommendation
  Questions on Tourism",,,10.1017/S1351324920000017,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce the first system towards the novel task of answering complex
multisentence recommendation questions in the tourism domain. Our solution uses
a pipeline of two modules: question understanding and answering. For question
understanding, we define an SQL-like query language that captures the semantic
intent of a question; it supports operators like subset, negation, preference
and similarity, which are often found in recommendation questions. We train and
compare traditional CRFs as well as bidirectional LSTM-based models for
converting a question to its semantic representation. We extend these models to
a semisupervised setting with partially labeled sequences gathered through
crowdsourcing. We find that our best model performs semi-supervised training of
BiDiLSTM+CRF with hand-designed features and CCM(Chang et al., 2007)
constraints. Finally, in an end to end QA system, our answering component
converts our question representation into queries fired on underlying knowledge
sources. Our experiments on two different answer corpora demonstrate that our
system can significantly outperform baselines with up to 20 pt higher accuracy
and 17 pt higher recall.
","[{'version': 'v1', 'created': 'Fri, 5 Jan 2018 16:38:05 GMT'}]",2020-09-30,"[['Contractor', 'Danish', ''], ['Patra', 'Barun', ''], ['Singla', 'Mausam', ''], ['Singla', 'Parag', '']]"
1336449,2008.08727,Aparna Elangovan,"Aparna Elangovan, Melissa Davis and Karin Verspoor","Assigning function to protein-protein interactions: a weakly supervised
  BioBERT based approach using PubMed abstracts",,,,,cs.CL cs.LG q-bio.GN,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Motivation: Protein-protein interactions (PPI) are critical to the function
of proteins in both normal and diseased cells, and many critical protein
functions are mediated by interactions.Knowledge of the nature of these
interactions is important for the construction of networks to analyse
biological data. However, only a small percentage of PPIs captured in protein
interaction databases have annotations of function available, e.g. only 4% of
PPI are functionally annotated in the IntAct database. Here, we aim to label
the function type of PPIs by extracting relationships described in PubMed
abstracts.
  Method: We create a weakly supervised dataset from the IntAct PPI database
containing interacting protein pairs with annotated function and associated
abstracts from the PubMed database. We apply a state-of-the-art deep learning
technique for biomedical natural language processing tasks, BioBERT, to build a
model - dubbed PPI-BioBERT - for identifying the function of PPIs. In order to
extract high quality PPI functions at large scale, we use an ensemble of
PPI-BioBERT models to improve uncertainty estimation and apply an interaction
type-specific threshold to counteract the effects of variations in the number
of training samples per interaction type.
  Results: We scan 18 million PubMed abstracts to automatically identify 3253
new typed PPIs, including phosphorylation and acetylation interactions, with an
overall precision of 46% (87% for acetylation) based on a human-reviewed
sample. This work demonstrates that analysis of biomedical abstracts for PPI
function extraction is a feasible approach to substantially increasing the
number of interactions annotated with function captured in online databases.
","[{'version': 'v1', 'created': 'Thu, 20 Aug 2020 01:42:28 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 09:57:29 GMT'}]",2020-09-30,"[['Elangovan', 'Aparna', ''], ['Davis', 'Melissa', ''], ['Verspoor', 'Karin', '']]"
1353516,2009.12013,Liyan Xu,"Liyan Xu, Jinho D. Choi",Revealing the Myth of Higher-Order Inference in Coreference Resolution,Accepted to EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper analyzes the impact of higher-order inference (HOI) on the task of
coreference resolution. HOI has been adapted by almost all recent coreference
resolution models without taking much investigation on its true effectiveness
over representation learning. To make a comprehensive analysis, we implement an
end-to-end coreference system as well as four HOI approaches, attended
antecedent, entity equalization, span clustering, and cluster merging, where
the latter two are our original methods. We find that given a high-performing
encoder such as SpanBERT, the impact of HOI is negative to marginal, providing
a new perspective of HOI to this task. Our best model using cluster merging
shows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 03:28:07 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Sep 2020 22:47:33 GMT'}]",2020-09-30,"[['Xu', 'Liyan', ''], ['Choi', 'Jinho D.', '']]"
1300422,2006.05908,Hansi Hettiarachchi,"Hansi Hettiarachchi, Mariam Adedoyin-Olowe, Jagdev Bhogal and Mohamed
  Medhat Gaber","Embed2Detect: Temporally Clustered Embedded Words for Event Detection in
  Social Media","Submitted to Journal of Machine Learning, Springer",,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Event detection in social media refers to automatic identification of
important information shared in social media platforms on a certain time.
Considering the dynamic nature and high volume of data production in data
streams, it is impractical to filter the events manually. Therefore, it is
important to have an automated mechanism to detect events in order to utilise
social media data effectively. Analysing the available literature, most of the
existing event detection methods are only focused on statistical and
syntactical features in data, even though the underlying semantics are also
important for an effective information retrieval from text, because they
describe the connections between words and their meanings. In this paper, we
propose a novel method termed Embed2Detect for event detection in social media
by combining the characteristics in prediction-based word embeddings and
hierarchical agglomerative clustering. The adoption of prediction-based word
embeddings incorporates the semantical features in the text to overcome a major
limitation available with previous approaches. This method is experimented on
two recent social media data sets which represent the sports and politics
domains. The results obtained from the experiments reveal that our approach is
capable of effective and efficient event detection with the proof of
significant improvements over baselines. For sports data set, Embed2Detect
achieved 27% higher F-measure than the best performed baseline method and for
political data set, it was an increase by 29%.
","[{'version': 'v1', 'created': 'Wed, 10 Jun 2020 15:52:52 GMT'}, {'version': 'v2', 'created': 'Thu, 11 Jun 2020 08:47:29 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Sep 2020 13:48:35 GMT'}]",2020-09-30,"[['Hettiarachchi', 'Hansi', ''], ['Adedoyin-Olowe', 'Mariam', ''], ['Bhogal', 'Jagdev', ''], ['Gaber', 'Mohamed Medhat', '']]"
1353559,2009.12056,Xuguang Wang,"Xuguang Wang, Linjun Shou, Ming Gong, Nan Duan and Daxin Jiang","No Answer is Better Than Wrong Answer: A Reflection Model for Document
  Level Machine Reading Comprehension",Accepted by Findings of EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Natural Questions (NQ) benchmark set brings new challenges to Machine
Reading Comprehension: the answers are not only at different levels of
granularity (long and short), but also of richer types (including no-answer,
yes/no, single-span and multi-span). In this paper, we target at this challenge
and handle all answer types systematically. In particular, we propose a novel
approach called Reflection Net which leverages a two-step training procedure to
identify the no-answer and wrong-answer cases. Extensive experiments are
conducted to verify the effectiveness of our approach. At the time of paper
writing (May.~20,~2020), our approach achieved the top 1 on both long and short
answer leaderboard, with F1 scores of 77.2 and 64.1, respectively.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 06:57:52 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 09:29:57 GMT'}]",2020-09-30,"[['Wang', 'Xuguang', ''], ['Shou', 'Linjun', ''], ['Gong', 'Ming', ''], ['Duan', 'Nan', ''], ['Jiang', 'Daxin', '']]"
1355105,2009.13602,Mahboobeh Parsapoor,"Jonathan Smith, Borna Ghotbi, Seungeun Yi, Mahboobeh Parsapoor",Non-Pharmaceutical Intervention Discovery with Topic Modeling,ML for Global Health (ICML 2020 Workshop),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the task of discovering categories of non-pharmaceutical
interventions during the evolving COVID-19 pandemic. We explore topic modeling
on two corpora with national and international scope. These models discover
existing categories when compared with human intervention labels while reduced
human effort needed.
","[{'version': 'v1', 'created': 'Thu, 10 Sep 2020 11:37:00 GMT'}]",2020-09-30,"[['Smith', 'Jonathan', ''], ['Ghotbi', 'Borna', ''], ['Yi', 'Seungeun', ''], ['Parsapoor', 'Mahboobeh', '']]"
1355158,2009.13655,Armen Aghajanyan,"Armen Aghajanyan, Jean Maillard, Akshat Shrivastava, Keith Diedrick,
  Mike Haeger, Haoran Li, Yashar Mehdad, Ves Stoyanov, Anuj Kumar, Mike Lewis,
  Sonal Gupta",Conversational Semantic Parsing,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The structured representation for semantic parsing in task-oriented assistant
systems is geared towards simple understanding of one-turn queries. Due to the
limitations of the representation, the session-based properties such as
co-reference resolution and context carryover are processed downstream in a
pipelined system. In this paper, we propose a semantic representation for such
task-oriented conversational systems that can represent concepts such as
co-reference and context carryover, enabling comprehensive understanding of
queries in a session. We release a new session-based, compositional
task-oriented parsing dataset of 20k sessions consisting of 60k utterances.
Unlike Dialog State Tracking Challenges, the queries in the dataset have
compositional forms. We propose a new family of Seq2Seq models for the
session-based parsing above, which achieve better or comparable performance to
the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve
the best known results on DSTC2 by up to 5 points for slot-carryover.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 22:08:00 GMT'}]",2020-09-30,"[['Aghajanyan', 'Armen', ''], ['Maillard', 'Jean', ''], ['Shrivastava', 'Akshat', ''], ['Diedrick', 'Keith', ''], ['Haeger', 'Mike', ''], ['Li', 'Haoran', ''], ['Mehdad', 'Yashar', ''], ['Stoyanov', 'Ves', ''], ['Kumar', 'Anuj', ''], ['Lewis', 'Mike', ''], ['Gupta', 'Sonal', '']]"
1314394,2007.02880,Prakamya Mishra,Prakamya Mishra and Pranav Mathur,"Contextualized Spoken Word Representations from Convolutional
  Autoencoders",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A lot of work has been done to build text-based language models for
performing different NLP tasks, but not much research has been done in the case
of audio-based language models. This paper proposes a Convolutional Autoencoder
based neural architecture to model syntactically and semantically adequate
contextualized representations of varying length spoken words. The use of such
representations can not only lead to great advances in the audio-based NLP
tasks but can also curtail the loss of information like tone, expression,
accent, etc while converting speech to text to perform these tasks. The
performance of the proposed model is validated by (1) examining the generated
vector space, and (2) evaluating its performance on three benchmark datasets
for measuring word similarities, against existing widely used text-based
language models that are trained on the transcriptions. The proposed model was
able to demonstrate its robustness when compared to the other two
language-based models.
","[{'version': 'v1', 'created': 'Mon, 6 Jul 2020 16:48:11 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 17:31:34 GMT'}]",2020-09-30,"[['Mishra', 'Prakamya', ''], ['Mathur', 'Pranav', '']]"
1301797,2006.07283,Shihan Wang,"Shihan Wang, Marijn Schraagen, Erik Tjong Kim Sang and Mehdi Dastani","Dutch General Public Reaction on Governmental COVID-19 Measures and
  Announcements in Twitter Data","17 pages, 8 figures",,,,cs.SI cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Public sentiment (the opinions, attitudes or feelings expressed by the
public) is a factor of interest for government, as it directly influences the
implementation of policies. Given the unprecedented nature of the COVID-19
crisis, having an up-to-date representation of public sentiment on governmental
measures and announcements is crucial. While the 'staying-at-home' policy makes
face-to-face interactions and interviews challenging, analysing real-time
Twitter data that reflects public opinion toward policy measures is a
cost-effective way to access public sentiment. In this context, we collect
streaming data using the Twitter API starting from the COVID-19 outbreak in the
Netherlands in February 2020, and track Dutch general public reactions on
governmental measures and announcements. We provide temporal analysis of tweet
frequency and public sentiment over the past seven months. We also identify
public attitudes towards two Dutch policies in case studies: one regarding
social distancing and one regarding wearing face masks. By presenting those
preliminary results, we aim to provide visibility into the social media
discussions around COVID-19 to the general public, scientists and policy
makers. The data collection and analysis will be updated and expanded over
time.
","[{'version': 'v1', 'created': 'Fri, 12 Jun 2020 16:03:58 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 10:10:35 GMT'}]",2020-09-30,"[['Wang', 'Shihan', ''], ['Schraagen', 'Marijn', ''], ['Sang', 'Erik Tjong Kim', ''], ['Dastani', 'Mehdi', '']]"
1355106,2009.13603,Fangyu Liu,"Fangyu Liu, Muhao Chen, Dan Roth, Nigel Collier",Visual Pivoting for (Unsupervised) Entity Alignment,Preprint. 11 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This work studies the use of visual semantic representations to align
entities in heterogeneous knowledge graphs (KGs). Images are natural components
of many existing KGs. By combining visual knowledge with other auxiliary
information, we show that the proposed new approach, EVA, creates a holistic
entity representation that provides strong signals for cross-graph entity
alignment. Besides, previous entity alignment methods require human labelled
seed alignment, restricting availability. EVA provides a completely
unsupervised solution by leveraging the visual similarity of entities to create
an initial seed dictionary (visual pivots). Experiments on benchmark data sets
DBP15k and DWY15k show that EVA offers state-of-the-art performance on both
monolingual and cross-lingual entity alignment tasks. Furthermore, we discover
that images are particularly useful to align long-tail KG entities, which
inherently lack the structural contexts necessary for capturing the
correspondences.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 20:09:40 GMT'}]",2020-09-30,"[['Liu', 'Fangyu', ''], ['Chen', 'Muhao', ''], ['Roth', 'Dan', ''], ['Collier', 'Nigel', '']]"
1354563,2009.13060,Duc Huy Huynh,"Huy Duc Huynh, Hang Thi-Thuy Do, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen","A Simple and Efficient Ensemble Classifier Combining Multiple Neural
  Network Models on Social Media Datasets in Vietnamese","Accepted by The 34th Pacific Asia Conference on Language, Information
  and Computation (PACLIC2020)",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Text classification is a popular topic of natural language processing, which
has currently attracted numerous research efforts worldwide. The significant
increase of data in social media requires the vast attention of researchers to
analyze such data. There are various studies in this field in many languages
but limited to the Vietnamese language. Therefore, this study aims to classify
Vietnamese texts on social media from three different Vietnamese benchmark
datasets. Advanced deep learning models are used and optimized in this study,
including CNN, LSTM, and their variants. We also implement the BERT, which has
never been applied to the datasets. Our experiments find a suitable model for
classification tasks on each specific dataset. To take advantage of single
models, we propose an ensemble model, combining the highest-performance models.
Our single models reach positive results on each dataset. Moreover, our
ensemble model achieves the best performance on all three datasets. We reach
86.96% of F1- score for the HSD-VLSP dataset, 65.79% of F1-score for the
UIT-VSMEC dataset, 92.79% and 89.70% for sentiments and topics on the UIT-VSFC
dataset, respectively. Therefore, our models achieve better performances as
compared to previous studies on these datasets.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 04:28:48 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 01:32:26 GMT'}]",2020-09-30,"[['Huynh', 'Huy Duc', ''], ['Do', 'Hang Thi-Thuy', ''], ['Van Nguyen', 'Kiet', ''], ['Nguyen', 'Ngan Luu-Thuy', '']]"
1355161,2009.13658,Zhiheng Huang,"Zhiheng Huang, Davis Liang, Peng Xu, Bing Xiang",Improve Transformer Models with Better Relative Position Embeddings,Accepted as Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer architectures rely on explicit position encodings in order to
preserve a notion of word order. In this paper, we argue that existing work
does not fully utilize position information. For example, the initial proposal
of a sinusoid embedding is fixed and not learnable. In this paper, we first
review absolute position embeddings and existing methods for relative position
embeddings. We then propose new techniques that encourage increased interaction
between query, key and relative position embeddings in the self-attention
mechanism. Our most promising approach is a generalization of the absolute
position embedding, improving results on SQuAD1.1 compared to previous position
embeddings approaches. In addition, we address the inductive property of
whether a position embedding can be robust enough to handle long sequences. We
demonstrate empirically that our relative position embedding method is
reasonably generalized and robust from the inductive perspective. Finally, we
show that our proposed method can be adopted as a near drop-in replacement for
improving the accuracy of large models with a small computational budget.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 22:18:58 GMT'}]",2020-09-30,"[['Huang', 'Zhiheng', ''], ['Liang', 'Davis', ''], ['Xu', 'Peng', ''], ['Xiang', 'Bing', '']]"
1279050,2004.14280,Jind\v{r}ich Libovick\'y,"Jind\v{r}ich Libovick\'y, Alexander Fraser","Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning
  Subword Systems","8 pages, 1 figure; Accepted to EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Applying the Transformer architecture on the character level usually requires
very deep architectures that are difficult and slow to train. These problems
can be partially overcome by incorporating a segmentation into tokens in the
model. We show that by initially training a subword model and then finetuning
it on characters, we can obtain a neural machine translation model that works
at the character level without requiring token segmentation. We use only the
vanilla 6-layer Transformer Base architecture. Our character-level models
better capture morphological phenomena and show more robustness to noise at the
expense of somewhat worse overall translation quality. Our study is a
significant step towards high-performance and easy to train character-based
models that are not extremely large.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 15:56:02 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 14:46:28 GMT'}]",2020-09-30,"[['Libovický', 'Jindřich', ''], ['Fraser', 'Alexander', '']]"
1355362,2009.13859,Inna Vogel,Inna Vogel and Meghana Meghana,"Fake News Spreader Detection on Twitter using Character N-Grams.
  Notebook for PAN at CLEF 2020","CLEF 2020 Labs and Workshops, Notebook Papers",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The authors of fake news often use facts from verified news sources and mix
them with misinformation to create confusion and provoke unrest among the
readers. The spread of fake news can thereby have serious implications on our
society. They can sway political elections, push down the stock price or crush
reputations of corporations or public figures. Several websites have taken on
the mission of checking rumors and allegations, but are often not fast enough
to check the content of all the news being disseminated. Especially social
media websites have offered an easy platform for the fast propagation of
information. Towards limiting fake news from being propagated among social
media users, the task of this year's PAN 2020 challenge lays the focus on the
fake news spreaders. The aim of the task is to determine whether it is possible
to discriminate authors that have shared fake news in the past from those that
have never done it. In this notebook, we describe our profiling system for the
fake news detection task on Twitter. For this, we conduct different feature
extraction techniques and learning experiments from a multilingual perspective,
namely English and Spanish. Our final submitted systems use character n-grams
as features in combination with a linear SVM for English and Logistic
Regression for the Spanish language. Our submitted models achieve an overall
accuracy of 73% and 79% on the English and Spanish official test set,
respectively. Our experiments show that it is difficult to differentiate
solidly fake news spreaders on Twitter from users who share credible
information leaving room for further investigations. Our model ranked 3rd out
of 72 competitors.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 08:32:32 GMT'}]",2020-09-30,"[['Vogel', 'Inna', ''], ['Meghana', 'Meghana', '']]"
1355619,2009.14116,Pawan Kumar,Pawan Kumar and Srikanta Bedathur,A Survey on Semantic Parsing from the perspective of Compositionality,,,,,cs.CL cs.GL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Different from previous surveys in semantic parsing (Kamath and Das, 2018)
and knowledge base question answering(KBQA)(Chakraborty et al., 2019; Zhu et
al., 2019; Hoffner et al., 2017) we try to takes a different perspective on the
study of semantic parsing. Specifically, we will focus on (a)meaning
composition from syntactical structure(Partee, 1975), and (b) the ability of
semantic parsers to handle lexical variation given the context of a knowledge
base (KB). In the following section after an introduction of the field of
semantic parsing and its uses in KBQA, we will describe meaning representation
using grammar formalism CCG (Steedman, 1996). We will discuss semantic
composition using formal languages in Section 2. In section 3 we will consider
systems that uses formal languages e.g. $\lambda$-calculus (Steedman, 1996),
$\lambda$-DCS (Liang, 2013). Section 4 and 5 consider semantic parser using
structured-language for logical form. Section 6 is on different benchmark
datasets ComplexQuestions (Bao et al.,2016) and GraphQuestions (Su et al.,
2016) that can be used to evaluate semantic parser on their ability to answer
complex questions that are highly compositional in nature.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 16:03:13 GMT'}]",2020-09-30,"[['Kumar', 'Pawan', ''], ['Bedathur', 'Srikanta', '']]"
1355627,2009.14124,Ethan Chau,"Ethan C. Chau, Lucy H. Lin, Noah A. Smith","Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank",Accepted to Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained multilingual contextual representations have shown great success,
but due to the limits of their pretraining data, their benefits do not apply
equally to all language varieties. This presents a challenge for language
varieties unfamiliar to these models, whose labeled \emph{and unlabeled} data
is too limited to train a monolingual model effectively. We propose the use of
additional language-specific pretraining and vocabulary augmentation to adapt
multilingual models to low-resource settings. Using dependency parsing of four
diverse low-resource language varieties as a case study, we show that these
methods significantly improve performance over baselines, especially in the
lowest-resource cases, and demonstrate the importance of the relationship
between such models' pretraining data and target language varieties.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 16:12:52 GMT'}]",2020-09-30,"[['Chau', 'Ethan C.', ''], ['Lin', 'Lucy H.', ''], ['Smith', 'Noah A.', '']]"
1355670,2009.14167,Zhe Gan,"Siqi Sun, Zhe Gan, Yu Cheng, Yuwei Fang, Shuohang Wang, Jingjing Liu","Contrastive Distillation on Intermediate Representations for Language
  Model Compression",Accepted by EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing language model compression methods mostly use a simple L2 loss to
distill knowledge in the intermediate representations of a large BERT model to
a smaller one. Although widely used, this objective by design assumes that all
the dimensions of hidden representations are independent, failing to capture
important structural knowledge in the intermediate layers of the teacher
network. To achieve better distillation efficacy, we propose Contrastive
Distillation on Intermediate Representations (CoDIR), a principled knowledge
distillation framework where the student is trained to distill knowledge
through intermediate layers of the teacher via a contrastive objective. By
learning to distinguish positive sample from a large set of negative samples,
CoDIR facilitates the student's exploitation of rich information in teacher's
hidden layers. CoDIR can be readily applied to compress large-scale language
models in both pre-training and finetuning stages, and achieves superb
performance on the GLUE benchmark, outperforming state-of-the-art compression
methods.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 17:31:43 GMT'}]",2020-09-30,"[['Sun', 'Siqi', ''], ['Gan', 'Zhe', ''], ['Cheng', 'Yu', ''], ['Fang', 'Yuwei', ''], ['Wang', 'Shuohang', ''], ['Liu', 'Jingjing', '']]"
1355487,2009.13984,Raymond Lee,"Nuobei Shi, Qin Zeng and Raymond Lee","The design and implementation of Language Learning Chatbot with XAI
  using Ontology and Transfer Learning","19 pages, 20 figures, published paper in International Conference on
  NLP & Big Data (NLPD 2020)","Dhinaharan Nagamalai et al. (Eds): CSEIT, WiMoNe, NCS, CIoT, CMLA,
  DMSE, NLPD - 2020 pp. 305-323, 2020. CS & IT - CSCP 2020",,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we proposed a transfer learning-based English language
learning chatbot, whose output generated by GPT-2 can be explained by
corresponding ontology graph rooted by fine-tuning dataset. We design three
levels for systematically English learning, including phonetics level for
speech recognition and pronunciation correction, semantic level for specific
domain conversation, and the simulation of free-style conversation in English -
the highest level of language chatbot communication as free-style conversation
agent. For academic contribution, we implement the ontology graph to explain
the performance of free-style conversation, following the concept of XAI
(Explainable Artificial Intelligence) to visualize the connections of neural
network in bionics, and explain the output sentence from language model. From
implementation perspective, our Language Learning agent integrated the
mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer
learning as back-end to interpret the responses by ontology graph.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 13:11:40 GMT'}]",2020-09-30,"[['Shi', 'Nuobei', ''], ['Zeng', 'Qin', ''], ['Lee', 'Raymond', '']]"
1355475,2009.13972,Xuemeng Hu,"Deyu Zhou, Xuemeng Hu, Rui Wang",Neural Topic Modeling by Incorporating Document Relationship Graph,Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Graph Neural Networks (GNNs) that capture the relationships between graph
nodes via message passing have been a hot research direction in the natural
language processing community. In this paper, we propose Graph Topic Model
(GTM), a GNN based neural topic model that represents a corpus as a document
relationship graph. Documents and words in the corpus become nodes in the graph
and are connected based on document-word co-occurrences. By introducing the
graph structure, the relationships between documents are established through
their shared words and thus the topical representation of a document is
enriched by aggregating information from its neighboring nodes using graph
convolution. Extensive experiments on three datasets were conducted and the
results demonstrate the effectiveness of the proposed approach.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 12:45:55 GMT'}]",2020-09-30,"[['Zhou', 'Deyu', ''], ['Hu', 'Xuemeng', ''], ['Wang', 'Rui', '']]"
1355474,2009.13971,Xuemeng Hu,"Xuemeng Hu, Rui Wang, Deyu Zhou, Yuxuan Xiong",Neural Topic Modeling with Cycle-Consistent Adversarial Training,Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Advances on deep generative models have attracted significant research
interest in neural topic modeling. The recently proposed Adversarial-neural
Topic Model models topics with an adversarially trained generator network and
employs Dirichlet prior to capture the semantic patterns in latent topics. It
is effective in discovering coherent topics but unable to infer topic
distributions for given documents or utilize available document labels. To
overcome such limitations, we propose Topic Modeling with Cycle-consistent
Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT
employs a generator network to interpret topics and an encoder network to infer
document topics. Adversarial training and cycle-consistent constraints are used
to encourage the generator and the encoder to produce realistic samples that
coordinate with each other. sToMCAT extends ToMCAT by incorporating document
labels into the topic modeling process to help discover more coherent topics.
The effectiveness of the proposed models is evaluated on
unsupervised/supervised topic modeling and text classification. The
experimental results show that our models can produce both coherent and
informative topics, outperforming a number of competitive baselines.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 12:41:27 GMT'}]",2020-09-30,"[['Hu', 'Xuemeng', ''], ['Wang', 'Rui', ''], ['Zhou', 'Deyu', ''], ['Xiong', 'Yuxuan', '']]"
1355408,2009.13905,Jacopo Amidei,Jacopo Amidei,Aligning Intraobserver Agreement by Transitivity,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Annotation reproducibility and accuracy rely on good consistency within
annotators. We propose a novel method for measuring within annotator
consistency or annotator Intraobserver Agreement (IA). The proposed approach is
based on transitivity, a measure that has been thoroughly studied in the
context of rational decision-making. The transitivity measure, in contrast with
the commonly used test-retest strategy for annotator IA, is less sensitive to
the several types of bias introduced by the test-retest strategy. We present a
representation theorem to the effect that relative judgement data that meet
transitivity can be mapped to a scale (in terms of measurement theory). We also
discuss a further application of transitivity as part of data collection design
for addressing the problem of the quadratic complexity of data collection of
relative judgements.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 09:55:04 GMT'}]",2020-09-30,"[['Amidei', 'Jacopo', '']]"
1355392,2009.13889,Ferdiant Joshua Muis,"Ferdiant Joshua Muis (1) and Ayu Purwarianti (1 and 2) ((1) Institut
  Teknologi Bandung, (2) U-CoE AI-VLB)","Sequence-to-Sequence Learning for Indonesian Automatic Question
  Generator","6 pages, 4 figures","2020 International Conference on Advanced Informatics: Concept,
  Theory and Application (ICAICTA)",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic question generation is defined as the task of automating the
creation of question given a various of textual data. Research in automatic
question generator (AQG) has been conducted for more than 10 years, mainly
focused on factoid question. In all these studies, the state-of-the-art is
attained using sequence-to-sequence approach. However, AQG system for
Indonesian has not ever been researched intensely. In this work we construct an
Indonesian automatic question generator, adapting the architecture from some
previous works. In summary, we used sequence-to-sequence approach using BiGRU,
BiLSTM, and Transformer with additional linguistic features, copy mechanism,
and coverage mechanism. Since there is no public large dan popular Indonesian
dataset for question generation, we translated SQuAD v2.0 factoid question
answering dataset, with additional Indonesian TyDiQA dev set for testing. The
system achieved BLEU1, BLEU2, BLEU3, BLEU4, and ROUGE-L score at 38,35, 20,96,
10,68, 5,78, and 43,4 for SQuAD, and 39.9, 20.78, 10.26, 6.31, 44.13 for
TyDiQA, respectively. The system performed well when the expected answers are
named entities and are syntactically close with the context explaining them.
Additionally, from native Indonesian perspective, the best questions generated
by our best models on their best cases are acceptable and reasonably useful.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 09:25:54 GMT'}]",2020-09-30,"[['Muis', 'Ferdiant Joshua', '', '1 and 2'], ['Purwarianti', 'Ayu', '', '1 and 2']]"
1355348,2009.13845,Xi Victoria Lin,"Tao Yu and Chien-Sheng Wu and Xi Victoria Lin and Bailin Wang and Yi
  Chern Tan and Xinyi Yang and Dragomir Radev and Richard Socher and Caiming
  Xiong",GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing,14 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present GraPPa, an effective pre-training approach for table semantic
parsing that learns a compositional inductive bias in the joint representations
of textual and tabular data. We construct synthetic question-SQL pairs over
high-quality tables via a synchronous context-free grammar (SCFG) induced from
existing text-to-SQL datasets. We pre-train our model on the synthetic data
using a novel text-schema linking objective that predicts the syntactic role of
a table field in the SQL for each question-SQL pair. To maintain the model's
ability to represent real-world data, we also include masked language modeling
(MLM) over several existing table-and-language datasets to regularize the
pre-training process. On four popular fully supervised and weakly supervised
table semantic parsing benchmarks, GraPPa significantly outperforms
RoBERTa-large as the feature representation layers and establishes new
state-of-the-art results on all of them.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 08:17:58 GMT'}]",2020-09-30,"[['Yu', 'Tao', ''], ['Wu', 'Chien-Sheng', ''], ['Lin', 'Xi Victoria', ''], ['Wang', 'Bailin', ''], ['Tan', 'Yi Chern', ''], ['Yang', 'Xinyi', ''], ['Radev', 'Dragomir', ''], ['Socher', 'Richard', ''], ['Xiong', 'Caiming', '']]"
1248180,2002.10757,Shiyao Cui,"Shiyao Cui, Bowen Yu, Tingwen Liu, Zhenyu Zhang, Xuebin Wang and
  Jinqiao Shi","Edge-Enhanced Graph Convolution Networks for Event Detection with
  Syntactic Relation",Accepted to Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Event detection (ED), a key subtask of information extraction, aims to
recognize instances of specific event types in text. Previous studies on the
task have verified the effectiveness of integrating syntactic dependency into
graph convolutional networks. However, these methods usually ignore dependency
label information, which conveys rich and useful linguistic knowledge for ED.
In this paper, we propose a novel architecture named Edge-Enhanced Graph
Convolution Networks (EE-GCN), which simultaneously exploits syntactic
structure and typed dependency label information to perform ED. Specifically,
an edge-aware node update module is designed to generate expressive word
representations by aggregating syntactically-connected words through specific
dependency types. Furthermore, to fully explore clues hidden in dependency
edges, a node-aware edge update module is introduced, which refines the
relation representations with contextual information. These two modules are
complementary to each other and work in a mutual promotion way. We conduct
experiments on the widely used ACE2005 dataset and the results show significant
improvement over competitive baseline methods.
","[{'version': 'v1', 'created': 'Tue, 25 Feb 2020 09:18:26 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 06:19:52 GMT'}]",2020-09-30,"[['Cui', 'Shiyao', ''], ['Yu', 'Bowen', ''], ['Liu', 'Tingwen', ''], ['Zhang', 'Zhenyu', ''], ['Wang', 'Xuebin', ''], ['Shi', 'Jinqiao', '']]"
1355330,2009.13827,Jiaming Shen,"Jiaming Shen and Wenda Qiu and Jingbo Shang and Michelle Vanni and
  Xiang Ren and Jiawei Han","SynSetExpan: An Iterative Framework for Joint Entity Set Expansion and
  Synonym Discovery",EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Entity set expansion and synonym discovery are two critical NLP tasks.
Previous studies accomplish them separately, without exploring their
interdependencies. In this work, we hypothesize that these two tasks are
tightly coupled because two synonymous entities tend to have similar
likelihoods of belonging to various semantic classes. This motivates us to
design SynSetExpan, a novel framework that enables two tasks to mutually
enhance each other. SynSetExpan uses a synonym discovery model to include
popular entities' infrequent synonyms into the set, which boosts the set
expansion recall. Meanwhile, the set expansion model, being able to determine
whether an entity belongs to a semantic class, can generate pseudo training
data to fine-tune the synonym discovery model towards better accuracy. To
facilitate the research on studying the interplays of these two tasks, we
create the first large-scale Synonym-Enhanced Set Expansion (SE2) dataset via
crowdsourcing. Extensive experiments on the SE2 dataset and previous benchmarks
demonstrate the effectiveness of SynSetExpan for both entity set expansion and
synonym discovery tasks.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 07:32:17 GMT'}]",2020-09-30,"[['Shen', 'Jiaming', ''], ['Qiu', 'Wenda', ''], ['Shang', 'Jingbo', ''], ['Vanni', 'Michelle', ''], ['Ren', 'Xiang', ''], ['Han', 'Jiawei', '']]"
1355329,2009.13826,Yanlin Li,"Yanlin Li, Shi An, Ruisheng Zhang",EEMC: Embedding Enhanced Multi-tag Classification,,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recently occurred representation learning make an attractive performance
in NLP and complex network, it is becoming a fundamental technology in machine
learning and data mining. How to use representation learning to improve the
performance of classifiers is a very significance research direction. We using
representation learning technology to map raw data(node of graph) to a
low-dimensional feature space. In this space, each raw data obtained a lower
dimensional vector representation, we do some simple linear operations for
those vectors to produce some virtual data, using those vectors and virtual
data to training multi-tag classifier. After that we measured the performance
of classifier by F1 score(Macro% F1 and Micro% F1). Our method make Macro F1
rise from 28 % - 450% and make average F1 score rise from 12 % - 224%. By
contrast, we trained the classifier directly with the lower dimensional vector,
and measured the performance of classifiers. We validate our algorithm on three
public data sets, we found that the virtual data helped the classifier greatly
improve the F1 score. Therefore, our algorithm is a effective way to improve
the performance of classifier. These result suggest that the virtual data
generated by simple linear operation, in representation space, still retains
the information of the raw data. It's also have great significance to the
learning of small sample data sets.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 07:29:34 GMT'}]",2020-09-30,"[['Li', 'Yanlin', ''], ['An', 'Shi', ''], ['Zhang', 'Ruisheng', '']]"
1355586,2009.14083,Vu Tran,Vu Tran and Minh Le Nguyen and Ken Satoh,"Building Legal Case Retrieval Systems with Lexical Matching and
  Summarization using A Pre-Trained Phrase Scoring Model",,,10.1145/3322640.3326740,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present our method for tackling the legal case retrieval task of the
Competition on Legal Information Extraction/Entailment 2019. Our approach is
based on the idea that summarization is important for retrieval. On one hand,
we adopt a summarization based model called encoded summarization which encodes
a given document into continuous vector space which embeds the summary
properties of the document. We utilize the resource of COLIEE 2018 on which we
train the document representation model. On the other hand, we extract lexical
features on different parts of a given query and its candidates. We observe
that by comparing different parts of the query and its candidates, we can
achieve better performance. Furthermore, the combination of the lexical
features with latent features by the summarization-based method achieves even
better performance. We have achieved the state-of-the-art result for the task
on the benchmark of the competition.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 15:10:59 GMT'}]",2020-09-30,"[['Tran', 'Vu', ''], ['Nguyen', 'Minh Le', ''], ['Satoh', 'Ken', '']]"
1355255,2009.13752,Shuang Zeng,"Shuang Zeng, Runxin Xu, Baobao Chang and Lei Li",Double Graph Based Reasoning for Document-level Relation Extraction,"Accepted as long paper to appear at the EMNLP 2020 main conference,
  11 pages, 3 figures",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Document-level relation extraction aims to extract relations among entities
within a document. Different from sentence-level relation extraction, it
requires reasoning over multiple sentences across a document. In this paper, we
propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs.
GAIN first constructs a heterogeneous mention-level graph (hMG) to model
complex interaction among different mentions across the document. It also
constructs an entity-level graph (EG), based on which we propose a novel path
reasoning mechanism to infer relations between entities. Experiments on the
public dataset, DocRED, show GAIN achieves a significant performance
improvement (2.85 on F1) over the previous state-of-the-art. Our code is
available at https://github.com/DreamInvoker/GAIN .
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 03:41:01 GMT'}]",2020-09-30,"[['Zeng', 'Shuang', ''], ['Xu', 'Runxin', ''], ['Chang', 'Baobao', ''], ['Li', 'Lei', '']]"
1224531,1912.12635,Konstantinos Kogkalidis,Konstantinos Kogkalidis and Michael Moortgat and Richard Moot,\AE THEL: Automatically Extracted Typelogical Derivations for Dutch,"8 pages plus abstract, LREC 2020","Proceedings of The 12th Language Resources and Evaluation
  Conference (2020)",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present {\AE}THEL, a semantic compositionality dataset for written Dutch.
{\AE}THEL consists of two parts. First, it contains a lexicon of supertags for
about 900 000 words in context. The supertags correspond to types of the simply
typed linear lambda-calculus, enhanced with dependency decorations that capture
grammatical roles supplementary to function-argument structures. On the basis
of these types, {\AE}THEL further provides 72 192 validated derivations,
presented in four formats: natural-deduction and sequent-style proofs, linear
logic proofnets and the associated programs (lambda terms) for meaning
composition. {\AE}THEL's types and derivations are obtained by means of an
extraction algorithm applied to the syntactic analyses of LASSY Small, the gold
standard corpus of written Dutch. We discuss the extraction algorithm and show
how `virtual elements' in the original LASSY annotation of unbounded
dependencies and coordination phenomena give rise to higher-order types. We
suggest some example usecases highlighting the benefits of a type-driven
approach at the syntax semantics interface. The following resources are
open-sourced with {\AE}THEL: the lexical mappings between words and types, a
subset of the dataset consisting of 7 924 semantic parses, and the Python code
that implements the extraction algorithm.
","[{'version': 'v1', 'created': 'Sun, 29 Dec 2019 11:31:11 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Mar 2020 15:26:59 GMT'}]",2020-09-30,"[['Kogkalidis', 'Konstantinos', ''], ['Moortgat', 'Michael', ''], ['Moot', 'Richard', '']]"
1351801,2009.10298,Paria Jamshid Lou,Paria Jamshid Lou and Mark Johnson,End-to-End Speech Recognition and Disfluency Removal,,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Disfluency detection is usually an intermediate step between an automatic
speech recognition (ASR) system and a downstream task. By contrast, this paper
aims to investigate the task of end-to-end speech recognition and disfluency
removal. We specifically explore whether it is possible to train an ASR model
to directly map disfluent speech into fluent transcripts, without relying on a
separate disfluency detection model. We show that end-to-end models do learn to
directly generate fluent transcripts; however, their performance is slightly
worse than a baseline pipeline approach consisting of an ASR system and a
disfluency detection model. We also propose two new metrics that can be used
for evaluating integrated ASR and disfluency models. The findings of this paper
can serve as a benchmark for further research on the task of end-to-end speech
recognition and disfluency removal in the future.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 03:11:37 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 08:13:32 GMT'}, {'version': 'v3', 'created': 'Mon, 28 Sep 2020 23:07:21 GMT'}]",2020-09-30,"[['Lou', 'Paria Jamshid', ''], ['Johnson', 'Mark', '']]"
1355223,2009.13720,Sharan Raja,"Sharan Raja, Rudraksh Tuwani","Adversarial Attacks Against Deep Learning Systems for ICD-9 Code
  Assignment",,,,,cs.LG cs.CL cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Manual annotation of ICD-9 codes is a time consuming and error-prone process.
Deep learning based systems tackling the problem of automated ICD-9 coding have
achieved competitive performance. Given the increased proliferation of
electronic medical records, such automated systems are expected to eventually
replace human coders. In this work, we investigate how a simple typo-based
adversarial attack strategy can impact the performance of state-of-the-art
models for the task of predicting the top 50 most frequent ICD-9 codes from
discharge summaries. Preliminary results indicate that a malicious adversary,
using gradient information, can craft specific perturbations, that appear as
regular human typos, for less than 3% of words in the discharge summary to
significantly affect the performance of the baseline model.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 01:45:11 GMT'}]",2020-09-30,"[['Raja', 'Sharan', ''], ['Tuwani', 'Rudraksh', '']]"
1355318,2009.13815,Yinfei Yang,"Yinfei Yang, Ning Jin, Kuo Lin, Mandy Guo, Daniel Cer","Neural Retrieval for Question Answering with Cross-Attention Supervised
  Data Augmentation",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural models that independently project questions and answers into a shared
embedding space allow for efficient continuous space retrieval from large
corpora. Independently computing embeddings for questions and answers results
in late fusion of information related to matching questions to their answers.
While critical for efficient retrieval, late fusion underperforms models that
make use of early fusion (e.g., a BERT based classifier with cross-attention
between question-answer pairs). We present a supervised data mining method
using an accurate early fusion model to improve the training of an efficient
late fusion retrieval model. We first train an accurate classification model
with cross-attention between questions and answers. The accurate
cross-attention model is then used to annotate additional passages in order to
generate weighted training examples for a neural retrieval model. The resulting
retrieval model with additional data significantly outperforms retrieval models
directly trained with gold annotations on Precision at $N$ (P@N) and Mean
Reciprocal Rank (MRR).
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 07:02:19 GMT'}]",2020-09-30,"[['Yang', 'Yinfei', ''], ['Jin', 'Ning', ''], ['Lin', 'Kuo', ''], ['Guo', 'Mandy', ''], ['Cer', 'Daniel', '']]"
1269930,2004.05160,Jind\v{r}ich Libovick\'y,"Jind\v{r}ich Libovick\'y, Rudolf Rosa, Alexander Fraser",On the Language Neutrality of Pre-trained Multilingual Representations,"12 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1911.03310. Accepted to Findings of EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual contextual embeddings, such as multilingual BERT and
XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work
probed the cross-linguality of the representations indirectly using zero-shot
transfer learning on morphological and syntactic tasks. We instead investigate
the language-neutrality of multilingual contextual embeddings directly and with
respect to lexical semantics. Our results show that contextual embeddings are
more language-neutral and, in general, more informative than aligned static
word-type embeddings, which are explicitly trained for language neutrality.
Contextual embeddings are still only moderately language-neutral by default, so
we propose two simple methods for achieving stronger language neutrality:
first, by unsupervised centering of the representation for each language and
second, by fitting an explicit projection on small parallel data. Besides, we
show how to reach state-of-the-art accuracy on language identification and
match the performance of statistical methods for word alignment of parallel
sentences without using parallel data.
","[{'version': 'v1', 'created': 'Thu, 9 Apr 2020 19:50:32 GMT'}, {'version': 'v2', 'created': 'Mon, 20 Apr 2020 11:44:10 GMT'}, {'version': 'v3', 'created': 'Thu, 23 Apr 2020 16:10:07 GMT'}, {'version': 'v4', 'created': 'Tue, 29 Sep 2020 18:48:19 GMT'}]",2020-10-01,"[['Libovický', 'Jindřich', ''], ['Rosa', 'Rudolf', ''], ['Fraser', 'Alexander', '']]"
1267956,2004.03186,Han Xu,"Xu Han, Tianyu Gao, Yankai Lin, Hao Peng, Yaoliang Yang, Chaojun Xiao,
  Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou","More Data, More Relations, More Context and More Openness: A Review and
  Outlook for Relation Extraction",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Relational facts are an important component of human knowledge, which are
hidden in vast amounts of text. In order to extract these facts from text,
people have been working on relation extraction (RE) for years. From early
pattern matching to current neural networks, existing RE methods have achieved
significant progress. Yet with explosion of Web text and emergence of new
relations, human knowledge is increasing drastically, and we thus require
""more"" from RE: a more powerful RE system that can robustly utilize more data,
efficiently learn more relations, easily handle more complicated context, and
flexibly generalize to more open domains. In this paper, we look back at
existing RE methods, analyze key challenges we are facing nowadays, and show
promising directions towards more powerful RE. We hope our view can advance
this field and inspire more efforts in the community.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 08:15:21 GMT'}, {'version': 'v2', 'created': 'Fri, 8 May 2020 11:56:52 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Sep 2020 09:15:29 GMT'}]",2020-10-01,"[['Han', 'Xu', ''], ['Gao', 'Tianyu', ''], ['Lin', 'Yankai', ''], ['Peng', 'Hao', ''], ['Yang', 'Yaoliang', ''], ['Xiao', 'Chaojun', ''], ['Liu', 'Zhiyuan', ''], ['Li', 'Peng', ''], ['Sun', 'Maosong', ''], ['Zhou', 'Jie', '']]"
1343546,2009.02043,Fredrik Olsson,"Fredrik Olsson, Magnus Sahlgren",Data Readiness for Natural Language Processing,,,,,cs.CY cs.AI cs.CL cs.DB cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This document concerns data readiness in the context of machine learning and
Natural Language Processing. It describes how an organization may proceed to
identify, make available, validate, and prepare data to facilitate automated
analysis methods. The contents of the document is based on the practical
challenges and frequently asked questions we have encountered in our work as an
applied research institute with helping organizations and companies, both in
the public and private sectors, to use data in their business processes.
","[{'version': 'v1', 'created': 'Fri, 4 Sep 2020 07:53:43 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 12:03:58 GMT'}]",2020-10-01,"[['Olsson', 'Fredrik', ''], ['Sahlgren', 'Magnus', '']]"
1164194,1908.06024,Pushkar Mishra,"Pushkar Mishra, Helen Yannakoudakis and Ekaterina Shutova",Tackling Online Abuse: A Survey of Automated Abuse Detection Methods,In preparation for Computational Linguistics,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Abuse on the Internet represents an important societal problem of our time.
Millions of Internet users face harassment, racism, personal attacks, and other
types of abuse on online platforms. The psychological effects of such abuse on
individuals can be profound and lasting. Consequently, over the past few years,
there has been a substantial research effort towards automated abuse detection
in the field of natural language processing (NLP). In this paper, we present a
comprehensive survey of the methods that have been proposed to date, thus
providing a platform for further development of this area. We describe the
existing datasets and review the computational approaches to abuse detection,
analyzing their strengths and limitations. We discuss the main trends that
emerge, highlight the challenges that remain, outline possible solutions, and
propose guidelines for ethics and explainability
","[{'version': 'v1', 'created': 'Tue, 13 Aug 2019 10:59:06 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 11:42:04 GMT'}]",2020-10-01,"[['Mishra', 'Pushkar', ''], ['Yannakoudakis', 'Helen', ''], ['Shutova', 'Ekaterina', '']]"
1337280,2008.09558,Artem Polyvyanyy,"Artem Polyvyanyy, Hanan Alkhammash, Claudio Di Ciccio, Luciano
  Garc\'ia-Ba\~nuelos, Anna Kalenkova, Sander J. J. Leemans, Jan Mendling,
  Alistair Moffat, Matthias Weidlich","Entropia: A Family of Entropy-Based Conformance Checking Measures for
  Process Mining",4 pages,,,,cs.AI cs.CL cs.FL cs.IT math.IT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a command-line tool, called Entropia, that implements a
family of conformance checking measures for process mining founded on the
notion of entropy from information theory. The measures allow quantifying
classical non-deterministic and stochastic precision and recall quality
criteria for process models automatically discovered from traces executed by
IT-systems and recorded in their event logs. A process model has ""good""
precision with respect to the log it was discovered from if it does not encode
many traces that are not part of the log, and has ""good"" recall if it encodes
most of the traces from the log. By definition, the measures possess useful
properties and can often be computed quickly.
","[{'version': 'v1', 'created': 'Fri, 21 Aug 2020 15:54:47 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 03:26:57 GMT'}]",2020-10-01,"[['Polyvyanyy', 'Artem', ''], ['Alkhammash', 'Hanan', ''], ['Di Ciccio', 'Claudio', ''], ['García-Bañuelos', 'Luciano', ''], ['Kalenkova', 'Anna', ''], ['Leemans', 'Sander J. J.', ''], ['Mendling', 'Jan', ''], ['Moffat', 'Alistair', ''], ['Weidlich', 'Matthias', '']]"
1279313,2004.14543,Linyang Li,"Linyang Li, Xipeng Qiu","TAVAT: Token-Aware Virtual Adversarial Training for Language
  Understanding",9 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Gradient-based adversarial training is widely used in improving the
robustness of neural networks, while it cannot be easily adapted to natural
language processing tasks since the embedding space is discrete. In natural
language processing fields, virtual adversarial training is introduced since
texts are discrete and cannot be perturbed by gradients directly.
Alternatively, virtual adversarial training, which generates perturbations on
the embedding space, is introduced in NLP tasks. Despite its success, existing
virtual adversarial training methods generate perturbations roughly constrained
by Frobenius normalization balls. To craft fine-grained perturbations, we
propose a Token-Aware Virtual Adversarial Training method. We introduce a
token-level accumulated perturbation vocabulary to initialize the perturbations
better and use a token-level normalization ball to constrain these
perturbations pertinently. Experiments show that our method improves the
performance of pre-trained models such as BERT and ALBERT in various tasks by a
considerable margin. The proposed method improves the score of the GLUE
benchmark from 78.3 to 80.9 using BERT model and it also enhances the
performance of sequence labeling and text classification tasks.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 02:03:24 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 05:28:10 GMT'}]",2020-10-01,"[['Li', 'Linyang', ''], ['Qiu', 'Xipeng', '']]"
1279987,2005.00192,Hwanhee Lee,"Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung
  Bui, Joongbo Shin and Kyomin Jung",KPQA: A Metric for Generative Question Answering Using Keyphrase Weights,"11 pages, 6 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the automatic evaluation of generative question answering (GenQA) systems,
it is difficult to assess the correctness of generated answers due to the
free-form of the answer. Moreover, there is a lack of benchmark datasets to
evaluate the suitability of existing metrics in terms of correctness. To study
a better metric for GenQA, we first create high-quality human judgments of
correctness on two standard GenQA datasets. Using our human-evaluation
datasets, we show that widely used n-gram similarity metrics do not correlate
with human judgments. To alleviate this problem, we propose a new metric for
evaluating the correctness of GenQA. Specifically, our new metric assigns
different weights to each token via keyphrase prediction, thereby judging
whether a generated answer sentence captures the key meaning of the reference
answer. Our proposed metric shows a significantly higher correlation with human
judgments than existing metrics in various datasets.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 03:24:36 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 09:28:59 GMT'}]",2020-10-01,"[['Lee', 'Hwanhee', ''], ['Yoon', 'Seunghyun', ''], ['Dernoncourt', 'Franck', ''], ['Kim', 'Doo Soon', ''], ['Bui', 'Trung', ''], ['Shin', 'Joongbo', ''], ['Jung', 'Kyomin', '']]"
1286420,2005.06625,Oguzhan Gencoglu,Oguzhan Gencoglu,Cyberbullying Detection with Fairness Constraints,"11 pages, 2 figures",,,,cs.CL cs.LG cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cyberbullying is a widespread adverse phenomenon among online social
interactions in today's digital society. While numerous computational studies
focus on enhancing the cyberbullying detection performance of machine learning
algorithms, proposed models tend to carry and reinforce unintended social
biases. In this study, we try to answer the research question of ""Can we
mitigate the unintended bias of cyberbullying detection models by guiding the
model training with fairness constraints?"". For this purpose, we propose a
model training scheme that can employ fairness constraints and validate our
approach with different datasets. We demonstrate that various types of
unintended biases can be successfully mitigated without impairing the model
quality. We believe our work contributes to the pursuit of unbiased,
transparent, and ethical machine learning solutions for cyber-social health.
","[{'version': 'v1', 'created': 'Sat, 9 May 2020 13:04:26 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 21:54:00 GMT'}]",2020-10-01,"[['Gencoglu', 'Oguzhan', '']]"
1275004,2004.10234,Hirofumi Inaguma,"Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson
  Enrique Yalta Soplin, Tomoki Hayashi, Shinji Watanabe",ESPnet-ST: All-in-One Speech Translation Toolkit,"Accepted at ACL 2020 System Demonstration (update Table1, fix typo)",,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present ESPnet-ST, which is designed for the quick development of
speech-to-speech translation systems in a single framework. ESPnet-ST is a new
project inside end-to-end speech processing toolkit, ESPnet, which integrates
or newly implements automatic speech recognition, machine translation, and
text-to-speech functions for speech translation. We provide all-in-one recipes
including data pre-processing, feature extraction, training, and decoding
pipelines for a wide range of benchmark datasets. Our reproducible results can
match or even outperform the current state-of-the-art performances; these
pre-trained models are downloadable. The toolkit is publicly available at
https://github.com/espnet/espnet.
","[{'version': 'v1', 'created': 'Tue, 21 Apr 2020 18:38:38 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 12:28:18 GMT'}]",2020-10-01,"[['Inaguma', 'Hirofumi', ''], ['Kiyono', 'Shun', ''], ['Duh', 'Kevin', ''], ['Karita', 'Shigeki', ''], ['Soplin', 'Nelson Enrique Yalta', ''], ['Hayashi', 'Tomoki', ''], ['Watanabe', 'Shinji', '']]"
1289189,2005.09394,Hirofumi Inaguma,"Hirofumi Inaguma, Masato Mimura, Tatsuya Kawahara",Enhancing Monotonic Multihead Attention for Streaming ASR,Accepted to Interspeech 2020,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate a monotonic multihead attention (MMA) by extending hard
monotonic attention to Transformer-based automatic speech recognition (ASR) for
online streaming applications. For streaming inference, all monotonic attention
(MA) heads should learn proper alignments because the next token is not
generated until all heads detect the corresponding token boundaries. However,
we found not all MA heads learn alignments with a na\""ive implementation. To
encourage every head to learn alignments properly, we propose HeadDrop
regularization by masking out a part of heads stochastically during training.
Furthermore, we propose to prune redundant heads to improve consensus among
heads for boundary detection and prevent delayed token generation caused by
such heads. Chunkwise attention on each MA head is extended to the multihead
counterpart. Finally, we propose head-synchronous beam search decoding to
guarantee stable streaming inference.
","[{'version': 'v1', 'created': 'Tue, 19 May 2020 12:39:38 GMT'}, {'version': 'v2', 'created': 'Sat, 23 May 2020 11:11:27 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Sep 2020 12:20:25 GMT'}]",2020-10-01,"[['Inaguma', 'Hirofumi', ''], ['Mimura', 'Masato', ''], ['Kawahara', 'Tatsuya', '']]"
1291644,2005.11849,Mamoru Komachi,Satoru Katsumata and Mamoru Komachi,"Stronger Baselines for Grammatical Error Correction Using Pretrained
  Encoder-Decoder Model",6 pages; AACL-IJCNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Studies on grammatical error correction (GEC) have reported the effectiveness
of pretraining a Seq2Seq model with a large amount of pseudodata. However, this
approach requires time-consuming pretraining for GEC because of the size of the
pseudodata. In this study, we explore the utility of bidirectional and
auto-regressive transformers (BART) as a generic pretrained encoder-decoder
model for GEC. With the use of this generic pretrained model for GEC, the
time-consuming pretraining can be eliminated. We find that monolingual and
multilingual BART models achieve high performance in GEC, with one of the
results being comparable to the current strong results in English GEC. Our
implementations are publicly available at GitHub
(https://github.com/Katsumata420/generic-pretrained-GEC).
","[{'version': 'v1', 'created': 'Sun, 24 May 2020 22:13:24 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 02:57:04 GMT'}]",2020-10-01,"[['Katsumata', 'Satoru', ''], ['Komachi', 'Mamoru', '']]"
1176682,1909.06644,Angus Addlesee,"Angus Addlesee, Arash Eshghi and Ioannis Konstas","Current Challenges in Spoken Dialogue Systems and Why They Are Critical
  for Those Living with Dementia","Published at Dialog for Good 2019 - Workshop on Speech and Language
  Technology Serving Society",Dialog for Good (2019),,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialogue technologies such as Amazon's Alexa have the potential to transform
the healthcare industry. However, current systems are not yet naturally
interactive: they are often turn-based, have naive end-of-turn detection and
completely ignore many types of verbal and visual feedback - such as
backchannels, hesitation markers, filled pauses, gaze, brow furrows and
disfluencies - that are crucial in guiding and managing the conversational
process. This is especially important in the healthcare industry as target
users of Spoken Dialogue Systems (SDSs) are likely to be frail, older,
distracted or suffer from cognitive decline which impacts their ability to make
effective use of current systems. In this paper, we outline some of the
challenges that are in urgent need of further research, including Incremental
Speech Recognition and a systematic study of the interactional patterns in
conversation that are potentially diagnostic of dementia, and how these might
inform research on and the design of the next generation of SDSs.
","[{'version': 'v1', 'created': 'Sat, 14 Sep 2019 18:03:57 GMT'}]",2020-10-01,"[['Addlesee', 'Angus', ''], ['Eshghi', 'Arash', ''], ['Konstas', 'Ioannis', '']]"
1334582,2008.06860,Sachin Saxena,Sachin Saxena,TextDecepter: Hard Label Black Box Attack on Text Classifiers,"8 pages, 11 tables",,,,cs.CL cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning has been proven to be susceptible to carefully crafted
samples, known as adversarial examples. The generation of these adversarial
examples helps to make the models more robust and give as an insight of the
underlying decision making of these models. Over the years, researchers have
successfully attacked image classifiers in, both, white and black-box setting.
Although, these methods are not directly applicable to texts as text data is
discrete in nature. In recent years, research on crafting adversarial examples
against textual applications has been on the rise. In this paper, we present a
novel approach for hard label black-box attacks against Natural Language
Processing (NLP) classifiers, where no model information is disclosed, and an
attacker can only query the model to get final decision of the classifier,
without confidence scores of the classes involved. Such attack scenario is
applicable to real world black-box models being used for security-sensitive
applications such as sentiment analysis and toxic content detection
","[{'version': 'v1', 'created': 'Sun, 16 Aug 2020 08:57:01 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 00:06:56 GMT'}]",2020-10-01,"[['Saxena', 'Sachin', '']]"
1295552,2006.01038,Lei Cui,"Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li,
  Ming Zhou",DocBank: A Benchmark Dataset for Document Layout Analysis,COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Document layout analysis usually relies on computer vision models to
understand documents while ignoring textual information that is vital to
capture. Meanwhile, high quality labeled datasets with both visual and textual
information are still insufficient. In this paper, we present \textbf{DocBank},
a benchmark dataset that contains 500K document pages with fine-grained
token-level annotations for document layout analysis. DocBank is constructed
using a simple yet effective way with weak supervision from the \LaTeX{}
documents available on the arXiv.com. With DocBank, models from different
modalities can be compared fairly and multi-modal approaches will be further
investigated and boost the performance of document layout analysis. We build
several strong baselines and manually split train/dev/test sets for evaluation.
Experiment results show that models trained on DocBank accurately recognize the
layout information for a variety of documents. The DocBank dataset is publicly
available at https://github.com/doc-analysis/DocBank.
","[{'version': 'v1', 'created': 'Mon, 1 Jun 2020 16:04:30 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 08:05:48 GMT'}]",2020-10-01,"[['Li', 'Minghao', ''], ['Xu', 'Yiheng', ''], ['Cui', 'Lei', ''], ['Huang', 'Shaohan', ''], ['Wei', 'Furu', ''], ['Li', 'Zhoujun', ''], ['Zhou', 'Ming', '']]"
1273815,2004.09045,Longbin Lai,"Lu Qin, Longbin Lai, Kongzhang Hao, Zhongxin Zhou, Yiwei Zhao, Yuxing
  Han, Xuemin Lin, Zhengping Qian, Jingren Zhou","Taming the Expressiveness and Programmability of Graph Analytical
  Queries",22 pages,,,,cs.CL cs.DB cs.PL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Graph database has enjoyed a boom in the last decade, and graph queries
accordingly gain a lot of attentions from both the academia and industry. We
focus on analytical queries in this paper. While analyzing existing
domain-specific languages (DSLs) for analytical queries regarding the
perspectives of completeness, expressiveness and programmability, we find out
that none of existing work has achieved a satisfactory coverage of these
perspectives. Motivated by this, we propose the \flash DSL, which is named
after the three primitive operators Filter, LocAl and PuSH. We prove that
\flash is Turing complete (completeness), and show that it achieves both good
expressiveness and programmability for analytical queries. We provide an
implementation of \flash based on code generation, and compare it with native
C++ codes and existing DSL using representative queries. The experiment results
demonstrate \flash's expressiveness, and its capability of programming complex
algorithms that achieve satisfactory runtime.
","[{'version': 'v1', 'created': 'Mon, 20 Apr 2020 04:08:28 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 07:28:06 GMT'}]",2020-10-01,"[['Qin', 'Lu', ''], ['Lai', 'Longbin', ''], ['Hao', 'Kongzhang', ''], ['Zhou', 'Zhongxin', ''], ['Zhao', 'Yiwei', ''], ['Han', 'Yuxing', ''], ['Lin', 'Xuemin', ''], ['Qian', 'Zhengping', ''], ['Zhou', 'Jingren', '']]"
1355765,2009.14262,Jiaqi Wang,"Chacha Chen, Chieh-Yang Huang, Yaqi Hou, Yang Shi, Enyan Dai, Jiaqi
  Wang","TEST_POSITIVE at W-NUT 2020 Shared Task-3: Joint Event Multi-task
  Learning for Slot Filling in Noisy Text","6 pages, 2 figures, 2 tables",,,,cs.CL cs.LG cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The competition of extracting COVID-19 events from Twitter is to develop
systems that can automatically extract related events from tweets. The built
system should identify different pre-defined slots for each event, in order to
answer important questions (e.g., Who is tested positive? What is the age of
the person? Where is he/she?). To tackle these challenges, we propose the Joint
Event Multi-task Learning (JOELIN) model. Through a unified global learning
framework, we make use of all the training data across different events to
learn and fine-tune the language model. Moreover, we implement a type-aware
post-processing procedure using named entity recognition (NER) to further
filter the predictions. JOELIN outperforms the BERT baseline by 17.2% in micro
F1.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 19:08:45 GMT'}]",2020-10-01,"[['Chen', 'Chacha', ''], ['Huang', 'Chieh-Yang', ''], ['Hou', 'Yaqi', ''], ['Shi', 'Yang', ''], ['Dai', 'Enyan', ''], ['Wang', 'Jiaqi', '']]"
1213943,1912.02047,Felix Stahlberg,Felix Stahlberg,Neural Machine Translation: A Review and Survey,"Extended version of ""Neural Machine Translation: A Review"" accepted
  by the Journal of Artificial Intelligence Research (JAIR)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The field of machine translation (MT), the automatic translation of written
text from one natural language into another, has experienced a major paradigm
shift in recent years. Statistical MT, which mainly relies on various
count-based models and which used to dominate MT research for decades, has
largely been superseded by neural machine translation (NMT), which tackles
translation with a single neural network. In this work we will trace back the
origins of modern NMT architectures to word and sentence embeddings and earlier
examples of the encoder-decoder network family. We will conclude with a survey
of recent trends in the field.
","[{'version': 'v1', 'created': 'Wed, 4 Dec 2019 15:16:03 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 18:17:04 GMT'}]",2020-10-01,"[['Stahlberg', 'Felix', '']]"
1301749,2006.07235,Marcos Zampieri,"Marcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi
  Karadzhov, Hamdy Mubarak, Leon Derczynski, Zeses Pitenis, \c{C}a\u{g}r{\i}
  \c{C}\""oltekin","SemEval-2020 Task 12: Multilingual Offensive Language Identification in
  Social Media (OffensEval 2020)","Proceedings of the International Workshop on Semantic Evaluation
  (SemEval-2020)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present the results and main findings of SemEval-2020 Task 12 on
Multilingual Offensive Language Identification in Social Media (OffensEval
2020). The task involves three subtasks corresponding to the hierarchical
taxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The
task featured five languages: English, Arabic, Danish, Greek, and Turkish for
Subtask A. In addition, English also featured Subtasks B and C. OffensEval 2020
was one of the most popular tasks at SemEval-2020 attracting a large number of
participants across all subtasks and also across all languages. A total of 528
teams signed up to participate in the task, 145 teams submitted systems during
the evaluation period, and 70 submitted system description papers.
","[{'version': 'v1', 'created': 'Fri, 12 Jun 2020 14:39:40 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 15:46:44 GMT'}]",2020-10-01,"[['Zampieri', 'Marcos', ''], ['Nakov', 'Preslav', ''], ['Rosenthal', 'Sara', ''], ['Atanasova', 'Pepa', ''], ['Karadzhov', 'Georgi', ''], ['Mubarak', 'Hamdy', ''], ['Derczynski', 'Leon', ''], ['Pitenis', 'Zeses', ''], ['Çöltekin', 'Çağrı', '']]"
1279995,2005.00200,Zhe Gan,"Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu","HERO: Hierarchical Encoder for Video+Language Omni-representation
  Pre-training",Accepted by EMNLP 2020,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present HERO, a novel framework for large-scale video+language
omni-representation learning. HERO encodes multimodal inputs in a hierarchical
structure, where local context of a video frame is captured by a Cross-modal
Transformer via multimodal fusion, and global video context is captured by a
Temporal Transformer. In addition to standard Masked Language Modeling (MLM)
and Masked Frame Modeling (MFM) objectives, we design two new pre-training
tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global
and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the
model predicts the right order of shuffled video frames. HERO is jointly
trained on HowTo100M and large-scale TV datasets to gain deep understanding of
complex social dynamics with multi-character interactions. Comprehensive
experiments demonstrate that HERO achieves new state of the art on multiple
benchmarks over Text-based Video/Video-moment Retrieval, Video Question
Answering (QA), Video-and-language Inference and Video Captioning tasks across
different domains. We also introduce two new challenging benchmarks How2QA and
How2R for Video QA and Retrieval, collected from diverse video content over
multimodalities.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 03:49:26 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 20:37:17 GMT'}]",2020-10-01,"[['Li', 'Linjie', ''], ['Chen', 'Yen-Chun', ''], ['Cheng', 'Yu', ''], ['Gan', 'Zhe', ''], ['Yu', 'Licheng', ''], ['Liu', 'Jingjing', '']]"
1314542,2007.03028,Chen-Han Tsai,"Chen-Han Tsai, Nahum Kiryati, Eli Konen, Miri Sklair-Levy, Arnaldo
  Mayer",Labeling of Multilingual Breast MRI Reports,"10 pages, 5 figures, Accepted to MICCAI LABELS Workshop 2020",,,,cs.CV cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Medical reports are an essential medium in recording a patient's condition
throughout a clinical trial. They contain valuable information that can be
extracted to generate a large labeled dataset needed for the development of
clinical tools. However, the majority of medical reports are stored in an
unregularized format, and a trained human annotator (typically a doctor) must
manually assess and label each case, resulting in an expensive and time
consuming procedure. In this work, we present a framework for developing a
multilingual breast MRI report classifier using a custom-built language
representation called LAMBR. Our proposed method overcomes practical challenges
faced in clinical settings, and we demonstrate improved performance in
extracting labels from medical reports when compared with conventional
approaches.
","[{'version': 'v1', 'created': 'Mon, 6 Jul 2020 19:22:44 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 09:06:47 GMT'}]",2020-10-01,"[['Tsai', 'Chen-Han', ''], ['Kiryati', 'Nahum', ''], ['Konen', 'Eli', ''], ['Sklair-Levy', 'Miri', ''], ['Mayer', 'Arnaldo', '']]"
1354146,2009.12643,Ning Shi,"Ning Shi, Ziheng Zeng, Haotian Zhang, Yichen Gong",Recurrent Inference in Text Editing,"12 pages, 4 figures, 3 tables, and 1 page appendix","Findings of the 2020 Conference on Empirical Methods in Natural
  Language Processing",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In neural text editing, prevalent sequence-to-sequence based approaches
directly map the unedited text either to the edited text or the editing
operations, in which the performance is degraded by the limited source text
encoding and long, varying decoding steps. To address this problem, we propose
a new inference method, Recurrence, that iteratively performs editing actions,
significantly narrowing the problem space. In each iteration, encoding the
partially edited text, Recurrence decodes the latent representation, generates
an action of short, fixed-length, and applies the action to complete a single
edit. For a comprehensive comparison, we introduce three types of text editing
tasks: Arithmetic Operators Restoration (AOR), Arithmetic Equation
Simplification (AES), Arithmetic Equation Correction (AEC). Extensive
experiments on these tasks with varying difficulties demonstrate that
Recurrence achieves improvements over conventional inference methods.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 17:06:29 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 04:12:05 GMT'}]",2020-10-01,"[['Shi', 'Ning', ''], ['Zeng', 'Ziheng', ''], ['Zhang', 'Haotian', ''], ['Gong', 'Yichen', '']]"
1355807,2009.14304,Saurabh Kulshreshtha,"Saurabh Kulshreshtha, Jos\'e Luis Redondo-Garc\'ia, Ching-Yun Chang","Cross-lingual Alignment Methods for Multilingual BERT: A Comparative
  Study",Accepted as a long paper in Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual BERT (mBERT) has shown reasonable capability for zero-shot
cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not
pre-trained with explicit cross-lingual supervision, transfer performance can
further be improved by aligning mBERT with cross-lingual signal. Prior work
proposes several approaches to align contextualised embeddings. In this paper
we analyse how different forms of cross-lingual supervision and various
alignment methods influence the transfer capability of mBERT in zero-shot
setting. Specifically, we compare parallel corpora vs. dictionary-based
supervision and rotational vs. fine-tuning based alignment methods. We evaluate
the performance of different alignment methodologies across eight languages on
two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we
propose a novel normalisation method which consistently improves the
performance of rotation-based alignment including a notable 3% F1 improvement
for distant and typologically dissimilar languages. Importantly we identify the
biases of the alignment methods to the type of task and proximity to the
transfer language. We also find that supervision from parallel corpus is
generally superior to dictionary alignments.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 20:56:57 GMT'}]",2020-10-01,"[['Kulshreshtha', 'Saurabh', ''], ['Redondo-García', 'José Luis', ''], ['Chang', 'Ching-Yun', '']]"
1276767,2004.11997,Samuel Bowman,"Samuel R. Bowman, Jennimaria Palomaki, Livio Baldini Soares, and Emily
  Pitler","New Protocols and Negative Results for Textual Entailment Data
  Collection",To appear at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language inference (NLI) data has proven useful in benchmarking and,
especially, as pretraining data for tasks requiring language understanding.
However, the crowdsourcing protocol that was used to collect this data has
known issues and was not explicitly optimized for either of these purposes, so
it is likely far from ideal. We propose four alternative protocols, each aimed
at improving either the ease with which annotators can produce sound training
examples or the quality and diversity of those examples. Using these
alternatives and a fifth baseline protocol, we collect and compare five new
8.5k-example training sets. In evaluations focused on transfer learning
applications, our results are solidly negative, with models trained on our
baseline dataset yielding good transfer performance to downstream tasks, but
none of our four new methods (nor the recent ANLI) showing any improvements
over that baseline. In a small silver lining, we observe that all four new
protocols, especially those where annotators edit pre-filled text boxes, reduce
previously observed issues with annotation artifacts.
","[{'version': 'v1', 'created': 'Fri, 24 Apr 2020 21:31:57 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 22:06:16 GMT'}]",2020-10-01,"[['Bowman', 'Samuel R.', ''], ['Palomaki', 'Jennimaria', ''], ['Soares', 'Livio Baldini', ''], ['Pitler', 'Emily', '']]"
1355962,2009.14459,Mireille Makary,Reda Khalaf and Mireille Makary,LEBANONUPRISING: a thorough study of Lebanese tweets,"9 pages, published at the CMLA 2020 conference",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies showed a huge interest in social networks sentiment analysis.
Twitter, which is a microblogging service, can be a great source of information
on how the users feel about a certain topic, or what their opinion is regarding
a social, economic and even political matter. On October 17, Lebanon witnessed
the start of a revolution; the LebanonUprising hashtag became viral on Twitter.
A dataset consisting of a 100,0000 tweets was collected between 18 and 21
October. In this paper, we conducted a sentiment analysis study for the tweets
in spoken Lebanese Arabic related to the LebanonUprising hashtag using
different machine learning algorithms. The dataset was manually annotated to
measure the precision and recall metrics and to compare between the different
algorithms. Furthermore, the work completed in this paper provides two more
contributions. The first is related to building a Lebanese to Modern Standard
Arabic mapping dictionary that was used for the preprocessing of the tweets and
the second is an attempt to move from sentiment analysis to emotion detection
using emojis, and the two emotions we tried to predict were the ""sarcastic"" and
""funny"" emotions. We built a training set from the tweets collected in October
2019 and then we used this set to predict sentiments and emotions of the tweets
we collected between May and August 2020. The analysis we conducted shows the
variation in sentiments, emotions and users between the two datasets. The
results we obtained seem satisfactory especially considering that there was no
previous or similar work done involving Lebanese Arabic tweets, to our
knowledge.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 05:50:08 GMT'}]",2020-10-01,"[['Khalaf', 'Reda', ''], ['Makary', 'Mireille', '']]"
1355764,2009.14261,Remesh Babu K R,"Dincy Davis, Reena Murali, Remesh Babu",Abusive Language Detection and Characterization of Twitter Behavior,"7 pages, 7 figures and 8 tables","International Journal of Computer Sciences and Engineering, Vol.8,
  Issue.7, July 2020",,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, abusive language detection in online content is performed using
Bidirectional Recurrent Neural Network (BiRNN) method. Here the main objective
is to focus on various forms of abusive behaviors on Twitter and to detect
whether a speech is abusive or not. The results are compared for various
abusive behaviors in social media, with Convolutional Neural Netwrok (CNN) and
Recurrent Neural Network (RNN) methods and proved that the proposed BiRNN is a
better deep learning model for automatic abusive speech detection.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 07:38:11 GMT'}]",2020-10-01,"[['Davis', 'Dincy', ''], ['Murali', 'Reena', ''], ['Babu', 'Remesh', '']]"
1356042,2009.14539,Marco Valentino,"Marco Valentino, Mokanarangan Thayaparan, Andr\'e Freitas",Explainable Natural Language Reasoning via Conceptual Unification,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents an abductive framework for multi-hop and interpretable
textual inference. The reasoning process is guided by the notions unification
power and plausibility of an explanation, computed through the interaction of
two major architectural components: (a) An analogical reasoning model that
ranks explanatory facts by leveraging unification patterns in a corpus of
explanations; (b) An abductive reasoning model that performs a search for the
best explanation, which is realised via conceptual abstraction and subsequent
unification. We demonstrate that the Step-wise Conceptual Unification can be
effective for unsupervised question answering, and as an explanation extractor
in combination with state-of-the-art Transformers. An empirical evaluation on
the Worldtree corpus and the ARC Challenge resulted in the following
conclusions: (1) The question answering model outperforms competitive neural
and multi-hop baselines without requiring any explicit training on answer
prediction; (2) When used as an explanation extractor, the proposed model
significantly improves the performance of Transformers, leading to
state-of-the-art results on the Worldtree corpus; (3) Analogical and abductive
reasoning are highly complementary for achieving sound explanatory inference, a
feature that demonstrates the impact of the unification patterns on performance
and interpretability.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 09:50:39 GMT'}]",2020-10-01,"[['Valentino', 'Marco', ''], ['Thayaparan', 'Mokanarangan', ''], ['Freitas', 'André', '']]"
1356013,2009.14510,Zihan Liu,"Zihan Liu, Genta Indra Winata, Peng Xu, Zhaojiang Lin, Pascale Fung","Cross-lingual Spoken Language Understanding with Regularized
  Representation Alignment",EMNLP-2020 Long Paper,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the promising results of current cross-lingual models for spoken
language understanding systems, they still suffer from imperfect cross-lingual
representation alignments between the source and target languages, which makes
the performance sub-optimal. To cope with this issue, we propose a
regularization approach to further align word-level and sentence-level
representations across languages without any external resource. First, we
regularize the representation of user utterances based on their corresponding
labels. Second, we regularize the latent variable model (Liu et al., 2019) by
leveraging adversarial training to disentangle the latent variables.
Experiments on the cross-lingual spoken language understanding task show that
our model outperforms current state-of-the-art methods in both few-shot and
zero-shot scenarios, and our model, trained on a few-shot setting with only 3\%
of the target language training data, achieves comparable performance to the
supervised training with all the training data.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 08:56:53 GMT'}]",2020-10-01,"[['Liu', 'Zihan', ''], ['Winata', 'Genta Indra', ''], ['Xu', 'Peng', ''], ['Lin', 'Zhaojiang', ''], ['Fung', 'Pascale', '']]"
1355966,2009.14463,Grigorii Guz,"Grigorii Guz, Peyman Bateni, Darius Muglich, Giuseppe Carenini",Neural RST-based Evaluation of Discourse Coherence,"8 pages, 5 figures, to be published in AACL 2020",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper evaluates the utility of Rhetorical Structure Theory (RST) trees
and relations in discourse coherence evaluation. We show that incorporating
silver-standard RST features can increase accuracy when classifying coherence.
We demonstrate this through our tree-recursive neural model, namely
RST-Recursive, which takes advantage of the text's RST features produced by a
state of the art RST parser. We evaluate our approach on the Grammarly Corpus
for Discourse Coherence (GCDC) and show that when ensembled with the current
state of the art, we can achieve the new state of the art accuracy on this
benchmark. Furthermore, when deployed alone, RST-Recursive achieves competitive
accuracy while having 62% fewer parameters.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 06:00:37 GMT'}]",2020-10-01,"[['Guz', 'Grigorii', ''], ['Bateni', 'Peyman', ''], ['Muglich', 'Darius', ''], ['Carenini', 'Giuseppe', '']]"
1356293,2009.14790,Hang Yan,"Hang Yan, Xiaonan Li, Xipeng Qiu",BERT for Monolingual and Cross-Lingual Reverse Dictionary,Accepted as EMNLP 2020 Findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reverse dictionary is the task to find the proper target word given the word
description. In this paper, we tried to incorporate BERT into this task.
However, since BERT is based on the byte-pair-encoding (BPE) subword encoding,
it is nontrivial to make BERT generate a word given the description. We propose
a simple but effective method to make BERT generate the target word for this
specific task. Besides, the cross-lingual reverse dictionary is the task to
find the proper target word described in another language. Previous models have
to keep two different word embeddings and learn to align these embeddings.
Nevertheless, by using the Multilingual BERT (mBERT), we can efficiently
conduct the cross-lingual reverse dictionary with one subword embedding, and
the alignment between languages is not necessary. More importantly, mBERT can
achieve remarkable cross-lingual reverse dictionary performance even without
the parallel corpus, which means it can conduct the cross-lingual reverse
dictionary with only corresponding monolingual data. Code is publicly available
at https://github.com/yhcc/BertForRD.git.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 17:00:10 GMT'}]",2020-10-01,"[['Yan', 'Hang', ''], ['Li', 'Xiaonan', ''], ['Qiu', 'Xipeng', '']]"
1356081,2009.14578,Shaoxiong Ji,"Shaoxiong Ji, Erik Cambria and Pekka Marttinen","Dilated Convolutional Attention Network for Medical Code Assignment from
  Clinical Text",The 3rd Clinical Natural Language Processing Workshop at EMNLP 2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Medical code assignment, which predicts medical codes from clinical texts, is
a fundamental task of intelligent medical information systems. The emergence of
deep models in natural language processing has boosted the development of
automatic assignment methods. However, recent advanced neural architectures
with flat convolutions or multi-channel feature concatenation ignore the
sequential causal constraint within a text sequence and may not learn
meaningful clinical text representations, especially for lengthy clinical notes
with long-term sequential dependency. This paper proposes a Dilated
Convolutional Attention Network (DCAN), integrating dilated convolutions,
residual connections, and label attention, for medical code assignment. It
adopts dilated convolutions to capture complex medical patterns with a
receptive field which increases exponentially with dilation size. Experiments
on a real-world clinical dataset empirically show that our model improves the
state of the art.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 11:55:58 GMT'}]",2020-10-01,"[['Ji', 'Shaoxiong', ''], ['Cambria', 'Erik', ''], ['Marttinen', 'Pekka', '']]"
1356312,2009.14809,Sanxing Chen,"Sanxing Chen, Aidan San, Xiaodong Liu, Yangfeng Ji","A Tale of Two Linkings: Dynamically Gating between Schema Linking and
  Structural Linking for Text-to-SQL Parsing",COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In Text-to-SQL semantic parsing, selecting the correct entities (tables and
columns) to output is both crucial and challenging; the parser is required to
connect the natural language (NL) question and the current SQL prediction with
the structured world, i.e., the database. We formulate two linking processes to
address this challenge: schema linking which links explicit NL mentions to the
database and structural linking which links the entities in the output SQL with
their structural relationships in the database schema. Intuitively, the effects
of these two linking processes change based on the entity being generated, thus
we propose to dynamically choose between them using a gating mechanism.
Integrating the proposed method with two graph neural network based semantic
parsers together with BERT representations demonstrates substantial gains in
parsing accuracy on the challenging Spider dataset. Analyses show that our
method helps to enhance the structure of the model output when generating
complicated SQL queries and offers explainable predictions.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 17:32:27 GMT'}]",2020-10-01,"[['Chen', 'Sanxing', ''], ['San', 'Aidan', ''], ['Liu', 'Xiaodong', ''], ['Ji', 'Yangfeng', '']]"
1355960,2009.14457,Subhojeet Pramanik,"Subhojeet Pramanik, Shashank Mujumdar, Hima Patel","Towards a Multi-modal, Multi-task Learning based Pre-training Framework
  for Document Representation Learning",Preprint,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose a multi-task learning-based framework that utilizes
a combination of self-supervised and supervised pre-training tasks to learn a
generic document representation. We design the network architecture and the
pre-training tasks to incorporate the multi-modal document information across
text, layout, and image dimensions and allow the network to work with
multi-page documents. We showcase the applicability of our pre-training
framework on a variety of different real-world document tasks such as document
classification, document information extraction, and document retrieval. We
conduct exhaustive experiments to compare performance against different
ablations of our framework and state-of-the-art baselines. We discuss the
current limitations and next steps for our work.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 05:39:04 GMT'}]",2020-10-01,"[['Pramanik', 'Subhojeet', ''], ['Mujumdar', 'Shashank', ''], ['Patel', 'Hima', '']]"
1355948,2009.14445,Soroush Vosoughi Dr,"Weicheng Ma, Ruibo Liu, Lili Wang and Soroush Vosoughi","Towards Improved Model Design for Authorship Identification: A Survey on
  Writing Style Understanding",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Authorship identification tasks, which rely heavily on linguistic styles,
have always been an important part of Natural Language Understanding (NLU)
research. While other tasks based on linguistic style understanding benefit
from deep learning methods, these methods have not behaved as well as
traditional machine learning methods in many authorship-based tasks. With these
tasks becoming more and more challenging, however, traditional machine learning
methods based on handcrafted feature sets are already approaching their
performance limits. Thus, in order to inspire future applications of deep
learning methods in authorship-based tasks in ways that benefit the extraction
of stylistic features, we survey authorship-based tasks and other tasks related
to writing style understanding. We first describe our survey results on the
current state of research in both sets of tasks and summarize existing
achievements and problems in authorship-related tasks. We then describe
outstanding methods in style-related tasks in general and analyze how they are
used in combination in the top-performing models. We are optimistic about the
applicability of these models to authorship-based tasks and hope our survey
will help advance research in this field.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 05:17:42 GMT'}]",2020-10-01,"[['Ma', 'Weicheng', ''], ['Liu', 'Ruibo', ''], ['Wang', 'Lili', ''], ['Vosoughi', 'Soroush', '']]"
1356327,2009.14824,Chantal Amrhein,Chantal Amrhein and Rico Sennrich,"On Romanization for Model Transfer Between Scripts in Neural Machine
  Translation",accepted at Findings of EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Transfer learning is a popular strategy to improve the quality of
low-resource machine translation. For an optimal transfer of the embedding
layer, the child and parent model should share a substantial part of the
vocabulary. This is not the case when transferring to languages with a
different script. We explore the benefit of romanization in this scenario. Our
results show that romanization entails information loss and is thus not always
superior to simpler vocabulary transfer methods, but can improve the transfer
between related languages with different scripts. We compare two romanization
tools and find that they exhibit different degrees of information loss, which
affects translation quality. Finally, we extend romanization to the target
side, showing that this can be a successful strategy when coupled with a simple
deromanization model.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 17:54:56 GMT'}]",2020-10-01,"[['Amrhein', 'Chantal', ''], ['Sennrich', 'Rico', '']]"
1356297,2009.14794,Xingyou Song,"Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
  Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller",Rethinking Attention with Performers,"36 pages. This is an updated version of a previous submission which
  can be found at arXiv:2006.03555. See
  https://github.com/google-research/google-research/tree/master/protein_lm for
  protein language model code, and
  https://github.com/google-research/google-research/tree/master/performer for
  Performer code",,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce Performers, Transformer architectures which can estimate regular
(softmax) full-rank-attention Transformers with provable accuracy, but using
only linear (as opposed to quadratic) space and time complexity, without
relying on any priors such as sparsity or low-rankness. To approximate softmax
attention-kernels, Performers use a novel Fast Attention Via positive
Orthogonal Random features approach (FAVOR+), which may be of independent
interest for scalable kernel methods. FAVOR+ can be also used to efficiently
model kernelizable attention mechanisms beyond softmax. This representational
power is crucial to accurately compare softmax with other kernels for the first
time on large-scale tasks, beyond the reach of regular Transformers, and
investigate optimal attention-kernels. Performers are linear architectures
fully compatible with regular Transformers and with strong theoretical
guarantees: unbiased or nearly-unbiased estimation of the attention matrix,
uniform convergence and low estimation variance. We tested Performers on a rich
set of tasks stretching from pixel-prediction through text models to protein
sequence modeling. We demonstrate competitive results with other examined
efficient sparse and dense attention methods, showcasing effectiveness of the
novel attention-learning paradigm leveraged by Performers.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 17:09:09 GMT'}]",2020-10-01,"[['Choromanski', 'Krzysztof', ''], ['Likhosherstov', 'Valerii', ''], ['Dohan', 'David', ''], ['Song', 'Xingyou', ''], ['Gane', 'Andreea', ''], ['Sarlos', 'Tamas', ''], ['Hawkins', 'Peter', ''], ['Davis', 'Jared', ''], ['Mohiuddin', 'Afroz', ''], ['Kaiser', 'Lukasz', ''], ['Belanger', 'David', ''], ['Colwell', 'Lucy', ''], ['Weller', 'Adrian', '']]"
1355898,2009.14395,Shamil Chollampatt,"Shamil Chollampatt, Raymond Hendy Susanto, Liling Tan, Ewa Szymanska",Can Automatic Post-Editing Improve NMT?,In EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic post-editing (APE) aims to improve machine translations, thereby
reducing human post-editing effort. APE has had notable success when used with
statistical machine translation (SMT) systems but has not been as successful
over neural machine translation (NMT) systems. This has raised questions on the
relevance of APE task in the current scenario. However, the training of APE
models has been heavily reliant on large-scale artificial corpora combined with
only limited human post-edited data. We hypothesize that APE models have been
underperforming in improving NMT translations due to the lack of adequate
supervision. To ascertain our hypothesis, we compile a larger corpus of human
post-edits of English to German NMT. We empirically show that a state-of-art
neural APE model trained on this corpus can significantly improve a strong
in-domain NMT system, challenging the current understanding in the field. We
further investigate the effects of varying training data sizes, using
artificial training data, and domain specificity for the APE task. We release
this new corpus under CC BY-NC-SA 4.0 license at
https://github.com/shamilcm/pedra.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 02:34:19 GMT'}]",2020-10-01,"[['Chollampatt', 'Shamil', ''], ['Susanto', 'Raymond Hendy', ''], ['Tan', 'Liling', ''], ['Szymanska', 'Ewa', '']]"
1355889,2009.14386,Hong-Kwang Jeff Kuo,"Hong-Kwang J. Kuo, Zolt\'an T\""uske, Samuel Thomas, Yinghui Huang,
  Kartik Audhkhasi, Brian Kingsbury, Gakuto Kurata, Zvi Kons, Ron Hoory, and
  Luis Lastras",End-to-End Spoken Language Understanding Without Full Transcripts,"5 pages, to be published in Interspeech 2020",,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  An essential component of spoken language understanding (SLU) is slot
filling: representing the meaning of a spoken utterance using semantic entity
labels. In this paper, we develop end-to-end (E2E) spoken language
understanding systems that directly convert speech input to semantic entities
and investigate if these E2E SLU models can be trained solely on semantic
entity annotations without word-for-word transcripts. Training such models is
very useful as they can drastically reduce the cost of data collection. We
created two types of such speech-to-entities models, a CTC model and an
attention-based encoder-decoder model, by adapting models trained originally
for speech recognition. Given that our experiments involve speech input, these
systems need to recognize both the entity label and words representing the
entity value correctly. For our speech-to-entities experiments on the ATIS
corpus, both the CTC and attention models showed impressive ability to skip
non-entity words: there was little degradation when trained on just entities
versus full transcripts. We also explored the scenario where the entities are
in an order not necessarily related to spoken order in the utterance. With its
ability to do re-ordering, the attention model did remarkably well, achieving
only about 2% degradation in speech-to-bag-of-entities F1 score.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 01:54:13 GMT'}]",2020-10-01,"[['Kuo', 'Hong-Kwang J.', ''], ['Tüske', 'Zoltán', ''], ['Thomas', 'Samuel', ''], ['Huang', 'Yinghui', ''], ['Audhkhasi', 'Kartik', ''], ['Kingsbury', 'Brian', ''], ['Kurata', 'Gakuto', ''], ['Kons', 'Zvi', ''], ['Hoory', 'Ron', ''], ['Lastras', 'Luis', '']]"
1355887,2009.14384,B Mansurov,B. Mansurov and A. Mansurov,Development of Word Embeddings for Uzbek Language,7 pages,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we share the process of developing word embeddings for the
Cyrillic variant of the Uzbek language. The result of our work is the first
publicly available set of word vectors trained on the word2vec, GloVe, and
fastText algorithms using a high-quality web crawl corpus developed in-house.
The developed word embeddings can be used in many natural language processing
downstream tasks.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 01:52:00 GMT'}]",2020-10-01,"[['Mansurov', 'B.', ''], ['Mansurov', 'A.', '']]"
1356225,2009.14722,Guoqing Luo,"Guoqing Luo, Jiaxin Pan, Min Peng","RDSGAN: Rank-based Distant Supervision Relation Extraction with
  Generative Adversarial Framework",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Distant supervision has been widely used for relation extraction but suffers
from noise labeling problem. Neural network models are proposed to denoise with
attention mechanism but cannot eliminate noisy data due to its non-zero
weights. Hard decision is proposed to remove wrongly-labeled instances from the
positive set though causes loss of useful information contained in removed
instances. In this paper, we propose a novel generative neural framework named
RDSGAN (Rank-based Distant Supervision GAN) which automatically generates valid
instances for distant supervision relation extraction. Our framework combines
soft attention and hard decision to learn the distribution of true positive
instances via adversarial training and selects valid instances conforming to
the distribution via rank-based distant supervision, which addresses the false
positive problem. Experimental results show the superiority of our framework
over strong baselines.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 14:59:09 GMT'}]",2020-10-01,"[['Luo', 'Guoqing', ''], ['Pan', 'Jiaxin', ''], ['Peng', 'Min', '']]"
1355878,2009.14375,Dhruv Kumar,"Olga Vechtomova, Gaurav Sahu, Dhruv Kumar",Generation of lyrics lines conditioned on music audio clips,"Accepted to First Workshop on NLP for Music and Audio (NLP4MusA) at
  ISMIR 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a system for generating novel lyrics lines conditioned on music
audio. A bimodal neural network model learns to generate lines conditioned on
any given short audio clip. The model consists of a spectrogram variational
autoencoder (VAE) and a text VAE. Both automatic and human evaluations
demonstrate effectiveness of our model in generating lines that have an
emotional impact matching a given audio clip. The system is intended to serve
as a creativity tool for songwriters.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 01:22:58 GMT'}]",2020-10-01,"[['Vechtomova', 'Olga', ''], ['Sahu', 'Gaurav', ''], ['Kumar', 'Dhruv', '']]"
1340274,2008.12552,Yashank Singh,"Yashank Singh, Niladri Chatterjee",Temporal Random Indexing of Context Vectors Applied to Event Detection,"8 pages, 12 figures",,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we explore new representations for encoding language data. The
general method of one-hot encoding grows linearly with the size of the word
corpus in space-complexity. We address this by using Random Indexing(RI) of
context vectors with non-zero entries. We propose a novel RI representation
where we exploit the effect imposing a probability distribution on the number
of randomized entries which leads to a class of RI representations. We also
propose an algorithm that is log linear in the size of word corpus to track the
semantic relationship of the query word to other words for suggesting the
events that are relevant to the word in question. Finally we run simulations on
the novel RI representations using the proposed algorithms for tweets relevant
to the word ""iPhone"" and present results. The RI representation is shown to be
faster and space efficient as compared to BoW embeddings.
","[{'version': 'v1', 'created': 'Fri, 28 Aug 2020 09:37:39 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 13:51:27 GMT'}]",2020-10-01,"[['Singh', 'Yashank', ''], ['Chatterjee', 'Niladri', '']]"
1355612,2009.14109,Jonathan K Kummerfeld,"Charles Welch, Rada Mihalcea, Jonathan K. Kummerfeld","Improving Low Compute Language Modeling with In-Domain Embedding
  Initialisation",To appear at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many NLP applications, such as biomedical data and technical support, have
10-100 million tokens of in-domain data and limited computational resources for
learning from it. How should we train a language model in this scenario? Most
language modeling research considers either a small dataset with a closed
vocabulary (like the standard 1 million token Penn Treebank), or the whole web
with byte-pair encoding. We show that for our target setting in English,
initialising and freezing input embeddings using in-domain data can improve
language model performance by providing a useful representation of rare words,
and this pattern holds across several different domains. In the process, we
show that the standard convention of tying input and output embeddings does not
improve perplexity when initializing with embeddings trained on in-domain data.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 15:48:58 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 15:40:39 GMT'}]",2020-10-01,"[['Welch', 'Charles', ''], ['Mihalcea', 'Rada', ''], ['Kummerfeld', 'Jonathan K.', '']]"
1355864,2009.14361,Angus Addlesee,Angus Addlesee and Pierre Albert,"Ethically Collecting Multi-Modal Spontaneous Conversations with People
  that have Cognitive Impairments","Published at LREC's Workshop on Legal and Ethical Issues in Human
  Language Technologies 2020","LREC Workshop on Legal and Ethical Issues in Human Language
  Technologies (2020) 15-20",,,cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In order to make spoken dialogue systems (such as Amazon Alexa or Google
Assistant) more accessible and naturally interactive for people with cognitive
impairments, appropriate data must be obtainable. Recordings of multi-modal
spontaneous conversations with vulnerable user groups are scarce however and
this valuable data is challenging to collect. Researchers that call for this
data are commonly inexperienced in ethical and legal issues around working with
vulnerable participants. Additionally, standard recording equipment is insecure
and should not be used to capture sensitive data. We spent a year consulting
experts on how to ethically capture and share recordings of multi-modal
spontaneous conversations with vulnerable user groups. In this paper we provide
guidance, collated from these experts, on how to ethically collect such data
and we present a new system - ""CUSCO"" - to capture, transport and exchange
sensitive data securely. This framework is intended to be easily followed and
implemented to encourage further publications of similar corpora. Using this
guide and secure recording system, researchers can review and refine their
ethical measures.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 00:57:33 GMT'}]",2020-10-01,"[['Addlesee', 'Angus', ''], ['Albert', 'Pierre', '']]"
1355851,2009.14348,Huaishao Luo,"Huaishao Luo, Yu Shi, Ming Gong, Linjun Shou, Tianrui Li","MaP: A Matrix-based Prediction Approach to Improve Span Extraction in
  Machine Reading Comprehension",to appear at AACL-IJCNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Span extraction is an essential problem in machine reading comprehension.
Most of the existing algorithms predict the start and end positions of an
answer span in the given corresponding context by generating two probability
vectors. In this paper, we propose a novel approach that extends the
probability vector to a probability matrix. Such a matrix can cover more
start-end position pairs. Precisely, to each possible start index, the method
always generates an end probability vector. Besides, we propose a
sampling-based training strategy to address the computational cost and memory
issue in the matrix training phase. We evaluate our method on SQuAD 1.1 and
three other question answering benchmarks. Leveraging the most competitive
models BERT and BiDAF as the backbone, our proposed approach can get consistent
improvements in all datasets, demonstrating the effectiveness of the proposed
method.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 23:53:50 GMT'}]",2020-10-01,"[['Luo', 'Huaishao', ''], ['Shi', 'Yu', ''], ['Gong', 'Ming', ''], ['Shou', 'Linjun', ''], ['Li', 'Tianrui', '']]"
1355838,2009.14335,Zewei Chu,"Zewei Chu, Karl Stratos, Kevin Gimpel","Natcat: Weakly Supervised Text Classification with Naturally Annotated
  Datasets",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We seek to improve text classification by leveraging naturally annotated
data. In particular, we construct a general purpose text categorization dataset
(NatCat) from three online resources: Wikipedia, Reddit, and Stack Exchange.
These datasets consist of document-category pairs derived from manual curation
that occurs naturally by their communities. We build general purpose text
classifiers by training on NatCat and evaluate them on a suite of 11 text
classification tasks (CatEval). We benchmark different modeling choices and
dataset combinations, and show how each task benefits from different NatCat
training resources.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 22:49:15 GMT'}]",2020-10-01,"[['Chu', 'Zewei', ''], ['Stratos', 'Karl', ''], ['Gimpel', 'Kevin', '']]"
1356161,2009.14658,Hongfei Xu,Hongfei Xu and Qiuhui Liu,Learning Hard Retrieval Cross Attention for Transformer,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Transformer translation model that based on the multi-head attention
mechanism can be parallelized easily and lead to competitive performance in
machine translation. The multi-head attention network performs the scaled
dot-product attention function in parallel, empowering the model by jointly
attending to information from different representation subspaces at different
positions. Though its advantages in parallelization, many previous works
suggest the computation of the attention mechanism is not sufficiently
efficient, especially when processing long sequences, and propose approaches to
improve its efficiency with long sentences. In this paper, we accelerate the
inference of the scaled dot-product attention in another perspective.
Specifically, instead of squeezing the sequence to attend, we simplify the
computation of the scaled dot-product attention by learning a hard retrieval
attention which only attends to one token in the sentence rather than all
tokens. Since the hard attention mechanism only attends to one position, the
matrix multiplication between attention probabilities and the value sequence in
the standard scaled dot-product attention can be replaced by a simple and
efficient retrieval operation. As a result, our hard retrieval attention
mechanism can empirically accelerate the scaled dot-product attention for both
long and short sequences by 66.5%, while performing competitively in a wide
range of machine translation tasks when using for cross attention networks.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 13:18:57 GMT'}]",2020-10-01,"[['Xu', 'Hongfei', ''], ['Liu', 'Qiuhui', '']]"
1355740,2009.14237,Andrew Head,"Andrew Head (UC Berkeley), Kyle Lo (Allen Institute for AI), Dongyeop
  Kang (UC Berkeley), Raymond Fok (University of Washington), Sam Skjonsberg
  (Allen Institute for AI), Daniel S. Weld (Allen Institute for AI, University
  of Washington), Marti A. Hearst (UC Berkeley)","Augmenting Scientific Papers with Just-in-Time, Position-Sensitive
  Definitions of Terms and Symbols","16 pages, 14 figures, 1 table. For associated video, see
  https://bit.ly/scholarphi-video-walkthrough",,,,cs.HC cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the central importance of research papers to scientific progress,
they can be difficult to read. Comprehension is often stymied when the
information needed to understand a passage resides somewhere else: in another
section, or in another paper. In this work, we envision how interfaces can
bring definitions of technical terms and symbols to readers when and where they
need them most. We introduce ScholarPhi, an augmented reading interface with
four novel features: (1) tooltips that surface position-sensitive definitions
from elsewhere in a paper, (2) a filter over the paper that ""declutters"" it to
reveal how the term or symbol is used across the paper, (3) automatic equation
diagrams that expose multiple definitions in parallel, and (4) an automatically
generated glossary of important terms and symbols. A usability study showed
that the tool helps researchers of all experience levels read papers.
Furthermore, researchers were eager to have ScholarPhi's definitions available
to support their everyday reading.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 18:11:19 GMT'}]",2020-10-01,"[['Head', 'Andrew', '', 'UC Berkeley'], ['Lo', 'Kyle', '', 'Allen Institute for AI'], ['Kang', 'Dongyeop', '', 'UC Berkeley'], ['Fok', 'Raymond', '', 'University of Washington'], ['Skjonsberg', 'Sam', '', 'Allen Institute for AI'], ['Weld', 'Daniel S.', '', 'Allen Institute for AI, University\n  of Washington'], ['Hearst', 'Marti A.', '', 'UC Berkeley']]"
1298069,2006.03555,Xingyou Song,"Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
  Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger,
  Lucy Colwell, Adrian Weller","Masked Language Modeling for Proteins via Linearly Scalable Long-Context
  Transformers","This arXiv submission has been deprecated. Please see ""Rethinking
  Attention with Performers"" at arXiv:2009.14794 for the most updated version
  of the paper",,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer models have achieved state-of-the-art results across a diverse
range of domains. However, concern over the cost of training the attention
mechanism to learn complex dependencies between distant inputs continues to
grow. In response, solutions that exploit the structure and sparsity of the
learned attention matrix have blossomed. However, real-world applications that
involve long sequences, such as biological sequence analysis, may fall short of
meeting these assumptions, precluding exploration of these models. To address
this challenge, we present a new Transformer architecture, Performer, based on
Fast Attention Via Orthogonal Random features (FAVOR). Our mechanism scales
linearly rather than quadratically in the number of tokens in the sequence, is
characterized by sub-quadratic space complexity and does not incorporate any
sparsity pattern priors. Furthermore, it provides strong theoretical
guarantees: unbiased estimation of the attention matrix and uniform
convergence. It is also backwards-compatible with pre-trained regular
Transformers. We demonstrate its effectiveness on the challenging task of
protein sequence modeling and provide detailed theoretical analysis.
","[{'version': 'v1', 'created': 'Fri, 5 Jun 2020 17:09:16 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Aug 2020 14:33:41 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Oct 2020 00:41:49 GMT'}]",2020-10-02,"[['Choromanski', 'Krzysztof', ''], ['Likhosherstov', 'Valerii', ''], ['Dohan', 'David', ''], ['Song', 'Xingyou', ''], ['Gane', 'Andreea', ''], ['Sarlos', 'Tamas', ''], ['Hawkins', 'Peter', ''], ['Davis', 'Jared', ''], ['Belanger', 'David', ''], ['Colwell', 'Lucy', ''], ['Weller', 'Adrian', '']]"
1356687,2010.00357,Hind Alatwi,"Hind Saleh Alatawi, Areej Maatog Alhothali and Kawthar Mustafa Moria","Detecting White Supremacist Hate Speech using Domain Specific Word
  Embedding with Deep Learning and BERT","32 pages,2 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  White supremacists embrace a radical ideology that considers white people
superior to people of other races. The critical influence of these groups is no
longer limited to social media; they also have a significant effect on society
in many ways by promoting racial hatred and violence. White supremacist hate
speech is one of the most recently observed harmful content on social
media.Traditional channels of reporting hate speech have proved inadequate due
to the tremendous explosion of information, and therefore, it is necessary to
find an automatic way to detect such speech in a timely manner. This research
investigates the viability of automatically detecting white supremacist hate
speech on Twitter by using deep learning and natural language processing
techniques. Through our experiments, we used two approaches, the first approach
is by using domain-specific embeddings which are extracted from white
supremacist corpus in order to catch the meaning of this white supremacist
slang with bidirectional Long Short-Term Memory (LSTM) deep learning model,
this approach reached a 0.74890 F1-score. The second approach is by using the
one of the most recent language model which is BERT, BERT model provides the
state of the art of most NLP tasks. It reached to a 0.79605 F1-score. Both
approaches are tested on a balanced dataset given that our experiments were
based on textual data only. The dataset was combined from dataset created from
Twitter and a Stormfront dataset compiled from that white supremacist forum.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 12:44:24 GMT'}]",2020-10-02,"[['Alatawi', 'Hind Saleh', ''], ['Alhothali', 'Areej Maatog', ''], ['Moria', 'Kawthar Mustafa', '']]"
1355073,2009.13570,Mihail Eric,"Shikib Mehri, Mihail Eric, Dilek Hakkani-Tur","DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented
  Dialogue","Benchmark hosted on:
  https://evalai.cloudcv.org/web/challenges/challenge-page/708/",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  A long-standing goal of task-oriented dialogue research is the ability to
flexibly adapt dialogue models to new domains. To progress research in this
direction, we introduce DialoGLUE (Dialogue Language Understanding Evaluation),
a public benchmark consisting of 7 task-oriented dialogue datasets covering 4
distinct natural language understanding tasks, designed to encourage dialogue
research in representation-based transfer, domain adaptation, and
sample-efficient task learning. We release several strong baseline models,
demonstrating performance improvements over a vanilla BERT architecture and
state-of-the-art results on 5 out of 7 tasks, by pre-training on a large
open-domain dialogue corpus and task-adaptive self-supervised training. Through
the DialoGLUE benchmark, the baseline methods, and our evaluation scripts, we
hope to facilitate progress towards the goal of developing more general
task-oriented dialogue models.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 18:36:23 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Oct 2020 00:00:19 GMT'}]",2020-10-02,"[['Mehri', 'Shikib', ''], ['Eric', 'Mihail', ''], ['Hakkani-Tur', 'Dilek', '']]"
1271641,2004.06871,Chien-Sheng Wu,"Chien-Sheng Wu, Steven Hoi, Richard Socher, and Caiming Xiong","TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented
  Dialogue",EMNLP 2020 camera-ready,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The underlying difference of linguistic patterns between general text and
task-oriented dialogue makes existing pre-trained language models less useful
in practice. In this work, we unify nine human-human and multi-turn
task-oriented dialogue datasets for language modeling. To better model dialogue
behavior during pre-training, we incorporate user and system tokens into the
masked language modeling. We propose a contrastive objective function to
simulate the response selection task. Our pre-trained task-oriented dialogue
BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream
task-oriented dialogue applications, including intention recognition, dialogue
state tracking, dialogue act prediction, and response selection. We also show
that TOD-BERT has a stronger few-shot ability that can mitigate the data
scarcity problem for task-oriented dialogue.
","[{'version': 'v1', 'created': 'Wed, 15 Apr 2020 04:09:05 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Apr 2020 18:10:32 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Oct 2020 16:34:52 GMT'}]",2020-10-02,"[['Wu', 'Chien-Sheng', ''], ['Hoi', 'Steven', ''], ['Socher', 'Richard', ''], ['Xiong', 'Caiming', '']]"
1301328,2006.06814,Jianhong Wang,"Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, Yunjie Gu","Modelling Hierarchical Structure between Dialogue Policy and Natural
  Language Generator with Option Framework for Task-oriented Dialogue System",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Designing task-oriented dialogue systems is a challenging research topic,
since it needs not only to generate utterances fulfilling user requests but
also to guarantee the comprehensibility. Many previous works trained end-to-end
(E2E) models with supervised learning (SL), however, the bias in annotated
system utterances remains as a bottleneck. Reinforcement learning (RL) deals
with the problem through using non-differentiable evaluation metrics (e.g., the
success rate) as rewards. Nonetheless, existing works with RL showed that the
comprehensibility of generated system utterances could be corrupted when
improving the performance on fulfilling user requests. In o gur work, we (1)
propose modelling the hierarchical structure between dialogue policy and
natural language generator (NLG) with the option framework, called HDNO, where
the latent dialogue act is applied to avoid designing specific dialogue act
representations; (2) train HDNO via hierarchical reinforcement learning (HRL),
as well as suggest the asynchronous updates between dialogue policy and NLG
during training to theoretically guarantee their convergence to a local
maximizer; and (3) propose using a discriminator modelled with language models
as an additional reward to further improve the comprehensibility. We test HDNO
on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in
comparison with word-level E2E model trained with RL, LaRL and HDSA, showing
improvements on the performance evaluated by automatic evaluation metrics and
human evaluation. Finally, we demonstrate the semantic meanings of latent
dialogue acts to show the ability of explanation.
","[{'version': 'v1', 'created': 'Thu, 11 Jun 2020 20:55:28 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Jul 2020 10:26:21 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Sep 2020 21:56:52 GMT'}]",2020-10-02,"[['Wang', 'Jianhong', ''], ['Zhang', 'Yuan', ''], ['Kim', 'Tae-Kyun', ''], ['Gu', 'Yunjie', '']]"
1209470,1911.10876,Yerai Doval,"Yerai Doval, Jes\'us Vilares, Carlos G\'omez-Rodr\'iguez",Towards robust word embeddings for noisy texts,"15 pages, 2 figures, 5 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Research on word embeddings has mainly focused on improving their performance
on standard corpora, disregarding the difficulties posed by noisy texts in the
form of tweets and other types of non-standard writing from social media. In
this work, we propose a simple extension to the skipgram model in which we
introduce the concept of bridge-words, which are artificial words added to the
model to strengthen the similarity between standard words and their noisy
variants. Our new embeddings outperform baseline models on noisy texts on a
wide range of evaluation tasks, both intrinsic and extrinsic, while retaining a
good performance on standard texts. To the best of our knowledge, this is the
first explicit approach at dealing with this type of noisy texts at the word
embedding level that goes beyond the support for out-of-vocabulary words.
","[{'version': 'v1', 'created': 'Mon, 25 Nov 2019 12:48:27 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Nov 2019 11:16:52 GMT'}, {'version': 'v3', 'created': 'Tue, 28 Jan 2020 12:00:13 GMT'}, {'version': 'v4', 'created': 'Wed, 30 Sep 2020 19:05:12 GMT'}]",2020-10-02,"[['Doval', 'Yerai', ''], ['Vilares', 'Jesús', ''], ['Gómez-Rodríguez', 'Carlos', '']]"
1351762,2009.10259,Weixin Liang,"Weixin Liang, James Zou, Zhou Yu",ALICE: Active Learning with Contrastive Natural Language Explanations,,EMNLP 2020,,,cs.CL cs.CV cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Training a supervised neural network classifier typically requires many
annotated training samples. Collecting and annotating a large number of data
points are costly and sometimes even infeasible. Traditional annotation process
uses a low-bandwidth human-machine communication interface: classification
labels, each of which only provides several bits of information. We propose
Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop
training framework that utilizes contrastive natural language explanations to
improve data efficiency in learning. ALICE learns to first use active learning
to select the most informative pairs of label classes to elicit contrastive
natural language explanations from experts. Then it extracts knowledge from
these explanations using a semantic parser. Finally, it incorporates the
extracted knowledge through dynamically changing the learning model's
structure. We applied ALICE in two visual recognition tasks, bird species
classification and social relationship classification. We found by
incorporating contrastive explanations, our models outperform baseline models
that are trained with 40-100% more training data. We found that adding 1
explanation leads to similar performance gain as adding 13-30 labeled training
data points.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 01:02:07 GMT'}]",2020-10-02,"[['Liang', 'Weixin', ''], ['Zou', 'James', ''], ['Yu', 'Zhou', '']]"
1351183,2009.09680,Haoyu Song,"Haoyu Song, Yan Wang, Wei-Nan Zhang, Zhengyu Zhao, Ting Liu, Xiaojiang
  Liu",Profile Consistency Identification for Open-domain Dialogue Agents,Accepted by EMNLP20,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Maintaining a consistent attribute profile is crucial for dialogue agents to
naturally converse with humans. Existing studies on improving attribute
consistency mainly explored how to incorporate attribute information in the
responses, but few efforts have been made to identify the consistency relations
between response and attribute profile. To facilitate the study of profile
consistency identification, we create a large-scale human-annotated dataset
with over 110K single-turn conversations and their key-value attribute
profiles. Explicit relation between response and profile is manually labeled.
We also propose a key-value structure information enriched BERT model to
identify the profile consistency, and it gained improvements over strong
baselines. Further evaluations on downstream tasks demonstrate that the profile
consistency identification model is conducive for improving dialogue
consistency.
","[{'version': 'v1', 'created': 'Mon, 21 Sep 2020 08:38:23 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 23:36:07 GMT'}]",2020-10-02,"[['Song', 'Haoyu', ''], ['Wang', 'Yan', ''], ['Zhang', 'Wei-Nan', ''], ['Zhao', 'Zhengyu', ''], ['Liu', 'Ting', ''], ['Liu', 'Xiaojiang', '']]"
1292808,2005.13013,Jason Phang,"Jason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun
  Liu, Clara Vania, Katharina Kann, Samuel R. Bowman","English Intermediate-Task Training Improves Zero-Shot Cross-Lingual
  Transfer Too",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Intermediate-task training---fine-tuning a pretrained model on an
intermediate task before fine-tuning again on the target task---often improves
model performance substantially on language understanding tasks in monolingual
English settings. We investigate whether English intermediate-task training is
still helpful on non-English target tasks. Using nine intermediate
language-understanding tasks, we evaluate intermediate-task transfer in a
zero-shot cross-lingual setting on the XTREME benchmark. We see large
improvements from intermediate training on the BUCC and Tatoeba sentence
retrieval tasks and moderate improvements on question-answering target tasks.
MNLI, SQuAD and HellaSwag achieve the best overall results as intermediate
tasks, while multi-task intermediate offers small additional improvements.
Using our best intermediate-task models for each target task, we obtain a 5.4
point improvement over XLM-R Large on the XTREME benchmark, setting the state
of the art as of June 2020. We also investigate continuing multilingual MLM
during intermediate-task training and using machine-translated
intermediate-task data, but neither consistently outperforms simply performing
English intermediate-task training.
","[{'version': 'v1', 'created': 'Tue, 26 May 2020 20:17:29 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 18:01:49 GMT'}]",2020-10-02,"[['Phang', 'Jason', ''], ['Calixto', 'Iacer', ''], ['Htut', 'Phu Mon', ''], ['Pruksachatkun', 'Yada', ''], ['Liu', 'Haokun', ''], ['Vania', 'Clara', ''], ['Kann', 'Katharina', ''], ['Bowman', 'Samuel R.', '']]"
1181917,1909.11879,Ali Septiandri,"Yosef Ardhito Winatmoko, Ali Akbar Septiandri, Arie Pratama Sutiono","Aspect and Opinion Term Extraction for Hotel Reviews using Transfer
  Learning and Auxiliary Labels","Updated from (Septiandri & Sutiono, 2019)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect and opinion term extraction is a critical step in Aspect-Based
Sentiment Analysis (ABSA). Our study focuses on evaluating transfer learning
using pre-trained BERT (Devlin et al., 2018) to classify tokens from hotel
reviews in bahasa Indonesia. The primary challenge is the language informality
of the review texts. By utilizing transfer learning from a multilingual model,
we achieved up to 2% difference on token level F1-score compared to the
state-of-the-art Bi-LSTM model with fewer training epochs (3 vs. 200 epochs).
The fine-tuned model clearly outperforms the Bi-LSTM model on the entity level.
Furthermore, we propose a method to include CRF with auxiliary labels as an
output layer for the BERT-based models. The CRF addition further improves the
F1-score for both token and entity level.
","[{'version': 'v1', 'created': 'Thu, 26 Sep 2019 04:17:48 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Oct 2019 11:02:03 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2020 07:09:56 GMT'}, {'version': 'v4', 'created': 'Fri, 28 Feb 2020 04:33:04 GMT'}, {'version': 'v5', 'created': 'Thu, 1 Oct 2020 10:14:02 GMT'}]",2020-10-02,"[['Winatmoko', 'Yosef Ardhito', ''], ['Septiandri', 'Ali Akbar', ''], ['Sutiono', 'Arie Pratama', '']]"
1288888,2005.09093,Shijie Wu,"Shijie Wu, Mark Dredze",Are All Languages Created Equal in Multilingual BERT?,RepL4NLP Workshop 2020 (Best Long Paper),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly
good cross-lingual performance on several NLP tasks, even without explicit
cross-lingual signals. However, these evaluations have focused on cross-lingual
transfer with high-resource languages, covering only a third of the languages
covered by mBERT. We explore how mBERT performs on a much wider set of
languages, focusing on the quality of representation for low-resource
languages, measured by within-language performance. We consider three tasks:
Named Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency
Parsing (54 languages each). mBERT does better than or comparable to baselines
on high resource languages but does much worse for low resource languages.
Furthermore, monolingual BERT models for these languages do even worse. Paired
with similar languages, the performance gap between monolingual BERT and mBERT
can be narrowed. We find that better models for low resource languages require
more efficient pretraining techniques or more data.
","[{'version': 'v1', 'created': 'Mon, 18 May 2020 21:15:39 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Oct 2020 02:46:15 GMT'}]",2020-10-02,"[['Wu', 'Shijie', ''], ['Dredze', 'Mark', '']]"
1280437,2005.00642,Wonseok Hwang,"Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, Minjoon Seo","Spatial Dependency Parsing for Semi-Structured Document Information
  Extraction",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Information Extraction (IE) for semi-structured document images is often
approached as a sequence tagging problem by classifying each recognized input
token into one of the IOB (Inside, Outside, and Beginning) categories. However,
such problem setup has two inherent limitations that (1) it cannot easily
handle complex spatial relationships and (2) it is not suitable for highly
structured information, which are nevertheless frequently observed in
real-world document images. To tackle these issues, we first formulate the IE
task as spatial dependency parsing problem that focuses on the relationship
among text segment nodes in the documents. Under this setup, we then propose
SPADE (SPAtial DEpendency parser) that models highly complex spatial
relationships and an arbitrary number of information layers in the documents in
an end-to-end manner. We evaluate it on various kinds of documents such as
receipts, name cards, forms, and invoices, and show that it achieves a similar
or better performance compared to strong baselines including BERT-based IOB
taggger, with up to 37.7% improvement.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 22:59:56 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 22:00:46 GMT'}]",2020-10-02,"[['Hwang', 'Wonseok', ''], ['Yim', 'Jinyeong', ''], ['Park', 'Seunghyun', ''], ['Yang', 'Sohee', ''], ['Seo', 'Minjoon', '']]"
1356639,2010.00309,Tianxiang Sun,"Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing
  Huang, Zheng Zhang",CoLAKE: Contextualized Language and Knowledge Embedding,Accepted by COLING 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the emerging branch of incorporating factual knowledge into pre-trained
language models such as BERT, most existing models consider shallow, static,
and separately pre-trained entity embeddings, which limits the performance
gains of these models. Few works explore the potential of deep contextualized
knowledge representation when injecting knowledge. In this paper, we propose
the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly
learns contextualized representation for both language and knowledge with the
extended MLM objective. Instead of injecting only entity embeddings, CoLAKE
extracts the knowledge context of an entity from large-scale knowledge bases.
To handle the heterogeneity of knowledge context and language context, we
integrate them in a unified data structure, word-knowledge graph (WK graph).
CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer
encoder. We conduct experiments on knowledge-driven tasks, knowledge probing
tasks, and language understanding tasks. Experimental results show that CoLAKE
outperforms previous counterparts on most of the tasks. Besides, CoLAKE
achieves surprisingly high performance on our synthetic task called
word-knowledge graph completion, which shows the superiority of simultaneously
contextualizing language and knowledge representation.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 11:39:32 GMT'}]",2020-10-02,"[['Sun', 'Tianxiang', ''], ['Shao', 'Yunfan', ''], ['Qiu', 'Xipeng', ''], ['Guo', 'Qipeng', ''], ['Hu', 'Yaru', ''], ['Huang', 'Xuanjing', ''], ['Zhang', 'Zheng', '']]"
1317904,2007.06390,Sherzod Hakimov,"Golsa Tahmasebzadeh, Sherzod Hakimov, Eric M\""uller-Budack, Ralph
  Ewerth",A Feature Analysis for Multimodal News Retrieval,CLEOPATRA Workshop co-located with ESWC 2020,CLEOPATRA Workshop co-located with ESWC 2020,,,cs.CL cs.IR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Content-based information retrieval is based on the information contained in
documents rather than using metadata such as keywords. Most information
retrieval methods are either based on text or image. In this paper, we
investigate the usefulness of multimodal features for cross-lingual news search
in various domains: politics, health, environment, sport, and finance. To this
end, we consider five feature types for image and text and compare the
performance of the retrieval system using different combinations. Experimental
results show that retrieval results can be improved when considering both
visual and textual information. In addition, it is observed that among textual
features entity overlap outperforms word embeddings, while geolocation
embeddings achieve better performance among visual features in the retrieval
task.
","[{'version': 'v1', 'created': 'Mon, 13 Jul 2020 14:09:29 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Oct 2020 08:38:47 GMT'}]",2020-10-02,"[['Tahmasebzadeh', 'Golsa', ''], ['Hakimov', 'Sherzod', ''], ['Müller-Budack', 'Eric', ''], ['Ewerth', 'Ralph', '']]"
1356693,2010.00363,Daichi Mochihashi,"Chihiro Shibata, Kei Uchiumi, Daichi Mochihashi","How LSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization
  on Natural Text",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Long Short-Term Memory recurrent neural network (LSTM) is widely used and
known to capture informative long-term syntactic dependencies. However, how
such information are reflected in its internal vectors for natural text has not
yet been sufficiently investigated. We analyze them by learning a language
model where syntactic structures are implicitly given. We empirically show that
the context update vectors, i.e. outputs of internal gates, are approximately
quantized to binary or ternary values to help the language model to count the
depth of nesting accurately, as Suzgun et al. (2019) recently show for
synthetic Dyck languages. For some dimensions in the context vector, we show
that their activations are highly correlated with the depth of phrase
structures, such as VP and NP. Moreover, with an $L_1$ regularization, we also
found that it can accurately predict whether a word is inside a phrase
structure or not from a small number of components of the context vector. Even
for the case of learning from raw text, context vectors are shown to still
correlate well with the phrase structures. Finally, we show that natural
clusters of the functional words and the part of speeches that trigger phrases
are represented in a small but principal subspace of the context-update vector
of LSTM.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 12:49:01 GMT'}]",2020-10-02,"[['Shibata', 'Chihiro', ''], ['Uchiumi', 'Kei', ''], ['Mochihashi', 'Daichi', '']]"
1356702,2010.00372,Haixia Liu,Haixia Liu,Citation Sentiment Changes Analysis,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Metrics for measuring the citation sentiment changes were introduced.
Citation sentiment changes can be observed from global citation sentiment
sequences (GCSSs). With respect to a cited paper, the citation sentiment
sequences were analysed across a collection of citing papers ordered by the
published time. For analysing GCSSs, Eddy Dissipation Rate (EDR) was adopted,
with the hypothesis that the GCSSs pattern differences can be spotted by EDR
based method. Preliminary evidence showed that EDR based method holds the
potential for analysing a publication's impact in a time series fashion.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 13:10:03 GMT'}]",2020-10-02,"[['Liu', 'Haixia', '']]"
1356719,2010.00389,Mokanarangan Thayaparan,"Mokanarangan Thayaparan, Marco Valentino, Andr\'e Freitas",A Survey on Explainability in Machine Reading Comprehension,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a systematic review of benchmarks and approaches for
explainability in Machine Reading Comprehension (MRC). We present how the
representation and inference challenges evolved and the steps which were taken
to tackle these challenges. We also present the evaluation methodologies to
assess the performance of explainable systems. In addition, we identify
persisting open research questions and highlight critical directions for future
work.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 13:26:58 GMT'}]",2020-10-02,"[['Thayaparan', 'Mokanarangan', ''], ['Valentino', 'Marco', ''], ['Freitas', 'André', '']]"
1356784,2010.00454,Claudia Kittask,"Claudia Kittask, Kirill Milintsevich, Kairit Sirts",Evaluating Multilingual BERT for Estonian,Baltic HLT 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, large pre-trained language models, such as BERT, have reached
state-of-the-art performance in many natural language processing tasks, but for
many languages, including Estonian, BERT models are not yet available. However,
there exist several multilingual BERT models that can handle multiple languages
simultaneously and that have been trained also on Estonian data. In this paper,
we evaluate four multilingual models---multilingual BERT, multilingual
distilled BERT, XLM and XLM-RoBERTa---on several NLP tasks including POS and
morphological tagging, NER and text classification. Our aim is to establish a
comparison between these multilingual BERT models and the existing baseline
neural models for these tasks. Our results show that multilingual BERT models
can generalise well on different Estonian NLP tasks outperforming all baselines
models for POS and morphological tagging and text classification, and reaching
the comparable level with the best baseline for NER, with XLM-RoBERTa achieving
the highest results compared with other multilingual models.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 14:48:31 GMT'}]",2020-10-02,"[['Kittask', 'Claudia', ''], ['Milintsevich', 'Kirill', ''], ['Sirts', 'Kairit', '']]"
1356792,2010.00462,Ly Antoine PhD,"Antoine Ly, Benno Uthayasooriyar, Tingting Wang","A survey on natural language processing (nlp) and applications in
  insurance",Preprint,,,,stat.ML cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text is the most widely used means of communication today. This data is
abundant but nevertheless complex to exploit within algorithms. For years,
scientists have been trying to implement different techniques that enable
computers to replicate some mechanisms of human reading. During the past five
years, research disrupted the capacity of the algorithms to unleash the value
of text data. It brings today, many opportunities for the insurance
industry.Understanding those methods and, above all, knowing how to apply them
is a major challenge and key to unleash the value of text data that have been
stored for many years. Processing language with computer brings many new
opportunities especially in the insurance sector where reports are central in
the information used by insurers. SCOR's Data Analytics team has been working
on the implementation of innovative tools or products that enable the use of
the latest research on text analysis. Understanding text mining techniques in
insurance enhances the monitoring of the underwritten risks and many processes
that finally benefit policyholders.This article proposes to explain
opportunities that Natural Language Processing (NLP) are providing to
insurance. It details different methods used today in practice traces back the
story of them. We also illustrate the implementation of certain methods using
open source libraries and python codes that we have developed to facilitate the
use of these techniques.After giving a general overview on the evolution of
text mining during the past few years,we share about how to conduct a full
study with text mining and share some examples to serve those models into
insurance products or services. Finally, we explained in more details every
step that composes a Natural Language Processing study to ensure the reader can
have a deep understanding on the implementation.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 14:56:18 GMT'}]",2020-10-02,"[['Ly', 'Antoine', ''], ['Uthayasooriyar', 'Benno', ''], ['Wang', 'Tingting', '']]"
1356820,2010.00490,Daniel Deutsch,"Daniel Deutsch, Tania Bedrax-Weiss, Dan Roth","Towards Question-Answering as an Automatic Metric for Evaluating the
  Content Quality of a Summary",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, there has been growing interest in using question-answering (QA)
models to evaluate the content quality of summaries. While previous work has
shown initial promising results in this direction, their experimentation has
been limited, leading to a poor understanding of the utility of QA in
evaluating summary content. In this work, we perform an extensive evaluation of
a QA-based metric for summary content quality, calculating its performance with
today's state-of-the-art models as well as estimating its potential upper-bound
performance. We analyze a proposed metric, QAEval, which is more widely
applicable than previous work. We show that QAEval already achieves
state-of-the-art performance at scoring summarization systems, beating all
other metrics including the gold-standard Pyramid Method, while its performance
on individual summaries is at best competitive to other automatic metrics.
Through a careful analysis of each component of QAEval, we identify the
performance bottlenecks and estimate that with human-level performance,
QAEval's summary-level results have the potential to approach that of the
Pyramid Method.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 15:33:09 GMT'}]",2020-10-02,"[['Deutsch', 'Daniel', ''], ['Bedrax-Weiss', 'Tania', ''], ['Roth', 'Dan', '']]"
1356832,2010.00502,Gautam Kishore Shahi,Gautam Kishore Shahi,AMUSED: An Annotation Framework of Multi-modal Social Media Data,"10 pages, 5 figures, 3 tables",,,,cs.SI cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present a semi-automated framework called AMUSED for
gathering multi-modal annotated data from the multiple social media platforms.
The framework is designed to mitigate the issues of collecting and annotating
social media data by cohesively combining machine and human in the data
collection process. From a given list of the articles from professional news
media or blog, AMUSED detects links to the social media posts from news
articles and then downloads contents of the same post from the respective
social media platform to gather details about that specific post. The framework
is capable of fetching the annotated data from multiple platforms like Twitter,
YouTube, Reddit. The framework aims to reduce the workload and problems behind
the data annotation from the social media platforms. AMUSED can be applied in
multiple application domains, as a use case, we have implemented the framework
for collecting COVID-19 misinformation data from different social media
platforms.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 15:50:41 GMT'}]",2020-10-02,"[['Shahi', 'Gautam Kishore', '']]"
1356844,2010.00514,Shaofei Huang,"Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong
  Han, Luoqi Liu, Bo Li",Referring Image Segmentation via Cross-Modal Progressive Comprehension,"Accepted by CVPR 2020. Code is available at
  https://github.com/spyflying/CMPC-Refseg",,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Referring image segmentation aims at segmenting the foreground masks of the
entities that can well match the description given in the natural language
expression. Previous approaches tackle this problem using implicit feature
interaction and fusion between visual and linguistic modalities, but usually
fail to explore informative words of the expression to well align features from
the two modalities for accurately identifying the referred entity. In this
paper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a
Text-Guided Feature Exchange (TGFE) module to effectively address the
challenging task. Concretely, the CMPC module first employs entity and
attribute words to perceive all the related entities that might be considered
by the expression. Then, the relational words are adopted to highlight the
correct entity as well as suppress other irrelevant ones by multimodal graph
reasoning. In addition to the CMPC module, we further leverage a simple yet
effective TGFE module to integrate the reasoned multimodal features from
different levels with the guidance of textual information. In this way,
features from multi-levels could communicate with each other and be refined
based on the textual context. We conduct extensive experiments on four popular
referring segmentation benchmarks and achieve new state-of-the-art
performances.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 16:02:30 GMT'}]",2020-10-02,"[['Huang', 'Shaofei', ''], ['Hui', 'Tianrui', ''], ['Liu', 'Si', ''], ['Li', 'Guanbin', ''], ['Wei', 'Yunchao', ''], ['Han', 'Jizhong', ''], ['Liu', 'Luoqi', ''], ['Li', 'Bo', '']]"
1356530,2010.00200,Michael Bendersky,"Michael Bendersky and Honglei Zhuang and Ji Ma and Shuguang Han and
  Keith Hall and Ryan McDonald",RRF102: Meeting the TREC-COVID Challenge with a 100+ Runs Ensemble,14 pages,,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we report the results of our participation in the TREC-COVID
challenge. To meet the challenge of building a search engine for rapidly
evolving biomedical collection, we propose a simple yet effective weighted
hierarchical rank fusion approach, that ensembles together 102 runs from (a)
lexical and semantic retrieval systems, (b) pre-trained and fine-tuned BERT
rankers, and (c) relevance feedback runs. Our ablation studies demonstrate the
contributions of each of these systems to the overall ensemble. The submitted
ensemble runs achieved state-of-the-art performance in rounds 4 and 5 of the
TREC-COVID challenge.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 05:27:51 GMT'}]",2020-10-02,"[['Bendersky', 'Michael', ''], ['Zhuang', 'Honglei', ''], ['Ma', 'Ji', ''], ['Han', 'Shuguang', ''], ['Hall', 'Keith', ''], ['McDonald', 'Ryan', '']]"
1356892,2010.00562,Jose Manuel Gomez-Perez,"Jose Manuel Gomez-Perez, Raul Ortega","ISAAQ -- Mastering Textbook Questions with Pre-trained Transformers and
  Bottom-Up and Top-Down Attention",Accepted for publication as a long paper in EMNLP2020,,,,cs.CL cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Textbook Question Answering is a complex task in the intersection of Machine
Comprehension and Visual Question Answering that requires reasoning with
multimodal information from text and diagrams. For the first time, this paper
taps on the potential of transformer language models and bottom-up and top-down
attention to tackle the language and visual understanding challenges this task
entails. Rather than training a language-visual transformer from scratch we
rely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up
and top-down attention to identify regions of interest corresponding to diagram
constituents and their relationships, improving the selection of relevant
visual information for each question and answer options. Our system ISAAQ
reports unprecedented success in all TQA question types, with accuracies of
81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice
questions. ISAAQ also demonstrates its broad applicability, obtaining
state-of-the-art results in other demanding datasets.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 17:28:47 GMT'}]",2020-10-02,"[['Gomez-Perez', 'Jose Manuel', ''], ['Ortega', 'Raul', '']]"
1356528,2010.00198,Binh Nguyen,"Thai Binh Nguyen, Quang Minh Nguyen, Thi Thu Hien Nguyen, Quoc Truong
  Do, Chi Mai Luong","Improving Vietnamese Named Entity Recognition from Speech Using Word
  Capitalization and Punctuation Recovery Models",Accepted in Interspeech 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Studies on the Named Entity Recognition (NER) task have shown outstanding
results that reach human parity on input texts with correct text formattings,
such as with proper punctuation and capitalization. However, such conditions
are not available in applications where the input is speech, because the text
is generated from a speech recognition system (ASR), and that the system does
not consider the text formatting. In this paper, we (1) presented the first
Vietnamese speech dataset for NER task, and (2) the first pre-trained public
large-scale monolingual language model for Vietnamese that achieved the new
state-of-the-art for the Vietnamese NER task by 1.3% absolute F1 score
comparing to the latest study. And finally, (3) we proposed a new pipeline for
NER task from speech that overcomes the text formatting problem by introducing
a text capitalization and punctuation recovery model (CaPu) into the pipeline.
The model takes input text from an ASR system and performs two tasks at the
same time, producing proper text formatting that helps to improve NER
performance. Experimental results indicated that the CaPu model helps to
improve by nearly 4% of F1-score.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 05:21:32 GMT'}]",2020-10-02,"[['Nguyen', 'Thai Binh', ''], ['Nguyen', 'Quang Minh', ''], ['Nguyen', 'Thi Thu Hien', ''], ['Do', 'Quoc Truong', ''], ['Luong', 'Chi Mai', '']]"
1356907,2010.00577,Michael Sejr Schlichtkrull,"Michael Sejr Schlichtkrull, Nicola De Cao, Ivan Titov","Interpreting Graph Neural Networks for NLP With Differentiable Edge
  Masking",,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Graph neural networks (GNNs) have become a popular approach to integrating
structural inductive biases into NLP models. However, there has been little
work on interpreting them, and specifically on understanding which parts of the
graphs (e.g. syntactic trees or co-reference structures) contribute to a
prediction. In this work, we introduce a post-hoc method for interpreting the
predictions of GNNs which identifies unnecessary edges. Given a trained GNN
model, we learn a simple classifier that, for every edge in every layer,
predicts if that edge can be dropped. We demonstrate that such a classifier can
be trained in a fully differentiable fashion, employing stochastic gates and
encouraging sparsity through the expected $L_0$ norm. We use our technique as
an attribution method to analyze GNN models for two tasks -- question answering
and semantic role labeling -- providing insights into the information flow in
these models. We show that we can drop a large proportion of edges without
deteriorating the performance of the model, while we can analyse the remaining
edges for interpreting model predictions.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 17:51:19 GMT'}]",2020-10-02,"[['Schlichtkrull', 'Michael Sejr', ''], ['De Cao', 'Nicola', ''], ['Titov', 'Ivan', '']]"
1356500,2010.00170,Ahmed Imtiaz Humayun,"Samiul Alam, Tahsin Reasat, Asif Shahriyar Sushmit, Sadi Mohammad
  Siddiquee, Fuad Rahman, Mahady Hasan, Ahmed Imtiaz Humayun","Multi-label Classification of Common Bengali Handwritten Graphemes:
  Dataset and Challenge","9 pages, 9 figures, Submitted to AAAI/IAAI-21. For dataset and
  challenge leaderboard, see kaggle.com/c/bengaliai-cv19",,,,cs.CV cs.CL cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Latin has historically led the state-of-the-art in handwritten optical
character recognition (OCR) research. Adapting existing systems from Latin to
alpha-syllabary languages is particularly challenging due to a sharp contrast
between their orthographies. The segmentation of graphical constituents
corresponding to characters becomes significantly hard due to a cursive writing
system and frequent use of diacritics in the alpha-syllabary family of
languages. We propose a labeling scheme based on graphemes (linguistic segments
of word formation) that makes segmentation inside alpha-syllabary words linear
and present the first dataset of Bengali handwritten graphemes that are
commonly used in an everyday context. The dataset is open-sourced as a part of
the Bengali.AI Handwritten Grapheme Classification Challenge on Kaggle to
benchmark vision algorithms for multi-label grapheme classification. From
competition proceedings, we see that deep learning methods can generalize to a
large span of uncommon graphemes even when they are absent during training.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 01:51:45 GMT'}]",2020-10-02,"[['Alam', 'Samiul', ''], ['Reasat', 'Tahsin', ''], ['Sushmit', 'Asif Shahriyar', ''], ['Siddiquee', 'Sadi Mohammad', ''], ['Rahman', 'Fuad', ''], ['Hasan', 'Mahady', ''], ['Humayun', 'Ahmed Imtiaz', '']]"
1269676,2004.04906,Wen-Tau Yih,"Vladimir Karpukhin, Barlas O\u{g}uz, Sewon Min, Patrick Lewis, Ledell
  Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih",Dense Passage Retrieval for Open-Domain Question Answering,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open-domain question answering relies on efficient passage retrieval to
select candidate contexts, where traditional sparse vector space models, such
as TF-IDF or BM25, are the de facto method. In this work, we show that
retrieval can be practically implemented using dense representations alone,
where embeddings are learned from a small number of questions and passages by a
simple dual-encoder framework. When evaluated on a wide range of open-domain QA
datasets, our dense retriever outperforms a strong Lucene-BM25 system largely
by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our
end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.
","[{'version': 'v1', 'created': 'Fri, 10 Apr 2020 04:53:17 GMT'}, {'version': 'v2', 'created': 'Sat, 2 May 2020 00:53:53 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Sep 2020 21:27:13 GMT'}]",2020-10-02,"[['Karpukhin', 'Vladimir', ''], ['Oğuz', 'Barlas', ''], ['Min', 'Sewon', ''], ['Lewis', 'Patrick', ''], ['Wu', 'Ledell', ''], ['Edunov', 'Sergey', ''], ['Chen', 'Danqi', ''], ['Yih', 'Wen-tau', '']]"
1356480,2010.00150,Lena Reed,"Lena Reed, Vrindavan Harrison, Shereen Oraby, Dilek Hakkani-Tur and
  Marilyn Walker","Learning from Mistakes: Combining Ontologies via Self-Training for
  Dialogue Generation","main paper 9 pages, 3 pages references, 2 pages supplementary
  material",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language generators (NLGs) for task-oriented dialogue typically take
a meaning representation (MR) as input. They are trained end-to-end with a
corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue
acts and domain attributes. Creation of such datasets is labor-intensive and
time-consuming. Therefore, dialogue systems for new domain ontologies would
benefit from using data for pre-existing ontologies. Here we explore, for the
first time, whether it is possible to train an NLG for a new larger ontology
using existing training sets for the restaurant domain, where each set is based
on a different ontology. We create a new, larger combined ontology, and then
train an NLG to produce utterances covering it. For example, if one dataset has
attributes for family-friendly and rating information, and the other has
attributes for decor and service, our aim is an NLG for the combined ontology
that can produce utterances that realize values for family-friendly, rating,
decor and service. Initial experiments with a baseline neural
sequence-to-sequence model show that this task is surprisingly challenging. We
then develop a novel self-training method that identifies (errorful) model
outputs, automatically constructs a corrected MR input to form a new (MR,
utterance) training pair, and then repeatedly adds these new instances back
into the training data. We then test the resulting model on a new test set. The
result is a self-trained model whose performance is an absolute 75.4%
improvement over the baseline model. We also report a human qualitative
evaluation of the final model showing that it achieves high naturalness,
semantic coherence and grammaticality
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 23:54:38 GMT'}]",2020-10-02,"[['Reed', 'Lena', ''], ['Harrison', 'Vrindavan', ''], ['Oraby', 'Shereen', ''], ['Hakkani-Tur', 'Dilek', ''], ['Walker', 'Marilyn', '']]"
1356463,2010.00133,Nikita Nangia,"Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel R. Bowman","CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked
  Language Models",EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained language models, especially masked language models (MLMs) have
seen success across many NLP tasks. However, there is ample evidence that they
use the cultural biases that are undoubtedly present in the corpora they are
trained on, implicitly creating harm with biased representations. To measure
some forms of social bias in language models against protected demographic
groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark
(CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing
with nine types of bias, like race, religion, and age. In CrowS-Pairs a model
is presented with two sentences: one that is more stereotyping and another that
is less stereotyping. The data focuses on stereotypes about historically
disadvantaged groups and contrasts them with advantaged groups. We find that
all three of the widely-used MLMs we evaluate substantially favor sentences
that express stereotypes in every category in CrowS-Pairs. As work on building
less biased models advances, this dataset can be used as a benchmark to
evaluate progress.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 22:38:40 GMT'}]",2020-10-02,"[['Nangia', 'Nikita', ''], ['Vania', 'Clara', ''], ['Bhalerao', 'Rasika', ''], ['Bowman', 'Samuel R.', '']]"
1356451,2010.00121,James Powell,"James Powell, Kari Sentz",Interactive Re-Fitting as a Technique for Improving Word Embeddings,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Word embeddings are a fixed, distributional representation of the context of
words in a corpus learned from word co-occurrences. While word embeddings have
proven to have many practical uses in natural language processing tasks, they
reflect the attributes of the corpus upon which they are trained. Recent work
has demonstrated that post-processing of word embeddings to apply information
found in lexical dictionaries can improve their quality. We build on this
post-processing technique by making it interactive. Our approach makes it
possible for humans to adjust portions of a word embedding space by moving sets
of words closer to one another. One motivating use case for this capability is
to enable users to identify and reduce the presence of bias in word embeddings.
Our approach allows users to trigger selective post-processing as they interact
with and assess potential bias in word embeddings.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 21:54:22 GMT'}]",2020-10-02,"[['Powell', 'James', ''], ['Sentz', 'Kari', '']]"
1356447,2010.00117,Yuning Mao,"Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, Jiawei Han","Multi-document Summarization with Maximal Marginal Relevance-guided
  Reinforcement Learning",EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While neural sequence learning methods have made significant progress in
single-document summarization (SDS), they produce unsatisfactory results on
multi-document summarization (MDS). We observe two major challenges when
adapting SDS advances to MDS: (1) MDS involves larger search space and yet more
limited training data, setting obstacles for neural methods to learn adequate
representations; (2) MDS needs to resolve higher information redundancy among
the source documents, which SDS methods are less effective to handle. To close
the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement
Learning for MDS, which unifies advanced neural SDS methods and statistical
measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising
candidates, which restrains the search space and thus leads to better
representation learning. Additionally, the explicit redundancy measure in MMR
helps the neural representation of the summary to better capture redundancy.
Extensive experiments demonstrate that RL-MMR achieves state-of-the-art
performance on benchmark MDS datasets. In particular, we show the benefits of
incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of
both learning effectiveness and efficiency.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 21:50:46 GMT'}]",2020-10-02,"[['Mao', 'Yuning', ''], ['Qu', 'Yanru', ''], ['Xie', 'Yiqing', ''], ['Ren', 'Xiang', ''], ['Han', 'Jiawei', '']]"
1355467,2009.13964,Yusheng Su,"Yusheng Su, Xu Han, Zhengyan Zhang, Peng Li, Zhiyuan Liu, Yankai Lin,
  Jie Zhou and Maosong Sun","Contextual Knowledge Selection and Embedding towards Enhanced
  Pre-Trained Language Models",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Several recent efforts have been devoted to enhancing pre-trained language
models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs
(KGs), and achieved consistent improvements on various knowledge-driven NLP
tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs
of KGs (""knowledge context""), regardless of that the knowledge required by PLMs
may change dynamically according to specific text (""textual context""). In this
paper, we propose a novel framework named DKPLM to dynamically select and embed
knowledge context according to textual context for PLMs, which can avoid the
effect of redundant and ambiguous knowledge in KGs that cannot match the input
text. Our experimental results show that DKPLM outperforms various baselines on
typical knowledge-driven NLP tasks, indicating the effectiveness of utilizing
dynamic knowledge context for language understanding. Besides the performance
improvements, the dynamically selected knowledge in DKPLM can describe the
semantics of text-related knowledge in a more interpretable form than the
conventional PLMs. Our source code and datasets will be available to provide
more details for DKPLM.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 12:29:04 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 09:31:29 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Oct 2020 09:04:48 GMT'}]",2020-10-02,"[['Su', 'Yusheng', ''], ['Han', 'Xu', ''], ['Zhang', 'Zhengyan', ''], ['Li', 'Peng', ''], ['Liu', 'Zhiyuan', ''], ['Lin', 'Yankai', ''], ['Zhou', 'Jie', ''], ['Sun', 'Maosong', '']]"
1253369,2003.03014,Julia Mendelsohn,"Julia Mendelsohn, Yulia Tsvetkov, Dan Jurafsky",A Framework for the Computational Linguistic Analysis of Dehumanization,"31 pages, 6 figures (Appendix is 4 pages, 4 figures). Submitted to
  Frontiers in Artificial Intelligence (Language and Computation)",Frontiers in Artificial Intelligence 3 (2020):55,10.3389/frai.2020.00055,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dehumanization is a pernicious psychological process that often leads to
extreme intergroup bias, hate speech, and violence aimed at targeted social
groups. Despite these serious consequences and the wealth of available data,
dehumanization has not yet been computationally studied on a large scale.
Drawing upon social psychology research, we create a computational linguistic
framework for analyzing dehumanizing language by identifying linguistic
correlates of salient components of dehumanization. We then apply this
framework to analyze discussions of LGBTQ people in the New York Times from
1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ
people over time. However, we find that the label homosexual has emerged to be
much more strongly associated with dehumanizing attitudes than other labels,
such as gay. Our proposed techniques highlight processes of linguistic
variation and change in discourses surrounding marginalized groups.
Furthermore, the ability to analyze dehumanizing language at a large scale has
implications for automatically detecting and understanding media bias as well
as abusive language online.
","[{'version': 'v1', 'created': 'Fri, 6 Mar 2020 03:02:12 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Jun 2020 20:00:19 GMT'}]",2020-10-02,"[['Mendelsohn', 'Julia', ''], ['Tsvetkov', 'Yulia', ''], ['Jurafsky', 'Dan', '']]"
1352843,2009.11340,Tanvi Dinkar,"Tanvi Dinkar, Pierre Colombo, Matthieu Labeau and Chlo\'e Clavel",The importance of fillers for text representations of speech transcripts,To appear in EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While being an essential component of spoken language, fillers (e.g.""um"" or
""uh"") often remain overlooked in Spoken Language Understanding (SLU) tasks. We
explore the possibility of representing them with deep contextualised
embeddings, showing improvements on modelling spoken language and two
downstream tasks - predicting a speaker's stance and expressed confidence.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 19:03:58 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Oct 2020 10:02:57 GMT'}]",2020-10-02,"[['Dinkar', 'Tanvi', ''], ['Colombo', 'Pierre', ''], ['Labeau', 'Matthieu', ''], ['Clavel', 'Chloé', '']]"
1254022,2003.03667,Thayer Alshaabi,"Thayer Alshaabi, David R. Dewhurst, Joshua R. Minot, Michael V.
  Arnold, Jane L. Adams, Christopher M. Danforth, and Peter Sheridan Dodds","The growing amplification of social media: Measuring temporal and social
  contagion dynamics for over 150 languages on Twitter for 2009--2020","26 pages (16 main, 10 appendix), 15 figures (7 main, 8 appendix), and
  4 online appendices available at
  http://compstorylab.org/storywrangler/papers/tlid/",,,,cs.CL cs.LG stat.AP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Working from a dataset of 118 billion messages running from the start of 2009
to the end of 2019, we identify and explore the relative daily use of over 150
languages on Twitter. We find that eight languages comprise 80% of all tweets,
with English, Japanese, Spanish, and Portuguese being the most dominant. To
quantify social spreading in each language over time, we compute the 'contagion
ratio': The balance of retweets to organic messages. We find that for the most
common languages on Twitter there is a growing tendency, though not universal,
to retweet rather than share new content. By the end of 2019, the contagion
ratios for half of the top 30 languages, including English and Spanish, had
reached above 1---the naive contagion threshold. In 2019, the top 5 languages
with the highest average daily ratios were, in order, Thai (7.3), Hindi, Tamil,
Urdu, and Catalan, while the bottom 5 were Russian, Swedish, Esperanto,
Cebuano, and Finnish (0.26). Further, we show that over time, the contagion
ratios for most common languages are growing more strongly than those of rare
languages.
","[{'version': 'v1', 'created': 'Sat, 7 Mar 2020 21:42:50 GMT'}, {'version': 'v2', 'created': 'Sun, 15 Mar 2020 04:43:52 GMT'}, {'version': 'v3', 'created': 'Fri, 10 Jul 2020 14:49:46 GMT'}, {'version': 'v4', 'created': 'Wed, 30 Sep 2020 21:40:16 GMT'}]",2020-10-02,"[['Alshaabi', 'Thayer', ''], ['Dewhurst', 'David R.', ''], ['Minot', 'Joshua R.', ''], ['Arnold', 'Michael V.', ''], ['Adams', 'Jane L.', ''], ['Danforth', 'Christopher M.', ''], ['Dodds', 'Peter Sheridan', '']]"
1353298,2009.11795,Boon Peng Yap,"Boon Peng Yap, Andrew Koh and Eng Siong Chng","Adapting BERT for Word Sense Disambiguation with Gloss Selection
  Objective and Example Sentences",Accepted to appear in Findings of EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Domain adaptation or transfer learning using pre-trained language models such
as BERT has proven to be an effective approach for many natural language
processing tasks. In this work, we propose to formulate word sense
disambiguation as a relevance ranking task, and fine-tune BERT on sequence-pair
ranking task to select the most probable sense definition given a context
sentence and a list of candidate sense definitions. We also introduce a data
augmentation technique for WSD using existing example sentences from WordNet.
Using the proposed training objective and data augmentation technique, our
models are able to achieve state-of-the-art results on the English all-words
benchmark datasets.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 16:37:04 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Oct 2020 06:06:39 GMT'}]",2020-10-02,"[['Yap', 'Boon Peng', ''], ['Koh', 'Andrew', ''], ['Chng', 'Eng Siong', '']]"
1356520,2010.00190,Long Xuan Ma,Longxuan Ma and Weinan Zhang and Runxin Sun and Ting Liu,"A Compare Aggregate Transformer for Understanding Document-grounded
  Dialogue","7pages, 3 figures, 6 tables, Findings of EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unstructured documents serving as external knowledge of the dialogues help to
generate more informative responses. Previous research focused on knowledge
selection (KS) in the document with dialogue. However, dialogue history that is
not related to the current dialogue may introduce noise in the KS processing.
In this paper, we propose a Compare Aggregate Transformer (CAT) to jointly
denoise the dialogue context and aggregate the document information for
response generation. We designed two different comparison mechanisms to reduce
noise (before and during decoding). In addition, we propose two metrics for
evaluating document utilization efficiency based on word overlap. Experimental
results on the CMUDoG dataset show that the proposed CAT model outperforms the
state-of-the-art approach and strong baselines.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 03:44:44 GMT'}]",2020-10-02,"[['Ma', 'Longxuan', ''], ['Zhang', 'Weinan', ''], ['Sun', 'Runxin', ''], ['Liu', 'Ting', '']]"
1356856,2010.00526,Qianying Liu,"Qianying Liu, Sicong Jiang, Yizhong Wang and Sujian Li",LiveQA: A Question Answering Dataset over Sports Live,"CCL 2020 Oral paper, 11 pages",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we introduce LiveQA, a new question answering dataset
constructed from play-by-play live broadcast. It contains 117k multiple-choice
questions written by human commentators for over 1,670 NBA games, which are
collected from the Chinese Hupu (https://nba.hupu.com/games) website. Derived
from the characteristics of sports games, LiveQA can potentially test the
reasoning ability across timeline-based live broadcasts, which is challenging
compared to the existing datasets. In LiveQA, the questions require
understanding the timeline, tracking events or doing mathematical computations.
Our preliminary experiments show that the dataset introduces a challenging
problem for question answering models, and a strong baseline model only
achieves the accuracy of 53.1\% and cannot beat the dominant option rule. We
release the code and data of this paper for future research.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 16:18:51 GMT'}]",2020-10-02,"[['Liu', 'Qianying', ''], ['Jiang', 'Sicong', ''], ['Wang', 'Yizhong', ''], ['Li', 'Sujian', '']]"
1334606,2008.06884,Shengyu Zhang,"Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu,
  Jin Yu, Hongxia Yang, Fei Wu",DeVLBert: Learning Deconfounded Visio-Linguistic Representations,"10 pages, 4 figures, to appear in ACM MM 2020 proceedings",,10.1145/3394171.3413518,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose to investigate the problem of out-of-domain
visio-linguistic pretraining, where the pretraining data distribution differs
from that of downstream data on which the pretrained model will be fine-tuned.
Existing methods for this problem are purely likelihood-based, leading to the
spurious correlations and hurt the generalization ability when transferred to
out-of-domain downstream tasks. By spurious correlation, we mean that the
conditional probability of one token (object or word) given another one can be
high (due to the dataset biases) without robust (causal) relationships between
them. To mitigate such dataset biases, we propose a Deconfounded
Visio-Linguistic Bert framework, abbreviated as DeVLBert, to perform
intervention-based learning. We borrow the idea of the backdoor adjustment from
the research field of causality and propose several neural-network based
architectures for Bert-style out-of-domain pretraining. The quantitative
results on three downstream tasks, Image Retrieval (IR), Zero-shot IR, and
Visual Question Answering, show the effectiveness of DeVLBert by boosting
generalization ability.
","[{'version': 'v1', 'created': 'Sun, 16 Aug 2020 11:09:22 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 12:00:56 GMT'}]",2020-10-05,"[['Zhang', 'Shengyu', ''], ['Jiang', 'Tan', ''], ['Wang', 'Tan', ''], ['Kuang', 'Kun', ''], ['Zhao', 'Zhou', ''], ['Zhu', 'Jianke', ''], ['Yu', 'Jin', ''], ['Yang', 'Hongxia', ''], ['Wu', 'Fei', '']]"
1279753,2004.14983,Nora Hollenstein,"Giuseppe Russo, Nora Hollenstein, Claudiu Musat, Ce Zhang","Control, Generate, Augment: A Scalable Framework for Multi-Attribute
  Text Generation",Accepted at Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce CGA, a conditional VAE architecture, to control, generate, and
augment text. CGA is able to generate natural English sentences controlling
multiple semantic and syntactic attributes by combining adversarial learning
with a context-aware loss and a cyclical word dropout routine. We demonstrate
the value of the individual model components in an ablation study. The
scalability of our approach is ensured through a single discriminator,
independently of the number of attributes. We show high quality, diversity and
attribute control in the generated sentences through a series of automatic and
human assessments. As the main application of our work, we test the potential
of this new NLG model in a data augmentation scenario. In a downstream NLP
task, the sentences generated by our CGA model show significant improvements
over a strong baseline, and a classification performance often comparable to
adding same amount of additional real data.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:31:16 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 12:23:16 GMT'}]",2020-10-05,"[['Russo', 'Giuseppe', ''], ['Hollenstein', 'Nora', ''], ['Musat', 'Claudiu', ''], ['Zhang', 'Ce', '']]"
1350657,2009.09154,Ameet Deshpande,Raeid Saqur and Ameet Deshpande,"CLEVR Parser: A Graph Parser Library for Geometric Learning on Language
  Grounded Image Scenes","Accepted at NLP-OSS, EMNLP 2020 (2nd Workshop for Natural Language
  Processing Open Source Software)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The CLEVR dataset has been used extensively in language grounded visual
reasoning in Machine Learning (ML) and Natural Language Processing (NLP)
domains. We present a graph parser library for CLEVR, that provides
functionalities for object-centric attributes and relationships extraction, and
construction of structural graph representations for dual modalities.
Structural order-invariant representations enable geometric learning and can
aid in downstream tasks like language grounding to vision, robotics,
compositionality, interpretability, and computational grammar construction. We
provide three extensible main components - parser, embedder, and visualizer
that can be tailored to suit specific learning setups. We also provide
out-of-the-box functionality for seamless integration with popular deep graph
neural network (GNN) libraries. Additionally, we discuss downstream usage and
applications of the library, and how it accelerates research for the NLP
research community.
","[{'version': 'v1', 'created': 'Sat, 19 Sep 2020 03:32:37 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Oct 2020 22:56:35 GMT'}]",2020-10-05,"[['Saqur', 'Raeid', ''], ['Deshpande', 'Ameet', '']]"
1357091,2010.00761,Hongyu Gong,"Hongyu Gong, Suma Bhat, Pramod Viswanath",Enriching Word Embeddings with Temporal and Spatial Information,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The meaning of a word is closely linked to sociocultural factors that can
change over time and location, resulting in corresponding meaning changes.
Taking a global view of words and their meanings in a widely used language,
such as English, may require us to capture more refined semantics for use in
time-specific or location-aware situations, such as the study of cultural
trends or language use. However, popular vector representations for words do
not adequately include temporal or spatial information. In this work, we
present a model for learning word representation conditioned on time and
location. In addition to capturing meaning changes over time and location, we
require that the resulting word embeddings retain salient semantic and
geometric properties. We train our model on time- and location-stamped corpora,
and show using both quantitative and qualitative evaluations that it can
capture semantics across time and locations. We note that our model compares
favorably with the state-of-the-art for time-specific embedding, and serves as
a new benchmark for location-specific embeddings.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 03:15:03 GMT'}]",2020-10-05,"[['Gong', 'Hongyu', ''], ['Bhat', 'Suma', ''], ['Viswanath', 'Pramod', '']]"
1357052,2010.00722,Ameet Deshpande,Ameet Deshpande and Mitesh M. Khapra,Evaluating a Generative Adversarial Framework for Information Retrieval,,,,,cs.LG cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in Generative Adversarial Networks (GANs) have resulted in
its widespread applications to multiple domains. A recent model, IRGAN, applies
this framework to Information Retrieval (IR) and has gained significant
attention over the last few years. In this focused work, we critically analyze
multiple components of IRGAN, while providing experimental and theoretical
evidence of some of its shortcomings. Specifically, we identify issues with the
constant baseline term in the policy gradients optimization and show that the
generator harms IRGAN's performance. Motivated by our findings, we propose two
models influenced by self-contrastive estimation and co-training which
outperform IRGAN on two out of the three tasks considered.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 23:11:23 GMT'}]",2020-10-05,"[['Deshpande', 'Ameet', ''], ['Khapra', 'Mitesh M.', '']]"
1357170,2010.00840,Peng Xu,"Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung,
  Anima Anandkumar and Bryan Catanzaro","MEGATRON-CNTRL: Controllable Story Generation with External Knowledge
  Using Large-Scale Language Models",Accepted in EMNLP 2020 main conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing pre-trained large language models have shown unparalleled generative
capabilities. However, they are not controllable. In this paper, we propose
MEGATRON-CNTRL, a novel framework that uses large-scale language models and
adds control to text generation by incorporating an external knowledge base.
Our framework consists of a keyword predictor, a knowledge retriever, a
contextual knowledge ranker, and a conditional text generator. As we do not
have access to ground-truth supervision for the knowledge ranker, we make use
of weak supervision from sentence embedding. The empirical results show that
our model generates more fluent, consistent, and coherent stories with less
repetition and higher diversity compared to prior work on the ROC story
dataset. We showcase the controllability of our model by replacing the keywords
used to generate stories and re-running the generation process. Human
evaluation results show that 77.5% of these stories are successfully controlled
by the new keywords. Furthermore, by scaling our model from 124 million to 8.3
billion parameters we demonstrate that larger models improve both the quality
of generation (from 74.5% to 93.0% for consistency) and controllability (from
77.5% to 91.5%).
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 08:07:12 GMT'}]",2020-10-05,"[['Xu', 'Peng', ''], ['Patwary', 'Mostofa', ''], ['Shoeybi', 'Mohammad', ''], ['Puri', 'Raul', ''], ['Fung', 'Pascale', ''], ['Anandkumar', 'Anima', ''], ['Catanzaro', 'Bryan', '']]"
1357126,2010.00796,Chenguang Zhu,"Donghan Yu, Chenguang Zhu, Yiming Yang, Michael Zeng",JAKET: Joint Pre-training of Knowledge Graph and Language Understanding,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graphs (KGs) contain rich information about world knowledge,
entities and relations. Thus, they can be great supplements to existing
pre-trained language models. However, it remains a challenge to efficiently
integrate information from KG into language modeling. And the understanding of
a knowledge graph requires related context. We propose a novel joint
pre-training framework, JAKET, to model both the knowledge graph and language.
The knowledge module and language module provide essential information to
mutually assist each other: the knowledge module produces embeddings for
entities in text while the language module generates context-aware initial
embeddings for entities and relations in the graph. Our design enables the
pre-trained model to easily adapt to unseen knowledge graphs in new domains.
Experimental results on several knowledge-aware NLP tasks show that our
proposed framework achieves superior performance by effectively leveraging
knowledge in language understanding.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 05:53:36 GMT'}]",2020-10-05,"[['Yu', 'Donghan', ''], ['Zhu', 'Chenguang', ''], ['Yang', 'Yiming', ''], ['Zeng', 'Michael', '']]"
1357114,2010.00784,Kristjan Arumae,"Kristjan Arumae, Qing Sun, and Parminder Bhatia","An Empirical Investigation Towards Efficient Multi-Domain Language Model
  Pre-training",arXiv admin note: text overlap with arXiv:2004.03794,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-training large language models has become a standard in the natural
language processing community. Such models are pre-trained on generic data
(e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the
same domain. However, in order to achieve state-of-the-art performance on out
of domain tasks such as clinical named entity recognition and relation
extraction, additional in domain pre-training is required. In practice, staged
multi-domain pre-training presents performance deterioration in the form of
catastrophic forgetting (CF) when evaluated on a generic benchmark such as
GLUE. In this paper we conduct an empirical investigation into known methods to
mitigate CF. We find that elastic weight consolidation provides best overall
scores yielding only a 0.33% drop in performance across seven generic tasks
while remaining competitive in bio-medical tasks. Furthermore, we explore
gradient and latent clustering based data selection techniques to improve
coverage when using elastic weight consolidation and experience replay methods.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 09:20:18 GMT'}]",2020-10-05,"[['Arumae', 'Kristjan', ''], ['Sun', 'Qing', ''], ['Bhatia', 'Parminder', '']]"
1266913,2004.02143,Deepak Gupta,"Deepak Gupta, Hardik Chauhan, Akella Ravi Tej, Asif Ekbal and Pushpak
  Bhattacharyya",Reinforced Multi-task Approach for Multi-hop Question Generation,Accepted for publication in COLING 2020,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Question generation (QG) attempts to solve the inverse of question answering
(QA) problem by generating a natural language question given a document and an
answer. While sequence to sequence neural models surpass rule-based systems for
QG, they are limited in their capacity to focus on more than one supporting
fact. For QG, we often require multiple supporting facts to generate
high-quality questions. Inspired by recent works on multi-hop reasoning in QA,
we take up Multi-hop question generation, which aims at generating relevant
questions based on supporting facts in the context. We employ multitask
learning with the auxiliary task of answer-aware supporting fact prediction to
guide the question generator. In addition, we also proposed a question-aware
reward function in a Reinforcement Learning (RL) framework to maximize the
utilization of the supporting facts. We demonstrate the effectiveness of our
approach through experiments on the multi-hop question answering dataset,
HotPotQA. Empirical evaluation shows our model to outperform the single-hop
neural question generation models on both automatic evaluation metrics such as
BLEU, METEOR, and ROUGE, and human evaluation metrics for quality and coverage
of the generated questions.
","[{'version': 'v1', 'created': 'Sun, 5 Apr 2020 10:16:59 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Apr 2020 05:31:39 GMT'}, {'version': 'v3', 'created': 'Fri, 2 Oct 2020 11:35:14 GMT'}]",2020-10-05,"[['Gupta', 'Deepak', ''], ['Chauhan', 'Hardik', ''], ['Tej', 'Akella Ravi', ''], ['Ekbal', 'Asif', ''], ['Bhattacharyya', 'Pushpak', '']]"
1357090,2010.00760,Jack FitzGerald,Jack G. M. FitzGerald,"STIL -- Simultaneous Slot Filling, Translation, Intent Classification,
  and Language Identification: Initial Results using mBART on MultiATIS++","4 pages; To be published at AACL 2020; For code, see:
  https://github.com/jgmfitz/stil-mbart-multiatispp-aacl2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Slot-filling, Translation, Intent classification, and Language
identification, or STIL, is a newly-proposed task for multilingual Natural
Language Understanding (NLU). By performing simultaneous slot filling and
translation into a single output language (English in this case), some portion
of downstream system components can be monolingual, reducing development and
maintenance cost. Results are given using the multilingual BART model (Liu et
al., 2020) fine-tuned on 7 languages using the MultiATIS++ dataset. When no
translation is performed, mBART's performance is comparable to the current
state of the art system (Cross-Lingual BERT by Xu et al. (2020)) for the
languages tested, with better average intent classification accuracy (96.07%
versus 95.50%) but worse average slot F1 (89.87% versus 90.81%). When
simultaneous translation is performed, average intent classification accuracy
degrades by only 1.7% relative and average slot F1 degrades by only 1.2%
relative.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 03:09:26 GMT'}]",2020-10-05,"[['FitzGerald', 'Jack G. M.', '']]"
1357077,2010.00747,Yuhao Zhang,"Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning,
  Curtis P. Langlotz","Contrastive Learning of Medical Visual Representations from Paired
  Images and Text",,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning visual representations of medical images is core to medical image
understanding but its progress has been held back by the small size of
hand-labeled datasets. Existing work commonly relies on transferring weights
from ImageNet pretraining, which is suboptimal due to drastically different
image characteristics, or rule-based label extraction from the textual report
data paired with medical images, which is inaccurate and hard to generalize. We
propose an alternative unsupervised strategy to learn medical visual
representations directly from the naturally occurring pairing of images and
textual data. Our method of pretraining medical image encoders with the paired
text data via a bidirectional contrastive objective between the two modalities
is domain-agnostic, and requires no additional expert input. We test our method
by transferring our pretrained weights to 4 medical image classification tasks
and 2 zero-shot retrieval tasks, and show that our method leads to image
representations that considerably outperform strong baselines in most settings.
Notably, in all 4 classification tasks, our method requires only 10% as much
labeled training data as an ImageNet initialized counterpart to achieve better
or comparable performance, demonstrating superior data efficiency.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 02:10:18 GMT'}]",2020-10-05,"[['Zhang', 'Yuhao', ''], ['Jiang', 'Hang', ''], ['Miura', 'Yasuhide', ''], ['Manning', 'Christopher D.', ''], ['Langlotz', 'Curtis P.', '']]"
1357065,2010.00735,Wentao Zhu,"Yufang Huang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian Hu, Feiyu
  Xu","Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style
  Transfer",COLING 2020,,,,cs.CV cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Unsupervised text style transfer is full of challenges due to the lack of
parallel data and difficulties in content preservation. In this paper, we
propose a novel neural approach to unsupervised text style transfer, which we
refer to as Cycle-consistent Adversarial autoEncoders (CAE) trained from
non-parallel data. CAE consists of three essential components: (1) LSTM
autoencoders that encode a text in one style into its latent representation and
decode an encoded representation into its original text or a transferred
representation into a style-transferred text, (2) adversarial style transfer
networks that use an adversarially trained generator to transform a latent
representation in one style into a representation in another style, and (3) a
cycle-consistent constraint that enhances the capacity of the adversarial style
transfer networks in content preservation. The entire CAE with these three
components can be trained end-to-end. Extensive experiments and in-depth
analyses on two widely-used public datasets consistently validate the
effectiveness of proposed CAE in both style transfer and content preservation
against several strong baselines in terms of four automatic evaluation metrics
and human evaluation.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 00:43:39 GMT'}]",2020-10-05,"[['Huang', 'Yufang', ''], ['Zhu', 'Wentao', ''], ['Xiong', 'Deyi', ''], ['Zhang', 'Yiye', ''], ['Hu', 'Changjian', ''], ['Xu', 'Feiyu', '']]"
1357041,2010.00711,Marina Danilevsky,"Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas,
  Prithviraj Sen",A Survey of the State of Explainable AI for Natural Language Processing,To appear in AACL-IJCNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years have seen important advances in the quality of state-of-the-art
models, but this has come at the expense of models becoming less interpretable.
This survey presents an overview of the current state of Explainable AI (XAI),
considered within the domain of Natural Language Processing (NLP). We discuss
the main categorization of explanations, as well as the various ways
explanations can be arrived at and visualized. We detail the operations and
explainability techniques currently available for generating explanations for
NLP model predictions, to serve as a resource for model developers in the
community. Finally, we point out the current gaps and encourage directions for
future work in this important research area.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 22:33:21 GMT'}]",2020-10-05,"[['Danilevsky', 'Marina', ''], ['Qian', 'Kun', ''], ['Aharonov', 'Ranit', ''], ['Katsis', 'Yannis', ''], ['Kawas', 'Ban', ''], ['Sen', 'Prithviraj', '']]"
1357184,2010.00854,Patrick Xia,"Patrick Xia, Shijie Wu, Benjamin Van Durme",Which *BERT? A Survey Organizing Contextualized Encoders,EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained contextualized text encoders are now a staple of the NLP
community. We present a survey on language representation learning with the aim
of consolidating a series of shared lessons learned across a variety of recent
efforts. While significant advancements continue at a rapid pace, we find that
enough has now been discovered, in different directions, that we can begin to
organize advances according to common themes. Through this organization, we
highlight important considerations when interpreting recent contributions and
choosing which model to use.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 08:34:34 GMT'}]",2020-10-05,"[['Xia', 'Patrick', ''], ['Wu', 'Shijie', ''], ['Van Durme', 'Benjamin', '']]"
1357040,2010.00710,Urvashi Khandelwal,"Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, Mike
  Lewis",Nearest Neighbor Machine Translation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce $k$-nearest-neighbor machine translation ($k$NN-MT), which
predicts tokens with a nearest neighbor classifier over a large datastore of
cached examples, using representations from a neural translation model for
similarity search. This approach requires no additional training and scales to
give the decoder direct access to billions of examples at test time, resulting
in a highly expressive model that consistently improves performance across many
settings. Simply adding nearest neighbor search improves a state-of-the-art
German-English translation model by 1.5 BLEU. $k$NN-MT allows a single model to
be adapted to diverse domains by using a domain-specific datastore, improving
results by an average of 9.2 BLEU over zero-shot transfer, and achieving new
state-of-the-art results---without training on these domains. A massively
multilingual model can also be specialized for particular language pairs, with
improvements of 3 BLEU for translating from English into German and Chinese.
Qualitatively, $k$NN-MT is easily interpretable; it combines source and target
context to retrieve highly relevant examples.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 22:24:46 GMT'}]",2020-10-05,"[['Khandelwal', 'Urvashi', ''], ['Fan', 'Angela', ''], ['Jurafsky', 'Dan', ''], ['Zettlemoyer', 'Luke', ''], ['Lewis', 'Mike', '']]"
1267479,2004.02709,Matt Gardner,"Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben
  Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth
  Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel
  Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang
  Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric
  Wallace, Ally Zhang, Ben Zhou",Evaluating Models' Local Decision Boundaries via Contrast Sets,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Standard test sets for supervised learning evaluate in-distribution
generalization. Unfortunately, when a dataset has systematic gaps (e.g.,
annotation artifacts), these evaluations are misleading: a model can learn
simple decision rules that perform well on the test set but do not capture a
dataset's intended capabilities. We propose a new annotation paradigm for NLP
that helps to close systematic gaps in the test data. In particular, after a
dataset is constructed, we recommend that the dataset authors manually perturb
the test instances in small but meaningful ways that (typically) change the
gold label, creating contrast sets. Contrast sets provide a local view of a
model's decision boundary, which can be used to more accurately evaluate a
model's true linguistic capabilities. We demonstrate the efficacy of contrast
sets by creating them for 10 diverse NLP datasets (e.g., DROP reading
comprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets
are not explicitly adversarial, model performance is significantly lower on
them than on the original test sets---up to 25\% in some cases. We release our
contrast sets as new evaluation benchmarks and encourage future dataset
construction efforts to follow similar annotation processes.
","[{'version': 'v1', 'created': 'Mon, 6 Apr 2020 14:47:18 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Oct 2020 21:26:57 GMT'}]",2020-10-05,"[['Gardner', 'Matt', ''], ['Artzi', 'Yoav', ''], ['Basmova', 'Victoria', ''], ['Berant', 'Jonathan', ''], ['Bogin', 'Ben', ''], ['Chen', 'Sihao', ''], ['Dasigi', 'Pradeep', ''], ['Dua', 'Dheeru', ''], ['Elazar', 'Yanai', ''], ['Gottumukkala', 'Ananth', ''], ['Gupta', 'Nitish', ''], ['Hajishirzi', 'Hanna', ''], ['Ilharco', 'Gabriel', ''], ['Khashabi', 'Daniel', ''], ['Lin', 'Kevin', ''], ['Liu', 'Jiangming', ''], ['Liu', 'Nelson F.', ''], ['Mulcaire', 'Phoebe', ''], ['Ning', 'Qiang', ''], ['Singh', 'Sameer', ''], ['Smith', 'Noah A.', ''], ['Subramanian', 'Sanjay', ''], ['Tsarfaty', 'Reut', ''], ['Wallace', 'Eric', ''], ['Zhang', 'Ally', ''], ['Zhou', 'Ben', '']]"
1357008,2010.00678,Yan Shvartzshnaider,"Yan Shvartzshnaider, Ananth Balashankar, Vikas Patidar, Thomas Wies,
  Lakshminarayanan Subramanian","Beyond The Text: Analysis of Privacy Statements through Syntactic and
  Semantic Role Labeling","11 pages, 4 figures",,,,cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper formulates a new task of extracting privacy parameters from a
privacy policy, through the lens of Contextual Integrity, an established social
theory framework for reasoning about privacy norms. Privacy policies, written
by lawyers, are lengthy and often comprise incomplete and vague statements. In
this paper, we show that traditional NLP tasks, including the recently proposed
Question-Answering based solutions, are insufficient to address the privacy
parameter extraction problem and provide poor precision and recall. We describe
4 different types of conventional methods that can be partially adapted to
address the parameter extraction task with varying degrees of success: Hidden
Markov Models, BERT fine-tuned models, Dependency Type Parsing (DP) and
Semantic Role Labeling (SRL). Based on a detailed evaluation across 36
real-world privacy policies of major enterprises, we demonstrate that a
solution combining syntactic DP coupled with type-specific SRL tasks provides
the highest accuracy for retrieving contextual privacy parameters from privacy
statements. We also observe that incorporating domain-specific knowledge is
critical to achieving high precision and recall, thus inspiring new NLP
research to address this important problem in the privacy domain.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 20:48:37 GMT'}]",2020-10-05,"[['Shvartzshnaider', 'Yan', ''], ['Balashankar', 'Ananth', ''], ['Patidar', 'Vikas', ''], ['Wies', 'Thomas', ''], ['Subramanian', 'Lakshminarayanan', '']]"
1357007,2010.00677,Jiaming Shen,Jiaming Shen and Heng Ji and Jiawei Han,"Near-imperceptible Neural Linguistic Steganography via Self-Adjusting
  Arithmetic Coding",EMNLP 2020,,,,cs.CL cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Linguistic steganography studies how to hide secret messages in natural
language cover texts. Traditional methods aim to transform a secret message
into an innocent text via lexical substitution or syntactical modification.
Recently, advances in neural language models (LMs) enable us to directly
generate cover text conditioned on the secret message. In this study, we
present a new linguistic steganography method which encodes secret messages
using self-adjusting arithmetic coding based on a neural language model. We
formally analyze the statistical imperceptibility of this method and
empirically show it outperforms the previous state-of-the-art methods on four
datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively.
Finally, human evaluations show that 51% of generated cover texts can indeed
fool eavesdroppers.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 20:40:23 GMT'}]",2020-10-05,"[['Shen', 'Jiaming', ''], ['Ji', 'Heng', ''], ['Han', 'Jiawei', '']]"
1356997,2010.00667,Hanjie Chen,"Hanjie Chen, Yangfeng Ji","Learning Variational Word Masks to Improve the Interpretability of
  Neural Text Classifiers",EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To build an interpretable neural text classifier, most of the prior work has
focused on designing inherently interpretable models or finding faithful
explanations. A new line of work on improving model interpretability has just
started, and many existing methods require either prior information or human
annotations as additional inputs in training. To address this limitation, we
propose the variational word mask (VMASK) method to automatically learn
task-specific important words and reduce irrelevant information on
classification, which ultimately improves the interpretability of model
predictions. The proposed method is evaluated with three neural text
classifiers (CNN, LSTM, and BERT) on seven benchmark text classification
datasets. Experiments show the effectiveness of VMASK in improving both model
prediction accuracy and interpretability.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 20:02:43 GMT'}]",2020-10-05,"[['Chen', 'Hanjie', ''], ['Ji', 'Yangfeng', '']]"
1356986,2010.00656,Rui Meng,"Rui Meng, Zhen Yue, Alyssa Glass","Predicting User Engagement Status for Online Evaluation of Intelligent
  Assistants",,,,,cs.CL cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Evaluation of intelligent assistants in large-scale and online settings
remains an open challenge. User behavior-based online evaluation metrics have
demonstrated great effectiveness for monitoring large-scale web search and
recommender systems. Therefore, we consider predicting user engagement status
as the very first and critical step to online evaluation for intelligent
assistants. In this work, we first proposed a novel framework for classifying
user engagement status into four categories -- fulfillment, continuation,
reformulation and abandonment. We then demonstrated how to design simple but
indicative metrics based on the framework to quantify user engagement levels.
We also aim for automating user engagement prediction with machine learning
methods. We compare various models and features for predicting engagement
status using four real-world datasets. We conducted detailed analyses on
features and failure cases to discuss the performance of current models as well
as challenges.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 19:33:27 GMT'}]",2020-10-05,"[['Meng', 'Rui', ''], ['Yue', 'Zhen', ''], ['Glass', 'Alyssa', '']]"
1356963,2010.00633,David Vilares,David Vilares and Carlos G\'omez-Rodr\'iguez,Discontinuous Constituent Parsing as Sequence Labeling,To appear in EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper reduces discontinuous parsing to sequence labeling. It first shows
that existing reductions for constituent parsing as labeling do not support
discontinuities. Second, it fills this gap and proposes to encode tree
discontinuities as nearly ordered permutations of the input sequence. Third, it
studies whether such discontinuous representations are learnable. The
experiments show that despite the architectural simplicity, under the right
representation, the models are fast and accurate.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 18:17:58 GMT'}]",2020-10-05,"[['Vilares', 'David', ''], ['Gómez-Rodríguez', 'Carlos', '']]"
1356237,2009.14734,Nikolaos Aletras,"Danae S\'anchez Villegas, Daniel Preo\c{t}iuc-Pietro, Nikolaos Aletras",Point-of-Interest Type Inference from Social Media Text,Accepted at AACL-IJCNLP 2020,,,,cs.CL cs.SI,http://creativecommons.org/licenses/by/4.0/,"  Physical places help shape how we perceive the experiences we have there. For
the first time, we study the relationship between social media text and the
type of the place from where it was posted, whether a park, restaurant, or
someplace else. To facilitate this, we introduce a novel data set of
$\sim$200,000 English tweets published from 2,761 different points-of-interest
in the U.S., enriched with place type information. We train classifiers to
predict the type of the location a tweet was sent from that reach a macro F1 of
43.67 across eight classes and uncover the linguistic markers associated with
each type of place. The ability to predict semantic place information from a
tweet has applications in recommendation systems, personalization services and
cultural geography.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 15:21:19 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 10:31:02 GMT'}]",2020-10-05,"[['Villegas', 'Danae Sánchez', ''], ['Preoţiuc-Pietro', 'Daniel', ''], ['Aletras', 'Nikolaos', '']]"
1356228,2009.14725,Kiet Nguyen Van,"Kiet Van Nguyen, Duc-Vu Nguyen, Anh Gia-Tuan Nguyen, Ngan Luu-Thuy
  Nguyen",A Vietnamese Dataset for Evaluating Machine Reading Comprehension,Accepted by COLING 2020,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Over 97 million inhabitants speak Vietnamese as the native language in the
world. However, there are few research studies on machine reading comprehension
(MRC) in Vietnamese, the task of understanding a document or text, and
answering questions related to it. Due to the lack of benchmark datasets for
Vietnamese, we present the Vietnamese Question Answering Dataset (ViQuAD), a
new dataset for the low-resource language as Vietnamese to evaluate MRC models.
This dataset comprises over 23,000 human-generated question-answer pairs based
on 5,109 passages of 174 Vietnamese articles from Wikipedia. In particular, we
propose a new process of dataset creation for Vietnamese MRC. Our in-depth
analyses illustrate that our dataset requires abilities beyond simple reasoning
like word matching and demands complicate reasoning such as single-sentence and
multiple-sentence inferences. Besides, we conduct experiments on
state-of-the-art MRC methods in English and Chinese as the first experimental
models on ViQuAD, which will be compared to further models. We also estimate
human performances on the dataset and compare it to the experimental results of
several powerful machine models. As a result, the substantial differences
between humans and the best model performances on the dataset indicate that
improvements can be explored on ViQuAD through future research. Our dataset is
freely available to encourage the research community to overcome challenges in
Vietnamese MRC.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 15:06:56 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 14:22:12 GMT'}]",2020-10-05,"[['Van Nguyen', 'Kiet', ''], ['Nguyen', 'Duc-Vu', ''], ['Nguyen', 'Anh Gia-Tuan', ''], ['Nguyen', 'Ngan Luu-Thuy', '']]"
1357015,2010.00685,Prithviraj Ammanabrolu,"Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim
  Rockt\""aschel, Jason Weston","How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and
  Act in Fantasy Worlds",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We seek to create agents that both act and communicate with other agents in
pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019)---a
large-scale crowd-sourced fantasy text-game---with a dataset of quests. These
contain natural language motivations paired with in-game goals and human
demonstrations; completing a quest might require dialogue or actions (or both).
We introduce a reinforcement learning system that (1) incorporates large-scale
language modeling-based and commonsense reasoning-based pre-training to imbue
the agent with relevant priors; and (2) leverages a factorized action space of
action commands and dialogue, balancing between the two. We conduct zero-shot
evaluations using held-out human expert demonstrations, showing that our agents
are able to act consistently and talk naturally with respect to their
motivations.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 21:06:21 GMT'}]",2020-10-05,"[['Ammanabrolu', 'Prithviraj', ''], ['Urbanek', 'Jack', ''], ['Li', 'Margaret', ''], ['Szlam', 'Arthur', ''], ['Rocktäschel', 'Tim', ''], ['Weston', 'Jason', '']]"
1357187,2010.00857,Vani Kanjirangat,"K Vani, Sandra Mitrovic, Alessandro Antonucci, Fabio Rinaldi","SST-BERT at SemEval-2020 Task 1: Semantic Shift Tracing by Clustering in
  BERT-based Embedding Spaces",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lexical semantic change detection (also known as semantic shift tracing) is a
task of identifying words that have changed their meaning over time.
Unsupervised semantic shift tracing, focal point of SemEval2020, is
particularly challenging. Given the unsupervised setup, in this work, we
propose to identify clusters among different occurrences of each target word,
considering these as representatives of different word meanings. As such,
disagreements in obtained clusters naturally allow to quantify the level of
semantic shift per each target word in four target languages. To leverage this
idea, clustering is performed on contextualized (BERT-based) embeddings of word
occurrences. The obtained results show that our approach performs well both
measured separately (per language) and overall, where we surpass all provided
SemEval baselines.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 08:38:40 GMT'}]",2020-10-05,"[['Vani', 'K', ''], ['Mitrovic', 'Sandra', ''], ['Antonucci', 'Alessandro', ''], ['Rinaldi', 'Fabio', '']]"
1174227,1909.04189,Sandeep Soni,"Sandeep Soni, Kristina Lerman, Jacob Eisenstein","Follow the Leader: Documents on the Leading Edge of Semantic Change Get
  More Citations","25 pages, 3 figures, To appear in the Journal of the Association of
  Information Sciences and Technology",,,,cs.CL cs.SI physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diachronic word embeddings -- vector representations of words over time --
offer remarkable insights into the evolution of language and provide a tool for
quantifying sociocultural change from text documents. Prior work has used such
embeddings to identify shifts in the meaning of individual words. However,
simply knowing that a word has changed in meaning is insufficient to identify
the instances of word usage that convey the historical or the newer meaning. In
this paper, we link diachronic word embeddings to documents, by situating those
documents as leaders or laggards with respect to ongoing semantic changes.
Specifically, we propose a novel method to quantify the degree of semantic
progressiveness in each word usage, and then show how these usages can be
aggregated to obtain scores for each document. We analyze two large collections
of documents, representing legal opinions and scientific articles. Documents
that are scored as semantically progressive receive a larger number of
citations, indicating that they are especially influential. Our work thus
provides a new technique for identifying lexical semantic leaders and
demonstrates a new link between progressive use of language and influence in a
citation network.
","[{'version': 'v1', 'created': 'Mon, 9 Sep 2019 22:43:02 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Oct 2020 19:41:11 GMT'}]",2020-10-05,"[['Soni', 'Sandeep', ''], ['Lerman', 'Kristina', ''], ['Eisenstein', 'Jacob', '']]"
1357234,2010.00904,Nicola De Cao,"Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni",Autoregressive Entity Retrieval,"18 pages, 6 figures, 8 tables",,,,cs.CL cs.IR cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Entities are at the center of how we represent and aggregate knowledge. For
instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one
per article). The ability to retrieve such entities given a query is
fundamental for knowledge-intensive tasks such as entity linking and
open-domain question answering. One way to understand current approaches is as
classifiers among atomic labels, one for each entity. Their weight vectors are
dense entity representations produced by encoding entity information such as
descriptions. This approach leads to several shortcomings: i) context and
entity affinity is mainly captured through a vector dot product, potentially
missing fine-grained interactions between the two; ii) a large memory footprint
is needed to store dense representations when considering large entity sets;
iii) an appropriately hard set of negative data has to be subsampled at
training time. We propose GENRE, the first system that retrieves entities by
generating their unique names, left to right, token-by-token in an
autoregressive fashion, and conditioned on the context. This enables to
mitigate the aforementioned technical issues: i) the autoregressive formulation
allows us to directly capture relations between context and entity name,
effectively cross encoding both; ii) the memory footprint is greatly reduced
because the parameters of our encoder-decoder architecture scale with
vocabulary size, not entity count; iii) the exact softmax loss can be
efficiently computed without the need to subsample negative data. We show the
efficacy of the approach with more than 20 datasets on entity disambiguation,
end-to-end entity linking and document retrieval tasks, achieving new SOTA, or
very competitive results while using a tiny fraction of the memory of competing
systems. Finally, we demonstrate that new entities can be added by simply
specifying their unambiguous name.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 10:13:31 GMT'}]",2020-10-05,"[['De Cao', 'Nicola', ''], ['Izacard', 'Gautier', ''], ['Riedel', 'Sebastian', ''], ['Petroni', 'Fabio', '']]"
1274754,2004.09984,Linyang Li,"Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu",BERT-ATTACK: Adversarial Attack Against BERT Using BERT,Accepted by EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversarial attacks for discrete data (such as texts) have been proved
significantly more challenging than continuous data (such as images) since it
is difficult to generate adversarial samples with gradient-based methods.
Current successful attack methods for texts usually adopt heuristic replacement
strategies on the character or word level, which remains challenging to find
the optimal solution in the massive space of possible combinations of
replacements while preserving semantic consistency and language fluency. In
this paper, we propose \textbf{BERT-Attack}, a high-quality and effective
method to generate adversarial samples using pre-trained masked language models
exemplified by BERT. We turn BERT against its fine-tuned models and other deep
neural models in downstream tasks so that we can successfully mislead the
target models to predict incorrectly. Our method outperforms state-of-the-art
attack strategies in both success rate and perturb percentage, while the
generated adversarial samples are fluent and semantically preserved. Also, the
cost of calculation is low, thus possible for large-scale generations. The code
is available at https://github.com/LinyangLee/BERT-Attack.
","[{'version': 'v1', 'created': 'Tue, 21 Apr 2020 13:30:02 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Sep 2020 02:02:23 GMT'}, {'version': 'v3', 'created': 'Fri, 2 Oct 2020 03:08:04 GMT'}]",2020-10-05,"[['Li', 'Linyang', ''], ['Ma', 'Ruotian', ''], ['Guo', 'Qipeng', ''], ['Xue', 'Xiangyang', ''], ['Qiu', 'Xipeng', '']]"
1357190,2010.00860,Claire N\'edellec,"Claire N\'edellec, Wiktoria Golik, Sophie Aubin, Robert Bossy","Building Large Lexicalized Ontologies from Text: a Use Case in Automatic
  Indexing of Biotechnology Patents",,"International Conference on Knowledge Engineering and Knowledge
  Management. EKAW 2010. Lecture Notes in Computer Science, vol 6317. (pp.
  514-523) Springer, Berlin, Heidelberg",10.1007/978-3-642-16438-5_41,,cs.AI cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  This paper presents a tool, TyDI, and methods experimented in the building of
a termino-ontology, i.e. a lexicalized ontology aimed at fine-grained
indexation for semantic search applications. TyDI provides facilities for
knowledge engineers and domain experts to efficiently collaborate to validate,
organize and conceptualize corpus extracted terms. A use case on biotechnology
patent search demonstrates TyDI's potential.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 08:42:56 GMT'}]",2020-10-05,"[['Nédellec', 'Claire', ''], ['Golik', 'Wiktoria', ''], ['Aubin', 'Sophie', ''], ['Bossy', 'Robert', '']]"
1357438,2010.01108,Dumitru-Clementin Cercel,"George-Eduard Zaharia, Dumitru-Clementin Cercel, Mihai Dascalu",Cross-Lingual Transfer Learning for Complex Word Identification,"accepted at ICTAI 2020, 7 pages, 5 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Complex Word Identification (CWI) is a task centered on detecting
hard-to-understand words, or groups of words, in texts from different areas of
expertise. The purpose of CWI is to highlight problematic structures that
non-native speakers would usually find difficult to understand. Our approach
uses zero-shot, one-shot, and few-shot learning techniques, alongside
state-of-the-art solutions for Natural Language Processing (NLP) tasks (i.e.,
Transformers). Our aim is to provide evidence that the proposed models can
learn the characteristics of complex words in a multilingual environment by
relying on the CWI shared task 2018 dataset available for four different
languages (i.e., English, German, Spanish, and also French). Our approach
surpasses state-of-the-art cross-lingual results in terms of macro F1-score on
English (0.774), German (0.782), and Spanish (0.734) languages, for the
zero-shot learning scenario. At the same time, our model also outperforms the
state-of-the-art monolingual result for German (0.795 macro F1-score).
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 17:09:47 GMT'}]",2020-10-05,"[['Zaharia', 'George-Eduard', ''], ['Cercel', 'Dumitru-Clementin', ''], ['Dascalu', 'Mihai', '']]"
1357412,2010.01082,Kurt Shuster,"Kurt Shuster, Eric Michael Smith, Da Ju, Jason Weston",Multi-Modal Open-Domain Dialogue,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work in open-domain conversational agents has demonstrated that
significant improvements in model engagingness and humanness metrics can be
achieved via massive scaling in both pre-training data and model size
(Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build
agents with human-like abilities, we must expand beyond handling just text. A
particularly important topic is the ability to see images and communicate about
what is perceived. With the goal of engaging humans in multi-modal dialogue, we
investigate combining components from state-of-the-art open-domain dialogue
agents with those from state-of-the-art vision models. We study incorporating
different image fusion schemes and domain-adaptive pre-training and fine-tuning
strategies, and show that our best resulting model outperforms strong existing
models in multi-modal dialogue while simultaneously performing as well as its
predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based
conversation. We additionally investigate and incorporate safety components in
our final model, and show that such efforts do not diminish model performance
with respect to engagingness metrics.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 16:20:39 GMT'}]",2020-10-05,"[['Shuster', 'Kurt', ''], ['Smith', 'Eric Michael', ''], ['Ju', 'Da', ''], ['Weston', 'Jason', '']]"
1357410,2010.01080,Moritz Wolf,"Moritz Wolf, Dana Ruiter, Ashwin Geet D'Sa, Liane Reiners, Jan
  Alexandersson, Dietrich Klakow",HUMAN: Hierarchical Universal Modular ANnotator,"7 pages, 4 figures, EMNLP - Demonstrations 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A lot of real-world phenomena are complex and cannot be captured by single
task annotations. This causes a need for subsequent annotations, with
interdependent questions and answers describing the nature of the subject at
hand. Even in the case a phenomenon is easily captured by a single task, the
high specialisation of most annotation tools can result in having to switch to
another tool if the task only slightly changes.
  We introduce HUMAN, a novel web-based annotation tool that addresses the
above problems by a) covering a variety of annotation tasks on both textual and
image data, and b) the usage of an internal deterministic state machine,
allowing the researcher to chain different annotation tasks in an
interdependent manner. Further, the modular nature of the tool makes it easy to
define new annotation tasks and integrate machine learning algorithms e.g., for
active learning. HUMAN comes with an easy-to-use graphical user interface that
simplifies the annotation task and management.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 16:20:30 GMT'}]",2020-10-05,"[['Wolf', 'Moritz', ''], ['Ruiter', 'Dana', ''], [""D'Sa"", 'Ashwin Geet', ''], ['Reiners', 'Liane', ''], ['Alexandersson', 'Jan', ''], ['Klakow', 'Dietrich', '']]"
1357393,2010.01063,Tomasz Limisiewicz,Tomasz Limisiewicz and David Mare\v{c}ek,Syntax Representation in Word Embeddings and Neural Networks -- A Survey,,"Proceedings of the 20th Conference ITAT 2020: Automata, Formal and
  Natural Languages Workshop",,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Neural networks trained on natural language processing tasks capture syntax
even though it is not provided as a supervision signal. This indicates that
syntactic analysis is essential to the understating of language in artificial
intelligence systems. This overview paper covers approaches of evaluating the
amount of syntactic information included in the representations of words for
different neural network architectures. We mainly summarize re-search on
English monolingual data on language modeling tasks and multilingual data for
neural machine translation systems and multilingual language models. We
describe which pre-trained models and representations of language are best
suited for transfer to syntactic tasks.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 15:44:58 GMT'}]",2020-10-05,"[['Limisiewicz', 'Tomasz', ''], ['Mareček', 'David', '']]"
1271756,2004.06986,Philipp Wicke,Philipp Wicke and Marianna M. Bolognesi,"Framing COVID-19: How we conceptualize and discuss the pandemic on
  Twitter","41 pages, 6 figures",PLOS ONE 2020,10.1371/journal.pone.0240010,,cs.CL cs.SI,http://creativecommons.org/licenses/by/4.0/,"  Doctors and nurses in these weeks are busy in the trenches, fighting against
a new invisible enemy: Covid-19. Cities are locked down and civilians are
besieged in their own homes, to prevent the spreading of the virus. War-related
terminology is commonly used to frame the discourse around epidemics and
diseases. Arguably the discourse around the current epidemic will make use of
war-related metaphors too,not only in public discourse and the media, but also
in the tweets written by non-experts of mass communication. We hereby present
an analysis of the discourse around #Covid-19, based on a corpus of 200k tweets
posted on Twitter during March and April 2020. Using topic modelling we first
analyze the topics around which the discourse can be classified. Then, we show
that the WAR framing is used to talk about specific topics, such as the virus
treatment, but not others, such as the effects of social distancing on the
population. We then measure and compare the popularity of the WAR frame to
three alternative figurative frames (MONSTER, STORM and TSUNAMI) and a literal
frame used as control (FAMILY). The results show that while the FAMILY literal
frame covers a wider portion of the corpus, among the figurative framings WAR
is the most frequently used, and thus arguably the most conventional one.
However, we conclude, this frame is not apt to elaborate the discourse around
many aspects involved in the current situation. Therefore, we conclude, in line
with previous suggestions, a plethora of framing options, or a metaphor menu,
may facilitate the communication of various aspects involved in the
Covid-19-related discourse on the social media, and thus support civilians in
the expression of their feelings, opinions and ideas during the current
pandemic.
","[{'version': 'v1', 'created': 'Wed, 15 Apr 2020 10:14:15 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 10:26:50 GMT'}]",2020-10-05,"[['Wicke', 'Philipp', ''], ['Bolognesi', 'Marianna M.', '']]"
1357384,2010.01054,Eric Malmi,"Eric Malmi, Aliaksei Severyn, Sascha Rothe",Unsupervised Text Style Transfer with Padded Masked Language Models,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose Masker, an unsupervised text-editing method for style transfer. To
tackle cases when no parallel source-target pairs are available, we train
masked language models (MLMs) for both the source and the target domain. Then
we find the text spans where the two models disagree the most in terms of
likelihood. This allows us to identify the source tokens to delete to transform
the source text to match the style of the target domain. The deleted tokens are
replaced with the target MLM, and by using a padded MLM variant, we avoid
having to predetermine the number of inserted tokens. Our experiments on
sentence fusion and sentiment transfer demonstrate that Masker performs
competitively in a fully unsupervised setting. Moreover, in low-resource
settings, it improves supervised methods' accuracy by over 10 percentage points
when pre-training them on silver training data generated by Masker.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 15:33:42 GMT'}]",2020-10-05,"[['Malmi', 'Eric', ''], ['Severyn', 'Aliaksei', ''], ['Rothe', 'Sascha', '']]"
1357387,2010.01057,Ikuya Yamada,"Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji
  Matsumoto","LUKE: Deep Contextualized Entity Representations with Entity-aware
  Self-attention",EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Entity representations are useful in natural language tasks involving
entities. In this paper, we propose new pretrained contextualized
representations of words and entities based on the bidirectional transformer.
The proposed model treats words and entities in a given text as independent
tokens, and outputs contextualized representations of them. Our model is
trained using a new pretraining task based on the masked language model of
BERT. The task involves predicting randomly masked words and entities in a
large entity-annotated corpus retrieved from Wikipedia. We also propose an
entity-aware self-attention mechanism that is an extension of the
self-attention mechanism of the transformer, and considers the types of tokens
(words or entities) when computing attention scores. The proposed model
achieves impressive empirical performance on a wide range of entity-related
tasks. In particular, it obtains state-of-the-art results on five well-known
datasets: Open Entity (entity typing), TACRED (relation classification),
CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),
and SQuAD 1.1 (extractive question answering). Our source code and pretrained
representations are available at https://github.com/studio-ousia/luke.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 15:38:03 GMT'}]",2020-10-05,"[['Yamada', 'Ikuya', ''], ['Asai', 'Akari', ''], ['Shindo', 'Hiroyuki', ''], ['Takeda', 'Hideaki', ''], ['Matsumoto', 'Yuji', '']]"
1357310,2010.00980,"Andreas R\""uckl\'e","Andreas R\""uckl\'e, Jonas Pfeiffer, Iryna Gurevych","MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on
  a Massive Scale",EMNLP-2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the zero-shot transfer capabilities of text matching models on a
massive scale, by self-supervised training on 140 source domains from community
question answering forums in English. We investigate the model performances on
nine benchmarks of answer selection and question similarity tasks, and show
that all 140 models transfer surprisingly well, where the large majority of
models substantially outperforms common IR baselines. We also demonstrate that
considering a broad selection of source domains is crucial for obtaining the
best zero-shot transfer performances, which contrasts the standard procedure
that merely relies on the largest and most similar domains. In addition, we
extensively study how to best combine multiple source domains. We propose to
incorporate self-supervised with supervised multi-task learning on all
available source domains. Our best zero-shot transfer model considerably
outperforms in-domain BERT and the previous state of the art on six benchmarks.
Fine-tuning of our model with in-domain data results in additional large gains
and achieves the new state of the art on all nine benchmarks.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 13:22:12 GMT'}]",2020-10-05,"[['Rücklé', 'Andreas', ''], ['Pfeiffer', 'Jonas', ''], ['Gurevych', 'Iryna', '']]"
1357240,2010.00910,Fei Mi,"Fei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang and Boi Faltings","Continual Learning for Natural Language Generation in Task-oriented
  Dialog Systems","Accepted as Long Paper at ""Findgins of EMNLP, 2020""",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language generation (NLG) is an essential component of task-oriented
dialog systems. Despite the recent success of neural approaches for NLG, they
are typically developed in an offline manner for particular domains. To better
fit real-life applications where new data come in a stream, we study NLG in a
""continual learning"" setting to expand its knowledge to new domains or
functionalities incrementally. The major challenge towards this goal is
catastrophic forgetting, meaning that a continually trained model tends to
forget the knowledge it has learned before. To this end, we propose a method
called ARPER (Adaptively Regularized Prioritized Exemplar Replay) by replaying
prioritized historical exemplars, together with an adaptive regularization
technique based on ElasticWeight Consolidation. Extensive experiments to
continually learn new domains and intents are conducted on MultiWoZ-2.0 to
benchmark ARPER with a wide range of techniques. Empirical results demonstrate
that ARPER significantly outperforms other methods by effectively mitigating
the detrimental catastrophic forgetting issue.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 10:32:29 GMT'}]",2020-10-05,"[['Mi', 'Fei', ''], ['Chen', 'Liangwei', ''], ['Zhao', 'Mengjie', ''], ['Huang', 'Minlie', ''], ['Faltings', 'Boi', '']]"
1251099,2003.00744,Dat Quoc Nguyen,Dat Quoc Nguyen and Anh Tuan Nguyen,PhoBERT: Pre-trained language models for Vietnamese,EMNLP 2020 (Findings),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the
first public large-scale monolingual language models pre-trained for
Vietnamese. Experimental results show that PhoBERT consistently outperforms the
recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and
improves the state-of-the-art in multiple Vietnamese-specific NLP tasks
including Part-of-speech tagging, Dependency parsing, Named-entity recognition
and Natural language inference. We release PhoBERT to facilitate future
research and downstream applications for Vietnamese NLP. Our PhoBERT models are
available at https://github.com/VinAIResearch/PhoBERT
","[{'version': 'v1', 'created': 'Mon, 2 Mar 2020 10:21:17 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Apr 2020 17:36:29 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Oct 2020 09:53:19 GMT'}]",2020-10-06,"[['Nguyen', 'Dat Quoc', ''], ['Nguyen', 'Anh Tuan', '']]"
1278988,2004.14218,Zihan Liu,"Zihan Liu, Genta Indra Winata, Andrea Madotto, Pascale Fung","Exploring Fine-tuning Techniques for Pre-trained Cross-lingual Models
  via Continual Learning",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, fine-tuning pre-trained language models (e.g., multilingual BERT)
to downstream cross-lingual tasks has shown promising results. However, the
fine-tuning process inevitably changes the parameters of the pre-trained model
and weakens its cross-lingual ability, which leads to sub-optimal performance.
To alleviate this problem, we leverage continual learning to preserve the
original cross-lingual ability of the pre-trained model when we fine-tune it to
downstream tasks. The experimental result shows that our fine-tuning methods
can better preserve the cross-lingual ability of the pre-trained model in a
sentence retrieval task. Our methods also achieve better performance than other
fine-tuning baselines on the zero-shot cross-lingual part-of-speech tagging and
named entity recognition tasks.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 14:07:18 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 08:43:24 GMT'}]",2020-10-06,"[['Liu', 'Zihan', ''], ['Winata', 'Genta Indra', ''], ['Madotto', 'Andrea', ''], ['Fung', 'Pascale', '']]"
1261916,2003.11561,Felipe Maia Polo,"Felipe Maia Polo, Itamar Ciochetti, Emerson Bertolo",Interpretable Approach in the Classification of Sequences of Legal Texts,,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning applications in the legal field are numerous and diverse. In
order to make contribution to both the machine learning community and the legal
community, we have made efforts to create a model compatible with the
classification of text sequences, valuing the interpretability of the results.
The purpose of this paper is to classify Brazilian legal proceedings in three
possible status classes, which are (i) archived proceedings, (ii) active
proceedings and (iii) suspended proceedings. Although working with portuguese
NLP, which can be hard due to lack of resources, our approach performed
remarkably well in the classification task. Furthermore, we were able to
extract and interpret the patterns learnt by the neural network besides
quantifying how those patterns relate to the classification task.
","[{'version': 'v1', 'created': 'Fri, 13 Mar 2020 19:40:57 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 20:36:55 GMT'}]",2020-10-06,"[['Polo', 'Felipe Maia', ''], ['Ciochetti', 'Itamar', ''], ['Bertolo', 'Emerson', '']]"
1351698,2009.10195,Nathan Ng,"Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi","SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving
  Out-of-Domain Robustness","16 pages, 8 figures, to be published in EMNLP 2020",,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Models that perform well on a training domain often fail to generalize to
out-of-domain (OOD) examples. Data augmentation is a common method used to
prevent overfitting and improve OOD generalization. However, in natural
language, it is difficult to generate new examples that stay on the underlying
data manifold. We introduce SSMBA, a data augmentation method for generating
synthetic training examples by using a pair of corruption and reconstruction
functions to move randomly on a data manifold. We investigate the use of SSMBA
in the natural language domain, leveraging the manifold assumption to
reconstruct corrupted text with masked language models. In experiments on
robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently
outperforms existing data augmentation methods and baseline models on both
in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews,
1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.
","[{'version': 'v1', 'created': 'Mon, 21 Sep 2020 22:02:33 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 22:47:00 GMT'}]",2020-10-06,"[['Ng', 'Nathan', ''], ['Cho', 'Kyunghyun', ''], ['Ghassemi', 'Marzyeh', '']]"
1278971,2004.14201,Ruize Wang,"Ruize Wang, Duyu Tang, Nan Duan, Wanjun Zhong, Zhongyu Wei, Xuanjing
  Huang, Daxin Jiang, Ming Zhou","Leveraging Declarative Knowledge in Text and First-Order Logic for
  Fine-Grained Propaganda Detection",Accepted as a long paper to EMNLP 2020,,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the detection of propagandistic text fragments in news articles.
Instead of merely learning from input-output datapoints in training data, we
introduce an approach to inject declarative knowledge of fine-grained
propaganda techniques. Specifically, we leverage the declarative knowledge
expressed in both first-order logic and natural language. The former refers to
the logical consistency between coarse- and fine-grained predictions, which is
used to regularize the training process with propositional Boolean expressions.
The latter refers to the literal definition of each propaganda technique, which
is utilized to get class representations for regularizing the model parameters.
We conduct experiments on Propaganda Techniques Corpus, a large manually
annotated dataset for fine-grained propaganda detection. Experiments show that
our method achieves superior performance, demonstrating that leveraging
declarative knowledge can help the model to make more accurate predictions.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 13:46:15 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 13:08:40 GMT'}]",2020-10-06,"[['Wang', 'Ruize', ''], ['Tang', 'Duyu', ''], ['Duan', 'Nan', ''], ['Zhong', 'Wanjun', ''], ['Wei', 'Zhongyu', ''], ['Huang', 'Xuanjing', ''], ['Jiang', 'Daxin', ''], ['Zhou', 'Ming', '']]"
1352655,2009.11152,Pierre Colombo,"Emile Chapuis and Pierre Colombo, Matteo Manica, Matthieu Labeau,
  Chloe Clavel",Hierarchical Pre-training for Sequence Labelling in Spoken Dialog,,EMNLP 2020,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sequence labelling tasks like Dialog Act and Emotion/Sentiment identification
are a key component of spoken dialog systems. In this work, we propose a new
approach to learn generic representations adapted to spoken dialog, which we
evaluate on a new benchmark we call Sequence labellIng evaLuatIon benChmark fOr
spoken laNguagE benchmark (\texttt{SILICONE}). \texttt{SILICONE} is
model-agnostic and contains 10 different datasets of various sizes. We obtain
our representations with a hierarchical encoder based on transformer
architectures, for which we extend two well-known pre-training objectives.
Pre-training is performed on OpenSubtitles: a large corpus of spoken dialog
containing over $2.3$ billion of tokens. We demonstrate how hierarchical
encoders achieve competitive results with consistently fewer parameters
compared to state-of-the-art models and we show their importance for both
pre-training and fine-tuning.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 13:54:57 GMT'}, {'version': 'v2', 'created': 'Sat, 3 Oct 2020 15:58:48 GMT'}]",2020-10-06,"[['Chapuis', 'Emile', ''], ['Colombo', 'Pierre', ''], ['Manica', 'Matteo', ''], ['Labeau', 'Matthieu', ''], ['Clavel', 'Chloe', '']]"
1275415,2004.10645,Sewon Min,"Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer",AmbigQA: Answering Ambiguous Open-domain Questions,Published as a conference paper at EMNLP 2020 (long),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ambiguity is inherent to open-domain question answering; especially when
exploring new topics, it can be difficult to ask questions that have a single,
unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain
question answering task which involves finding every plausible answer, and then
rewriting the question for each one to resolve the ambiguity. To study this
task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open,
an existing open-domain QA benchmark. We find that over half of the questions
in NQ-open are ambiguous, with diverse sources of ambiguity such as event and
entity references. We also present strong baseline models for AmbigQA which we
show benefit from weakly supervised learning that incorporates NQ-open,
strongly suggesting our new task and data will support significant future
research effort. Our data and baselines are available at
https://nlp.cs.washington.edu/ambigqa.
","[{'version': 'v1', 'created': 'Wed, 22 Apr 2020 15:42:13 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 03:28:21 GMT'}]",2020-10-06,"[['Min', 'Sewon', ''], ['Michael', 'Julian', ''], ['Hajishirzi', 'Hannaneh', ''], ['Zettlemoyer', 'Luke', '']]"
972842,1804.11149,Sheikh Shams Azam,"Sheikh Shams Azam, Manoj Raju, Venkatesh Pagidimarri, Vamsi
  Kasivajjala",Q-Map: Clinical Concept Mining from Clinical Documents,"6 pages, 1 figure","International Journal of Computer and Information Engineering,
  12(9), 2018, 691 - 696",10.5281/zenodo.1474513,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Over the past decade, there has been a steep rise in the data-driven analysis
in major areas of medicine, such as clinical decision support system, survival
analysis, patient similarity analysis, image analytics etc. Most of the data in
the field are well-structured and available in numerical or categorical formats
which can be used for experiments directly. But on the opposite end of the
spectrum, there exists a wide expanse of data that is intractable for direct
analysis owing to its unstructured nature which can be found in the form of
discharge summaries, clinical notes, procedural notes which are in human
written narrative format and neither have any relational model nor any standard
grammatical structure. An important step in the utilization of these texts for
such studies is to transform and process the data to retrieve structured
information from the haystack of irrelevant data using information retrieval
and data mining techniques. To address this problem, the authors present Q-Map
in this paper, which is a simple yet robust system that can sift through
massive datasets with unregulated formats to retrieve structured information
aggressively and efficiently. It is backed by an effective mining technique
which is based on a string matching algorithm that is indexed on curated
knowledge sources, that is both fast and configurable. The authors also briefly
examine its comparative performance with MetaMap, one of the most reputed tools
for medical concepts retrieval and present the advantages the former displays
over the latter.
","[{'version': 'v1', 'created': 'Mon, 30 Apr 2018 12:19:03 GMT'}, {'version': 'v2', 'created': 'Mon, 16 Jul 2018 12:24:39 GMT'}]",2020-10-06,"[['Azam', 'Sheikh Shams', ''], ['Raju', 'Manoj', ''], ['Pagidimarri', 'Venkatesh', ''], ['Kasivajjala', 'Vamsi', '']]"
1268645,2004.03875,Dayiheng Liu,"Dayiheng Liu, Yeyun Gong, Jie Fu, Wei Liu, Yu Yan, Bo Shao, Daxin
  Jiang, Jiancheng Lv, Nan Duan","Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News
  Multi-Headline Generation",Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  News headline generation aims to produce a short sentence to attract readers
to read the news. One news article often contains multiple keyphrases that are
of interest to different users, which can naturally have multiple reasonable
headlines. However, most existing methods focus on the single headline
generation. In this paper, we propose generating multiple headlines with
keyphrases of user interests, whose main idea is to generate multiple
keyphrases of interest to users for the news first, and then generate multiple
keyphrase-relevant headlines. We propose a multi-source Transformer decoder,
which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered
article, and (c) original article to generate keyphrase-relevant, high-quality,
and diverse headlines. Furthermore, we propose a simple and effective method to
mine the keyphrases of interest in the news article and build a first
large-scale keyphrase-aware news headline corpus, which contains over 180K
aligned triples of $<$news article, headline, keyphrase$>$. Extensive
experimental comparisons on the real-world dataset show that the proposed
method achieves state-of-the-art results in terms of quality and diversity
","[{'version': 'v1', 'created': 'Wed, 8 Apr 2020 08:30:05 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 03:02:07 GMT'}]",2020-10-06,"[['Liu', 'Dayiheng', ''], ['Gong', 'Yeyun', ''], ['Fu', 'Jie', ''], ['Liu', 'Wei', ''], ['Yan', 'Yu', ''], ['Shao', 'Bo', ''], ['Jiang', 'Daxin', ''], ['Lv', 'Jiancheng', ''], ['Duan', 'Nan', '']]"
1339731,2008.12009,Ananya B Sai,"Ananya B. Sai, Akash Kumar Mohankumar, Mitesh M. Khapra",A Survey of Evaluation Metrics Used for NLG Systems,A condensed version of this paper is submitted to ACM CSUR,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The success of Deep Learning has created a surge in interest in a wide a
range of Natural Language Generation (NLG) tasks. Deep Learning has not only
pushed the state of the art in several existing NLG tasks but has also
facilitated researchers to explore various newer NLG tasks such as image
captioning. Such rapid progress in NLG has necessitated the development of
accurate automatic evaluation metrics that would allow us to track the progress
in the field of NLG. However, unlike classification tasks, automatically
evaluating NLG systems in itself is a huge challenge. Several works have shown
that early heuristic-based metrics such as BLEU, ROUGE are inadequate for
capturing the nuances in the different NLG tasks. The expanding number of NLG
models and the shortcomings of the current metrics has led to a rapid surge in
the number of evaluation metrics proposed since 2014. Moreover, various
evaluation metrics have shifted from using pre-determined heuristic-based
formulae to trained transformer models. This rapid change in a relatively short
time has led to the need for a survey of the existing NLG metrics to help
existing and new researchers to quickly come up to speed with the developments
that have happened in NLG evaluation in the last few years. Through this
survey, we first wish to highlight the challenges and difficulties in
automatically evaluating NLG systems. Then, we provide a coherent taxonomy of
the evaluation metrics to organize the existing metrics and to better
understand the developments in the field. We also describe the different
metrics in detail and highlight their key contributions. Later, we discuss the
main shortcomings identified in the existing metrics and describe the
methodology used to evaluate evaluation metrics. Finally, we discuss our
suggestions and recommendations on the next steps forward to improve the
automatic evaluation metrics.
","[{'version': 'v1', 'created': 'Thu, 27 Aug 2020 09:25:05 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 17:38:22 GMT'}]",2020-10-06,"[['Sai', 'Ananya B.', ''], ['Mohankumar', 'Akash Kumar', ''], ['Khapra', 'Mitesh M.', '']]"
1267104,2004.02334,Thamme Gowda,"Thamme Gowda, Jonathan May",Finding the Optimal Vocabulary Size for Neural Machine Translation,,,,,cs.CL cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We cast neural machine translation (NMT) as a classification task in an
autoregressive setting and analyze the limitations of both classification and
autoregression components. Classifiers are known to perform better with
balanced class distributions during training. Since the Zipfian nature of
languages causes imbalanced classes, we explore its effect on NMT. We analyze
the effect of various vocabulary sizes on NMT performance on multiple languages
with many data sizes, and reveal an explanation for why certain vocabulary
sizes are better than others.
","[{'version': 'v1', 'created': 'Sun, 5 Apr 2020 22:17:34 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 15:19:16 GMT'}]",2020-10-06,"[['Gowda', 'Thamme', ''], ['May', 'Jonathan', '']]"
1278968,2004.14198,Muqiao Yang,"Yao-Hung Hubert Tsai, Martin Q. Ma, Muqiao Yang, Ruslan Salakhutdinov,
  and Louis-Philippe Morency","Multimodal Routing: Improving Local and Global Interpretability of
  Multimodal Language Analysis",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The human language can be expressed through multiple sources of information
known as modalities, including tones of voice, facial gestures, and spoken
language. Recent multimodal learning with strong performances on human-centric
tasks such as sentiment analysis and emotion recognition are often black-box,
with very limited interpretability. In this paper we propose Multimodal
Routing, which dynamically adjusts weights between input modalities and output
representations differently for each input sample. Multimodal routing can
identify relative importance of both individual modalities and cross-modality
features. Moreover, the weight assignment by routing allows us to interpret
modality-prediction relationships not only globally (i.e. general trends over
the whole dataset), but also locally for each single input sample, meanwhile
keeping competitive performance compared to state-of-the-art methods.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 13:42:22 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 04:56:42 GMT'}]",2020-10-06,"[['Tsai', 'Yao-Hung Hubert', ''], ['Ma', 'Martin Q.', ''], ['Yang', 'Muqiao', ''], ['Salakhutdinov', 'Ruslan', ''], ['Morency', 'Louis-Philippe', '']]"
1200580,1911.01986,Xuan Phi Nguyen,"Xuan-Phi Nguyen, Shafiq Joty, Wu Kui, Ai Ti Aw",Data Diversification: A Simple Strategy For Neural Machine Translation,Accepted as a conference paper at NeurIPS 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce Data Diversification: a simple but effective strategy to boost
neural machine translation (NMT) performance. It diversifies the training data
by using the predictions of multiple forward and backward models and then
merging them with the original dataset on which the final NMT model is trained.
Our method is applicable to all NMT models. It does not require extra
monolingual data like back-translation, nor does it add more computations and
parameters like ensembles of models. Our method achieves state-of-the-art BLEU
scores of 30.7 and 43.7 in the WMT'14 English-German and English-French
translation tasks, respectively. It also substantially improves on 8 other
translation tasks: 4 IWSLT tasks (English-German and English-French) and 4
low-resource translation tasks (English-Nepali and English-Sinhala). We
demonstrate that our method is more effective than knowledge distillation and
dual learning, it exhibits strong correlation with ensembles of models, and it
trades perplexity off for better BLEU score. We have released our source code
at https://github.com/nxphi47/data_diversification
","[{'version': 'v1', 'created': 'Tue, 5 Nov 2019 18:25:42 GMT'}, {'version': 'v2', 'created': 'Thu, 9 Apr 2020 10:32:18 GMT'}, {'version': 'v3', 'created': 'Wed, 3 Jun 2020 16:06:04 GMT'}, {'version': 'v4', 'created': 'Sun, 4 Oct 2020 15:08:17 GMT'}]",2020-10-06,"[['Nguyen', 'Xuan-Phi', ''], ['Joty', 'Shafiq', ''], ['Kui', 'Wu', ''], ['Aw', 'Ai Ti', '']]"
1269889,2004.05119,Siddhant Garg,"Siddhant Garg, Rohit Kumar Sharma, Yingyu Liang",Beyond Fine-tuning: Few-Sample Sentence Embedding Transfer,Accepted at AACL-IJCNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Fine-tuning (FT) pre-trained sentence embedding models on small datasets has
been shown to have limitations. In this paper we show that concatenating the
embeddings from the pre-trained model with those from a simple sentence
embedding model trained only on the target data, can improve over the
performance of FT for few-sample tasks. To this end, a linear classifier is
trained on the combined embeddings, either by freezing the embedding model
weights or training the classifier and embedding models end-to-end. We perform
evaluation on seven small datasets from NLP tasks and show that our approach
with end-to-end training outperforms FT with negligible computational overhead.
Further, we also show that sophisticated combination techniques like CCA and
KCCA do not work as well in practice as concatenation. We provide theoretical
analysis to explain this empirical observation.
","[{'version': 'v1', 'created': 'Fri, 10 Apr 2020 16:57:06 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 16:57:39 GMT'}]",2020-10-06,"[['Garg', 'Siddhant', ''], ['Sharma', 'Rohit Kumar', ''], ['Liang', 'Yingyu', '']]"
1268490,2004.03720,Kaj Bostrom,Kaj Bostrom and Greg Durrett,Byte Pair Encoding is Suboptimal for Language Model Pretraining,"5 pages, 3 figures. To be published in Findings of EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The success of pretrained transformer language models (LMs) in natural
language processing has led to a wide range of pretraining setups. In
particular, these models employ a variety of subword tokenization methods, most
notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the
WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling
(Kudo, 2018), to segment text. However, to the best of our knowledge, the
literature does not contain a direct evaluation of the impact of tokenization
on language model pretraining. We analyze differences between BPE and unigram
LM tokenization, finding that the latter method recovers subword units that
align more closely with morphology and avoids problems stemming from BPE's
greedy construction procedure. We then compare the fine-tuned task performance
of identical transformer masked language models pretrained with these
tokenizations. Across downstream tasks and two languages (English and
Japanese), we find that the unigram LM tokenization method matches or
outperforms BPE. We hope that developers of future pretrained LMs will consider
adopting the unigram LM method over the more prevalent BPE.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 21:21:06 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 17:35:44 GMT'}]",2020-10-06,"[['Bostrom', 'Kaj', ''], ['Durrett', 'Greg', '']]"
1289716,2005.09921,Shota Horiguchi,"Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, Yawen Xue, Kenji
  Nagamatsu","End-to-End Speaker Diarization for an Unknown Number of Speakers with
  Encoder-Decoder Based Attractors",Accepted to INTERSPEECH 2020,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end speaker diarization for an unknown number of speakers is addressed
in this paper. Recently proposed end-to-end speaker diarization outperformed
conventional clustering-based speaker diarization, but it has one drawback: it
is less flexible in terms of the number of speakers. This paper proposes a
method for encoder-decoder based attractor calculation (EDA), which first
generates a flexible number of attractors from a speech embedding sequence.
Then, the generated multiple attractors are multiplied by the speech embedding
sequence to produce the same number of speaker activities. The speech embedding
sequence is extracted using the conventional self-attentive end-to-end neural
speaker diarization (SA-EEND) network. In a two-speaker condition, our method
achieved a 2.69 % diarization error rate (DER) on simulated mixtures and a 8.07
% DER on the two-speaker subset of CALLHOME, while vanilla SA-EEND attained
4.56 % and 9.54 %, respectively. In unknown numbers of speakers conditions, our
method attained a 15.29 % DER on CALLHOME, while the x-vector-based clustering
method achieved a 19.43 % DER.
","[{'version': 'v1', 'created': 'Wed, 20 May 2020 09:08:41 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Aug 2020 10:31:15 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Oct 2020 07:12:42 GMT'}]",2020-10-06,"[['Horiguchi', 'Shota', ''], ['Fujita', 'Yusuke', ''], ['Watanabe', 'Shinji', ''], ['Xue', 'Yawen', ''], ['Nagamatsu', 'Kenji', '']]"
1277086,2004.12316,Peixiang Zhong,"Peixiang Zhong, Chen Zhang, Hao Wang, Yong Liu, Chunyan Miao",Towards Persona-Based Empathetic Conversational Models,Accepted to EMNLP 2020,,,,cs.CL cs.AI cs.HC cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Empathetic conversational models have been shown to improve user satisfaction
and task outcomes in numerous domains. In Psychology, persona has been shown to
be highly correlated to personality, which in turn influences empathy. In
addition, our empirical analysis also suggests that persona plays an important
role in empathetic conversations. To this end, we propose a new task towards
persona-based empathetic conversations and present the first empirical study on
the impact of persona on empathetic responding. Specifically, we first present
a novel large-scale multi-domain dataset for persona-based empathetic
conversations. We then propose CoBERT, an efficient BERT-based response
selection model that obtains the state-of-the-art performance on our dataset.
Finally, we conduct extensive experiments to investigate the impact of persona
on empathetic responding. Notably, our results show that persona improves
empathetic responding more when CoBERT is trained on empathetic conversations
than non-empathetic ones, establishing an empirical link between persona and
empathy in human conversations.
","[{'version': 'v1', 'created': 'Sun, 26 Apr 2020 08:51:01 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Apr 2020 01:55:05 GMT'}, {'version': 'v3', 'created': 'Thu, 30 Apr 2020 03:40:56 GMT'}, {'version': 'v4', 'created': 'Wed, 16 Sep 2020 06:48:24 GMT'}, {'version': 'v5', 'created': 'Wed, 23 Sep 2020 08:23:51 GMT'}, {'version': 'v6', 'created': 'Mon, 5 Oct 2020 09:21:06 GMT'}]",2020-10-06,"[['Zhong', 'Peixiang', ''], ['Zhang', 'Chen', ''], ['Wang', 'Hao', ''], ['Liu', 'Yong', ''], ['Miao', 'Chunyan', '']]"
1134966,1906.02780,Nader Akoury,"Nader Akoury, Kalpesh Krishna, Mohit Iyyer","Syntactically Supervised Transformers for Faster Neural Machine
  Translation","9 pages, 5 figures, accepted to ACL 2019",Association for Computational Linguistics (2019) 1269-1281,10.18653/v1/P19-1122,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Standard decoders for neural machine translation autoregressively generate a
single target token per time step, which slows inference especially for long
outputs. While architectural advances such as the Transformer fully parallelize
the decoder computations at training time, inference still proceeds
sequentially. Recent developments in non- and semi- autoregressive decoding
produce multiple tokens per time step independently of the others, which
improves inference speed but deteriorates translation quality. In this work, we
propose the syntactically supervised Transformer (SynST), which first
autoregressively predicts a chunked parse tree before generating all of the
target tokens in one shot conditioned on the predicted parse. A series of
controlled experiments demonstrates that SynST decodes sentences ~ 5x faster
than the baseline autoregressive Transformer while achieving higher BLEU scores
than most competing methods on En-De and En-Fr datasets.
","[{'version': 'v1', 'created': 'Thu, 6 Jun 2019 19:16:16 GMT'}]",2020-10-06,"[['Akoury', 'Nader', ''], ['Krishna', 'Kalpesh', ''], ['Iyyer', 'Mohit', '']]"
1267414,2004.02644,Pedro Henrique Martins,Pedro Henrique Martins and Zita Marinho and Andr\'e F. T. Martins,Sparse Text Generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current state-of-the-art text generators build on powerful language models
such as GPT-2, achieving impressive performance. However, to avoid degenerate
text, they require sampling from a modified softmax, via temperature parameters
or ad-hoc truncation techniques, as in top-$k$ or nucleus sampling. This
creates a mismatch between training and testing conditions. In this paper, we
use the recently introduced entmax transformation to train and sample from a
natively sparse language model, avoiding this mismatch. The result is a text
generator with favorable performance in terms of fluency and consistency, fewer
repetitions, and n-gram diversity closer to human text. In order to evaluate
our model, we propose three new metrics for comparing sparse or truncated
distributions: $\epsilon$-perplexity, sparsemax score, and Jensen-Shannon
divergence. Human-evaluated experiments in story completion and dialogue
generation show that entmax sampling leads to more engaging and coherent
stories and conversations.
","[{'version': 'v1', 'created': 'Mon, 6 Apr 2020 13:09:10 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 11:17:53 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Oct 2020 11:20:54 GMT'}]",2020-10-06,"[['Martins', 'Pedro Henrique', ''], ['Marinho', 'Zita', ''], ['Martins', 'André F. T.', '']]"
1134544,1906.02358,Nisansa de Silva,Nisansa de Silva,"Survey on Publicly Available Sinhala Natural Language Processing Tools
  and Research",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.
","[{'version': 'v1', 'created': 'Wed, 5 Jun 2019 23:36:06 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Jun 2019 20:03:25 GMT'}, {'version': 'v3', 'created': 'Fri, 5 Jul 2019 06:33:19 GMT'}, {'version': 'v4', 'created': 'Mon, 22 Jul 2019 02:21:31 GMT'}, {'version': 'v5', 'created': 'Mon, 13 Jan 2020 07:07:39 GMT'}, {'version': 'v6', 'created': 'Sun, 4 Oct 2020 04:59:22 GMT'}]",2020-10-06,"[['de Silva', 'Nisansa', '']]"
1201608,1911.03014,Simeng Han,"Simeng Han, Xiang Lin and Shafiq Joty",Resurrecting Submodularity for Neural Text Generation,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Submodularity is desirable for a variety of objectives in content selection
where the current neural encoder-decoder framework is inadequate. However, it
has so far not been explored in the neural encoder-decoder system for text
generation. In this work, we define diminishing attentions with submodular
functions and in turn, prove the submodularity of the effective neural
coverage. The greedy algorithm approximating the solution to the submodular
maximization problem is not suited to attention score optimization in
auto-regressive generation. Therefore instead of following how submodular
function has been widely used, we propose a simplified yet principled solution.
The resulting attention module offers an architecturally simple and empirically
effective method to improve the coverage of neural text generation. We run
experiments on three directed text generation tasks with different levels of
recovering rate, across two modalities, three different neural model
architectures and two training strategy variations. The results and analyses
demonstrate that our method generalizes well across these settings, produces
texts of good quality and outperforms state-of-the-art baselines.
","[{'version': 'v1', 'created': 'Fri, 8 Nov 2019 03:17:54 GMT'}, {'version': 'v2', 'created': 'Sat, 13 Jun 2020 16:47:33 GMT'}, {'version': 'v3', 'created': 'Sun, 4 Oct 2020 08:09:57 GMT'}]",2020-10-06,"[['Han', 'Simeng', ''], ['Lin', 'Xiang', ''], ['Joty', 'Shafiq', '']]"
1201748,1911.03154,Yun Chen,"Yun Chen, Liangyou Li, Xin Jiang, Xiao Chen, Qun Liu","A General Framework for Adaptation of Neural Machine Translation to
  Simultaneous Translation",Accepted by AACL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the success of neural machine translation (NMT), simultaneous neural
machine translation (SNMT), the task of translating in real time before a full
sentence has been observed, remains challenging due to the syntactic structure
difference and simultaneity requirements. In this paper, we propose a general
framework for adapting neural machine translation to translate simultaneously.
Our framework contains two parts: prefix translation that utilizes a
consecutive NMT model to translate source prefixes and a stopping criterion
that determines when to stop the prefix translation. Experiments on three
translation corpora and two language pairs show the efficacy of the proposed
framework on balancing the quality and latency in adapting NMT to perform
simultaneous translation.
","[{'version': 'v1', 'created': 'Fri, 8 Nov 2019 09:55:37 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 02:39:36 GMT'}]",2020-10-06,"[['Chen', 'Yun', ''], ['Li', 'Liangyou', ''], ['Jiang', 'Xin', ''], ['Chen', 'Xiao', ''], ['Liu', 'Qun', '']]"
1201956,1911.03362,Nikolay Bogoychev Dr,Nikolay Bogoychev and Rico Sennrich,"Domain, Translationese and Noise in Synthetic Data for Neural Machine
  Translation",,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The quality of neural machine translation can be improved by leveraging
additional monolingual resources to create synthetic training data. Source-side
monolingual data can be (forward-)translated into the target language for
self-training; target-side monolingual data can be back-translated. It has been
widely reported that back-translation delivers superior results, but could this
be due to artefacts in the test sets? We perform a case study using
French-English news translation task and separate test sets based on their
original languages. We show that forward translation delivers superior gains in
terms of BLEU on sentences that were originally in the source language,
complementing previous studies which show large improvements with
back-translation on sentences that were originally in the target language. To
better understand when and why forward and back-translation are effective, we
study the role of domains, translationese, and noise. While translationese
effects are well known to influence MT evaluation, we also find evidence that
news data from different languages shows subtle domain differences, which is
another explanation for varying performance on different portions of the test
set. We perform additional low-resource experiments which demonstrate that
forward translation is more sensitive to the quality of the initial translation
system than back-translation, and tends to perform worse in low-resource
settings.
","[{'version': 'v1', 'created': 'Wed, 6 Nov 2019 17:30:57 GMT'}, {'version': 'v2', 'created': 'Sat, 3 Oct 2020 17:14:02 GMT'}]",2020-10-06,"[['Bogoychev', 'Nikolay', ''], ['Sennrich', 'Rico', '']]"
1253094,2003.02739,Farhad Nooralahzadeh,"Farhad Nooralahzadeh, Giannis Bekoulis, Johannes Bjerva, Isabelle
  Augenstein",Zero-Shot Cross-Lingual Transfer with Meta Learning,Accepted as long paper in EMNLP2020 main conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning what to share between tasks has been a topic of great importance
recently, as strategic sharing of knowledge has been shown to improve
downstream task performance. This is particularly important for multilingual
applications, as most languages in the world are under-resourced. Here, we
consider the setting of training models on multiple different languages at the
same time, when little or no data is available for languages other than
English. We show that this challenging setup can be approached using
meta-learning, where, in addition to training a source language model, another
model learns to select which training instances are the most beneficial to the
first. We experiment using standard supervised, zero-shot cross-lingual, as
well as few-shot cross-lingual settings for different natural language
understanding tasks (natural language inference, question answering). Our
extensive experimental setup demonstrates the consistent effectiveness of
meta-learning for a total of 15 languages. We improve upon the state-of-the-art
for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA
dataset). A comprehensive error analysis indicates that the correlation of
typological features between languages can partly explain when parameter
sharing learned via meta-learning is beneficial.
","[{'version': 'v1', 'created': 'Thu, 5 Mar 2020 16:07:32 GMT'}, {'version': 'v2', 'created': 'Thu, 2 Apr 2020 14:40:52 GMT'}, {'version': 'v3', 'created': 'Wed, 29 Apr 2020 09:12:11 GMT'}, {'version': 'v4', 'created': 'Mon, 5 Oct 2020 13:18:00 GMT'}]",2020-10-06,"[['Nooralahzadeh', 'Farhad', ''], ['Bekoulis', 'Giannis', ''], ['Bjerva', 'Johannes', ''], ['Augenstein', 'Isabelle', '']]"
1347055,2009.05552,Qi Huang,"Tong Gao, Qi Huang, Raymond J. Mooney",Systematic Generalization on gSCAN with Language Conditioned Embedding,"Accepted by AACL-IJCNLP 2020. Huang and Gao share co-first
  authorship, authors contribute equally and are listed in alphabetical order",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Systematic Generalization refers to a learning algorithm's ability to
extrapolate learned behavior to unseen situations that are distinct but
semantically similar to its training data. As shown in recent work,
state-of-the-art deep learning models fail dramatically even on tasks for which
they are designed when the test set is systematically different from the
training data. We hypothesize that explicitly modeling the relations between
objects in their contexts while learning their representations will help
achieve systematic generalization. Therefore, we propose a novel method that
learns objects' contextualized embeddings with dynamic message passing
conditioned on the input natural language and end-to-end trainable with other
downstream deep learning modules. To our knowledge, this model is the first one
that significantly outperforms the provided baseline and reaches
state-of-the-art performance on grounded-SCAN (gSCAN), a grounded natural
language navigation dataset designed to require systematic generalization in
its test splits.
","[{'version': 'v1', 'created': 'Fri, 11 Sep 2020 17:35:05 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 20:59:57 GMT'}]",2020-10-06,"[['Gao', 'Tong', ''], ['Huang', 'Qi', ''], ['Mooney', 'Raymond J.', '']]"
1349118,2009.07615,Junfan Chen,"Junfan Chen, Richong Zhang, Yongyi Mao, Jie Xu",Neural Dialogue State Tracking with Temporally Expressive Networks,Accepted by Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialogue state tracking (DST) is an important part of a spoken dialogue
system. Existing DST models either ignore temporal feature dependencies across
dialogue turns or fail to explicitly model temporal state dependencies in a
dialogue. In this work, we propose Temporally Expressive Networks (TEN) to
jointly model the two types of temporal dependencies in DST. The TEN model
utilizes the power of recurrent networks and probabilistic graphical models.
Evaluating on standard datasets, TEN is demonstrated to be effective in
improving the accuracy of turn-level-state prediction and the state
aggregation.
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 11:53:00 GMT'}, {'version': 'v2', 'created': 'Sat, 3 Oct 2020 07:36:07 GMT'}]",2020-10-06,"[['Chen', 'Junfan', ''], ['Zhang', 'Richong', ''], ['Mao', 'Yongyi', ''], ['Xu', 'Jie', '']]"
1350445,2009.08942,Tuhin Chakrabarty Mr,"Tuhin Chakrabarty, Smaranda Muresan, Nanyun Peng","Generating similes effortlessly like a Pro: A Style Transfer Approach
  for Simile Generation",EMNLP 2020,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Literary tropes, from poetry to stories, are at the crux of human imagination
and communication. Figurative language such as a simile go beyond plain
expressions to give readers new insights and inspirations. In this paper, we
tackle the problem of simile generation. Generating a simile requires proper
understanding for effective mapping of properties between two concepts. To this
end, we first propose a method to automatically construct a parallel corpus by
transforming a large number of similes collected from Reddit to their literal
counterpart using structured common sense knowledge. We then propose to
fine-tune a pretrained sequence to sequence model, BART~\cite{lewis2019bart},
on the literal-simile pairs to gain generalizability, so that we can generate
novel similes given a literal sentence. Experiments show that our approach
generates $88\%$ novel similes that do not share properties with the training
data. Human evaluation on an independent set of literal statements shows that
our model generates similes better than two literary experts
\textit{37\%}\footnote{We average 32.6\% and 41.3\% for 2 humans.} of the
times, and three baseline systems including a recent metaphor generation model
\textit{71\%}\footnote{We average 82\% ,63\% and 68\% for three baselines.} of
the times when compared pairwise.\footnote{The simile in the title is generated
by our best model. Input: Generating similes effortlessly, output: Generating
similes \textit{like a Pro}.} We also show how replacing literal sentences with
similes from our best model in machine generated stories improves evocativeness
and leads to better acceptance by human judges.
","[{'version': 'v1', 'created': 'Fri, 18 Sep 2020 17:37:13 GMT'}, {'version': 'v2', 'created': 'Sat, 3 Oct 2020 05:47:43 GMT'}]",2020-10-06,"[['Chakrabarty', 'Tuhin', ''], ['Muresan', 'Smaranda', ''], ['Peng', 'Nanyun', '']]"
1350977,2009.09474,Ehsan Doostmohammadi,"Ehsan Doostmohammadi, Minoo Nassajian, Adel Rahimi","Persian Ezafe Recognition Using Transformers and Its Role in
  Part-Of-Speech Tagging",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ezafe is a grammatical particle in some Iranian languages that links two
words together. Regardless of the important information it conveys, it is
almost always not indicated in Persian script, resulting in mistakes in reading
complex sentences and errors in natural language processing tasks. In this
paper, we experiment with different machine learning methods to achieve
state-of-the-art results in the task of ezafe recognition. Transformer-based
methods, BERT and XLMRoBERTa, achieve the best results, the latter achieving
2.68% F1-score more than the previous state-of-the-art. We, moreover, use ezafe
information to improve Persian part-of-speech tagging results and show that
such information will not be useful to transformer-based methods and explain
why that might be the case.
","[{'version': 'v1', 'created': 'Sun, 20 Sep 2020 17:01:43 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 19:52:11 GMT'}]",2020-10-06,"[['Doostmohammadi', 'Ehsan', ''], ['Nassajian', 'Minoo', ''], ['Rahimi', 'Adel', '']]"
1268059,2004.03289,Yo Joong Choe,"Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, Hyungjoon Soh","KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language
  Understanding","Findings of EMNLP 2020. Datasets available at
  https://github.com/kakaobrain/KorNLUDatasets",,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Natural language inference (NLI) and semantic textual similarity (STS) are
key tasks in natural language understanding (NLU). Although several benchmark
datasets for those tasks have been released in English and a few other
languages, there are no publicly available NLI or STS datasets in the Korean
language. Motivated by this, we construct and release new datasets for Korean
NLI and STS, dubbed KorNLI and KorSTS, respectively. Following previous
approaches, we machine-translate existing English training sets and manually
translate development and test sets into Korean. To accelerate research on
Korean NLU, we also establish baselines on KorNLI and KorSTS. Our datasets are
publicly available at https://github.com/kakaobrain/KorNLUDatasets.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 11:49:15 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Apr 2020 04:15:35 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Oct 2020 09:28:51 GMT'}]",2020-10-06,"[['Ham', 'Jiyeon', ''], ['Choe', 'Yo Joong', ''], ['Park', 'Kyubyong', ''], ['Choi', 'Ilji', ''], ['Soh', 'Hyungjoon', '']]"
1274583,2004.09813,Nils Reimers,"Nils Reimers, Iryna Gurevych","Making Monolingual Sentence Embeddings Multilingual using Knowledge
  Distillation",Accepted at EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  We present an easy and efficient method to extend existing sentence embedding
models to new languages. This allows to create multilingual versions from
previously monolingual models. The training is based on the idea that a
translated sentence should be mapped to the same location in the vector space
as the original sentence. We use the original (monolingual) model to generate
sentence embeddings for the source language and then train a new system on
translated sentences to mimic the original model. Compared to other methods for
training multilingual sentence embeddings, this approach has several
advantages: It is easy to extend existing models with relatively few samples to
new languages, it is easier to ensure desired properties for the vector space,
and the hardware requirements for training is lower. We demonstrate the
effectiveness of our approach for 50+ languages from various language families.
Code to extend sentence embeddings models to more than 400 languages is
publicly available.
","[{'version': 'v1', 'created': 'Tue, 21 Apr 2020 08:20:25 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 06:30:56 GMT'}]",2020-10-06,"[['Reimers', 'Nils', ''], ['Gurevych', 'Iryna', '']]"
1350920,2009.09417,Byung-Ju Choi,"Byung-Ju Choi, Jimin Hong, David Keetae Park, Sang Wan Lee","F^2-Softmax: Diversifying Neural Text Generation via Frequency
  Factorized Softmax",EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite recent advances in neural text generation, encoding the rich
diversity in human language remains elusive. We argue that the sub-optimal text
generation is mainly attributable to the imbalanced token distribution, which
particularly misdirects the learning model when trained with the
maximum-likelihood objective. As a simple yet effective remedy, we propose two
novel methods, F^2-Softmax and MefMax, for a balanced training even with the
skewed frequency distribution. MefMax assigns tokens uniquely to frequency
classes, trying to group tokens with similar frequencies and equalize frequency
mass between the classes. F^2-Softmax then decomposes a probability
distribution of the target token into a product of two conditional
probabilities of (i) frequency class, and (ii) token from the target frequency
class. Models learn more uniform probability distributions because they are
confined to subsets of vocabularies. Significant performance gains on seven
relevant metrics suggest the supremacy of our approach in improving not only
the diversity but also the quality of generated texts.
","[{'version': 'v1', 'created': 'Sun, 20 Sep 2020 12:03:58 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 08:46:58 GMT'}]",2020-10-06,"[['Choi', 'Byung-Ju', ''], ['Hong', 'Jimin', ''], ['Park', 'David Keetae', ''], ['Lee', 'Sang Wan', '']]"
1358335,2010.02005,Maaike de Boer,Maaike Burghoorn and Maaike H.T. de Boer and Stephan Raaijmakers,Gender prediction using limited Twitter Data,"6 pages, 2 figures",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer models have shown impressive performance on a variety of NLP
tasks. Off-the-shelf, pre-trained models can be fine-tuned for specific NLP
classification tasks, reducing the need for large amounts of additional
training data. However, little research has addressed how much data is required
to accurately fine-tune such pre-trained transformer models, and how much data
is needed for accurate prediction. This paper explores the usability of BERT (a
Transformer model for word embedding) for gender prediction on social media.
Forensic applications include detecting gender obfuscation, e.g. males posing
as females in chat rooms. A Dutch BERT model is fine-tuned on different samples
of a Dutch Twitter dataset labeled for gender, varying in the number of tweets
used per person. The results show that finetuning BERT contributes to good
gender classification performance (80% F1) when finetuned on only 200 tweets
per person. But when using just 20 tweets per person, the performance of our
classifier deteriorates non-steeply (to 70% F1). These results show that even
with relatively small amounts of data, BERT can be fine-tuned to accurately
help predict the gender of Twitter users, and, consequently, that it is
possible to determine gender on the basis of just a low volume of tweets. This
opens up an operational perspective on the swift detection of gender.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 11:46:07 GMT'}]",2020-10-06,"[['Burghoorn', 'Maaike', ''], ['de Boer', 'Maaike H. T.', ''], ['Raaijmakers', 'Stephan', '']]"
1247683,2002.10260,Alessandro Raganato,"Alessandro Raganato, Yves Scherrer and J\""org Tiedemann","Fixed Encoder Self-Attention Patterns in Transformer-Based Machine
  Translation",Accepted to Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer-based models have brought a radical change to neural machine
translation. A key feature of the Transformer architecture is the so-called
multi-head attention mechanism, which allows the model to focus simultaneously
on different parts of the input. However, recent works have shown that most
attention heads learn simple, and often redundant, positional patterns. In this
paper, we propose to replace all but one attention head of each encoder layer
with simple fixed -- non-learnable -- attentive patterns that are solely based
on position and do not require any external knowledge. Our experiments with
different data sizes and multiple language pairs show that fixing the attention
heads on the encoder side of the Transformer at training time does not impact
the translation quality and even increases BLEU scores by up to 3 points in
low-resource scenarios.
","[{'version': 'v1', 'created': 'Mon, 24 Feb 2020 13:53:06 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Apr 2020 18:36:07 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Oct 2020 16:10:31 GMT'}]",2020-10-06,"[['Raganato', 'Alessandro', ''], ['Scherrer', 'Yves', ''], ['Tiedemann', 'Jörg', '']]"
1357675,2010.01345,Zhao Meng,"Zhao Meng, Roger Wattenhofer","A Geometry-Inspired Attack for Generating Natural Language Adversarial
  Examples",COLING 2020 Long Paper,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating adversarial examples for natural language is hard, as natural
language consists of discrete symbols, and examples are often of variable
lengths. In this paper, we propose a geometry-inspired attack for generating
natural language adversarial examples. Our attack generates adversarial
examples by iteratively approximating the decision boundary of Deep Neural
Networks (DNNs). Experiments on two datasets with two different models show
that our attack fools natural language models with high success rates, while
only replacing a few words. Human evaluation shows that adversarial examples
generated by our attack are hard for humans to recognize. Further experiments
show that adversarial training can improve model robustness against our attack.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 12:58:47 GMT'}]",2020-10-06,"[['Meng', 'Zhao', ''], ['Wattenhofer', 'Roger', '']]"
1313614,2007.02100,Alberto Cetoli,Alberto Cetoli,Pynsett: A programmable relation extractor,Accepted for publication in SEMAPRO2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper proposes a programmable relation extraction method for the English
language by parsing texts into semantic graphs. A person can define rules in
plain English that act as matching patterns onto the graph representation.
These rules are designed to capture the semantic content of the documents,
allowing for flexibility and ad-hoc entities. Relation extraction is a complex
task that typically requires sizable training corpora. The method proposed here
is ideal for extracting specialized ontologies in a limited collection of
documents.
","[{'version': 'v1', 'created': 'Sat, 4 Jul 2020 14:03:48 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 22:52:29 GMT'}]",2020-10-06,"[['Cetoli', 'Alberto', '']]"
1357740,2010.01410,Hariharan Sezhiyan,"David Gros, Hariharan Sezhiyan, Prem Devanbu, Zhou Yu","Code to Comment ""Translation"": Data, Metrics, Baselining & Evaluation",,,,,cs.SE cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The relationship of comments to code, and in particular, the task of
generating useful comments given the code, has long been of interest. The
earliest approaches have been based on strong syntactic theories of
comment-structures, and relied on textual templates. More recently, researchers
have applied deep learning methods to this task, and specifically, trainable
generative translation models which are known to work very well for Natural
Language translation (e.g., from German to English). We carefully examine the
underlying assumption here: that the task of generating comments sufficiently
resembles the task of translating between natural languages, and so similar
models and evaluation metrics could be used. We analyze several recent
code-comment datasets for this task: CodeNN, DeepCom, FunCom, and DocString. We
compare them with WMT19, a standard dataset frequently used to train state of
the art natural language translators. We found some interesting differences
between the code-comment data and the WMT19 natural language data. Next, we
describe and conduct some studies to calibrate BLEU (which is commonly used as
a measure of comment quality). using ""affinity pairs"" of methods, from
different projects, in the same project, in the same class, etc; Our study
suggests that the current performance on some datasets might need to be
improved substantially. We also argue that fairly naive information retrieval
(IR) methods do well enough at this task to be considered a reasonable
baseline. Finally, we make some suggestions on how our findings might be used
in future research in this area.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 18:57:26 GMT'}]",2020-10-06,"[['Gros', 'David', ''], ['Sezhiyan', 'Hariharan', ''], ['Devanbu', 'Prem', ''], ['Yu', 'Zhou', '']]"
1357747,2010.01417,Kun Xu,"Kun Xu and Haochen Tan and Linfeng Song and Han Wu and Haisong Zhang
  and Linqi Song and Dong Yu",Semantic Role Labeling Guided Multi-turn Dialogue ReWriter,To appear in EMNLP 2020,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  For multi-turn dialogue rewriting, the capacity of effectively modeling the
linguistic knowledge in dialog context and getting rid of the noises is
essential to improve its performance. Existing attentive models attend to all
words without prior focus, which results in inaccurate concentration on some
dispensable words. In this paper, we propose to use semantic role labeling
(SRL), which highlights the core semantic information of who did what to whom,
to provide additional guidance for the rewriter model. Experiments show that
this information significantly improves a RoBERTa-based model that already
outperforms previous state-of-the-art systems.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 19:50:04 GMT'}]",2020-10-06,"[['Xu', 'Kun', ''], ['Tan', 'Haochen', ''], ['Song', 'Linfeng', ''], ['Wu', 'Han', ''], ['Zhang', 'Haisong', ''], ['Song', 'Linqi', ''], ['Yu', 'Dong', '']]"
1357759,2010.01429,Krenare Pireva Nuci,Rinor Hajrizi and Krenare Pireva Nu\c{c}i,Aspect-Based Sentiment Analysis in Education Domain,"Sentiment Analysis, 8 pages",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Analysis of a large amount of data has always brought value to institutions
and organizations. Lately, people's opinions expressed through text have become
a very important aspect of this analysis. In response to this challenge, a
natural language processing technique known as Aspect-Based Sentiment Analysis
(ABSA) has emerged. Having the ability to extract the polarity for each aspect
of opinions separately, ABSA has found itself useful in a wide range of
domains. Education is one of the domains in which ABSA can be successfully
utilized. Being able to understand and find out what students like and don't
like most about a course, professor, or teaching methodology can be of great
importance for the respective institutions. While this task represents a unique
NLP challenge, many studies have proposed different approaches to tackle the
problem. In this work, we present a comprehensive review of the existing work
in ABSA with a focus in the education domain. A wide range of methodologies are
discussed and conclusions are drawn.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 21:51:47 GMT'}]",2020-10-06,"[['Hajrizi', 'Rinor', ''], ['Nuçi', 'Krenare Pireva', '']]"
1357777,2010.01447,Shiquan Yang,"Shiquan Yang, Rui Zhang, Sarah Erfani","GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented
  Dialogue Systems","11 pages, 5 figures, Accepted as an EMNLP 2020 Long Paper",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end task-oriented dialogue systems aim to generate system responses
directly from plain text inputs. There are two challenges for such systems: one
is how to effectively incorporate external knowledge bases (KBs) into the
learning framework; the other is how to accurately capture the semantics of
dialogue history. In this paper, we address these two challenges by exploiting
the graph structural information in the knowledge base and in the dependency
parsing tree of the dialogue. To effectively leverage the structural
information in dialogue history, we propose a new recurrent cell architecture
which allows representation learning on graphs. To exploit the relations
between entities in KBs, the model combines multi-hop reasoning ability based
on the graph structure. Experimental results show that the proposed model
achieves consistent improvement over state-of-the-art models on two different
task-oriented dialogue datasets.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 00:04:40 GMT'}]",2020-10-06,"[['Yang', 'Shiquan', ''], ['Zhang', 'Rui', ''], ['Erfani', 'Sarah', '']]"
1289995,2005.10200,Dat Quoc Nguyen,"Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen",BERTweet: A pre-trained language model for English Tweets,In Proceedings of EMNLP 2020: System Demonstrations,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present BERTweet, the first public large-scale pre-trained language model
for English Tweets. Our BERTweet, having the same architecture as BERT-base
(Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu
et al., 2019). Experiments show that BERTweet outperforms strong baselines
RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better
performance results than the previous state-of-the-art models on three Tweet
NLP tasks: Part-of-speech tagging, Named-entity recognition and text
classification. We release BERTweet under the MIT License to facilitate future
research and applications on Tweet data. Our BERTweet is available at
https://github.com/VinAIResearch/BERTweet
","[{'version': 'v1', 'created': 'Wed, 20 May 2020 17:05:57 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 10:00:24 GMT'}]",2020-10-06,"[['Nguyen', 'Dat Quoc', ''], ['Vu', 'Thanh', ''], ['Nguyen', 'Anh Tuan', '']]"
1357780,2010.01450,Kexin Huang,"Yue Yu, Kexin Huang, Chao Zhang, Lucas M. Glass, Jimeng Sun, and Cao
  Xiao","SumGNN: Multi-typed Drug Interaction Prediction via Efficient Knowledge
  Graph Summarization",,,,,cs.LG cs.CL cs.IR q-bio.QM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Thanks to the increasing availability of drug-drug interactions (DDI)
datasets and large biomedical knowledge graphs (KGs), accurate detection of
adverse DDI using machine learning models becomes possible. However, it remains
largely an open problem how to effectively utilize large and noisy biomedical
KG for DDI detection. Due to its sheer size and amount of noise in KGs, it is
often less beneficial to directly integrate KGs with other smaller but higher
quality data (e.g., experimental data). Most of existing approaches ignore KGs
altogether. Some tries to directly integrate KGs with other data via graph
neural networks with limited success. Furthermore most previous works focus on
binary DDI prediction whereas the multi-typed DDI pharmacological effect
prediction is more meaningful but harder task.
  To fill the gaps, we propose a new method SumGNN:~{\it knowledge
summarization graph neural network}, which is enabled by a subgraph extraction
module that can efficiently anchor on relevant subgraphs from a KG, a
self-attention based subgraph summarization scheme to generate reasoning path
within the subgraph, and a multi-channel knowledge and data integration module
that utilizes massive external biomedical knowledge for significantly improved
multi-typed DDI predictions. SumGNN outperforms the best baseline by up to
5.54\%, and performance gain is particularly significant in low data relation
types. In addition, SumGNN provides interpretable prediction via the generated
reasoning paths for each prediction.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 00:14:57 GMT'}]",2020-10-06,"[['Yu', 'Yue', ''], ['Huang', 'Kexin', ''], ['Zhang', 'Chao', ''], ['Glass', 'Lucas M.', ''], ['Sun', 'Jimeng', ''], ['Xiao', 'Cao', '']]"
1357784,2010.01454,Soujanya Poria,"Navonil Majumder, Pengfei Hong, Shanshan Peng, Jiankun Lu, Deepanway
  Ghosal, Alexander Gelbukh, Rada Mihalcea, Soujanya Poria",MIME: MIMicking Emotions for Empathetic Response Generation,EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Current approaches to empathetic response generation view the set of emotions
expressed in the input text as a flat structure, where all the emotions are
treated uniformly. We argue that empathetic responses often mimic the emotion
of the user to a varying degree, depending on its positivity or negativity and
content. We show that the consideration of this polarity-based emotion clusters
and emotional mimicry results in improved empathy and contextual relevance of
the response as compared to the state-of-the-art. Also, we introduce
stochasticity into the emotion mixture that yields emotionally more varied
empathetic responses than the previous work. We demonstrate the importance of
these factors to empathetic response generation using both automatic- and
human-based evaluations. The implementation of MIME is publicly available at
https://github.com/declare-lab/MIME.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 00:35:47 GMT'}]",2020-10-06,"[['Majumder', 'Navonil', ''], ['Hong', 'Pengfei', ''], ['Peng', 'Shanshan', ''], ['Lu', 'Jiankun', ''], ['Ghosal', 'Deepanway', ''], ['Gelbukh', 'Alexander', ''], ['Mihalcea', 'Rada', ''], ['Poria', 'Soujanya', '']]"
1357805,2010.01475,Dayiheng Liu,"Dayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng Chen, Jiancheng Lv,
  Nan Duan and Ming Zhou","Tell Me How to Ask Again: Question Data Augmentation with Controllable
  Rewriting in Continuous Space",Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose a novel data augmentation method, referred to as
Controllable Rewriting based Question Data Augmentation (CRQDA), for machine
reading comprehension (MRC), question generation, and question-answering
natural language inference tasks. We treat the question data augmentation task
as a constrained question rewriting problem to generate context-relevant,
high-quality, and diverse question data samples. CRQDA utilizes a Transformer
autoencoder to map the original discrete question into a continuous embedding
space. It then uses a pre-trained MRC model to revise the question
representation iteratively with gradient-based optimization. Finally, the
revised question representations are mapped back into the discrete space, which
serve as additional question data. Comprehensive experiments on SQuAD 2.0,
SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of
CRQDA
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 03:13:46 GMT'}]",2020-10-06,"[['Liu', 'Dayiheng', ''], ['Gong', 'Yeyun', ''], ['Fu', 'Jie', ''], ['Yan', 'Yu', ''], ['Chen', 'Jiusheng', ''], ['Lv', 'Jiancheng', ''], ['Duan', 'Nan', ''], ['Zhou', 'Ming', '']]"
1357810,2010.01480,Junyi Li,"Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas
  Jing Yuan and Ji-Rong Wen","Knowledge-Enhanced Personalized Review Generation with Capsule Graph
  Neural Network",Accepted by CIKM 2020 (Long Paper),,10.1145/3340531.3411893,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Personalized review generation (PRG) aims to automatically produce review
text reflecting user preference, which is a challenging natural language
generation task. Most of previous studies do not explicitly model factual
description of products, tending to generate uninformative content. Moreover,
they mainly focus on word-level generation, but cannot accurately reflect more
abstractive user preference in multiple aspects. To address the above issues,
we propose a novel knowledge-enhanced PRG model based on capsule graph neural
network~(Caps-GNN). We first construct a heterogeneous knowledge graph (HKG)
for utilizing rich item attributes. We adopt Caps-GNN to learn graph capsules
for encoding underlying characteristics from the HKG. Our generation process
contains two major steps, namely aspect sequence generation and sentence
generation. First, based on graph capsules, we adaptively learn aspect capsules
for inferring the aspect sequence. Then, conditioned on the inferred aspect
label, we design a graph-based copy mechanism to generate sentences by
incorporating related entities or words from HKG. To our knowledge, we are the
first to utilize knowledge graph for the PRG task. The incorporated KG
information is able to enhance user preference at both aspect and word levels.
Extensive experiments on three real-world datasets have demonstrated the
effectiveness of our model on the PRG task.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 03:54:40 GMT'}]",2020-10-06,"[['Li', 'Junyi', ''], ['Li', 'Siqing', ''], ['Zhao', 'Wayne Xin', ''], ['He', 'Gaole', ''], ['Wei', 'Zhicheng', ''], ['Yuan', 'Nicholas Jing', ''], ['Wen', 'Ji-Rong', '']]"
1357816,2010.01486,Saadia Gabriel,"Saadia Gabriel, Chandra Bhagavatula, Vered Shwartz, Ronan Le Bras,
  Maxwell Forbes, Yejin Choi",Paragraph-Level Commonsense Transformers with Recurrent Memory,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human understanding of narrative texts requires making commonsense inferences
beyond what is stated in the text explicitly. A recent model, COMeT, can
generate such inferences along several dimensions such as pre- and
post-conditions, motivations, and mental-states of the participants. However,
COMeT was trained on short phrases, and is therefore discourse-agnostic. When
presented with each sentence of a multi-sentence narrative, it might generate
inferences that are inconsistent with the rest of the narrative.
  We present the task of discourse-aware commonsense inference. Given a
sentence within a narrative, the goal is to generate commonsense inferences
along predefined dimensions, while maintaining coherence with the rest of the
narrative. Such large-scale paragraph-level annotation is hard to get and
costly, so we use available sentence-level annotations to efficiently and
automatically construct a distantly supervised corpus.
  Using this corpus, we train PARA-COMeT, a discourse-aware model that
incorporates paragraph-level information to generate coherent commonsense
inferences from narratives. PARA-COMeT captures both semantic knowledge
pertaining to prior world knowledge, and episodic knowledge involving how
current events relate to prior and future events in a narrative. Our results
confirm that PARA-COMeT outperforms the sentence-level baselines, particularly
in generating inferences that are both coherent and novel.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 05:24:12 GMT'}]",2020-10-06,"[['Gabriel', 'Saadia', ''], ['Bhagavatula', 'Chandra', ''], ['Shwartz', 'Vered', ''], ['Bras', 'Ronan Le', ''], ['Forbes', 'Maxwell', ''], ['Choi', 'Yejin', '']]"
1357825,2010.01495,Yifan Gao,"Yifan Gao, Piji Li, Wei Bi, Xiaojiang Liu, Michael R. Lyu, Irwin King","Dialogue Generation on Infrequent Sentence Functions via Structured
  Meta-Learning","EMNLP 2020, Findings, 10 pages, 4 figures",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Sentence function is an important linguistic feature indicating the
communicative purpose in uttering a sentence. Incorporating sentence functions
into conversations has shown improvements in the quality of generated
responses. However, the number of utterances for different types of
fine-grained sentence functions is extremely imbalanced. Besides a small number
of high-resource sentence functions, a large portion of sentence functions is
infrequent. Consequently, dialogue generation conditioned on these infrequent
sentence functions suffers from data deficiency. In this paper, we investigate
a structured meta-learning (SML) approach for dialogue generation on infrequent
sentence functions. We treat dialogue generation conditioned on different
sentence functions as separate tasks, and apply model-agnostic meta-learning to
high-resource sentence functions data. Furthermore, SML enhances meta-learning
effectiveness by promoting knowledge customization among different sentence
functions but simultaneously preserving knowledge generalization for similar
sentence functions. Experimental results demonstrate that SML not only improves
the informativeness and relevance of generated responses, but also can generate
responses consistent with the target sentence functions.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 07:13:36 GMT'}]",2020-10-06,"[['Gao', 'Yifan', ''], ['Li', 'Piji', ''], ['Bi', 'Wei', ''], ['Liu', 'Xiaojiang', ''], ['Lyu', 'Michael R.', ''], ['King', 'Irwin', '']]"
1357826,2010.01496,Oana-Maria Camburu,Oana-Maria Camburu,Explaining Deep Neural Networks,"PhD Thesis, University of Oxford",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks are becoming more and more popular due to their
revolutionary success in diverse areas, such as computer vision, natural
language processing, and speech recognition. However, the decision-making
processes of these models are generally not interpretable to users. In various
domains, such as healthcare, finance, or law, it is critical to know the
reasons behind a decision made by an artificial intelligence system. Therefore,
several directions for explaining neural models have recently been explored.
  In this thesis, I investigate two major directions for explaining deep neural
networks. The first direction consists of feature-based post-hoc explanatory
methods, that is, methods that aim to explain an already trained and fixed
model (post-hoc), and that provide explanations in terms of input features,
such as tokens for text and superpixels for images (feature-based). The second
direction consists of self-explanatory neural models that generate natural
language explanations, that is, models that have a built-in module that
generates explanations for the predictions of the model.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 07:23:13 GMT'}]",2020-10-06,"[['Camburu', 'Oana-Maria', '']]"
1358228,2010.01898,Jie Huang,"Jie Huang, Zilong Wang, Kevin Chen-Chuan Chang, Wen-mei Hwu, Jinjun
  Xiong",Exploring Semantic Capacity of Terms,Accepted to EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce and study semantic capacity of terms. For example, the semantic
capacity of artificial intelligence is higher than that of linear regression
since artificial intelligence possesses a broader meaning scope. Understanding
semantic capacity of terms will help many downstream tasks in natural language
processing. For this purpose, we propose a two-step model to investigate
semantic capacity of terms, which takes a large text corpus as input and can
evaluate semantic capacity of terms if the text corpus can provide enough
co-occurrence information of terms. Extensive experiments in three fields
demonstrate the effectiveness and rationality of our model compared with
well-designed baselines and human-level evaluations.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 10:26:36 GMT'}]",2020-10-06,"[['Huang', 'Jie', ''], ['Wang', 'Zilong', ''], ['Chang', 'Kevin Chen-Chuan', ''], ['Hwu', 'Wen-mei', ''], ['Xiong', 'Jinjun', '']]"
1357639,2010.01309,Amirmohammad Kazemeini,"Amirmohammad Kazameini, Samin Fatehi, Yash Mehta, Sauleh Eetemadi,
  Erik Cambria","Personality Trait Detection Using Bagged SVM over BERT Word Embedding
  Ensembles",,"Proceedings of the The Fourth Widening Natural Language Processing
  Workshop (2020)",,,cs.CL cs.AI cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the automatic prediction of personality traits has received
increasing attention and has emerged as a hot topic within the field of
affective computing. In this work, we present a novel deep learning-based
approach for automated personality detection from text. We leverage state of
the art advances in natural language understanding, namely the BERT language
model to extract contextualized word embeddings from textual data for automated
author personality detection. Our primary goal is to develop a computationally
efficient, high-performance personality prediction model which can be easily
used by a large number of people without access to huge computation resources.
Our extensive experiments with this ideology in mind, led us to develop a novel
model which feeds contextualized embeddings along with psycholinguistic
features toa Bagged-SVM classifier for personality trait prediction. Our model
outperforms the previous state of the art by 1.04% and, at the same time is
significantly more computationally efficient to train. We report our results on
the famous gold standard Essays dataset for personality detection.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 09:25:51 GMT'}]",2020-10-06,"[['Kazameini', 'Amirmohammad', ''], ['Fatehi', 'Samin', ''], ['Mehta', 'Yash', ''], ['Eetemadi', 'Sauleh', ''], ['Cambria', 'Erik', '']]"
1357832,2010.01502,Qi Jia,"Qi Jia, Yizhu Liu, Siyu Ren, Kenny Q. Zhu, Haifeng Tang",Multi-turn Response Selection using Dialogue Dependency Relations,Accepted for publication as a long paper in EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-turn response selection is a task designed for developing dialogue
agents. The performance on this task has a remarkable improvement with
pre-trained language models. However, these models simply concatenate the turns
in dialogue history as the input and largely ignore the dependencies between
the turns. In this paper, we propose a dialogue extraction algorithm to
transform a dialogue history into threads based on their dependency relations.
Each thread can be regarded as a self-contained sub-dialogue. We also propose
Thread-Encoder model to encode threads and candidates into compact
representations by pre-trained Transformers and finally get the matching score
through an attention layer. The experiments show that dependency relations are
helpful for dialogue context understanding, and our model outperforms the
state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results
on UbuntuV2.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 08:00:19 GMT'}]",2020-10-06,"[['Jia', 'Qi', ''], ['Liu', 'Yizhu', ''], ['Ren', 'Siyu', ''], ['Zhu', 'Kenny Q.', ''], ['Tang', 'Haifeng', '']]"
1357618,2010.01288,Jiahui Gao,"Jiahui Gao, Yi Zhou, Philip L. H. Yu and Jiuxiang Gu",Unsupervised Cross-lingual Image Captioning,8 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most recent image captioning works are conducted in English as the majority
of image-caption datasets are in English. However, there are a large amount of
non-native English speakers worldwide. Generating image captions in different
languages is worth exploring. In this paper, we present a novel unsupervised
method to generate image captions without using any caption corpus. Our method
relies on 1) a cross-lingual auto-encoding, which learns the scene graph
mapping function along with the scene graph encoders and sentence decoders on
machine translation parallel corpora, and 2) an unsupervised feature mapping,
which seeks to map the encoded scene graph features from image modality to
sentence modality. By leveraging cross-lingual auto-encoding, cross-modal
feature mapping, and adversarial learning, our method can learn an image
captioner to generate captions in different languages. We verify the
effectiveness of our proposed method on the Chinese image caption generation.
The comparisons against several baseline methods demonstrate the effectiveness
of our approach.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 06:14:06 GMT'}]",2020-10-06,"[['Gao', 'Jiahui', ''], ['Zhou', 'Yi', ''], ['Yu', 'Philip L. H.', ''], ['Gu', 'Jiuxiang', '']]"
1357598,2010.01268,Zihao Fu,"Zihao Fu, Bei Shi, Wai Lam, Lidong Bing, Zhiyuan Liu",Partially-Aligned Data-to-Text Generation with Distant Supervision,To appear EMNLP 2020. 11 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Data-to-Text task aims to generate human-readable text for describing
some given structured data enabling more interpretability. However, the typical
generation task is confined to a few particular domains since it requires
well-aligned data which is difficult and expensive to obtain. Using
partially-aligned data is an alternative way of solving the dataset scarcity
problem. This kind of data is much easier to obtain since it can be produced
automatically. However, using this kind of data induces the over-generation
problem posing difficulties for existing models, which tends to add unrelated
excerpts during the generation procedure. In order to effectively utilize
automatically annotated partially-aligned datasets, we extend the traditional
generation task to a refined task called Partially-Aligned Data-to-Text
Generation (PADTG) which is more practical since it utilizes automatically
annotated data for training and thus considerably expands the application
domains. To tackle this new task, we propose a novel distant supervision
generation framework. It firstly estimates the input data's supportiveness for
each target word with an estimator and then applies a supportiveness adaptor
and a rebalanced beam search to harness the over-generation problem in the
training and generation phases respectively. We also contribute a
partially-aligned dataset (The data and source code of this paper can be
obtained from https://github.com/fuzihaofzh/distant_supervision_nlg by sampling
sentences from Wikipedia and automatically extracting corresponding KB triples
for each sentence from Wikidata. The experimental results show that our
framework outperforms all baseline models as well as verify the feasibility of
utilizing partially-aligned data.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 03:18:52 GMT'}]",2020-10-06,"[['Fu', 'Zihao', ''], ['Shi', 'Bei', ''], ['Lam', 'Wai', ''], ['Bing', 'Lidong', ''], ['Liu', 'Zhiyuan', '']]"
1278944,2004.14174,John Morris,"John X. Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, Yanjun Qi",Reevaluating Adversarial Examples in Natural Language,15 pages; 9 Tables; 5 Figures,,,,cs.CL cs.AI cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State-of-the-art attacks on NLP models lack a shared definition of a what
constitutes a successful attack. We distill ideas from past work into a unified
framework: a successful natural language adversarial example is a perturbation
that fools the model and follows some linguistic constraints. We then analyze
the outputs of two state-of-the-art synonym substitution attacks. We find that
their perturbations often do not preserve semantics, and 38% introduce
grammatical errors. Human surveys reveal that to successfully preserve
semantics, we need to significantly increase the minimum cosine similarities
between the embeddings of swapped words and between the sentence encodings of
original and perturbed sentences.With constraints adjusted to better preserve
semantics and grammaticality, the attack success rate drops by over 70
percentage points.
","[{'version': 'v1', 'created': 'Sat, 25 Apr 2020 03:09:48 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 04:16:23 GMT'}]",2020-10-06,"[['Morris', 'John X.', ''], ['Lifland', 'Eli', ''], ['Lanchantin', 'Jack', ''], ['Ji', 'Yangfeng', ''], ['Qi', 'Yanjun', '']]"
1358502,2010.02172,Tiago Pimentel,"Tiago Pimentel, Rowan Hall Maudslay, Dami\'an Blasi, Ryan Cotterell",Speakers Fill Lexical Semantic Gaps with Context,Accepted for publication at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lexical ambiguity is widespread in language, allowing for the reuse of
economical word forms and therefore making language more efficient. If
ambiguous words cannot be disambiguated from context, however, this gain in
efficiency might make language less clear---resulting in frequent
miscommunication. For a language to be clear and efficiently encoded, we posit
that the lexical ambiguity of a word type should correlate with how much
information context provides about it, on average. To investigate whether this
is the case, we operationalise the lexical ambiguity of a word as the entropy
of meanings it can take, and provide two ways to estimate this---one which
requires human annotation (using WordNet), and one which does not (using BERT),
making it readily applicable to a large number of languages. We validate these
measures by showing that, on six high-resource languages, there are significant
Pearson correlations between our BERT-based estimate of ambiguity and the
number of synonyms a word has in WordNet (e.g. $\rho = 0.40$ in English). We
then test our main hypothesis---that a word's lexical ambiguity should
negatively correlate with its contextual uncertainty---and find significant
correlations on all 18 typologically diverse languages we analyse. This
suggests that, in the presence of ambiguity, speakers compensate by making
contexts more informative.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 17:19:10 GMT'}]",2020-10-06,"[['Pimentel', 'Tiago', ''], ['Maudslay', 'Rowan Hall', ''], ['Blasi', 'Damián', ''], ['Cotterell', 'Ryan', '']]"
1358509,2010.02179,Chieh-Yang Huang,"Yun-Hsuan Jen, Chieh-Yang Huang, Mei-Hua Chen, Ting-Hao 'Kenneth'
  Huang, Lun-Wei Ku","Assessing the Helpfulness of Learning Materials with Inference-Based
  Learner-Like Agent","9 pages, to appear in EMNLP 2020 as a long paper",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many English-as-a-second language learners have trouble using near-synonym
words (e.g., small vs.little; briefly vs.shortly) correctly, and often look for
example sentences to learn how two nearly synonymous terms differ. Prior work
uses hand-crafted scores to recommend sentences but has difficulty in adopting
such scores to all the near-synonyms as near-synonyms differ in various ways.
We notice that the helpfulness of the learning material would reflect on the
learners' performance. Thus, we propose the inference-based learner-like agent
to mimic learner behavior and identify good learning materials by examining the
agent's performance. To enable the agent to behave like a learner, we leverage
entailment modeling's capability of inferring answers from the provided
materials. Experimental results show that the proposed agent is equipped with
good learner-like behavior to achieve the best performance in both
fill-in-the-blank (FITB) and good example sentence selection tasks. We further
conduct a classroom user study with college ESL learners. The results of the
user study show that the proposed agent can find out example sentences that
help students learn more easily and efficiently. Compared to other models, the
proposed agent improves the score of more than 17% of students after learning.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 17:24:57 GMT'}]",2020-10-06,"[['Jen', 'Yun-Hsuan', ''], ['Huang', 'Chieh-Yang', ''], ['Chen', 'Mei-Hua', ''], ['Huang', ""Ting-Hao 'Kenneth'"", ''], ['Ku', 'Lun-Wei', '']]"
1358510,2010.02180,Tiago Pimentel,"Tiago Pimentel, Naomi Saphra, Adina Williams, Ryan Cotterell",Pareto Probing: Trading Off Accuracy for Complexity,"Tiago Pimentel and Naomi Saphra contributed equally to this work.
  Accepted for publication at EMNLP 2020. Code available in
  https://github.com/rycolab/pareto-probing",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The question of how to probe contextual word representations in a way that is
principled and useful has seen significant recent attention. In our
contribution to this discussion, we argue, first, for a probe metric that
reflects the trade-off between probe complexity and performance: the Pareto
hypervolume. To measure complexity, we present a number of parametric and
non-parametric metrics. Our experiments with such metrics show that probe's
performance curves often fail to align with widely accepted rankings between
language representations (with, e.g., non-contextual representations
outperforming contextual ones). These results lead us to argue, second, that
common simplistic probe tasks such as POS labeling and dependency arc labeling,
are inadequate to evaluate the properties encoded in contextual word
representations. We propose full dependency parsing as an example probe task,
and demonstrate it with the Pareto hypervolume. In support of our arguments,
the results of this illustrative experiment conform closer to accepted rankings
among contextual word representations.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 17:27:31 GMT'}]",2020-10-06,"[['Pimentel', 'Tiago', ''], ['Saphra', 'Naomi', ''], ['Williams', 'Adina', ''], ['Cotterell', 'Ryan', '']]"
1358524,2010.02194,Alexis Conneau,"Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur
  Celebi, Michael Auli, Ves Stoyanov, Alexis Conneau",Self-training Improves Pre-training for Natural Language Understanding,8 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unsupervised pre-training has led to much recent progress in natural language
understanding. In this paper, we study self-training as another way to leverage
unlabeled data through semi-supervised learning. To obtain additional data for
a specific task, we introduce SentAugment, a data augmentation method which
computes task-specific query embeddings from labeled data to retrieve sentences
from a bank of billions of unlabeled sentences crawled from the web. Unlike
previous semi-supervised methods, our approach does not require in-domain
unlabeled data and is therefore more generally applicable. Experiments show
that self-training is complementary to strong RoBERTa baselines on a variety of
tasks. Our augmentation approach leads to scalable and effective self-training
with improvements of up to 2.6% on standard text classification benchmarks.
Finally, we also show strong gains on knowledge-distillation and few-shot
learning.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 17:52:25 GMT'}]",2020-10-06,"[['Du', 'Jingfei', ''], ['Grave', 'Edouard', ''], ['Gunel', 'Beliz', ''], ['Chaudhary', 'Vishrav', ''], ['Celebi', 'Onur', ''], ['Auli', 'Michael', ''], ['Stoyanov', 'Ves', ''], ['Conneau', 'Alexis', '']]"
1356483,2010.00153,Zining Zhu,"Zining Zhu, Chuer Pan, Mohamed Abdalla, Frank Rudzicz",Examining the rhetorical capacities of neural language models,EMNLP 2020 BlackboxNLP Workshop,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, neural language models (LMs) have demonstrated impressive abilities
in generating high-quality discourse. While many recent papers have analyzed
the syntactic aspects encoded in LMs, there has been no analysis to date of the
inter-sentential, rhetorical knowledge. In this paper, we propose a method that
quantitatively evaluates the rhetorical capacities of neural LMs. We examine
the capacities of neural LMs understanding the rhetoric of discourse by
evaluating their abilities to encode a set of linguistic features derived from
Rhetorical Structure Theory (RST). Our experiments show that BERT-based LMs
outperform other Transformer LMs, revealing the richer discourse knowledge in
their intermediate layer representations. In addition, GPT-2 and XLNet
apparently encode less rhetorical knowledge, and we suggest an explanation
drawing from linguistic philosophy. Our method shows an avenue towards
quantifying the rhetorical capacities of neural LMs.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 00:18:43 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 22:16:11 GMT'}]",2020-10-06,"[['Zhu', 'Zining', ''], ['Pan', 'Chuer', ''], ['Abdalla', 'Mohamed', ''], ['Rudzicz', 'Frank', '']]"
1279347,2004.14577,Bonan Min,"Hayley Ross, Jonathon Cai, Bonan Min","Exploring Contextualized Neural Language Models for Temporal Dependency
  Parsing",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Extracting temporal relations between events and time expressions has many
applications such as constructing event timelines and time-related question
answering. It is a challenging problem which requires syntactic and semantic
information at sentence or discourse levels, which may be captured by deep
contextualized language models (LMs) such as BERT (Devlin et al., 2019). In
this paper, we develop several variants of BERT-based temporal dependency
parser, and show that BERT significantly improves temporal dependency parsing
(Zhang and Xue, 2018a). We also present a detailed analysis on why deep
contextualized neural LMs help and where they may fall short. Source code and
resources are made available at https://github.com/bnmin/tdp_ranking.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 03:59:13 GMT'}, {'version': 'v2', 'created': 'Sat, 3 Oct 2020 00:25:39 GMT'}]",2020-10-06,"[['Ross', 'Hayley', ''], ['Cai', 'Jonathon', ''], ['Min', 'Bonan', '']]"
1356577,2010.00247,Fandong Meng,"Fandong Meng, Jianhao Yan, Yijin Liu, Yuan Gao, Xianfeng Zeng, Qinsong
  Zeng, Peng Li, Ming Chen, Jie Zhou, Sifan Liu and Hao Zhou",WeChat Neural Machine Translation Systems for WMT20,"Accepted at WMT 2020. Our Chinese to English system achieved the
  highest case-sensitive BLEU score among all submissions",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We participate in the WMT 2020 shared news translation task on Chinese to
English. Our system is based on the Transformer (Vaswani et al., 2017a) with
effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our
experiments, we employ data selection, several synthetic data generation
approaches (i.e., back-translation, knowledge distillation, and iterative
in-domain knowledge transfer), advanced finetuning approaches and self-bleu
based model ensemble. Our constrained Chinese to English system achieves 36.9
case-sensitive BLEU score, which is the highest among all submissions.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 08:15:09 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 16:01:01 GMT'}]",2020-10-06,"[['Meng', 'Fandong', ''], ['Yan', 'Jianhao', ''], ['Liu', 'Yijin', ''], ['Gao', 'Yuan', ''], ['Zeng', 'Xianfeng', ''], ['Zeng', 'Qinsong', ''], ['Li', 'Peng', ''], ['Chen', 'Ming', ''], ['Zhou', 'Jie', ''], ['Liu', 'Sifan', ''], ['Zhou', 'Hao', '']]"
1356845,2010.00515,Shaofei Huang,"Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang,
  Jizhong Han","Linguistic Structure Guided Context Modeling for Referring Image
  Segmentation","Accepted by ECCV 2020. Code is available at
  https://github.com/spyflying/LSCM-Refseg",,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Referring image segmentation aims to predict the foreground mask of the
object referred by a natural language sentence. Multimodal context of the
sentence is crucial to distinguish the referent from the background. Existing
methods either insufficiently or redundantly model the multimodal context. To
tackle this problem, we propose a ""gather-propagate-distribute"" scheme to model
multimodal context by cross-modal interaction and implement this scheme as a
novel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM
module builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which
guides all the words to include valid multimodal context of the sentence while
excluding disturbing ones through three steps over the multimodal feature,
i.e., gathering, constrained propagation and distributing. Extensive
experiments on four benchmarks demonstrate that our method outperforms all the
previous state-of-the-arts.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 16:03:51 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 03:19:48 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Oct 2020 08:49:43 GMT'}]",2020-10-06,"[['Hui', 'Tianrui', ''], ['Liu', 'Si', ''], ['Huang', 'Shaofei', ''], ['Li', 'Guanbin', ''], ['Yu', 'Sansi', ''], ['Zhang', 'Faxi', ''], ['Han', 'Jizhong', '']]"
1356901,2010.00571,Julian Eisenschlos,"Julian Martin Eisenschlos, Syrine Krichene, Thomas M\""uller",Understanding tables with intermediate pre-training,Accepted to EMNLP Findings 2020,,,,cs.CL cs.AI cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Table entailment, the binary classification task of finding if a sentence is
supported or refuted by the content of a table, requires parsing language and
table structure as well as numerical and discrete reasoning. While there is
extensive work on textual entailment, table entailment is less well studied. We
adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize
entailment. Motivated by the benefits of data augmentation, we create a
balanced dataset of millions of automatically created training examples which
are learned in an intermediate step prior to fine-tuning. This new data is not
only useful for table entailment, but also for SQA (Iyyer et al., 2017), a
sequential table QA task. To be able to use long examples as input of BERT
models, we evaluate table pruning techniques as a pre-processing step to
drastically improve the training and prediction efficiency at a moderate drop
in accuracy. The different methods set the new state-of-the-art on the TabFact
(Chen et al., 2020) and SQA datasets.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 17:43:27 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 12:26:40 GMT'}]",2020-10-06,"[['Eisenschlos', 'Julian Martin', ''], ['Krichene', 'Syrine', ''], ['Müller', 'Thomas', '']]"
1357480,2010.01150,Xiang Dai,Xiang Dai and Sarvnaz Karimi and Ben Hachey and Cecile Paris,"Cost-effective Selection of Pretraining Data: A Case Study of
  Pretraining BERT on Social Media",Findings of EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recent studies on domain-specific BERT models show that effectiveness on
downstream tasks can be improved when models are pretrained on in-domain data.
Often, the pretraining data used in these models are selected based on their
subject matter, e.g., biology or computer science. Given the range of
applications using social media text, and its unique language variety, we
pretrain two models on tweets and forum text respectively, and empirically
demonstrate the effectiveness of these two resources. In addition, we
investigate how similarity measures can be used to nominate in-domain
pretraining data. We publicly release our pretrained models at
https://bit.ly/35RpTf0.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 18:06:31 GMT'}]",2020-10-06,"[['Dai', 'Xiang', ''], ['Karimi', 'Sarvnaz', ''], ['Hachey', 'Ben', ''], ['Paris', 'Cecile', '']]"
1357495,2010.01165,Zeljko Kraljevic,"Zeljko Kraljevic, Thomas Searle, Anthony Shek, Lukasz Roguski, Kawsar
  Noor, Daniel Bean, Aurelie Mascio, Leilei Zhu, Amos A Folarin, Angus Roberts,
  Rebecca Bendayan, Mark P Richardson, Robert Stewart, Anoop D Shah, Wai Keong
  Wong, Zina Ibrahim, James T Teo, Richard JB Dobson","Multi-domain Clinical Natural Language Processing with MedCAT: the
  Medical Concept Annotation Toolkit","Preprint: 44 Pages, 5 Figures",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Electronic health records (EHR) contain large volumes of unstructured text,
requiring the application of Information Extraction (IE) technologies to enable
clinical analysis. We present the open source Medical Concept Annotation
Toolkit (MedCAT) that provides: a) a novel self-supervised machine learning
algorithm for extracting concepts using any concept vocabulary including
UMLS/SNOMED-CT; b) a feature-rich annotation interface for customizing and
training IE models; and c) integrations to the broader CogStack ecosystem for
vendor-agnostic health system deployment. We show improved performance in
extracting UMLS concepts from open datasets ( F1 0.467-0.791 vs 0.384-0.691).
Further real-world validation demonstrates SNOMED-CT extraction at 3 large
London hospitals with self-supervised training over ~8.8B words from ~17M
clinical records and further fine-tuning with ~6K clinician annotated examples.
We show strong transferability ( F1 >0.94) between hospitals, datasets and
concept types indicating cross-domain EHR-agnostic utility for accelerated
clinical and research use cases.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 19:01:02 GMT'}]",2020-10-06,"[['Kraljevic', 'Zeljko', ''], ['Searle', 'Thomas', ''], ['Shek', 'Anthony', ''], ['Roguski', 'Lukasz', ''], ['Noor', 'Kawsar', ''], ['Bean', 'Daniel', ''], ['Mascio', 'Aurelie', ''], ['Zhu', 'Leilei', ''], ['Folarin', 'Amos A', ''], ['Roberts', 'Angus', ''], ['Bendayan', 'Rebecca', ''], ['Richardson', 'Mark P', ''], ['Stewart', 'Robert', ''], ['Shah', 'Anoop D', ''], ['Wong', 'Wai Keong', ''], ['Ibrahim', 'Zina', ''], ['Teo', 'James T', ''], ['Dobson', 'Richard JB', '']]"
1357499,2010.01169,Vineeth Ravi,"Vineeth Ravi, Selim Amrouni, Andrea Stefanucci, Prashant Reddy, and
  Manuela Veloso","AI pptX: Robust Continuous Learning for Document Generation with AI
  Insights","We introduce a novel framework called ""AI pptX""for creating and
  modifying documents as well as extract insights in the form of natural
  language sentences from data. Accepted at NeurIPS 2019: Workshop on Robust AI
  in Financial Services: Data, Fairness, Explainability, Trustworthiness, and
  Privacy 10 pages in total (8 pages + 2 (Acknowledgements, references etc.)) ,
  5 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Business analysts create billions of slide decks, reports and documents
annually. Most of these documents have well-defined structure comprising of
similar content generated from data. We present 'AI pptX', a novel AI framework
for creating and modifying documents as well as extract insights in the form of
natural language sentences from data. AI pptX has three main components: (i) a
component that translates users' natural language input into 'skills' that
encapsulate content editing and formatting commands, (ii) a robust continuously
learning component that interacts with users, and (iii) a component that
automatically generates hierarchical insights in the form of natural language
sentences. We illustrate (i) and (ii) with a study of 18 human users tasked to
create a presentation deck and observe the learning capability from a decrease
in user-input commands by up to 45%. We demonstrate the robust learning
capability of AI pptX with experimental simulations of non-collaborative users.
We illustrate (i) and (iii) by automatically generating insights in natural
language using a data set from the Electricity Transmission Network of France
(RTE); we show that a complex statistical analysis of series can automatically
be distilled into easily interpretable explanations called AI Insights.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 19:06:29 GMT'}]",2020-10-06,"[['Ravi', 'Vineeth', ''], ['Amrouni', 'Selim', ''], ['Stefanucci', 'Andrea', ''], ['Reddy', 'Prashant', ''], ['Veloso', 'Manuela', '']]"
1357569,2010.01239,Mingda Chen,"Mingda Chen, Zewei Chu, Karl Stratos, Kevin Gimpel","Mining Knowledge for Natural Language Inference from Wikipedia
  Categories",Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Accurate lexical entailment (LE) and natural language inference (NLI) often
require large quantities of costly annotations. To alleviate the need for
labeled data, we introduce WikiNLI: a resource for improving model performance
on NLI and LE tasks. It contains 428,899 pairs of phrases constructed from
naturally annotated category hierarchies in Wikipedia. We show that we can
improve strong baselines such as BERT and RoBERTa by pretraining them on
WikiNLI and transferring the models on downstream tasks. We conduct systematic
comparisons with phrases extracted from other knowledge bases such as WordNet
and Wikidata to find that pretraining on WikiNLI gives the best performance. In
addition, we construct WikiNLI in other languages, and show that pretraining on
them improves performance on NLI tasks of corresponding languages.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 00:45:01 GMT'}]",2020-10-06,"[['Chen', 'Mingda', ''], ['Chu', 'Zewei', ''], ['Stratos', 'Karl', ''], ['Gimpel', 'Kevin', '']]"
1357593,2010.01263,Nikolaos Pappas,"Xuhui Zhou, Nikolaos Pappas, Noah A. Smith",Multilevel Text Alignment with Cross-Document Attention,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text alignment finds application in tasks such as citation recommendation and
plagiarism detection. Existing alignment methods operate at a single,
predefined level and cannot learn to align texts at, for example, sentence and
document levels. We propose a new learning approach that equips previously
established hierarchical attention encoders for representing documents with a
cross-document attention component, enabling structural comparisons across
different levels (document-to-document and sentence-to-document). Our component
is weakly supervised from document pairs and can align at multiple levels. Our
evaluation on predicting document-to-document relationships and
sentence-to-document relationships on the tasks of citation recommendation and
plagiarism detection shows that our approach outperforms previously established
hierarchical, attention encoders based on recurrent and transformer
contextualization that are unaware of structural correspondence between
documents.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 02:52:28 GMT'}]",2020-10-06,"[['Zhou', 'Xuhui', ''], ['Pappas', 'Nikolaos', ''], ['Smith', 'Noah A.', '']]"
1357602,2010.01272,Mucheng Ren,"Mucheng Ren, Xiubo Geng, Tao Qin, Heyan Huang, Daxin Jiang",Towards Interpretable Reasoning over Paragraph Effects in Situation,14 pages. Accepted as EMNLP2020 Long paper,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We focus on the task of reasoning over paragraph effects in situation, which
requires a model to understand the cause and effect described in a background
paragraph, and apply the knowledge to a novel situation. Existing works ignore
the complicated reasoning process and solve it with a one-step ""black box""
model. Inspired by human cognitive processes, in this paper we propose a
sequential approach for this task which explicitly models each step of the
reasoning process with neural network modules. In particular, five reasoning
modules are designed and learned in an end-to-end manner, which leads to a more
interpretable model. Experimental results on the ROPES dataset demonstrate the
effectiveness and explainability of our proposed approach.
","[{'version': 'v1', 'created': 'Sat, 3 Oct 2020 04:03:52 GMT'}]",2020-10-06,"[['Ren', 'Mucheng', ''], ['Geng', 'Xiubo', ''], ['Qin', 'Tao', ''], ['Huang', 'Heyan', ''], ['Jiang', 'Daxin', '']]"
1357842,2010.01512,Chen Zhang,"Chen Zhang, Qiuchi Li, Dawei Song, Benyou Wang",A Multi-task Learning Framework for Opinion Triplet Extraction,"10 pages, 4 figures, 3 tables. Accepted to EMNLP 2020 Findings. Repo:
  https://github.com/GeneZC/OTE-MTL",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The state-of-the-art Aspect-based Sentiment Analysis (ABSA) approaches are
mainly based on either detecting aspect terms and their corresponding sentiment
polarities, or co-extracting aspect and opinion terms. However, the extraction
of aspect-sentiment pairs lacks opinion terms as a reference, while
co-extraction of aspect and opinion terms would not lead to meaningful pairs
without determining their sentiment dependencies. To address the issue, we
present a novel view of ABSA as an opinion triplet extraction task, and propose
a multi-task learning framework to jointly extract aspect terms and opinion
terms, and simultaneously parses sentiment dependencies between them with a
biaffine scorer. At inference phase, the extraction of triplets is facilitated
by a triplet decoding method based on the above outputs. We evaluate the
proposed framework on four SemEval benchmarks for ASBA. The results demonstrate
that our approach significantly outperforms a range of strong baselines and
state-of-the-art approaches.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 08:31:54 GMT'}]",2020-10-06,"[['Zhang', 'Chen', ''], ['Li', 'Qiuchi', ''], ['Song', 'Dawei', ''], ['Wang', 'Benyou', '']]"
1357856,2010.01526,Sahil Shah,"Sahil Shah, Vihari Piratla, Soumen Chakrabarti, Sunita Sarawagi",NLP Service APIs and Models for Efficient Registration of New Clients,"Accepted to Findings of EMNLP, 2020",,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State-of-the-art NLP inference uses enormous neural architectures and models
trained for GPU-months, well beyond the reach of most consumers of NLP. This
has led to one-size-fits-all public API-based NLP service models by major AI
companies, serving large numbers of clients. Neither (hardware deficient)
clients nor (heavily subscribed) servers can afford traditional fine tuning.
Many clients own little or no labeled data. We initiate a study of adaptation
of centralized NLP services to clients, and present one practical and
lightweight approach. Each client uses an unsupervised, corpus-based sketch to
register to the service. The server uses an auxiliary network to map the sketch
to an abstract vector representation, which then informs the main labeling
network. When a new client registers with its sketch, it gets immediate
accuracy benefits. We demonstrate the success of the proposed architecture
using sentiment labeling, NER, and predictive language modeling
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 09:47:40 GMT'}]",2020-10-06,"[['Shah', 'Sahil', ''], ['Piratla', 'Vihari', ''], ['Chakrabarti', 'Soumen', ''], ['Sarawagi', 'Sunita', '']]"
1357865,2010.01535,Wenjuan Han,"Wenjuan Han, Yong Jiang, Hwee Tou Ng, Kewei Tu",A Survey of Unsupervised Dependency Parsing,COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Syntactic dependency parsing is an important task in natural language
processing. Unsupervised dependency parsing aims to learn a dependency parser
from sentences that have no annotation of their correct parse trees. Despite
its difficulty, unsupervised parsing is an interesting research direction
because of its capability of utilizing almost unlimited unannotated text data.
It also serves as the basis for other research in low-resource parsing. In this
paper, we survey existing approaches to unsupervised dependency parsing,
identify two major classes of approaches, and discuss recent trends. We hope
that our survey can provide insights for researchers and facilitate future
research on this topic.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 10:51:22 GMT'}]",2020-10-06,"[['Han', 'Wenjuan', ''], ['Jiang', 'Yong', ''], ['Ng', 'Hwee Tou', ''], ['Tu', 'Kewei', '']]"
1358065,2010.01735,Lu Zhang,"Lu Zhang, Mo Yu, Tian Gao, Yue Yu",MCMH: Learning Multi-Chain Multi-Hop Rules for Knowledge Graph Reasoning,Accepted Findings of EMNLP2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-hop reasoning approaches over knowledge graphs infer a missing
relationship between entities with a multi-hop rule, which corresponds to a
chain of relationships. We extend existing works to consider a generalized form
of multi-hop rules, where each rule is a set of relation chains. To learn such
generalized rules efficiently, we propose a two-step approach that first
selects a small set of relation chains as a rule and then evaluates the
confidence of the target relationship by jointly scoring the selected chains. A
game-theoretical framework is proposed to this end to simultaneously optimize
the rule selection and prediction steps. Empirical results show that our
multi-chain multi-hop (MCMH) rules result in superior results compared to the
standard single-chain approaches, justifying both our formulation of
generalized rules and the effectiveness of the proposed learning framework.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 01:32:20 GMT'}]",2020-10-06,"[['Zhang', 'Lu', ''], ['Yu', 'Mo', ''], ['Gao', 'Tian', ''], ['Yu', 'Yue', '']]"
1358067,2010.01737,Yinghao Li,"Yinghao Li (Georgia Institute of Technology), Rui Feng (Georgia
  Institute of Technology), Isaac Rehg (Georgia Institute of Technology), Chao
  Zhang (Georgia Institute of Technology)",Transformer-Based Neural Text Generation with Syntactic Guidance,"11 pages, 4 figures and 5 tables",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We study the problem of using (partial) constituency parse trees as syntactic
guidance for controlled text generation. Existing approaches to this problem
use recurrent structures, which not only suffer from the long-term dependency
problem but also falls short in modeling the tree structure of the syntactic
guidance. We propose to leverage the parallelism of Transformer to better
incorporate parse trees. Our method first expands a partial template
constituency parse tree to a full-fledged parse tree tailored for the input
source text, and then uses the expanded tree to guide text generation. The
effectiveness of our model in this process hinges upon two new attention
mechanisms: 1) a path attention mechanism that forces one node to attend to
only other nodes located in its path in the syntax tree to better incorporate
syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder
to dynamically attend to information from multiple encoders. Our experiments in
the controlled paraphrasing task show that our method outperforms SOTA models
both semantically and syntactically, improving the best baseline's BLEU score
from 11.83 to 26.27.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 01:33:58 GMT'}]",2020-10-06,"[['Li', 'Yinghao', '', 'Georgia Institute of Technology'], ['Feng', 'Rui', '', 'Georgia\n  Institute of Technology'], ['Rehg', 'Isaac', '', 'Georgia Institute of Technology'], ['Zhang', 'Chao', '', 'Georgia Institute of Technology']]"
1358069,2010.01739,Thuy-Trang Vu,"Thuy-Trang Vu, Dinh Phung and Gholamreza Haffari","Effective Unsupervised Domain Adaptation with Adversarially Trained
  Language Models",EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has shown the importance of adaptation of broad-coverage
contextualised embedding models on the domain of the target task of interest.
Current self-supervised adaptation methods are simplistic, as the training
signal comes from a small percentage of \emph{randomly} masked-out tokens. In
this paper, we show that careful masking strategies can bridge the knowledge
gap of masked language models (MLMs) about the domains more effectively by
allocating self-supervision where it is needed. Furthermore, we propose an
effective training strategy by adversarially masking out those tokens which are
harder to reconstruct by the underlying MLM. The adversarial objective leads to
a challenging combinatorial optimisation problem over \emph{subsets} of tokens,
which we tackle efficiently through relaxation to a variational lowerbound and
dynamic programming. On six unsupervised domain adaptation tasks involving
named entity recognition, our method strongly outperforms the random masking
strategy and achieves up to +1.64 F1 score improvements.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 01:49:47 GMT'}]",2020-10-06,"[['Vu', 'Thuy-Trang', ''], ['Phung', 'Dinh', ''], ['Haffari', 'Gholamreza', '']]"
1183413,1909.13375,Elad Segal,"Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson, Jonathan Berant",A Simple and Effective Model for Answering Multi-span Questions,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Models for reading comprehension (RC) commonly restrict their output space to
the set of all single contiguous spans from the input, in order to alleviate
the learning problem and avoid the need for a model that generates text
explicitly. However, forcing an answer to be a single span can be restrictive,
and some recent datasets also include multi-span questions, i.e., questions
whose answer is a set of non-contiguous spans in the text. Naturally, models
that return single spans cannot answer these questions. In this work, we
propose a simple architecture for answering multi-span questions by casting the
task as a sequence tagging problem, namely, predicting for each input token
whether it should be part of the output or not. Our model substantially
improves performance on span extraction questions from DROP and Quoref by 9.9
and 5.5 EM points respectively.
","[{'version': 'v1', 'created': 'Sun, 29 Sep 2019 21:49:36 GMT'}, {'version': 'v2', 'created': 'Wed, 2 Oct 2019 05:51:55 GMT'}, {'version': 'v3', 'created': 'Mon, 27 Apr 2020 16:42:28 GMT'}, {'version': 'v4', 'created': 'Mon, 5 Oct 2020 14:02:00 GMT'}]",2020-10-06,"[['Segal', 'Elad', ''], ['Efrat', 'Avia', ''], ['Shoham', 'Mor', ''], ['Globerson', 'Amir', ''], ['Berant', 'Jonathan', '']]"
1358075,2010.01745,Diego Ramirez-Echavarria,"Diego Ramirez-Echavarria, Antonis Bikakis, Luke Dickens, Rob Miller,
  Andreas Vlachidis",On the Effects of Knowledge-Augmented Data in Word Embeddings,"10 pages, 5 figures, submitted to ACL 2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper investigates techniques for knowledge injection into word
embeddings learned from large corpora of unannotated data. These
representations are trained with word cooccurrence statistics and do not
commonly exploit syntactic and semantic information from linguistic knowledge
bases, which potentially limits their transferability to domains with differing
language distributions or usages. We propose a novel approach for linguistic
knowledge injection through data augmentation to learn word embeddings that
enforce semantic relationships from the data, and systematically evaluate the
impact it has on the resulting representations. We show our knowledge
augmentation approach improves the intrinsic characteristics of the learned
embeddings while not significantly altering their results on a downstream text
classification task.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 02:14:13 GMT'}]",2020-10-06,"[['Ramirez-Echavarria', 'Diego', ''], ['Bikakis', 'Antonis', ''], ['Dickens', 'Luke', ''], ['Miller', 'Rob', ''], ['Vlachidis', 'Andreas', '']]"
1358101,2010.01771,Dongqin Xu,"Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, Guodong Zhou",Improving AMR Parsing with Sequence-to-Sequence Pre-training,Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the literature, the research on abstract meaning representation (AMR)
parsing is much restricted by the size of human-curated dataset which is
critical to build an AMR parser with good performance. To alleviate such data
size restriction, pre-trained models have been drawing more and more attention
in AMR parsing. However, previous pre-trained models, like BERT, are
implemented for general purpose which may not work as expected for the specific
task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq)
AMR parsing and propose a seq2seq pre-training approach to build pre-trained
models in both single and joint way on three relevant tasks, i.e., machine
translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the
vanilla fine-tuning method to a multi-task learning fine-tuning method that
optimizes for the performance of AMR parsing while endeavors to preserve the
response of pre-trained models. Extensive experimental results on two English
benchmark datasets show that both the single and joint pre-trained models
significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0),
which reaches the state of the art. The result is very encouraging since we
achieve this with seq2seq models rather than complex models. We make our code
and model available at https://github.com/xdqkid/S2S-AMR-Parser.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 04:32:47 GMT'}]",2020-10-06,"[['Xu', 'Dongqin', ''], ['Li', 'Junhui', ''], ['Zhu', 'Muhua', ''], ['Zhang', 'Min', ''], ['Zhou', 'Guodong', '']]"
1358111,2010.01781,Tengfei Ma,"Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa and Shouling Ji","Unsupervised Reference-Free Summary Quality Evaluation via Contrastive
  Learning",Long Paper in EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Evaluation of a document summarization system has been a critical factor to
impact the success of the summarization task. Previous approaches, such as
ROUGE, mainly consider the informativeness of the assessed summary and require
human-generated references for each test summary. In this work, we propose to
evaluate the summary qualities without reference summaries by unsupervised
contrastive learning. Specifically, we design a new metric which covers both
linguistic qualities and semantic informativeness based on BERT. To learn the
metric, for each summary, we construct different types of negative samples with
respect to different aspects of the summary qualities, and train our model with
a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our
new evaluation method outperforms other metrics even without reference
summaries. Furthermore, we show that our method is general and transferable
across datasets.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 05:04:14 GMT'}]",2020-10-06,"[['Wu', 'Hanlu', ''], ['Ma', 'Tengfei', ''], ['Wu', 'Lingfei', ''], ['Manyumwa', 'Tariro', ''], ['Ji', 'Shouling', '']]"
1299627,2006.05113,Lukas Muttenthaler,"Lukas Muttenthaler, Nora Hollenstein, Maria Barrett",Human brain activity for machine attention,,,,,cs.CL cs.LG q-bio.NC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cognitively inspired NLP leverages human-derived data to teach machines about
language processing mechanisms. Recently, neural networks have been augmented
with behavioral data to solve a range of NLP tasks spanning syntax and
semantics. We are the first to exploit neuroscientific data, namely
electroencephalography (EEG), to inform a neural attention model about language
processing of the human brain. The challenge in working with EEG data is that
features are exceptionally rich and need extensive pre-processing to isolate
signals specific to text processing. We devise a method for finding such EEG
features to supervise machine attention through combining theoretically
motivated cropping with random forest tree splits. After this dimensionality
reduction, the pre-processed EEG features are capable of distinguishing two
reading tasks retrieved from a publicly available EEG corpus. We apply these
features to regularise attention on relation classification and show that EEG
is more informative than strong baselines. This improvement depends on both the
cognitive load of the task and the EEG frequency domain. Hence, informing
neural attention models with EEG signals is beneficial but requires further
investigation to understand which dimensions are the most useful across NLP
tasks.
","[{'version': 'v1', 'created': 'Tue, 9 Jun 2020 08:39:07 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 22:06:31 GMT'}]",2020-10-06,"[['Muttenthaler', 'Lukas', ''], ['Hollenstein', 'Nora', ''], ['Barrett', 'Maria', '']]"
1358116,2010.01786,Tanmoy Chakraborty,"Alvin Dey, Tanya Chowdhury, Yash Kumar Atri, Tanmoy Chakraborty","Corpora Evaluation and System Bias Detection in Multi-document
  Summarization","11 pages, 3 tables, 5 figures, Accepted in the Findings of EMNLP,
  2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-document summarization (MDS) is the task of reflecting key points from
any set of documents into a concise text paragraph. In the past, it has been
used to aggregate news, tweets, product reviews, etc. from various sources.
Owing to no standard definition of the task, we encounter a plethora of
datasets with varying levels of overlap and conflict between participating
documents. There is also no standard regarding what constitutes summary
information in MDS. Adding to the challenge is the fact that new systems report
results on a set of chosen datasets, which might not correlate with their
performance on the other datasets. In this paper, we study this heterogeneous
task with the help of a few widely used MDS corpora and a suite of
state-of-the-art models. We make an attempt to quantify the quality of
summarization corpus and prescribe a list of points to consider while proposing
a new MDS corpus. Next, we analyze the reason behind the absence of an MDS
system which achieves superior performance across all corpora. We then observe
the extent to which system metrics are influenced, and bias is propagated due
to corpus properties. The scripts to reproduce the experiments in this work are
available at https://github.com/LCS2-IIITD/summarization_bias.git.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 05:25:43 GMT'}]",2020-10-06,"[['Dey', 'Alvin', ''], ['Chowdhury', 'Tanya', ''], ['Atri', 'Yash Kumar', ''], ['Chakraborty', 'Tanmoy', '']]"
1046032,1811.01399,Peifeng Wang,"Peifeng Wang, Jialong Han, Chenliang Li, Rong Pan","Logic Attention Based Neighborhood Aggregation for Inductive Knowledge
  Graph Embedding",Accepted by AAAI 2019,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graph embedding aims at modeling entities and relations with
low-dimensional vectors. Most previous methods require that all entities should
be seen during training, which is unpractical for real-world knowledge graphs
with new entities emerging on a daily basis. Recent efforts on this issue
suggest training a neighborhood aggregator in conjunction with the conventional
entity and relation embeddings, which may help embed new entities inductively
via their existing neighbors. However, their neighborhood aggregators neglect
the unordered and unequal natures of an entity's neighbors. To this end, we
summarize the desired properties that may lead to effective neighborhood
aggregators. We also introduce a novel aggregator, namely, Logic Attention
Network (LAN), which addresses the properties by aggregating neighbors with
both rules- and network-based attention weights. By comparing with conventional
aggregators on two knowledge graph completion tasks, we experimentally validate
LAN's superiority in terms of the desired properties.
","[{'version': 'v1', 'created': 'Sun, 4 Nov 2018 16:39:28 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 04:06:14 GMT'}]",2020-10-06,"[['Wang', 'Peifeng', ''], ['Han', 'Jialong', ''], ['Li', 'Chenliang', ''], ['Pan', 'Rong', '']]"
1298848,2006.04334,Shahan Ali Memon,"Shahan Ali Memon, Aman Tyagi, David R. Mortensen, Kathleen M. Carley","Characterizing Sociolinguistic Variation in the Competing Vaccination
  Communities","11 pages, 4 tables, 1 figure, 1 algorithm, accepted to SBP-BRiMS 2020
  -- International Conference on Social Computing, Behavioral-Cultural Modeling
  & Prediction and Behavior Representation in Modeling and Simulation",,,,cs.SI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Public health practitioners and policy makers grapple with the challenge of
devising effective message-based interventions for debunking public health
misinformation in cyber communities. ""Framing"" and ""personalization"" of the
message is one of the key features for devising a persuasive messaging
strategy. For an effective health communication, it is imperative to focus on
""preference-based framing"" where the preferences of the target sub-community
are taken into consideration. To achieve that, it is important to understand
and hence characterize the target sub-communities in terms of their social
interactions. In the context of health-related misinformation, vaccination
remains to be the most prevalent topic of discord. Hence, in this paper, we
conduct a sociolinguistic analysis of the two competing vaccination communities
on Twitter: ""pro-vaxxers"" or individuals who believe in the effectiveness of
vaccinations, and ""anti-vaxxers"" or individuals who are opposed to
vaccinations. Our data analysis show significant linguistic variation between
the two communities in terms of their usage of linguistic intensifiers,
pronouns, and uncertainty words. Our network-level analysis show significant
differences between the two communities in terms of their network density,
echo-chamberness, and the EI index. We hypothesize that these sociolinguistic
differences can be used as proxies to characterize and understand these
communities to devise better message interventions.
","[{'version': 'v1', 'created': 'Mon, 8 Jun 2020 03:05:28 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Jul 2020 09:39:51 GMT'}, {'version': 'v3', 'created': 'Sun, 4 Oct 2020 13:28:50 GMT'}]",2020-10-06,"[['Memon', 'Shahan Ali', ''], ['Tyagi', 'Aman', ''], ['Mortensen', 'David R.', ''], ['Carley', 'Kathleen M.', '']]"
1358121,2010.01791,Zi Lin,"Zi Lin, Jeremiah Zhe Liu, Zi Yang, Nan Hua, Dan Roth","Pruning Redundant Mappings in Transformer Models via Spectral-Normalized
  Identity Prior",Findings of EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Traditional (unstructured) pruning methods for a Transformer model focus on
regularizing the individual weights by penalizing them toward zero. In this
work, we explore spectral-normalized identity priors (SNIP), a structured
pruning approach that penalizes an entire residual module in a Transformer
model toward an identity mapping. Our method identifies and discards
unimportant non-linear mappings in the residual connections by applying a
thresholding operator on the function norm. It is applicable to any structured
module, including a single attention head, an entire attention block, or a
feed-forward subnetwork. Furthermore, we introduce spectral normalization to
stabilize the distribution of the post-activation values of the Transformer
layers, further improving the pruning effectiveness of the proposed
methodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to
demonstrate that SNIP achieves effective pruning results while maintaining
comparable performance. Specifically, we improve the performance over the
state-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 05:40:56 GMT'}]",2020-10-06,"[['Lin', 'Zi', ''], ['Liu', 'Jeremiah Zhe', ''], ['Yang', 'Zi', ''], ['Hua', 'Nan', ''], ['Roth', 'Dan', '']]"
1296677,2006.02163,Xuan Phi Nguyen,"Xuan-Phi Nguyen, Shafiq Joty, Wu Kui, Ai Ti Aw","Cross-model Back-translated Distillation for Unsupervised Machine
  Translation",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent unsupervised machine translation (UMT) systems usually employ three
main principles: initialization, language modeling and iterative
back-translation, though they may apply them differently. Crucially, iterative
back-translation and denoising auto-encoding for language modeling provide data
diversity to train the UMT systems. However, the gains from these
diversification processes has seemed to plateau. We introduce a novel component
to the standard UMT framework called Cross-model Back-translated Distillation
(CBD), that is aimed to induce another level of data diversification that
existing principles lack. CBD is applicable to all previous UMT approaches. In
our experiments, it boosts the performance of the standard UMT methods by
1.5-2.0 BLEU. In particular, in WMT'14 English-French, WMT'16 German-English
and English-Romanian, CBD outperforms cross-lingual masked language model (XLM)
by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields 1.5--3.3 BLEU
improvements in IWSLT English-French and English-German tasks. Through
extensive experimental analyses, we show that CBD is effective because it
embraces data diversity while other similar variants do not.
","[{'version': 'v1', 'created': 'Wed, 3 Jun 2020 10:57:21 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 14:07:14 GMT'}]",2020-10-06,"[['Nguyen', 'Xuan-Phi', ''], ['Joty', 'Shafiq', ''], ['Kui', 'Wu', ''], ['Aw', 'Ai Ti', '']]"
1358155,2010.01825,Yoav Levine,"Yoav Levine, Barak Lenz, Opher Lieber, Omri Abend, Kevin Leyton-Brown,
  Moshe Tennenholtz, Yoav Shoham",PMI-Masking: Principled masking of correlated spans,,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Masking tokens uniformly at random constitutes a common flaw in the
pretraining of Masked Language Models (MLMs) such as BERT. We show that such
uniform masking allows an MLM to minimize its training objective by latching
onto shallow local signals, leading to pretraining inefficiency and suboptimal
downstream performance. To address this flaw, we propose PMI-Masking, a
principled masking strategy based on the concept of Pointwise Mutual
Information (PMI), which jointly masks a token n-gram if it exhibits high
collocation over the corpus. PMI-Masking motivates, unifies, and improves upon
prior more heuristic approaches that attempt to address the drawback of random
uniform token masking, such as whole-word masking, entity/phrase masking, and
random-span masking. Specifically, we show experimentally that PMI-Masking
reaches the performance of prior masking approaches in half the training time,
and consistently improves performance at the end of training.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 07:19:52 GMT'}]",2020-10-06,"[['Levine', 'Yoav', ''], ['Lenz', 'Barak', ''], ['Lieber', 'Opher', ''], ['Abend', 'Omri', ''], ['Leyton-Brown', 'Kevin', ''], ['Tennenholtz', 'Moshe', ''], ['Shoham', 'Yoav', '']]"
1358229,2010.01899,Xin Lv,"Xin Lv, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Wei Zhang, Yichi
  Zhang, Hao Kong, Suhui Wu","Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse
  Knowledge Graph",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-hop reasoning has been widely studied in recent years to seek an
effective and interpretable method for knowledge graph (KG) completion. Most
previous reasoning methods are designed for dense KGs with enough paths between
entities, but cannot work well on those sparse KGs that only contain sparse
paths for reasoning. On the one hand, sparse KGs contain less information,
which makes it difficult for the model to choose correct paths. On the other
hand, the lack of evidential paths to target entities also makes the reasoning
process difficult. To solve these problems, we propose a multi-hop reasoning
model named DacKGR over sparse KGs, by applying novel dynamic anticipation and
completion strategies: (1) The anticipation strategy utilizes the latent
prediction of embedding-based models to make our model perform more potential
path search over sparse KGs. (2) Based on the anticipation information, the
completion strategy dynamically adds edges as additional actions during the
path search, which further alleviates the sparseness problem of KGs. The
experimental results on five datasets sampled from Freebase, NELL and Wikidata
show that our method outperforms state-of-the-art baselines. Our codes and
datasets can be obtained from https://github.com/THU-KEG/DacKGR
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 10:28:03 GMT'}]",2020-10-06,"[['Lv', 'Xin', ''], ['Han', 'Xu', ''], ['Hou', 'Lei', ''], ['Li', 'Juanzi', ''], ['Liu', 'Zhiyuan', ''], ['Zhang', 'Wei', ''], ['Zhang', 'Yichi', ''], ['Kong', 'Hao', ''], ['Wu', 'Suhui', '']]"
1358047,2010.01717,Nader Akoury,"Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng,
  Mohit Iyyer","STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story
  Generation",Accepted as a long paper to EMNLP 2020,,,,cs.CL cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Systems for story generation are asked to produce plausible and enjoyable
stories given an input context. This task is underspecified, as a vast number
of diverse stories can originate from a single input. The large output space
makes it difficult to build and evaluate story generation models, as (1)
existing datasets lack rich enough contexts to meaningfully guide models, and
(2) existing evaluations (both crowdsourced and automatic) are unreliable for
assessing long-form creative text. To address these issues, we introduce a
dataset and evaluation platform built from STORIUM, an online collaborative
storytelling community. Our author-generated dataset contains 6K lengthy
stories (125M tokens) with fine-grained natural language annotations (e.g.,
character goals and attributes) interspersed throughout each narrative, forming
a robust source for guiding models. We evaluate language models fine-tuned on
our dataset by integrating them onto STORIUM, where real authors can query a
model for suggested story continuations and then edit them. Automatic metrics
computed over these edits correlate well with both user ratings of generated
stories and qualitative feedback from semi-structured user interviews. We
release both the STORIUM dataset and evaluation platform to spur more
principled research into story generation.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 23:26:09 GMT'}]",2020-10-06,"[['Akoury', 'Nader', ''], ['Wang', 'Shufan', ''], ['Whiting', 'Josh', ''], ['Hood', 'Stephen', ''], ['Peng', 'Nanyun', ''], ['Iyyer', 'Mohit', '']]"
1358043,2010.01713,Anshuman Mishra,"Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Li, Pavan
  Kapanipathi, Kartik Talamadupula",Reading Comprehension as Natural Language Inference: A Semantic Analysis,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the recent past, Natural language Inference (NLI) has gained significant
attention, particularly given its promise for downstream NLP tasks. However,
its true impact is limited and has not been well studied. Therefore, in this
paper, we explore the utility of NLI for one of the most prominent downstream
tasks, viz. Question Answering (QA). We transform the one of the largest
available MRC dataset (RACE) to an NLI form, and compare the performances of a
state-of-the-art model (RoBERTa) on both these forms. We propose new
characterizations of questions, and evaluate the performance of QA and NLI
models on these categories. We highlight clear categories for which the model
is able to perform better when the data is presented in a coherent entailment
form, and a structured question-answer concatenation form, respectively.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 22:50:59 GMT'}]",2020-10-06,"[['Mishra', 'Anshuman', ''], ['Patel', 'Dhruvesh', ''], ['Vijayakumar', 'Aparna', ''], ['Li', 'Xiang', ''], ['Kapanipathi', 'Pavan', ''], ['Talamadupula', 'Kartik', '']]"
1358033,2010.01703,Zhong-Qiu Wang,Zhong-Qiu Wang and Peidong Wang and DeLiang Wang,"Multi-microphone Complex Spectral Mapping for Utterance-wise and
  Continuous Speaker Separation","10 pages, in submission",,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose multi-microphone complex spectral mapping, a simple way of
applying deep learning for time-varying non-linear beamforming, for offline
utterance-wise and block-online continuous speaker separation in reverberant
conditions, aiming at both speaker separation and dereverberation. Assuming a
fixed array geometry between training and testing, we train deep neural
networks (DNN) to predict the real and imaginary (RI) components of target
speech at a reference microphone from the RI components of multiple
microphones. We then integrate multi-microphone complex spectral mapping with
beamforming and post-filtering to further improve separation, and combine it
with frame-level speaker counting for block-online continuous speaker
separation (CSS). Although our system is trained on simulated room impulse
responses (RIR) based on a fixed number of microphones arranged in a given
geometry, it generalizes well to a real array with the same geometry.
State-of-the-art separation performance is obtained on the simulated two-talker
SMS-WSJ corpus and the real-recorded LibriCSS dataset.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 22:13:13 GMT'}]",2020-10-06,"[['Wang', 'Zhong-Qiu', ''], ['Wang', 'Peidong', ''], ['Wang', 'DeLiang', '']]"
1358024,2010.01694,St\'ephane Aroca-Ouellette,"Stephane Aroca-Ouellette, Frank Rudzicz",On Losses for Modern Language Models,"Accepted to EMNLP 2020. 9 Pages + 3 Pages of References and
  Appendices (12 Pages total)",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  BERT set many state-of-the-art results over varied NLU benchmarks by
pre-training over two tasks: masked language modelling (MLM) and next sentence
prediction (NSP), the latter of which has been highly criticized. In this
paper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen
possible auxiliary pre-training tasks, of which seven are novel to modern
language models, and 3) investigate different ways to include multiple tasks
into pre-training. We show that NSP is detrimental to training due to its
context splitting and shallow semantic signal. We also identify six auxiliary
pre-training tasks -- sentence ordering, adjacent sentence prediction, TF
prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant
-- that outperform a pure MLM baseline. Finally, we demonstrate that using
multiple tasks in a multi-task pre-training framework provides better results
than using any single auxiliary task. Using these methods, we outperform BERT
Base on the GLUE benchmark using fewer than a quarter of the training tokens.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 21:44:15 GMT'}]",2020-10-06,"[['Aroca-Ouellette', 'Stephane', ''], ['Rudzicz', 'Frank', '']]"
1357879,2010.01549,Faria Huq,"Faria Huq, Anindya Iqbal, Nafees Ahmed","Holistic static and animated 3D scene generation from diverse text
  descriptions",,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a framework for holistic static and animated 3D scene generation
from diverse text descriptions. Prior works of scene generation rely on static
rule-based entity extraction from natural language description. However, this
limits the usability of a practical solution. To overcome this limitation, we
use one of state-of-the-art architecture - TransformerXL. Instead of rule-based
extraction, our framework leverages the rich contextual encoding which allows
us to process a larger range (diverse) of possible natural language
descriptions. We empirically show how our proposed mechanism generalizes even
on novel combinations of object-features during inference. We also show how our
framework can jointly generate static and animated 3D scene efficiently. We
modify CLEVR to generate a large, scalable dataset - Integrated static and
animated 3D scene (Iscene). Data preparation code and pre-trained model
available at - https://github.com/oaishi/3DScene_from_text.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 11:31:21 GMT'}]",2020-10-06,"[['Huq', 'Faria', ''], ['Iqbal', 'Anindya', ''], ['Ahmed', 'Nafees', '']]"
1357884,2010.01554,Sina Ahmadi,"Sina Ahmadi, Hossein Hassani, Daban Q. Jaff","Leveraging Multilingual News Websites for Building a Kurdish Parallel
  Corpus","11 pages, under review in the ACM Transactions on Asian and
  Low-Resource Language Information Processing (TALLIP) Corpus available at
  https://github.com/KurdishBLARK/InterdialectCorpus",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Machine translation has been a major motivation of development in natural
language processing. Despite the burgeoning achievements in creating more
efficient machine translation systems thanks to deep learning methods, parallel
corpora have remained indispensable for progress in the field. In an attempt to
create parallel corpora for the Kurdish language, in this paper, we describe
our approach in retrieving potentially-alignable news articles from
multi-language websites and manually align them across dialects and languages
based on lexical similarity and transliteration of scripts. We present a corpus
containing 12,327 translation pairs in the two major dialects of Kurdish,
Sorani and Kurmanji. We also provide 1,797 and 650 translation pairs in
English-Kurmanji and English-Sorani. The corpus is publicly available under the
CC BY-NC-SA 4.0 license.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 11:52:50 GMT'}]",2020-10-06,"[['Ahmadi', 'Sina', ''], ['Hassani', 'Hossein', ''], ['Jaff', 'Daban Q.', '']]"
1357886,2010.01556,Qianying Liu,"Qianying Liu, Wenyu Guan, Sujian Li, Fei Cheng, Daisuke Kawahara and
  Sadao Kurohashi",Reverse Operation based Data Augmentation for Solving Math Word Problems,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatically solving math word problems is a critical task in the field of
natural language processing. Recent models have reached their performance
bottleneck and require more high-quality data for training. Inspired by human
double-checking mechanism, we propose a reverse operation based data
augmentation method that makes use of mathematical logic to produce new
high-quality math problems and introduce new knowledge points that can give
supervision for new mathematical reasoning logic. We apply the augmented data
on two SOTA math word problem solving models. Experimental results show the
effectiveness of our approach\footnote{We will release our code and data after
the paper is accepted.}.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 11:59:59 GMT'}]",2020-10-06,"[['Liu', 'Qianying', ''], ['Guan', 'Wenyu', ''], ['Li', 'Sujian', ''], ['Cheng', 'Fei', ''], ['Kawahara', 'Daisuke', ''], ['Kurohashi', 'Sadao', '']]"
1358316,2010.01986,Wanzheng Zhu,"Wanzheng Zhu, Chao Zhang, Shuochao Yao, Xiaobin Gao, Jiawei Han","A Spherical Hidden Markov Model for Semantics-Rich Human Mobility
  Modeling",,,,,cs.LG cs.AI cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the problem of modeling human mobility from semantic trace data,
wherein each GPS record in a trace is associated with a text message that
describes the user's activity. Existing methods fall short in unveiling human
movement regularities, because they either do not model the text data at all or
suffer from text sparsity severely. We propose SHMM, a multi-modal spherical
hidden Markov model for semantics-rich human mobility modeling. Under the
hidden Markov assumption, SHMM models the generation process of a given trace
by jointly considering the observed location, time, and text at each step of
the trace. The distinguishing characteristic of SHMM is the text modeling part.
We use fixed-size vector representations to encode the semantics of the text
messages, and model the generation of the l2-normalized text embeddings on a
unit sphere with the von Mises-Fisher (vMF) distribution. Compared with other
alternatives like multi-variate Gaussian, our choice of the vMF distribution
not only incurs much fewer parameters, but also better leverages the
discriminative power of text embeddings in a directional metric space. The
parameter inference for the vMF distribution is non-trivial since it involves
functional inversion of ratios of Bessel functions. We theoretically prove
that: 1) the classical Expectation-Maximization algorithm can work with vMF
distributions; and 2) while closed-form solutions are hard to be obtained for
the M-step, Newton's method is guaranteed to converge to the optimal solution
with quadratic convergence rate. We have performed extensive experiments on
both synthetic and real-life data. The results on synthetic data verify our
theoretical analysis; while the results on real-life data demonstrate that SHMM
learns meaningful semantics-rich mobility models, outperforms state-of-the-art
mobility models for next location prediction, and incurs lower training cost.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 13:18:38 GMT'}]",2020-10-06,"[['Zhu', 'Wanzheng', ''], ['Zhang', 'Chao', ''], ['Yao', 'Shuochao', ''], ['Gao', 'Xiaobin', ''], ['Han', 'Jiawei', '']]"
1305577,2006.11063,Ilya Gusev,Ilya Gusev,Dataset for Automatic Summarization of Russian News,"Version 3, accepted to AINL 2020","In: AINL 2020. Communications in Computer and Information Science,
  vol 1292. Springer, Cham (2020)",10.1007/978-3-030-59082-6_9,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic text summarization has been studied in a variety of domains and
languages. However, this does not hold for the Russian language. To overcome
this issue, we present Gazeta, the first dataset for summarization of Russian
news. We describe the properties of this dataset and benchmark several
extractive and abstractive models. We demonstrate that the dataset is a valid
task for methods of text summarization for Russian. Additionally, we prove the
pretrained mBART model to be useful for Russian text summarization.
","[{'version': 'v1', 'created': 'Fri, 19 Jun 2020 10:44:06 GMT'}, {'version': 'v2', 'created': 'Tue, 14 Jul 2020 23:42:10 GMT'}, {'version': 'v3', 'created': 'Wed, 22 Jul 2020 17:12:48 GMT'}]",2020-10-06,"[['Gusev', 'Ilya', '']]"
1357950,2010.01620,Cheng Zhang,"Cheng Zhang, Jie Wang",Meta Sequence Learning and Its Applications,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a meta-sequence representation of sentences and demonstrate how to
use meta sequence learning to generate adequate question-answer pairs (QAPs)
over a given article. A meta sequence is a sequence of vectors of semantic and
syntactic tags. On a given declarative sentence, a trained model converts it to
a meta sequence, finds a matched meta sequence in its learned database, and
uses the corresponding meta sequence for interrogative sentence to generate
QAPs. We show that, trained on a small dataset, our method generates
efficiently, on the official SAT practice reading tests, a large number of
syntactically and semantically correct QAPs with high accuracy.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 16:28:13 GMT'}]",2020-10-06,"[['Zhang', 'Cheng', ''], ['Wang', 'Jie', '']]"
1358279,2010.01949,Kellen Gillespie,"Kellen Gillespie, Ioannis C. Konstantakopoulos, Xingzhi Guo, Vishal
  Thanvantri Vasudevan, Abhinav Sethy","Improving Device Directedness Classification of Utterances with Semantic
  Lexical Features",Accepted and Published at ICASSP 2020,"2020 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Barcelona, Spain, 2020, pp. 7859-7863",10.1109/ICASSP40776.2020.9054304,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  User interactions with personal assistants like Alexa, Google Home and Siri
are typically initiated by a wake term or wakeword. Several personal assistants
feature ""follow-up"" modes that allow users to make additional interactions
without the need of a wakeword. For the system to only respond when
appropriate, and to ignore speech not intended for it, utterances must be
classified as device-directed or non-device-directed. State-of-the-art systems
have largely used acoustic features for this task, while others have used only
lexical features or have added LM-based lexical features. We propose a
directedness classifier that combines semantic lexical features with a
lightweight acoustic feature and show it is effective in classifying
directedness. The mixed-domain lexical and acoustic feature model is able to
achieve 14% relative reduction of EER over a state-of-the-art acoustic-only
baseline model. Finally, we successfully apply transfer learning and
semi-supervised learning to the model to improve accuracy even further.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 20:13:58 GMT'}]",2020-10-06,"[['Gillespie', 'Kellen', ''], ['Konstantakopoulos', 'Ioannis C.', ''], ['Guo', 'Xingzhi', ''], ['Vasudevan', 'Vishal Thanvantri', ''], ['Sethy', 'Abhinav', '']]"
1358492,2010.02162,Zequn Sun,"Zequn Sun, Muhao Chen, Wei Hu, Chengming Wang, Jian Dai, Wei Zhang",Knowledge Association with Hyperbolic Knowledge Graph Embeddings,EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Capturing associations for knowledge graphs (KGs) through entity alignment,
entity type inference and other related tasks benefits NLP applications with
comprehensive knowledge representations. Recent related methods built on
Euclidean embeddings are challenged by the hierarchical structures and
different scales of KGs. They also depend on high embedding dimensions to
realize enough expressiveness. Differently, we explore with low-dimensional
hyperbolic embeddings for knowledge association. We propose a hyperbolic
relational graph neural network for KG embedding and capture knowledge
associations with a hyperbolic transformation. Extensive experiments on entity
alignment and type inference demonstrate the effectiveness and efficiency of
our method.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 17:11:35 GMT'}]",2020-10-06,"[['Sun', 'Zequn', ''], ['Chen', 'Muhao', ''], ['Hu', 'Wei', ''], ['Wang', 'Chengming', ''], ['Dai', 'Jian', ''], ['Zhang', 'Wei', '']]"
1357983,2010.01653,Ilias Chalkidis,"Ilias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos
  Malakasiotis, Nikolaos Aletras and Ion Androutsopoulos","An Empirical Study on Large-Scale Multi-Label Text Classification
  Including Few and Zero-Shot Labels","9 pages, long paper at EMNLP 2020 proceedings",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale Multi-label Text Classification (LMTC) has a wide range of
Natural Language Processing (NLP) applications and presents interesting
challenges. First, not all labels are well represented in the training set, due
to the very large label set and the skewed label distributions of LMTC
datasets. Also, label hierarchies and differences in human labelling guidelines
may affect graph-aware annotation proximity. Finally, the label hierarchies are
periodically updated, requiring LMTC models capable of zero-shot
generalization. Current state-of-the-art LMTC models employ Label-Wise
Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label
classification; (2) may use the label hierarchy to improve zero-shot learning,
although this practice is vastly understudied; and (3) have not been combined
with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art
results in several NLP benchmarks. Here, for the first time, we empirically
evaluate a battery of LMTC methods from vanilla LWANs to hierarchical
classification approaches and transfer learning, on frequent, few, and
zero-shot learning on three datasets from different domains. We show that
hierarchical methods based on Probabilistic Label Trees (PLTs) outperform
LWANs. Furthermore, we show that Transformer-based approaches outperform the
state-of-the-art in two of the datasets, and we propose a new state-of-the-art
method which combines BERT with LWANs. Finally, we propose new models that
leverage the label hierarchy to improve few and zero-shot learning, considering
on each dataset a graph-aware annotation proximity measure that we introduce.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 18:55:47 GMT'}]",2020-10-06,"[['Chalkidis', 'Ilias', ''], ['Fergadiotis', 'Manos', ''], ['Kotitsas', 'Sotiris', ''], ['Malakasiotis', 'Prodromos', ''], ['Aletras', 'Nikolaos', ''], ['Androutsopoulos', 'Ion', '']]"
1357988,2010.01658,Wei-Jen Ko,Wei-Jen Ko and Avik Ray and Yilin Shen and Hongxia Jin,Generating Dialogue Responses from a Semantic Latent Space,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing open-domain dialogue generation models are usually trained to mimic
the gold response in the training set using cross-entropy loss on the
vocabulary. However, a good response does not need to resemble the gold
response, since there are multiple possible responses to a given prompt. In
this work, we hypothesize that the current models are unable to integrate
information from multiple semantically similar valid responses of a prompt,
resulting in the generation of generic and uninformative responses. To address
this issue, we propose an alternative to the end-to-end classification on
vocabulary. We learn the pair relationship between the prompts and responses as
a regression task on a latent space instead. In our novel dialog generation
model, the representations of semantically related sentences are close to each
other on the latent space. Human evaluation showed that learning the task on a
continuous space can generate responses that are both relevant and informative.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 19:06:16 GMT'}]",2020-10-06,"[['Ko', 'Wei-Jen', ''], ['Ray', 'Avik', ''], ['Shen', 'Yilin', ''], ['Jin', 'Hongxia', '']]"
1357997,2010.01667,Luyu Gao,"Luyu Gao, Xinyi Wang, Graham Neubig","Improving Target-side Lexical Transfer in Multilingual Neural Machine
  Translation",Accepted to Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To improve the performance of Neural Machine Translation~(NMT) for
low-resource languages~(LRL), one effective strategy is to leverage parallel
data from a related high-resource language~(HRL). However, multilingual data
has been found more beneficial for NMT models that translate from the LRL to a
target language than the ones that translate into the LRLs. In this paper, we
aim to improve the effectiveness of multilingual transfer for NMT models that
translate \emph{into} the LRL, by designing a better decoder word embedding.
Extending upon a general-purpose multilingual encoding method Soft Decoupled
Encoding~\citep{SDE}, we propose DecSDE, an efficient character n-gram based
embedding specifically designed for the NMT decoder. Our experiments show that
DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English
to four different languages.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 19:42:40 GMT'}]",2020-10-06,"[['Gao', 'Luyu', ''], ['Wang', 'Xinyi', ''], ['Neubig', 'Graham', '']]"
1358002,2010.01672,Jiaao Chen,"Jiaao Chen, Diyi Yang","Multi-View Sequence-to-Sequence Models with Conversational Structure for
  Abstractive Dialogue Summarization",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text summarization is one of the most challenging and interesting problems in
NLP. Although much attention has been paid to summarizing structured text like
news reports or encyclopedia articles, summarizing conversations---an essential
part of human-human/machine interaction where most important pieces of
information are scattered across various utterances of different
speakers---remains relatively under-investigated. This work proposes a
multi-view sequence-to-sequence model by first extracting conversational
structures of unstructured daily chats from different views to represent
conversations and then utilizing a multi-view decoder to incorporate different
views to generate dialogue summaries. Experiments on a large-scale dialogue
summarization corpus demonstrated that our methods significantly outperformed
previous state-of-the-art models via both automatic evaluations and human
judgment. We also discussed specific challenges that current approaches faced
with this task. We have publicly released our code at
https://github.com/GT-SALT/Multi-View-Seq2Seq.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 20:12:44 GMT'}]",2020-10-06,"[['Chen', 'Jiaao', ''], ['Yang', 'Diyi', '']]"
1358253,2010.01923,Tianyu Gao,"Hao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng Li, Zhiyuan Liu,
  Maosong Sun, Jie Zhou","Learning from Context or Names? An Empirical Study on Neural Relation
  Extraction",Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural models have achieved remarkable success on relation extraction (RE)
benchmarks. However, there is no clear understanding which type of information
affects existing RE models to make decisions and how to further improve the
performance of these models. To this end, we empirically study the effect of
two main information sources in text: textual context and entity mentions
(names). We find that (i) while context is the main source to support the
predictions, RE models also heavily rely on the information from entity
mentions, most of which is type information, and (ii) existing datasets may
leak shallow heuristics via entity mentions and thus contribute to the high
performance on RE benchmarks. Based on the analyses, we propose an
entity-masked contrastive pre-training framework for RE to gain a deeper
understanding on both textual context and type information while avoiding rote
memorization of entities or use of superficial cues in mentions. We carry out
extensive experiments to support our views, and show that our framework can
improve the effectiveness and robustness of neural models in different RE
scenarios. All the code and datasets are released at
https://github.com/thunlp/RE-Context-or-Names.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 11:21:59 GMT'}]",2020-10-06,"[['Peng', 'Hao', ''], ['Gao', 'Tianyu', ''], ['Han', 'Xu', ''], ['Lin', 'Yankai', ''], ['Li', 'Peng', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', ''], ['Zhou', 'Jie', '']]"
1358007,2010.01677,Jiaao Chen,"Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang, Diyi Yang",Local Additivity Based Data Augmentation for Semi-supervised NER,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Named Entity Recognition (NER) is one of the first stages in deep language
understanding yet current NER models heavily rely on human-annotated data. In
this work, to alleviate the dependence on labeled data, we propose a Local
Additivity based Data Augmentation (LADA) method for semi-supervised NER, in
which we create virtual samples by interpolating sequences close to each other.
Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA
performs interpolations among tokens within one sentence, and Inter-LADA
samples different sentences to interpolate. Through linear additions between
sampled training data, LADA creates an infinite amount of labeled data and
improves both entity and context learning. We further extend LADA to the
semi-supervised setting by designing a novel consistency loss for unlabeled
data. Experiments conducted on two NER benchmarks demonstrate the effectiveness
of our methods over several strong baselines. We have publicly released our
code at https://github.com/GT-SALT/LADA.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 20:46:26 GMT'}]",2020-10-06,"[['Chen', 'Jiaao', ''], ['Wang', 'Zhenghui', ''], ['Tian', 'Ran', ''], ['Yang', 'Zichao', ''], ['Yang', 'Diyi', '']]"
1358008,2010.01678,Xi Ye,"Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett",Optimal Neural Program Synthesis from Multimodal Specifications,,,,,cs.CL cs.PL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal program synthesis, which leverages different types of user input
to synthesize a desired program, is an attractive way to scale program
synthesis to challenging settings; however, it requires integrating noisy
signals from the user (like natural language) with hard constraints on the
program's behavior. This paper proposes an optimal neural synthesis approach
where the goal is to find a program that satisfies user-provided constraints
while also maximizing the program's score with respect to a neural model.
Specifically, we focus on multimodal synthesis tasks in which the user intent
is expressed using combination of natural language (NL) and input-output
examples. At the core of our method is a top-down recurrent neural model that
places distributions over abstract syntax trees conditioned on the NL input.
This model not only allows for efficient search over the space of syntactically
valid programs, but it allows us to leverage automated program analysis
techniques for pruning the search space based on infeasibility of partial
programs with respect to the user's constraints. The experimental results on a
multimodal synthesis dataset (StructuredRegex) show that our method
substantially outperforms prior state-of-the-art techniques in terms of
accuracy %, finds model-optimal programs more frequently, and explores fewer
states during search.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 20:51:21 GMT'}]",2020-10-06,"[['Ye', 'Xi', ''], ['Chen', 'Qiaochu', ''], ['Dillig', 'Isil', ''], ['Durrett', 'Greg', '']]"
1358013,2010.01683,Wenlin Yao,"Wenlin Yao, Cheng Zhang, Shiva Saravanan, Ruihong Huang, Ali Mostafavi","Weakly-supervised Fine-grained Event Recognition on Social Media Texts
  for Disaster Management","In Proceedings of the AAAI 2020 (AI for Social Impact Track). Link:
  https://aaai.org/ojs/index.php/AAAI/article/view/5391",,10.1609/aaai.v34i01.5391,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  People increasingly use social media to report emergencies, seek help or
share information during disasters, which makes social networks an important
tool for disaster management. To meet these time-critical needs, we present a
weakly supervised approach for rapidly building high-quality classifiers that
label each individual Twitter message with fine-grained event categories. Most
importantly, we propose a novel method to create high-quality labeled data in a
timely manner that automatically clusters tweets containing an event keyword
and asks a domain expert to disambiguate event word senses and label clusters
quickly. In addition, to process extremely noisy and often rather short
user-generated messages, we enrich tweet representations using preceding
context tweets and reply tweets in building event recognition classifiers. The
evaluation on two hurricanes, Harvey and Florence, shows that using only 1-2
person-hours of human supervision, the rapidly trained weakly supervised
classifiers outperform supervised classifiers trained using more than ten
thousand annotated tweets created in over 50 person-hours.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 21:06:45 GMT'}]",2020-10-06,"[['Yao', 'Wenlin', ''], ['Zhang', 'Cheng', ''], ['Saravanan', 'Shiva', ''], ['Huang', 'Ruihong', ''], ['Mostafavi', 'Ali', '']]"
1357987,2010.01657,Wei-Jen Ko,"Wei-Jen Ko and Te-Yuan Chen and Yiyan Huang and Greg Durrett and Junyi
  Jessy Li",Inquisitive Question Generation for High Level Text Comprehension,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inquisitive probing questions come naturally to humans in a variety of
settings, but is a challenging task for automatic systems. One natural type of
question to ask tries to fill a gap in knowledge during text comprehension,
like reading a news article: we might ask about background information, deeper
reasons behind things occurring, or more. Despite recent progress with
data-driven approaches, generating such questions is beyond the range of models
trained on existing datasets.
  We introduce INQUISITIVE, a dataset of ~19K questions that are elicited while
a person is reading through a document. Compared to existing datasets,
INQUISITIVE questions target more towards high-level (semantic and discourse)
comprehension of text. We show that readers engage in a series of pragmatic
strategies to seek information. Finally, we evaluate question generation models
based on GPT-2 and show that our model is able to generate reasonable questions
although the task is challenging, and highlight the importance of context to
generate INQUISITIVE questions.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 19:03:39 GMT'}]",2020-10-06,"[['Ko', 'Wei-Jen', ''], ['Chen', 'Te-Yuan', ''], ['Huang', 'Yiyan', ''], ['Durrett', 'Greg', ''], ['Li', 'Junyi Jessy', '']]"
1279744,2004.14974,David Wadden,"David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van
  Zuylen, Arman Cohan, Hannaneh Hajishirzi",Fact or Fiction: Verifying Scientific Claims,"EMNLP 2020. GitHub: https://github.com/allenai/scifact. Leaderboard
  and demo: https://scifact.apps.allenai.org",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We introduce scientific claim verification, a new task to select abstracts
from the research literature containing evidence that SUPPORTS or REFUTES a
given scientific claim, and to identify rationales justifying each decision. To
study this task, we construct SciFact, a dataset of 1.4K expert-written
scientific claims paired with evidence-containing abstracts annotated with
labels and rationales. We develop baseline models for SciFact, and demonstrate
that simple domain adaptation techniques substantially improve performance
compared to models trained on Wikipedia or political news. We show that our
system is able to verify claims related to COVID-19 by identifying evidence
from the CORD-19 corpus. Our experiments indicate that SciFact will provide a
challenging testbed for the development of new systems designed to retrieve and
reason over corpora containing specialized domain knowledge. Data and code for
this new task are publicly available at https://github.com/allenai/scifact. A
leaderboard and COVID-19 fact-checking demo are available at
https://scifact.apps.allenai.org.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:22:57 GMT'}, {'version': 'v2', 'created': 'Fri, 1 May 2020 17:15:15 GMT'}, {'version': 'v3', 'created': 'Wed, 10 Jun 2020 20:49:43 GMT'}, {'version': 'v4', 'created': 'Thu, 17 Sep 2020 19:14:04 GMT'}, {'version': 'v5', 'created': 'Thu, 1 Oct 2020 08:16:27 GMT'}, {'version': 'v6', 'created': 'Sat, 3 Oct 2020 04:31:06 GMT'}]",2020-10-06,"[['Wadden', 'David', ''], ['Lin', 'Shanchuan', ''], ['Lo', 'Kyle', ''], ['Wang', 'Lucy Lu', ''], ['van Zuylen', 'Madeleine', ''], ['Cohan', 'Arman', ''], ['Hajishirzi', 'Hannaneh', '']]"
1357791,2010.01461,Yuncong Li,"Yuncong Li, Cunxiang Yin and Sheng-hua Zhong","Sentence Constituent-Aware Aspect-Category Sentiment Analysis with Graph
  Attention Networks",Long paper accepted by NLPCC 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect category sentiment analysis (ACSA) aims to predict the sentiment
polarities of the aspect categories discussed in sentences. Since a sentence
usually discusses one or more aspect categories and expresses different
sentiments toward them, various attention-based methods have been developed to
allocate the appropriate sentiment words for the given aspect category and
obtain promising results. However, most of these methods directly use the given
aspect category to find the aspect category-related sentiment words, which may
cause mismatching between the sentiment words and the aspect categories when an
unrelated sentiment word is semantically meaningful for the given aspect
category. To mitigate this problem, we propose a Sentence Constituent-Aware
Network (SCAN) for aspect-category sentiment analysis. SCAN contains two graph
attention modules and an interactive loss function. The graph attention modules
generate representations of the nodes in sentence constituency parse trees for
the aspect category detection (ACD) task and the ACSA task, respectively. ACD
aims to detect aspect categories discussed in sentences and is a auxiliary
task. For a given aspect category, the interactive loss function helps the ACD
task to find the nodes which can predict the aspect category but can't predict
other aspect categories. The sentiment words in the nodes then are used to
predict the sentiment polarity of the aspect category by the ACSA task. The
experimental results on five public datasets demonstrate the effectiveness of
SCAN.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 01:23:17 GMT'}]",2020-10-06,"[['Li', 'Yuncong', ''], ['Yin', 'Cunxiang', ''], ['Zhong', 'Sheng-hua', '']]"
1144221,1906.12035,Xipeng Qiu,"Xipeng Qiu, Hengzhi Pei, Hang Yan, Xuanjing Huang","A Concise Model for Multi-Criteria Chinese Word Segmentation with
  Transformer Encoder",Findings of EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-criteria Chinese word segmentation (MCCWS) aims to exploit the
relations among the multiple heterogeneous segmentation criteria and further
improve the performance of each single criterion. Previous work usually regards
MCCWS as different tasks, which are learned together under the multi-task
learning framework. In this paper, we propose a concise but effective unified
model for MCCWS, which is fully-shared for all the criteria. By leveraging the
powerful ability of the Transformer encoder, the proposed unified model can
segment Chinese text according to a unique criterion-token indicating the
output criterion. Besides, the proposed unified model can segment both
simplified and traditional Chinese and has an excellent transfer capability.
Experiments on eight datasets with different criteria show that our model
outperforms our single-criterion baseline model and other multi-criteria
models. Source codes of this paper are available on Github
https://github.com/acphile/MCCWS.
","[{'version': 'v1', 'created': 'Fri, 28 Jun 2019 04:08:15 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 11:02:37 GMT'}]",2020-10-06,"[['Qiu', 'Xipeng', ''], ['Pei', 'Hengzhi', ''], ['Yan', 'Hang', ''], ['Huang', 'Xuanjing', '']]"
1172598,1909.02560,Zhouxing Shi,"Zhouxing Shi, Minlie Huang","Robustness to Modification with Shared Words in Paraphrase
  Identification",Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Revealing the robustness issues of natural language processing models and
improving their robustness is important to their performance under difficult
situations. In this paper, we study the robustness of paraphrase identification
models from a new perspective -- via modification with shared words, and we
show that the models have significant robustness issues when facing such
modifications. To modify an example consisting of a sentence pair, we either
replace some words shared by both sentences or introduce new shared words. We
aim to construct a valid new example such that a target model makes a wrong
prediction. To find a modification solution, we use beam search constrained by
heuristic rules, and we leverage a BERT masked language model for generating
substitution words compatible with the context. Experiments show that the
performance of the target models has a dramatic drop on the modified examples,
thereby revealing the robustness issue. We also show that adversarial training
can mitigate this issue.
","[{'version': 'v1', 'created': 'Thu, 5 Sep 2019 17:59:15 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Sep 2019 17:46:33 GMT'}, {'version': 'v3', 'created': 'Sun, 10 Nov 2019 11:52:40 GMT'}, {'version': 'v4', 'created': 'Sat, 2 May 2020 04:38:11 GMT'}, {'version': 'v5', 'created': 'Mon, 5 Oct 2020 06:20:50 GMT'}]",2020-10-06,"[['Shi', 'Zhouxing', ''], ['Huang', 'Minlie', '']]"
1358221,2010.01891,Dat Quoc Nguyen,"Anh Tuan Nguyen, Mai Hoang Dao and Dat Quoc Nguyen",A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese,EMNLP 2020 (Findings),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semantic parsing is an important NLP task. However, Vietnamese is a
low-resource language in this research area. In this paper, we present the
first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese.
We extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et
al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two
baselines with key configurations and find that: automatic Vietnamese word
segmentation improves the parsing results of both baselines; the normalized
pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema
linking; latent syntactic features extracted from a neural dependency parser
for Vietnamese also improve the results; and the monolingual language model
PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher
performances than the recent best multilingual language model XLM-R (Conneau et
al., 2020).
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 09:54:51 GMT'}]",2020-10-06,"[['Nguyen', 'Anh Tuan', ''], ['Dao', 'Mai Hoang', ''], ['Nguyen', 'Dat Quoc', '']]"
1358328,2010.01998,Angel Daza,Angel Daza and Anette Frank,X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset,To be presented at the EMNLP 2020 Conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Even though SRL is researched for many languages, major improvements have
mostly been obtained for English, for which more resources are available. In
fact, existing multilingual SRL datasets contain disparate annotation styles or
come from different domains, hampering generalization in multilingual learning.
In this work, we propose a method to automatically construct an SRL corpus that
is parallel in four languages: English, French, German, Spanish, with unified
predicate and role annotations that are fully comparable across languages. We
apply high-quality machine translation to the English CoNLL-09 dataset and use
multilingual BERT to project its high-quality annotations to the target
languages. We include human-validated test sets that we use to measure the
projection quality, and show that projection is denser and more precise than a
strong baseline. Finally, we train different SOTA models on our novel corpus
for mono- and multilingual SRL, showing that the multilingual annotations
improve performance especially for the weaker languages.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 13:34:20 GMT'}]",2020-10-06,"[['Daza', 'Angel', ''], ['Frank', 'Anette', '']]"
1240348,2002.02925,Canwen Xu,Canwen Xu and Wangchunshu Zhou and Tao Ge and Furu Wei and Ming Zhou,BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose a novel model compression approach to effectively
compress BERT by progressive module replacing. Our approach first divides the
original BERT into several modules and builds their compact substitutes. Then,
we randomly replace the original modules with their substitutes to train the
compact modules to mimic the behavior of the original modules. We progressively
increase the probability of replacement through the training. In this way, our
approach brings a deeper level of interaction between the original and compact
models. Compared to the previous knowledge distillation approaches for BERT
compression, our approach does not introduce any additional loss function. Our
approach outperforms existing knowledge distillation approaches on GLUE
benchmark, showing a new perspective of model compression.
","[{'version': 'v1', 'created': 'Fri, 7 Feb 2020 17:52:16 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Feb 2020 18:45:41 GMT'}, {'version': 'v3', 'created': 'Wed, 25 Mar 2020 15:20:44 GMT'}, {'version': 'v4', 'created': 'Sat, 3 Oct 2020 12:18:50 GMT'}]",2020-10-06,"[['Xu', 'Canwen', ''], ['Zhou', 'Wangchunshu', ''], ['Ge', 'Tao', ''], ['Wei', 'Furu', ''], ['Zhou', 'Ming', '']]"
1239915,2002.02492,Sean Welleck,"Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang,
  Kyunghyun Cho","Consistency of a Recurrent Language Model With Respect to Incomplete
  Decoding",EMNLP 2020,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite strong performance on a variety of tasks, neural sequence models
trained with maximum likelihood have been shown to exhibit issues such as
length bias and degenerate repetition. We study the related issue of receiving
infinite-length sequences from a recurrent language model when using common
decoding algorithms. To analyze this issue, we first define inconsistency of a
decoding algorithm, meaning that the algorithm can yield an infinite-length
sequence that has zero probability under the model. We prove that commonly used
incomplete decoding algorithms - greedy search, beam search, top-k sampling,
and nucleus sampling - are inconsistent, despite the fact that recurrent
language models are trained to produce sequences of finite length. Based on
these insights, we propose two remedies which address inconsistency: consistent
variants of top-k and nucleus sampling, and a self-terminating recurrent
language model. Empirical results show that inconsistency occurs in practice,
and that the proposed methods prevent inconsistency.
","[{'version': 'v1', 'created': 'Thu, 6 Feb 2020 19:56:15 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 22:36:49 GMT'}]",2020-10-06,"[['Welleck', 'Sean', ''], ['Kulikov', 'Ilia', ''], ['Kim', 'Jaedeok', ''], ['Pang', 'Richard Yuanzhe', ''], ['Cho', 'Kyunghyun', '']]"
1239231,2002.01808,Ruize Wang,"Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu
  ji, Guihong Cao, Daxin Jiang, Ming Zhou",K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the problem of injecting knowledge into large pre-trained models
like BERT and RoBERTa. Existing methods typically update the original
parameters of pre-trained models when injecting knowledge. However, when
multiple kinds of knowledge are injected, they may suffer from catastrophic
forgetting. To address this, we propose K-Adapter, which remains the original
parameters of the pre-trained model fixed and supports continual knowledge
infusion. Taking RoBERTa as the pre-trained model, K-Adapter has a neural
adapter for each kind of infused knowledge, like a plug-in connected to
RoBERTa. There is no information flow between different adapters, thus
different adapters are efficiently trained in a distributed way. We inject two
kinds of knowledge, including factual knowledge obtained from automatically
aligned text-triplets on Wikipedia and Wikidata, and linguistic knowledge
obtained from dependency parsing. Results on three knowledge-driven tasks
(total six datasets) including relation classification, entity typing and
question answering demonstrate that each adapter improves the performance, and
the combination of both adapters brings further improvements. Probing
experiments further indicate that K-Adapter captures richer factual and
commonsense knowledge than RoBERTa.
","[{'version': 'v1', 'created': 'Wed, 5 Feb 2020 14:30:49 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Feb 2020 03:30:44 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Feb 2020 06:29:54 GMT'}, {'version': 'v4', 'created': 'Sun, 4 Oct 2020 16:11:45 GMT'}]",2020-10-06,"[['Wang', 'Ruize', ''], ['Tang', 'Duyu', ''], ['Duan', 'Nan', ''], ['Wei', 'Zhongyu', ''], ['Huang', 'Xuanjing', ''], ['ji', 'Jianshu', ''], ['Cao', 'Guihong', ''], ['Jiang', 'Daxin', ''], ['Zhou', 'Ming', '']]"
1358453,2010.02123,Yung-Sung Chuang,"Yung-Sung Chuang, Shang-Yu Su, Yun-Nung Chen",Lifelong Language Knowledge Distillation,EMNLP 2020 long paper,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is challenging to perform lifelong language learning (LLL) on a stream of
different tasks without any performance degradation comparing to the multi-task
counterparts. To address this issue, we present Lifelong Language Knowledge
Distillation (L2KD), a simple but efficient method that can be easily applied
to existing LLL architectures in order to mitigate the degradation.
Specifically, when the LLL model is trained on a new task, we assign a teacher
model to first learn the new task, and pass the knowledge to the LLL model via
knowledge distillation. Therefore, the LLL model can better adapt to the new
task while keeping the previously learned knowledge. Experiments show that the
proposed L2KD consistently improves previous state-of-the-art models, and the
degradation comparing to multi-task models in LLL tasks is well mitigated for
both sequence generation and text classification tasks.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 16:10:11 GMT'}]",2020-10-06,"[['Chuang', 'Yung-Sung', ''], ['Su', 'Shang-Yu', ''], ['Chen', 'Yun-Nung', '']]"
1280191,2005.00396,Philipp Dufter,"Philipp Dufter, Hinrich Sch\""utze",Identifying Necessary Elements for BERT's Multilinguality,EMNLP2020 CRV,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It has been shown that multilingual BERT (mBERT) yields high quality
multilingual representations and enables effective zero-shot transfer. This is
surprising given that mBERT does not use any crosslingual signal during
training. While recent literature has studied this phenomenon, the reasons for
the multilinguality are still somewhat obscure. We aim to identify
architectural properties of BERT and linguistic properties of languages that
are necessary for BERT to become multilingual. To allow for fast
experimentation we propose an efficient setup with small BERT models trained on
a mix of synthetic and natural data. Overall, we identify four architectural
and two linguistic elements that influence multilinguality. Based on our
insights, we experiment with a multilingual pretraining setup that modifies the
masking strategy using VecMap, i.e., unsupervised embedding alignment.
Experiments on XNLI with three languages indicate that our findings transfer
from our small setup to larger scale settings.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 14:27:14 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 13:26:22 GMT'}]",2020-10-06,"[['Dufter', 'Philipp', ''], ['Schütze', 'Hinrich', '']]"
1285704,2005.05909,John Morris,"John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin and
  Yanjun Qi","TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and
  Adversarial Training in NLP","6 pages. More details are shared at
  https://github.com/QData/TextAttack",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While there has been substantial research using adversarial attacks to
analyze NLP models, each attack is implemented in its own code repository. It
remains challenging to develop NLP attacks and utilize them to improve model
performance. This paper introduces TextAttack, a Python framework for
adversarial attacks, data augmentation, and adversarial training in NLP.
TextAttack builds attacks from four components: a goal function, a set of
constraints, a transformation, and a search method. TextAttack's modular design
enables researchers to easily construct attacks from combinations of novel and
existing components. TextAttack provides implementations of 16 adversarial
attacks from the literature and supports a variety of models and datasets,
including BERT and other transformers, and all GLUE tasks. TextAttack also
includes data augmentation and adversarial training modules for using
components of adversarial attacks to improve model accuracy and robustness.
TextAttack is democratizing NLP: anyone can try data augmentation and
adversarial training on any model or dataset, with just a few lines of code.
Code and tutorials are available at https://github.com/QData/TextAttack.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 21:33:35 GMT'}, {'version': 'v2', 'created': 'Wed, 13 May 2020 17:37:21 GMT'}, {'version': 'v3', 'created': 'Wed, 22 Jul 2020 19:33:10 GMT'}, {'version': 'v4', 'created': 'Mon, 5 Oct 2020 00:10:24 GMT'}]",2020-10-06,"[['Morris', 'John X.', ''], ['Lifland', 'Eli', ''], ['Yoo', 'Jin Yong', ''], ['Grigsby', 'Jake', ''], ['Jin', 'Di', ''], ['Qi', 'Yanjun', '']]"
1358470,2010.02140,Jan Deriu,"Jan Deriu and Don Tuggener and Pius von D\""aniken and Jon Ander Campos
  and Alvaro Rodrigo and Thiziri Belkacem and Aitor Soroa and Eneko Agirre and
  Mark Cieliebak","Spot The Bot: A Robust and Efficient Framework for the Evaluation of
  Conversational Dialogue Systems",,,,,cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The lack of time-efficient and reliable evaluation methods hamper the
development of conversational dialogue systems (chatbots). Evaluations
requiring humans to converse with chatbots are time and cost-intensive, put
high cognitive demands on the human judges, and yield low-quality results. In
this work, we introduce \emph{Spot The Bot}, a cost-efficient and robust
evaluation framework that replaces human-bot conversations with conversations
between bots. Human judges then only annotate for each entity in a conversation
whether they think it is human or not (assuming there are humans participants
in these conversations). These annotations then allow us to rank chatbots
regarding their ability to mimic the conversational behavior of humans. Since
we expect that all bots are eventually recognized as such, we incorporate a
metric that measures which chatbot can uphold human-like behavior the longest,
i.e., \emph{Survival Analysis}. This metric has the ability to correlate a
bot's performance to certain of its characteristics (e.g., \ fluency or
sensibleness), yielding interpretable results. The comparably low cost of our
framework allows for frequent evaluations of chatbots during their evaluation
cycle. We empirically validate our claims by applying \emph{Spot The Bot} to
three domains, evaluating several state-of-the-art chatbots, and drawing
comparisons to related work. The framework is released as a ready-to-use tool.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 16:37:52 GMT'}]",2020-10-06,"[['Deriu', 'Jan', ''], ['Tuggener', 'Don', ''], ['von Däniken', 'Pius', ''], ['Campos', 'Jon Ander', ''], ['Rodrigo', 'Alvaro', ''], ['Belkacem', 'Thiziri', ''], ['Soroa', 'Aitor', ''], ['Agirre', 'Eneko', ''], ['Cieliebak', 'Mark', '']]"
1352926,2009.11423,Jacob Andreas,"Semantic Machines, Jacob Andreas, John Bufe, David Burkett, Charles
  Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner,
  Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill,
  Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo
  Lanman, Percy Liang, Christopher H Lin, Ilya Lintsbakh, Andy McGovern,
  Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth,
  Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon
  Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela
  Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, Alexander Zotov",Task-Oriented Dialogue as Dataflow Synthesis,TACL 2020,,10.1162/tacl_a_00333,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We describe an approach to task-oriented dialogue in which dialogue state is
represented as a dataflow graph. A dialogue agent maps each user utterance to a
program that extends this graph. Programs include metacomputation operators for
reference and revision that reuse dataflow fragments from previous turns. Our
graph-based state enables the expression and manipulation of complex user
intents, and explicit metacomputation makes these intents easier for learned
models to predict. We introduce a new dataset, SMCalFlow, featuring complex
dialogues about events, weather, places, and people. Experiments show that
dataflow graphs and metacomputation substantially improve representability and
predictability in these natural dialogues. Additional experiments on the
MultiWOZ dataset show that our dataflow representation enables an otherwise
off-the-shelf sequence-to-sequence model to match the best existing
task-specific state tracking model. The SMCalFlow dataset and code for
replicating experiments are available at
https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 00:35:26 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Oct 2020 20:01:18 GMT'}]",2020-10-06,"[['Machines', 'Semantic', ''], ['Andreas', 'Jacob', ''], ['Bufe', 'John', ''], ['Burkett', 'David', ''], ['Chen', 'Charles', ''], ['Clausman', 'Josh', ''], ['Crawford', 'Jean', ''], ['Crim', 'Kate', ''], ['DeLoach', 'Jordan', ''], ['Dorner', 'Leah', ''], ['Eisner', 'Jason', ''], ['Fang', 'Hao', ''], ['Guo', 'Alan', ''], ['Hall', 'David', ''], ['Hayes', 'Kristin', ''], ['Hill', 'Kellie', ''], ['Ho', 'Diana', ''], ['Iwaszuk', 'Wendy', ''], ['Jha', 'Smriti', ''], ['Klein', 'Dan', ''], ['Krishnamurthy', 'Jayant', ''], ['Lanman', 'Theo', ''], ['Liang', 'Percy', ''], ['Lin', 'Christopher H', ''], ['Lintsbakh', 'Ilya', ''], ['McGovern', 'Andy', ''], ['Nisnevich', 'Aleksandr', ''], ['Pauls', 'Adam', ''], ['Petters', 'Dmitrij', ''], ['Read', 'Brent', ''], ['Roth', 'Dan', ''], ['Roy', 'Subhro', ''], ['Rusak', 'Jesse', ''], ['Short', 'Beth', ''], ['Slomin', 'Div', ''], ['Snyder', 'Ben', ''], ['Striplin', 'Stephon', ''], ['Su', 'Yu', ''], ['Tellman', 'Zachary', ''], ['Thomson', 'Sam', ''], ['Vorobev', 'Andrei', ''], ['Witoszko', 'Izabela', ''], ['Wolfe', 'Jason', ''], ['Wray', 'Abby', ''], ['Zhang', 'Yuchen', ''], ['Zotov', 'Alexander', '']]"
1358340,2010.02010,Griffin Adams,"Griffin Adams, Mert Ketenci, Adler Perotte, Noemie Elhadad","Zero-Shot Clinical Acronym Expansion with a Hierarchical Metadata-Based
  Latent Variable Model",31 pages,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce Latent Meaning Cells, a deep latent variable model which learns
contextualized representations of words by combining local lexical context and
metadata. Metadata can refer to granular context, such as section type, or to
more global context, such as unique document ids. Reliance on metadata for
contextualized representation learning is apropos in the clinical domain where
text is semi-structured and expresses high variation in topics. We evaluate the
LMC model on the task of clinical acronym expansion across three datasets. The
LMC significantly outperforms a diverse set of baselines at a fraction of the
pre-training cost and learns clinically coherent representations.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 00:28:30 GMT'}]",2020-10-06,"[['Adams', 'Griffin', ''], ['Ketenci', 'Mert', ''], ['Perotte', 'Adler', ''], ['Elhadad', 'Noemie', '']]"
1280447,2005.00652,Bhargavi Paranjape,"Bhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi,
  Luke Zettlemoyer","An Information Bottleneck Approach for Controlling Conciseness in
  Rationale Extraction",EMNLP 2020 accepted paper,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Decisions of complex language understanding models can be rationalized by
limiting their inputs to a relevant subsequence of the original text. A
rationale should be as concise as possible without significantly degrading task
performance, but this balance can be difficult to achieve in practice. In this
paper, we show that it is possible to better manage this trade-off by
optimizing a bound on the Information Bottleneck (IB) objective. Our fully
unsupervised approach jointly learns an explainer that predicts sparse binary
masks over sentences, and an end-task predictor that considers only the
extracted rationale. Using IB, we derive a learning objective that allows
direct control of mask sparsity levels through a tunable sparse prior.
Experiments on ERASER benchmark tasks demonstrate significant gains over
norm-minimization techniques for both task performance and agreement with human
rationales. Furthermore, we find that in the semi-supervised setting, a modest
amount of gold rationales (25% of training examples) closes the gap with a
model that uses the full input.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 23:26:41 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 16:57:54 GMT'}]",2020-10-06,"[['Paranjape', 'Bhargavi', ''], ['Joshi', 'Mandar', ''], ['Thickstun', 'John', ''], ['Hajishirzi', 'Hannaneh', ''], ['Zettlemoyer', 'Luke', '']]"
1358208,2010.01878,Mathieu Rita,"Mathieu Rita, Rahma Chaabouni, Emmanuel Dupoux","""LazImpa"": Lazy and Impatient neural agents learn to communicate
  efficiently",Accepted to CoNLL 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous work has shown that artificial neural agents naturally develop
surprisingly non-efficient codes. This is illustrated by the fact that in a
referential game involving a speaker and a listener neural networks optimizing
accurate transmission over a discrete channel, the emergent messages fail to
achieve an optimal length. Furthermore, frequent messages tend to be longer
than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA)
observed in all natural languages. Here, we show that near-optimal and
ZLA-compatible messages can emerge, but only if both the speaker and the
listener are modified. We hence introduce a new communication system,
""LazImpa"", where the speaker is made increasingly lazy, i.e. avoids long
messages, and the listener impatient, i.e.,~seeks to guess the intended content
as soon as possible.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 09:25:53 GMT'}]",2020-10-06,"[['Rita', 'Mathieu', ''], ['Chaabouni', 'Rahma', ''], ['Dupoux', 'Emmanuel', '']]"
1287594,2005.07799,Dan Lim,"Dan Lim, Won Jang, Gyeonghwan O, Heayoung Park, Bongwan Kim, Jaesam
  Yoon","JDI-T: Jointly trained Duration Informed Transformer for Text-To-Speech
  without Explicit Alignment",Accepted for publication in Interspeech 2020,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose Jointly trained Duration Informed Transformer (JDI-T), a
feed-forward Transformer with a duration predictor jointly trained without
explicit alignments in order to generate an acoustic feature sequence from an
input text. In this work, inspired by the recent success of the duration
informed networks such as FastSpeech and DurIAN, we further simplify its
sequential, two-stage training pipeline to a single-stage training.
Specifically, we extract the phoneme duration from the autoregressive
Transformer on the fly during the joint training instead of pretraining the
autoregressive model and using it as a phoneme duration extractor. To our best
knowledge, it is the first implementation to jointly train the feed-forward
Transformer without relying on a pre-trained phoneme duration extractor in a
single training pipeline. We evaluate the effectiveness of the proposed model
on the publicly available Korean Single speaker Speech (KSS) dataset compared
to the baseline text-to-speech (TTS) models trained by ESPnet-TTS.
","[{'version': 'v1', 'created': 'Fri, 15 May 2020 22:06:13 GMT'}, {'version': 'v2', 'created': 'Fri, 19 Jun 2020 01:42:11 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Oct 2020 02:48:58 GMT'}]",2020-10-06,"[['Lim', 'Dan', ''], ['Jang', 'Won', ''], ['O', 'Gyeonghwan', ''], ['Park', 'Heayoung', ''], ['Kim', 'Bongwan', ''], ['Yoon', 'Jaesam', '']]"
1358227,2010.01897,Urszula Wali\'nska,"Piotr Janiszewski, Mateusz Skiba, Urszula Wali\'nska","PUM at SemEval-2020 Task 12: Aggregation of Transformer-based models'
  features for offensive language recognition","7 pages, 0 figures. Proceedings of the International Workshop on
  Semantic Evaluation (SemEval-2020)",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we describe the PUM team's entry to the SemEval-2020 Task 12.
Creating our solution involved leveraging two well-known pretrained models used
in natural language processing: BERT and XLNet, which achieve state-of-the-art
results in multiple NLP tasks. The models were fine-tuned for each subtask
separately and features taken from their hidden layers were combined and fed
into a fully connected neural network. The model using aggregated Transformer
features can serve as a powerful tool for offensive language identification
problem. Our team was ranked 7th out of 40 in Sub-task C - Offense target
identification with 64.727% macro F1-score and 64th out of 85 in Sub-task A -
Offensive language identification (89.726% F1-score).
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 10:25:29 GMT'}]",2020-10-06,"[['Janiszewski', 'Piotr', ''], ['Skiba', 'Mateusz', ''], ['Walińska', 'Urszula', '']]"
1286414,2005.06619,Ramit Debnath,Ramit Debnath and Ronita Bardhan,"India nudges to contain COVID-19 pandemic: a reactive public policy
  analysis using machine-learning based topic modelling",25 pages with 10 figures and 9 tables,PLoS ONE 15(9): e0238972 (2020),10.1371/journal.pone.0238972,,cs.CY cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  India locked down 1.3 billion people on March 25, 2020 in the wake of
COVID-19 pandemic. The economic cost of it was estimated at USD 98 billion,
while the social costs are still unknown. This study investigated how
government formed reactive policies to fight coronavirus across its policy
sectors. Primary data was collected from the Press Information Bureau (PIB) in
the form press releases of government plans, policies, programme initiatives
and achievements. A text corpus of 260,852 words was created from 396 documents
from the PIB. An unsupervised machine-based topic modelling using Latent
Dirichlet Allocation (LDA) algorithm was performed on the text corpus. It was
done to extract high probability topics in the policy sectors. The
interpretation of the extracted topics was made through a nudge theoretic lens
to derive the critical policy heuristics of the government. Results showed that
most interventions were targeted to generate endogenous nudge by using external
triggers. Notably, the nudges from the Prime Minister of India was critical in
creating herd effect on lockdown and social distancing norms across the nation.
A similar effect was also observed around the public health (e.g., masks in
public spaces; Yoga and Ayurveda for immunity), transport (e.g., old trains
converted to isolation wards), micro, small and medium enterprises (e.g., rapid
production of PPE and masks), science and technology sector (e.g., diagnostic
kits, robots and nano-technology), home affairs (e.g., surveillance and
lockdown), urban (e.g. drones, GIS-tools) and education (e.g., online
learning). A conclusion was drawn on leveraging these heuristics are crucial
for lockdown easement planning.
","[{'version': 'v1', 'created': 'Thu, 14 May 2020 04:14:09 GMT'}, {'version': 'v2', 'created': 'Sat, 3 Oct 2020 08:09:16 GMT'}]",2020-10-06,"[['Debnath', 'Ramit', ''], ['Bardhan', 'Ronita', '']]"
1170202,1909.00164,Ying Luo,"Ying Luo, Hai Zhao, Junlang Zhan",Named Entity Recognition Only from Word Embeddings,Accepted by EMNLP2020,,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural network models have helped named entity (NE) recognition achieve
amazing performance without handcrafting features. However, existing systems
require large amounts of human annotated training data. Efforts have been made
to replace human annotations with external knowledge (e.g., NE dictionary,
part-of-speech tags), while it is another challenge to obtain such effective
resources. In this work, we propose a fully unsupervised NE recognition model
which only needs to take informative clues from pre-trained word embeddings. We
first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture
Model on word embeddings for entity span detection and type prediction, and
then further design an instance selector based on reinforcement learning to
distinguish positive sentences from noisy sentences and refine these
coarse-grained annotations through neural networks. Extensive experiments on
CoNLL benchmark datasets demonstrate that our proposed light NE recognition
model achieves remarkable performance without using any annotated lexicon or
corpus.
","[{'version': 'v1', 'created': 'Sat, 31 Aug 2019 08:22:13 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 15:22:32 GMT'}]",2020-10-06,"[['Luo', 'Ying', ''], ['Zhao', 'Hai', ''], ['Zhan', 'Junlang', '']]"
1233289,2001.07876,Xingbo Wang,"Xingbo Wang, Haipeng Zeng, Yong Wang, Aoyu Wu, Zhida Sun, Xiaojuan Ma,
  Huamin Qu","VoiceCoach: Interactive Evidence-based Training for Voice Modulation
  Skills in Public Speaking",Accepted by CHI '20,"Proceedings of the 2020 CHI Conference on Human Factors in
  Computing Systems",10.1145/3313831.3376726,,cs.HC cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The modulation of voice properties, such as pitch, volume, and speed, is
crucial for delivering a successful public speech. However, it is challenging
to master different voice modulation skills. Though many guidelines are
available, they are often not practical enough to be applied in different
public speaking situations, especially for novice speakers. We present
VoiceCoach, an interactive evidence-based approach to facilitate the effective
training of voice modulation skills. Specifically, we have analyzed the voice
modulation skills from 2623 high-quality speeches (i.e., TED Talks) and use
them as the benchmark dataset. Given a voice input, VoiceCoach automatically
recommends good voice modulation examples from the dataset based on the
similarity of both sentence structures and voice modulation skills. Immediate
and quantitative visual feedback is provided to guide further improvement. The
expert interviews and the user study provide support for the effectiveness and
usability of VoiceCoach.
","[{'version': 'v1', 'created': 'Wed, 22 Jan 2020 04:52:06 GMT'}]",2020-10-06,"[['Wang', 'Xingbo', ''], ['Zeng', 'Haipeng', ''], ['Wang', 'Yong', ''], ['Wu', 'Aoyu', ''], ['Sun', 'Zhida', ''], ['Ma', 'Xiaojuan', ''], ['Qu', 'Huamin', '']]"
1354009,2009.12506,Sashank Santhanam,"Sashank Santhanam, Zhuo Cheng, Brodie Mather, Bonnie Dorr, Archna
  Bhatia, Bryanna Hebenstreit, Alan Zemel, Adam Dalton, Tomek Strzalkowski and
  Samira Shaikh",Learning to Plan and Realize Separately for Open-Ended Dialogue Systems,Accepted at EMNLP 2020 (Findings),,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Achieving true human-like ability to conduct a conversation remains an
elusive goal for open-ended dialogue systems. We posit this is because extant
approaches towards natural language generation (NLG) are typically construed as
end-to-end architectures that do not adequately model human generation
processes. To investigate, we decouple generation into two separate phases:
planning and realization. In the planning phase, we train two planners to
generate plans for response utterances. The realization phase uses response
plans to produce an appropriate response. Through rigorous evaluations, both
automated and human, we demonstrate that decoupling the process into planning
and realization performs better than an end-to-end approach.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 02:31:42 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 23:57:43 GMT'}]",2020-10-06,"[['Santhanam', 'Sashank', ''], ['Cheng', 'Zhuo', ''], ['Mather', 'Brodie', ''], ['Dorr', 'Bonnie', ''], ['Bhatia', 'Archna', ''], ['Hebenstreit', 'Bryanna', ''], ['Zemel', 'Alan', ''], ['Dalton', 'Adam', ''], ['Strzalkowski', 'Tomek', ''], ['Shaikh', 'Samira', '']]"
1358383,2010.02053,Federico L\'opez,"Federico L\'opez, Michael Strube","A Fully Hyperbolic Neural Model for Hierarchical Multi-Class
  Classification","16 pages, accepted at Findings of EMNLP2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Label inventories for fine-grained entity typing have grown in size and
complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic
spaces offer a mathematically appealing approach for learning hierarchical
representations of symbolic data. However, it is not clear how to integrate
hyperbolic components into downstream tasks. This is the first work that
proposes a fully hyperbolic model for multi-class multi-label classification,
which performs all operations in hyperbolic space. We evaluate the proposed
model on two challenging datasets and compare to different baselines that
operate under Euclidean assumptions. Our hyperbolic model infers the latent
hierarchy from the class distribution, captures implicit hyponymic relations in
the inventory, and shows performance on par with state-of-the-art methods on
fine-grained classification with remarkable reduction of the parameter size. A
thorough analysis sheds light on the impact of each component in the final
prediction and showcases its ease of integration with Euclidean layers.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 14:42:56 GMT'}]",2020-10-06,"[['López', 'Federico', ''], ['Strube', 'Michael', '']]"
1358199,2010.01869,Alessio Miaschi,"Alessio Miaschi, Dominique Brunato, Felice Dell'Orletta, Giulia
  Venturi",Linguistic Profiling of a Neural Language Model,Accepted to COLING 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we investigate the linguistic knowledge learned by a Neural
Language Model (NLM) before and after a fine-tuning process and how this
knowledge affects its predictions during several classification problems. We
use a wide set of probing tasks, each of which corresponds to a distinct
sentence-level feature extracted from different levels of linguistic
annotation. We show that BERT is able to encode a wide range of linguistic
characteristics, but it tends to lose this information when trained on specific
downstream tasks. We also find that BERT's capacity to encode different kind of
linguistic properties has a positive influence on its predictions: the more it
stores readable linguistic information, the higher will be its capacity of
predicting the correct label.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 09:09:01 GMT'}]",2020-10-06,"[['Miaschi', 'Alessio', ''], ['Brunato', 'Dominique', ''], [""Dell'Orletta"", 'Felice', ''], ['Venturi', 'Giulia', '']]"
1358387,2010.02057,No\'e Tits,Jean-Benoit Delbrouck and No\'e Tits and St\'ephane Dupont,"Modulated Fusion using Transformer for Linguistic-Acoustic Emotion
  Recognition",EMNLP 2020 workshop: NLP Beyond Text (NLPBT),,,,cs.CL cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper aims to bring a new lightweight yet powerful solution for the task
of Emotion Recognition and Sentiment Analysis. Our motivation is to propose two
architectures based on Transformers and modulation that combine the linguistic
and acoustic inputs from a wide range of datasets to challenge, and sometimes
surpass, the state-of-the-art in the field. To demonstrate the efficiency of
our models, we carefully evaluate their performances on the IEMOCAP, MOSI,
MOSEI and MELD dataset. The experiments can be directly replicated and the code
is fully open for future researches.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 14:46:20 GMT'}]",2020-10-06,"[['Delbrouck', 'Jean-Benoit', ''], ['Tits', 'Noé', ''], ['Dupont', 'Stéphane', '']]"
1358480,2010.02150,Saurabh Gupta,"Saurabh Gupta, Huy H. Nguyen, Junichi Yamagishi and Isao Echizen","Viable Threat on News Reading: Generating Biased News Using Natural
  Language Models","11 pages, 4 figures, 6 tables, Accepted at NLP+CSS Workshop at EMNLP
  2020",,,,cs.CL cs.CY cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in natural language generation has raised serious
concerns. High-performance language models are widely used for language
generation tasks because they are able to produce fluent and meaningful
sentences. These models are already being used to create fake news. They can
also be exploited to generate biased news, which can then be used to attack
news aggregators to change their reader's behavior and influence their bias. In
this paper, we use a threat model to demonstrate that the publicly available
language models can reliably generate biased news content based on an input
original news. We also show that a large number of high-quality biased news
articles can be generated using controllable text generation. A subjective
evaluation with 80 participants demonstrated that the generated biased news is
generally fluent, and a bias evaluation with 24 participants demonstrated that
the bias (left or right) is usually evident in the generated articles and can
be easily identified.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 16:55:39 GMT'}]",2020-10-06,"[['Gupta', 'Saurabh', ''], ['Nguyen', 'Huy H.', ''], ['Yamagishi', 'Junichi', ''], ['Echizen', 'Isao', '']]"
1359142,2010.02812,Lucas Torroba Hennigen,"Lucas Torroba Hennigen, Adina Williams, Ryan Cotterell",Intrinsic Probing through Dimension Selection,To appear EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most modern NLP systems make use of pre-trained contextual representations
that attain astonishingly high performance on a variety of tasks. Such high
performance should not be possible unless some form of linguistic structure
inheres in these representations, and a wealth of research has sprung up on
probing for it. In this paper, we draw a distinction between intrinsic probing,
which examines how linguistic information is structured within a
representation, and the extrinsic probing popular in prior work, which only
argues for the presence of such information by showing that it can be
successfully extracted. To enable intrinsic probing, we propose a novel
framework based on a decomposable multivariate Gaussian probe that allows us to
determine whether the linguistic information in word embeddings is dispersed or
focal. We then probe fastText and BERT for various morphosyntactic attributes
across 36 languages. We find that most attributes are reliably encoded by only
a few neurons, with fastText concentrating its linguistic structure more than
BERT.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 15:21:08 GMT'}]",2020-10-07,"[['Hennigen', 'Lucas Torroba', ''], ['Williams', 'Adina', ''], ['Cotterell', 'Ryan', '']]"
1359145,2010.02815,Valentina Pyatkin,"Valentina Pyatkin, Ayal Klein, Reut Tsarfaty, Ido Dagan","QADiscourse -- Discourse Relations as QA Pairs: Representation,
  Crowdsourcing and Baselines",To appear at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Discourse relations describe how two propositions relate to one another, and
identifying them automatically is an integral part of natural language
understanding. However, annotating discourse relations typically requires
expert annotators. Recently, different semantic aspects of a sentence have been
represented and crowd-sourced via question-and-answer (QA) pairs. This paper
proposes a novel representation of discourse relations as QA pairs, which in
turn allows us to crowd-source wide-coverage data annotated with discourse
relations, via an intuitively appealing interface for composing such questions
and answers. Based on our proposed representation, we collect a novel and
wide-coverage QADiscourse dataset, and present baseline algorithms for
predicting QADiscourse relations.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 15:25:15 GMT'}]",2020-10-07,"[['Pyatkin', 'Valentina', ''], ['Klein', 'Ayal', ''], ['Tsarfaty', 'Reut', ''], ['Dagan', 'Ido', '']]"
1358741,2010.02411,Abd AlRahman AlMomani,Abd AlRahman AlMomani and Erik Bollt,"ERFit: Entropic Regression Fit Matlab Package, for Data-Driven System
  Identification of Underlying Dynamic Equations","7 pages, 2 figures",,,,math.DS cs.CL stat.CO stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Data-driven sparse system identification becomes the general framework for a
wide range of problems in science and engineering. It is a problem of growing
importance in applied machine learning and artificial intelligence algorithms.
In this work, we developed the Entropic Regression Software Package (ERFit), a
MATLAB package for sparse system identification using the entropic regression
method. The code requires minimal supervision, with a wide range of options
that make it adapt easily to different problems in science and engineering. The
ERFit is available at https://github.com/almomaa/ERFit-Package
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 01:07:15 GMT'}]",2020-10-07,"[['AlMomani', 'Abd AlRahman', ''], ['Bollt', 'Erik', '']]"
1358743,2010.02413,Belinda Z. Li,"Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad and Wen-tau
  Yih",Efficient One-Pass End-to-End Entity Linking for Questions,"9 pages, EMNLP 2020",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present ELQ, a fast end-to-end entity linking model for questions, which
uses a biencoder to jointly perform mention detection and linking in one pass.
Evaluated on WebQSP and GraphQuestions with extended annotations that cover
multiple entities per question, ELQ outperforms the previous state of the art
by a large margin of +12.7% and +19.6% F1, respectively. With a very fast
inference time (1.57 examples/s on a single CPU), ELQ can be useful for
downstream question answering systems. In a proof-of-concept experiment, we
demonstrate that using ELQ significantly improves the downstream QA performance
of GraphRetriever (arXiv:1911.03868). Code and data available at
https://github.com/facebookresearch/BLINK/tree/master/elq
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 01:14:10 GMT'}]",2020-10-07,"[['Li', 'Belinda Z.', ''], ['Min', 'Sewon', ''], ['Iyer', 'Srinivasan', ''], ['Mehdad', 'Yashar', ''], ['Yih', 'Wen-tau', '']]"
1348876,2009.07373,Rujun Han,"Rujun Han, Yichao Zhou, Nanyun Peng","Domain Knowledge Empowered Structured Neural Net for End-to-End Event
  Temporal Relation Extraction",Appear in EMNLP'20,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Extracting event temporal relations is a critical task for information
extraction and plays an important role in natural language understanding. Prior
systems leverage deep learning and pre-trained language models to improve the
performance of the task. However, these systems often suffer from two
short-comings: 1) when performing maximum a posteriori (MAP) inference based on
neural models, previous systems only used structured knowledge that are assumed
to be absolutely correct, i.e., hard constraints; 2) biased predictions on
dominant temporal relations when training with a limited amount of data. To
address these issues, we propose a framework that enhances deep neural network
with distributional constraints constructed by probabilistic domain knowledge.
We solve the constrained inference problem via Lagrangian Relaxation and apply
it on end-to-end event temporal relation extraction tasks. Experimental results
show our framework is able to improve the baseline neural network models with
strong statistical significance on two widely used datasets in news and
clinical domains.
","[{'version': 'v1', 'created': 'Tue, 15 Sep 2020 22:20:27 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 16:58:49 GMT'}]",2020-10-07,"[['Han', 'Rujun', ''], ['Zhou', 'Yichao', ''], ['Peng', 'Nanyun', '']]"
1202225,1911.03631,Yuwei Fang,"Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, Jingjing
  Liu",Hierarchical Graph Network for Multi-hop Question Answering,Accepted to EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present Hierarchical Graph Network (HGN) for multi-hop
question answering. To aggregate clues from scattered texts across multiple
paragraphs, a hierarchical graph is created by constructing nodes on different
levels of granularity (questions, paragraphs, sentences, entities), the
representations of which are initialized with pre-trained contextual encoders.
Given this hierarchical graph, the initial node representations are updated
through graph propagation, and multi-hop reasoning is performed via traversing
through the graph edges for each subsequent sub-task (e.g., paragraph
selection, supporting facts extraction, answer prediction). By weaving
heterogeneous nodes into an integral unified graph, this hierarchical
differentiation of node granularity enables HGN to support different question
answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark
demonstrate that the proposed model achieves new state of the art,
outperforming existing multi-hop QA approaches.
","[{'version': 'v1', 'created': 'Sat, 9 Nov 2019 07:18:47 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Apr 2020 19:40:03 GMT'}, {'version': 'v3', 'created': 'Sun, 27 Sep 2020 05:00:08 GMT'}, {'version': 'v4', 'created': 'Tue, 6 Oct 2020 08:17:58 GMT'}]",2020-10-07,"[['Fang', 'Yuwei', ''], ['Sun', 'Siqi', ''], ['Gan', 'Zhe', ''], ['Pillai', 'Rohit', ''], ['Wang', 'Shuohang', ''], ['Liu', 'Jingjing', '']]"
1358788,2010.02458,Zhao Wang,Zhao Wang and Aron Culotta,Identifying Spurious Correlations for Robust Text Classification,Findings of EMNLP-2020,Findings of EMNLP-2020,,,cs.LG cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The predictions of text classifiers are often driven by spurious correlations
-- e.g., the term `Spielberg' correlates with positively reviewed movies, even
though the term itself does not semantically convey a positive sentiment. In
this paper, we propose a method to distinguish spurious and genuine
correlations in text classification. We treat this as a supervised
classification problem, using features derived from treatment effect estimators
to distinguish spurious correlations from ""genuine"" ones. Due to the generic
nature of these features and their small dimensionality, we find that the
approach works well even with limited training examples, and that it is
possible to transport the word classifier to new domains. Experiments on four
datasets (sentiment classification and toxicity detection) suggest that using
this approach to inform feature selection also leads to more robust
classification, as measured by improved worst-case accuracy on the samples
affected by spurious correlations.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 03:49:22 GMT'}]",2020-10-07,"[['Wang', 'Zhao', ''], ['Culotta', 'Aron', '']]"
1358773,2010.02443,Yue Dong,"Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung and
  Jingjing Liu",Multi-Fact Correction in Abstractive Text Summarization,"12 pages, accepted at EMNLP2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained neural abstractive summarization systems have dominated
extractive strategies on news summarization performance, at least in terms of
ROUGE. However, system-generated abstractive summaries often face the pitfall
of factual inconsistency: generating incorrect facts with respect to the source
text. To address this challenge, we propose Span-Fact, a suite of two factual
correction models that leverages knowledge learned from question answering
models to make corrections in system-generated summaries via span selection.
Our models employ single or multi-masking strategies to either iteratively or
auto-regressively replace entities in order to ensure semantic consistency
w.r.t. the source text, while retaining the syntactic structure of summaries
generated by abstractive summarization models. Experiments show that our models
significantly boost the factual consistency of system-generated summaries
without sacrificing summary quality in terms of both automatic metrics and
human evaluation.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 02:51:02 GMT'}]",2020-10-07,"[['Dong', 'Yue', ''], ['Wang', 'Shuohang', ''], ['Gan', 'Zhe', ''], ['Cheng', 'Yu', ''], ['Cheung', 'Jackie Chi Kit', ''], ['Liu', 'Jingjing', '']]"
1358778,2010.02448,Huayang Li,"Huayang Li, Lemao Liu, Guoping Huang, Shuming Shi","On the Branching Bias of Syntax Extracted from Pre-trained Language
  Models",EMNLP 2020 findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many efforts have been devoted to extracting constituency trees from
pre-trained language models, often proceeding in two stages: feature definition
and parsing. However, this kind of methods may suffer from the branching bias
issue, which will inflate the performances on languages with the same branch it
biases to. In this work, we propose quantitatively measuring the branching bias
by comparing the performance gap on a language and its reversed language, which
is agnostic to both language models and extracting methods. Furthermore, we
analyze the impacts of three factors on the branching bias, namely parsing
algorithms, feature definitions, and language models. Experiments show that
several existing works exhibit branching biases, and some implementations of
these three factors can introduce the branching bias.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 03:09:14 GMT'}]",2020-10-07,"[['Li', 'Huayang', ''], ['Liu', 'Lemao', ''], ['Huang', 'Guoping', ''], ['Shi', 'Shuming', '']]"
1358737,2010.02407,Vipul Raheja,Vipul Raheja and Dimitrios Alikaniotis,Adversarial Grammatical Error Correction,"13 Pages, EMNLP 2020",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Recent works in Grammatical Error Correction (GEC) have leveraged the
progress in Neural Machine Translation (NMT), to learn rewrites from parallel
corpora of grammatically incorrect and corrected sentences, achieving
state-of-the-art results. At the same time, Generative Adversarial Networks
(GANs) have been successful in generating realistic texts across many different
tasks by learning to directly minimize the difference between human-generated
and synthetic text. In this work, we present an adversarial learning approach
to GEC, using the generator-discriminator framework. The generator is a
Transformer model, trained to produce grammatically correct sentences given
grammatically incorrect ones. The discriminator is a sentence-pair
classification model, trained to judge a given pair of grammatically
incorrect-correct sentences on the quality of grammatical correction. We
pre-train both the discriminator and the generator on parallel texts and then
fine-tune them further using a policy gradient method that assigns high rewards
to sentences which could be true corrections of the grammatically incorrect
text. Experimental results on FCE, CoNLL-14, and BEA-19 datasets show that
Adversarial-GEC can achieve competitive GEC quality compared to NMT-based
baselines.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 00:31:33 GMT'}]",2020-10-07,"[['Raheja', 'Vipul', ''], ['Alikaniotis', 'Dimitrios', '']]"
1358796,2010.02466,Zhao Wang,"Zhao Wang, Jennifer Cutler, Aron Culotta","Are Words Commensurate with Actions? Quantifying Commitment to a Cause
  from Online Public Messaging","In IEEE International Conference on Data Mining (ICDM) Workshop on
  Data science for Human Performance in Social Networks, 2017",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Public entities such as companies and politicians increasingly use online
social networks to communicate directly with their constituencies. Often, this
public messaging is aimed at aligning the entity with a particular cause or
issue, such as the environment or public health. However, as a consumer or
voter, it can be difficult to assess an entity's true commitment to a cause
based on public messaging. In this paper, we present a text classification
approach to categorize a message according to its commitment level toward a
cause. We then compare the volume of such messages with external ratings based
on entities' actions (e.g., a politician's voting record with respect to the
environment or a company's rating from environmental non-profits). We find that
by distinguishing between low- and high- level commitment messages, we can more
reliably identify truly committed entities. Furthermore, by measuring the
discrepancy between classified messages and external ratings, we can identify
entities whose public messaging does not align with their actions, thereby
providing a methodology to identify potentially ""inauthentic"" messaging
campaigns.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 04:12:28 GMT'}]",2020-10-07,"[['Wang', 'Zhao', ''], ['Cutler', 'Jennifer', ''], ['Culotta', 'Aron', '']]"
1358797,2010.02467,Jianmo Ni,"Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley","Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on
  Chest X-rays","7 pages, 2 figures, to be published in Findings of EMNLP 2020",,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic medical image report generation has drawn growing attention due to
its potential to alleviate radiologists' workload. Existing work on report
generation often trains encoder-decoder networks to generate complete reports.
However, such models are affected by data bias (e.g.~label imbalance) and face
common issues inherent in text generation models (e.g.~repetition). In this
work, we focus on reporting abnormal findings on radiology images; instead of
training on complete radiology reports, we propose a method to identify
abnormal findings from the reports in addition to grouping them with
unsupervised clustering and minimal rules. We formulate the task as cross-modal
retrieval and propose Conditional Visual-Semantic Embeddings to align images
and fine-grained abnormal findings in a joint embedding space. We demonstrate
that our method is able to retrieve abnormal findings and outperforms existing
generation models on both clinical correctness and text generation metrics.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 04:18:18 GMT'}]",2020-10-07,"[['Ni', 'Jianmo', ''], ['Hsu', 'Chun-Nan', ''], ['Gentili', 'Amilcare', ''], ['McAuley', 'Julian', '']]"
1358803,2010.02473,Zhirui Zhang,"Hao-Ran Wei, Zhirui Zhang, Boxing Chen, Weihua Luo",Iterative Domain-Repaired Back-Translation,EMNLP 2020 long paper,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we focus on the domain-specific translation with low
resources, where in-domain parallel corpora are scarce or nonexistent. One
common and effective strategy for this case is exploiting in-domain monolingual
data with the back-translation method. However, the synthetic parallel data is
very noisy because they are generated by imperfect out-of-domain systems,
resulting in the poor performance of domain adaptation. To address this issue,
we propose a novel iterative domain-repaired back-translation framework, which
introduces the Domain-Repair (DR) model to refine translations in synthetic
bilingual data. To this end, we construct corresponding data for the DR model
training by round-trip translating the monolingual sentences, and then design
the unified training framework to optimize paired DR and NMT models jointly.
Experiments on adapting NMT models between specific domains and from the
general domain to specific domains demonstrate the effectiveness of our
proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over
unadapted models and back-translation.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 04:38:09 GMT'}]",2020-10-07,"[['Wei', 'Hao-Ran', ''], ['Zhang', 'Zhirui', ''], ['Chen', 'Boxing', ''], ['Luo', 'Weihua', '']]"
1358764,2010.02434,Wen-Chin Huang,"Wen-Chin Huang, Tomoki Hayashi, Shinji Watanabe, Tomoki Toda","The Sequence-to-Sequence Baseline for the Voice Conversion Challenge
  2020: Cascading ASR and TTS","Accepted to the ISCA Joint Workshop for the Blizzard Challenge and
  Voice Conversion Challenge 2020",,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents the sequence-to-sequence (seq2seq) baseline system for
the voice conversion challenge (VCC) 2020. We consider a naive approach for
voice conversion (VC), which is to first transcribe the input speech with an
automatic speech recognition (ASR) model, followed using the transcriptions to
generate the voice of the target with a text-to-speech (TTS) model. We revisit
this method under a sequence-to-sequence (seq2seq) framework by utilizing
ESPnet, an open-source end-to-end speech processing toolkit, and the many
well-configured pretrained models provided by the community. Official
evaluation results show that our system comes out top among the participating
systems in terms of conversion similarity, demonstrating the promising ability
of seq2seq models to convert speaker identity. The implementation is made
open-source at: https://github.com/espnet/espnet/tree/master/egs/vcc20.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 02:27:38 GMT'}]",2020-10-07,"[['Huang', 'Wen-Chin', ''], ['Hayashi', 'Tomoki', ''], ['Watanabe', 'Shinji', ''], ['Toda', 'Tomoki', '']]"
1358735,2010.02405,Yi Yang,Yi Yang and Arzoo Katiyar,"Simple and Effective Few-Shot Named Entity Recognition with Structured
  Nearest Neighbor Learning",Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a simple few-shot named entity recognition (NER) system based on
nearest neighbor learning and structured inference. Our system uses a
supervised NER model trained on the source domain, as a feature extractor.
Across several test domains, we show that a nearest neighbor classifier in this
feature-space is far more effective than the standard meta-learning approaches.
We further propose a cheap but effective method to capture the label
dependencies between entity tags without expensive CRF training. We show that
our method of combining structured decoding with nearest neighbor learning
achieves state-of-the-art performance on standard few-shot NER evaluation
tasks, improving F1 scores by $6\%$ to $16\%$ absolute points over prior
meta-learning based systems.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 00:25:50 GMT'}]",2020-10-07,"[['Yang', 'Yi', ''], ['Katiyar', 'Arzoo', '']]"
1134602,1906.02416,Alexander Terenin,"Alexander Terenin, M{\aa}ns Magnusson, Leif Jonsson",Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models,,"Conference on Empirical Methods in Natural Language Processing,
  2020",,,stat.ML cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To scale non-parametric extensions of probabilistic topic models such as
Latent Dirichlet allocation to larger data sets, practitioners rely
increasingly on parallel and distributed systems. In this work, we study
data-parallel training for the hierarchical Dirichlet process (HDP) topic
model. Based upon a representation of certain conditional distributions within
an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic
model. This sampler utilizes all available sources of sparsity found in natural
language - an important way to make computation efficient. We benchmark our
method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using
a single multi-core machine in under four days.
","[{'version': 'v1', 'created': 'Thu, 6 Jun 2019 05:04:08 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 12:00:09 GMT'}]",2020-10-07,"[['Terenin', 'Alexander', ''], ['Magnusson', 'Måns', ''], ['Jonsson', 'Leif', '']]"
1358726,2010.02396,Arbi Haza Nasution,"Arbi Haza Nasution, Yohei Murakami, Toru Ishida","Plan Optimization to Bilingual Dictionary Induction for Low-Resource
  Language Families","29 pages, 16 figures, 9 tables, accepted for publication in ACM
  TALLIP",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Creating bilingual dictionary is the first crucial step in enriching
low-resource languages. Especially for the closely-related ones, it has been
shown that the constraint-based approach is useful for inducing bilingual
lexicons from two bilingual dictionaries via the pivot language. However, if
there are no available machine-readable dictionaries as input, we need to
consider manual creation by bilingual native speakers. To reach a goal of
comprehensively create multiple bilingual dictionaries, even if we already have
several existing machine-readable bilingual dictionaries, it is still difficult
to determine the execution order of the constraint-based approach to reducing
the total cost. Plan optimization is crucial in composing the order of
bilingual dictionaries creation with the consideration of the methods and their
costs. We formalize the plan optimization for creating bilingual dictionaries
by utilizing Markov Decision Process (MDP) with the goal to get a more accurate
estimation of the most feasible optimal plan with the least total cost before
fully implementing the constraint-based bilingual lexicon induction. We model a
prior beta distribution of bilingual lexicon induction precision with language
similarity and polysemy of the topology as $\alpha$ and $\beta$ parameters. It
is further used to model cost function and state transition probability. We
estimated the cost of all investment plan as a baseline for evaluating the
proposed MDP-based approach with total cost as an evaluation metric. After
utilizing the posterior beta distribution in the first batch of experiments to
construct the prior beta distribution in the second batch of experiments, the
result shows 61.5\% of cost reduction compared to the estimated all investment
plan and 39.4\% of cost reduction compared to the estimated MDP optimal plan.
The MDP-based proposal outperformed the baseline on the total cost.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 23:43:40 GMT'}]",2020-10-07,"[['Nasution', 'Arbi Haza', ''], ['Murakami', 'Yohei', ''], ['Ishida', 'Toru', '']]"
1358687,2010.02357,Tsvetomila Mihaylova,"Tsvetomila Mihaylova, Vlad Niculae, Andr\'e F. T. Martins","Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent
  Structure Learning",EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Latent structure models are a powerful tool for modeling language data: they
can mitigate the error propagation and annotation bottleneck in pipeline
systems, while simultaneously uncovering linguistic insights about the data.
One challenge with end-to-end training of these models is the argmax operation,
which has null gradient. In this paper, we focus on surrogate gradients, a
popular strategy to deal with this problem. We explore latent structure
learning through the angle of pulling back the downstream learning objective.
In this paradigm, we discover a principled motivation for both the
straight-through estimator (STE) as well as the recently-proposed SPIGOT - a
variant of STE for structured models. Our perspective leads to new algorithms
in the same family. We empirically compare the known and the novel pulled-back
estimators against the popular alternatives, yielding new insight for
practitioners and revealing intriguing failure cases.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 21:56:00 GMT'}]",2020-10-07,"[['Mihaylova', 'Tsvetomila', ''], ['Niculae', 'Vlad', ''], ['Martins', 'André F. T.', '']]"
1255929,2003.05574,Magdalena Biesialska,"Katarzyna Biesialska, Magdalena Biesialska and Henryk Rybinski",Sentiment Analysis with Contextual Embeddings and Self-Attention,"Accepted at the 25th International Symposium on Methodologies for
  Intelligent Systems (ISMIS 2020)",,10.1007/978-3-030-59491-6_4,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In natural language the intended meaning of a word or phrase is often
implicit and depends on the context. In this work, we propose a simple yet
effective method for sentiment analysis using contextual embeddings and a
self-attention mechanism. The experimental results for three languages,
including morphologically rich Polish and German, show that our model is
comparable to or even outperforms state-of-the-art models. In all cases the
superiority of models leveraging contextual embeddings is demonstrated.
Finally, this work is intended as a step towards introducing a universal,
multilingual sentiment classifier.
","[{'version': 'v1', 'created': 'Thu, 12 Mar 2020 02:19:51 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 23:02:42 GMT'}]",2020-10-07,"[['Biesialska', 'Katarzyna', ''], ['Biesialska', 'Magdalena', ''], ['Rybinski', 'Henryk', '']]"
1234309,2001.08896,Urmish Thakker,"Urmish Thakker, Paul Whatmough, Zhi-Gang Liu, Matthew Mattina, Jesse
  Beu",Compressing Language Models using Doped Kronecker Products,"Link to Workshop
  (https://research.fb.com/programs/on-device-intelligence-workshop/)","Presented at On-device Intelligence Workshop at Third Conference
  on Machine Learning and Systems (MLSys) 2020",,,cs.LG cs.CL stat.ML,http://creativecommons.org/publicdomain/zero/1.0/,"  Kronecker Products (KP) have been used to compress IoT RNN Applications by
15-38x compression factors, achieving better results than traditional
compression methods. However when KP is applied to large Natural Language
Processing tasks, it leads to significant accuracy loss (approx 26%). This
paper proposes a way to recover accuracy otherwise lost when applying KP to
large NLP tasks, by allowing additional degrees of freedom in the KP matrix.
More formally, we propose doping, a process of adding an extremely sparse
overlay matrix on top of the pre-defined KP structure. We call this compression
method doped kronecker product compression. To train these models, we present a
new solution to the phenomenon of co-matrix adaption (CMA), which uses a new
regularization scheme called co matrix dropout regularization (CMR). We present
experimental results that demonstrate compression of a large language model
with LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At
25x compression, an equivalent pruned network leads to 7.9% loss in perplexity
score, while HMD and LMF lead to 15% and 27% loss in perplexity score
respectively.
","[{'version': 'v1', 'created': 'Fri, 24 Jan 2020 06:07:21 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Jan 2020 05:36:36 GMT'}, {'version': 'v3', 'created': 'Fri, 6 Mar 2020 15:58:58 GMT'}, {'version': 'v4', 'created': 'Tue, 6 Oct 2020 07:07:22 GMT'}]",2020-10-07,"[['Thakker', 'Urmish', ''], ['Whatmough', 'Paul', ''], ['Liu', 'Zhi-Gang', ''], ['Mattina', 'Matthew', ''], ['Beu', 'Jesse', '']]"
1353533,2009.12030,Guanglin Niu,"Guanglin Niu, Bo Li, Yongfei Zhang, Shiliang Pu, Jingyang Li","AutoETER: Automated Entity Type Representation for Knowledge Graph
  Embedding","10 pages, 3 figures, the full version of a paper accepted to EMNLP
  2020 Findings",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in Knowledge Graph Embedding (KGE) allow for representing
entities and relations in continuous vector spaces. Some traditional KGE models
leveraging additional type information can improve the representation of
entities which however totally rely on the explicit types or neglect the
diverse type representations specific to various relations. Besides, none of
the existing methods is capable of inferring all the relation patterns of
symmetry, inversion and composition as well as the complex properties of 1-N,
N-1 and N-N relations, simultaneously. To explore the type information for any
KG, we develop a novel KGE framework with Automated Entity TypE Representation
(AutoETER), which learns the latent type embedding of each entity by regarding
each relation as a translation operation between the types of two entities with
a relation-aware projection mechanism. Particularly, our designed automated
type representation learning mechanism is a pluggable module which can be
easily incorporated with any KGE model. Besides, our approach could model and
infer all the relation patterns and complex relations. Experiments on four
datasets demonstrate the superior performance of our model compared to
state-of-the-art baselines on link prediction tasks, and the visualization of
type clustering provides clearly the explanation of type embeddings and
verifies the effectiveness of our model.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 04:27:35 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 13:52:59 GMT'}]",2020-10-07,"[['Niu', 'Guanglin', ''], ['Li', 'Bo', ''], ['Zhang', 'Yongfei', ''], ['Pu', 'Shiliang', ''], ['Li', 'Jingyang', '']]"
1359194,2010.02864,Avi Shmidman,"Avi Shmidman, Joshua Guedalia, Shaltiel Shmidman, Moshe Koppel, Reut
  Tsarfaty","A Novel Challenge Set for Hebrew Morphological Disambiguation and
  Diacritics Restoration",,"Findings of EMNLP, 2020",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One of the primary tasks of morphological parsers is the disambiguation of
homographs. Particularly difficult are cases of unbalanced ambiguity, where one
of the possible analyses is far more frequent than the others. In such cases,
there may not exist sufficient examples of the minority analyses in order to
properly evaluate performance, nor to train effective classifiers. In this
paper we address the issue of unbalanced morphological ambiguities in Hebrew.
We offer a challenge set for Hebrew homographs -- the first of its kind --
containing substantial attestation of each analysis of 21 Hebrew homographs. We
show that the current SOTA of Hebrew disambiguation performs poorly on cases of
unbalanced ambiguity. Leveraging our new dataset, we achieve a new
state-of-the-art for all 21 words, improving the overall average F1 score from
0.67 to 0.95. Our resulting annotated datasets are made publicly available for
further research.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 16:34:03 GMT'}]",2020-10-07,"[['Shmidman', 'Avi', ''], ['Guedalia', 'Joshua', ''], ['Shmidman', 'Shaltiel', ''], ['Koppel', 'Moshe', ''], ['Tsarfaty', 'Reut', '']]"
1359170,2010.02840,Ruiqi Zhong,"Ruiqi Zhong, Tao Yu, Dan Klein",Semantic Evaluation for Text-to-SQL with Distilled Test Suites,EMNLP 2020 Long Paper,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose test suite accuracy to approximate semantic accuracy for
Text-to-SQL models. Our method distills a small test suite of databases that
achieves high code coverage for the gold query from a large number of randomly
generated databases. At evaluation time, it computes the denotation accuracy of
the predicted queries on the distilled test suite, hence calculating a tight
upper-bound for semantic accuracy efficiently. We use our proposed method to
evaluate 21 models submitted to the Spider leader board and manually verify
that our method is always correct on 100 examples. In contrast, the current
Spider metric leads to a 2.5% false negative rate on average and 8.1% in the
worst case, indicating that test suite accuracy is needed. Our implementation,
along with distilled test suites for eleven Text-to-SQL datasets, is publicly
available.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 16:04:12 GMT'}]",2020-10-07,"[['Zhong', 'Ruiqi', ''], ['Yu', 'Tao', ''], ['Klein', 'Dan', '']]"
1358100,2010.01770,John Morris,John X. Morris,Second-Order NLP Adversarial Examples,8 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversarial example generation methods in NLP rely on models like language
models or sentence encoders to determine if potential adversarial examples are
valid. In these methods, a valid adversarial example fools the model being
attacked, and is determined to be semantically or syntactically valid by a
second model. Research to date has counted all such examples as errors by the
attacked model. We contend that these adversarial examples may not be flaws in
the attacked model, but flaws in the model that determines validity. We term
such invalid inputs second-order adversarial examples. We propose the
constraint robustness curve and associated metric ACCS as tools for evaluating
the robustness of a constraint to second-order adversarial examples. To
generate this curve, we design an adversarial attack to run directly on the
semantic similarity models. We test on two constraints, the Universal Sentence
Encoder (USE) and BERTScore. Our findings indicate that such second-order
examples exist, but are typically less common than first-order adversarial
examples in state-of-the-art models. They also indicate that USE is effective
as constraint on NLP adversarial examples, while BERTScore is nearly
ineffectual. Code for running the experiments in this paper is available at
https://github.com/jxmorris12/second-order-adversarial-examples.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 04:32:38 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 01:20:53 GMT'}]",2020-10-07,"[['Morris', 'John X.', '']]"
1359160,2010.02830,Swarnadeep Saha,"Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal",PRover: Proof Generation for Interpretable Reasoning over Rules,EMNLP 2020 (15 pages),,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work by Clark et al. (2020) shows that transformers can act as 'soft
theorem provers' by answering questions over explicitly provided knowledge in
natural language. In our work, we take a step closer to emulating formal
theorem provers, by proposing PROVER, an interpretable transformer-based model
that jointly answers binary questions over rule-bases and generates the
corresponding proofs. Our model learns to predict nodes and edges corresponding
to proof graphs in an efficient constrained training paradigm. During
inference, a valid proof, satisfying a set of global constraints is generated.
We conduct experiments on synthetic, hand-authored, and human-paraphrased
rule-bases to show promising results for QA and proof generation, with strong
generalization performance. First, PROVER generates proofs with an accuracy of
87%, while retaining or improving performance on the QA task, compared to
RuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained
on questions requiring lower depths of reasoning, it generalizes significantly
better to higher depths (up to 15% improvement). Third, PROVER obtains near
perfect QA accuracy of 98% using only 40% of the training data. However,
generating proofs for questions requiring higher depths of reasoning becomes
challenging, and the accuracy drops to 65% for 'depth 5', indicating
significant scope for future work. Our code and models are publicly available
at https://github.com/swarnaHub/PRover
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 15:47:53 GMT'}]",2020-10-07,"[['Saha', 'Swarnadeep', ''], ['Ghosh', 'Sayan', ''], ['Srivastava', 'Shashank', ''], ['Bansal', 'Mohit', '']]"
1358729,2010.02399,Ameet Deshpande,"Ameet Deshpande, Karthik Narasimhan",Guiding Attention for Self-Supervised Learning with Transformers,"Accepted to Findings of EMNLP, 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose a simple and effective technique to allow for
efficient self-supervised learning with bi-directional Transformers. Our
approach is motivated by recent studies demonstrating that self-attention
patterns in trained models contain a majority of non-linguistic regularities.
We propose a computationally efficient auxiliary loss function to guide
attention heads to conform to such patterns. Our method is agnostic to the
actual pre-training objective and results in faster convergence of models as
well as better performance on downstream tasks compared to the baselines,
achieving state of the art results in low-resource settings. Surprisingly, we
also find that linguistic properties of attention heads are not necessarily
correlated with language modeling performance.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 00:04:08 GMT'}]",2020-10-07,"[['Deshpande', 'Ameet', ''], ['Narasimhan', 'Karthik', '']]"
1358707,2010.02377,Alexander Hoyle,"Alexander Hoyle, Pranav Goel, Philip Resnik",Improving Neural Topic Models using Knowledge Distillation,Accepted to EMNLP 2020,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Topic models are often used to identify human-interpretable topics to help
make sense of large document collections. We use knowledge distillation to
combine the best attributes of probabilistic topic models and pretrained
transformers. Our modular method can be straightforwardly applied with any
neural topic model to improve topic quality, which we demonstrate using two
models having disparate architectures, obtaining state-of-the-art topic
coherence. We show that our adaptable framework not only improves performance
in the aggregate over all estimated topics, as is commonly reported, but also
in head-to-head comparisons of aligned topics.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 22:49:16 GMT'}]",2020-10-07,"[['Hoyle', 'Alexander', ''], ['Goel', 'Pranav', ''], ['Resnik', 'Philip', '']]"
1348868,2009.07365,Matthias Lindemann,"Matthias Lindemann, Jonas Groschwitz, Alexander Koller",Fast semantic parsing with well-typedness guarantees,"Accepted at EMNLP 2020, camera-ready version",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  AM dependency parsing is a linguistically principled method for neural
semantic parsing with high accuracy across multiple graphbanks. It relies on a
type system that models semantic valency but makes existing parsers slow. We
describe an A* parser and a transition-based parser for AM dependency parsing
which guarantee well-typedness and improve parsing speed by up to 3 orders of
magnitude, while maintaining or improving accuracy.
","[{'version': 'v1', 'created': 'Tue, 15 Sep 2020 21:54:01 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 14:49:04 GMT'}]",2020-10-07,"[['Lindemann', 'Matthias', ''], ['Groschwitz', 'Jonas', ''], ['Koller', 'Alexander', '']]"
1353026,2009.11523,Nikolaos Pappas,"Nikolaos Pappas, Phoebe Mulcaire, Noah A. Smith",Grounded Compositional Outputs for Adaptive Language Modeling,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language models have emerged as a central component across NLP, and a great
deal of progress depends on the ability to cheaply adapt them (e.g., through
finetuning) to new domains and tasks. A language model's vocabulary$-$typically
selected before training and permanently fixed later$-$affects its size and is
part of what makes it resistant to such adaptation. Prior work has used
compositional input embeddings based on surface forms to ameliorate this issue.
In this work, we go one step beyond and propose a fully compositional output
embedding layer for language models, which is further grounded in information
from a structured lexicon (WordNet), namely semantically related words and
free-text definitions. To our knowledge, the result is the first word-level
language model with a size that does not depend on the training vocabulary. We
evaluate the model on conventional language modeling as well as challenging
cross-domain settings with an open vocabulary, finding that it matches or
outperforms previous state-of-the-art output embedding methods and adaptation
approaches. Our analysis attributes the improvements to sample efficiency: our
model is more accurate for low-frequency words.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 07:21:14 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 18:26:38 GMT'}]",2020-10-07,"[['Pappas', 'Nikolaos', ''], ['Mulcaire', 'Phoebe', ''], ['Smith', 'Noah A.', '']]"
1164690,1908.06520,Ugur Kursuncu,"Ugur Kursuncu, Manas Gaur, Carlos Castillo, Amanuel Alambo, K.
  Thirunarayan, Valerie Shalin, Dilshod Achilov, I. Budak Arpinar, Amit Sheth","Modeling Islamist Extremist Communications on Social Media using
  Contextual Dimensions: Religion, Ideology, and Hate",22 pages,,,,cs.SI cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Terror attacks have been linked in part to online extremist content. Although
tens of thousands of Islamist extremism supporters consume such content, they
are a small fraction relative to peaceful Muslims. The efforts to contain the
ever-evolving extremism on social media platforms have remained inadequate and
mostly ineffective. Divergent extremist and mainstream contexts challenge
machine interpretation, with a particular threat to the precision of
classification algorithms. Our context-aware computational approach to the
analysis of extremist content on Twitter breaks down this persuasion process
into building blocks that acknowledge inherent ambiguity and sparsity that
likely challenge both manual and automated classification. We model this
process using a combination of three contextual dimensions -- religion,
ideology, and hate -- each elucidating a degree of radicalization and
highlighting independent features to render them computationally accessible. We
utilize domain-specific knowledge resources for each of these contextual
dimensions such as Qur'an for religion, the books of extremist ideologues and
preachers for political ideology and a social media hate speech corpus for
hate. Our study makes three contributions to reliable analysis: (i) Development
of a computational approach rooted in the contextual dimensions of religion,
ideology, and hate that reflects strategies employed by online Islamist
extremist groups, (ii) An in-depth analysis of relevant tweet datasets with
respect to these dimensions to exclude likely mislabeled users, and (iii) A
framework for understanding online radicalization as a process to assist
counter-programming. Given the potentially significant social impact, we
evaluate the performance of our algorithms to minimize mislabeling, where our
approach outperforms a competitive baseline by 10.2% in precision.
","[{'version': 'v1', 'created': 'Sun, 18 Aug 2019 21:46:19 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Aug 2019 21:28:01 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Oct 2020 22:31:08 GMT'}]",2020-10-07,"[['Kursuncu', 'Ugur', ''], ['Gaur', 'Manas', ''], ['Castillo', 'Carlos', ''], ['Alambo', 'Amanuel', ''], ['Thirunarayan', 'K.', ''], ['Shalin', 'Valerie', ''], ['Achilov', 'Dilshod', ''], ['Arpinar', 'I. Budak', ''], ['Sheth', 'Amit', '']]"
1358714,2010.02384,Tejas Srinivasan,"Tejas Srinivasan, Ramon Sanabria, Florian Metze and Desmond Elliott",Fine-Grained Grounding for Multimodal Speech Recognition,Accepted to Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal automatic speech recognition systems integrate information from
images to improve speech recognition quality, by grounding the speech in the
visual context. While visual signals have been shown to be useful for
recovering entities that have been masked in the audio, these models should be
capable of recovering a broader range of word types. Existing systems rely on
global visual features that represent the entire image, but localizing the
relevant regions of the image will make it possible to recover a larger set of
words, such as adjectives and verbs. In this paper, we propose a model that
uses finer-grained visual information from different parts of the image, using
automatic object proposals. In experiments on the Flickr8K Audio Captions
Corpus, we find that our model improves over approaches that use global visual
features, that the proposals enable the model to recover entities and other
related words, such as adjectives, and that improvements are due to the model's
ability to localize the correct proposals.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 23:06:24 GMT'}]",2020-10-07,"[['Srinivasan', 'Tejas', ''], ['Sanabria', 'Ramon', ''], ['Metze', 'Florian', ''], ['Elliott', 'Desmond', '']]"
1198215,1910.14296,Junru Zhou,"Junru Zhou, Zhuosheng Zhang, Hai Zhao, Shuailiang Zhang",LIMIT-BERT : Linguistic Informed Multi-Task BERT,"EMNLP 2020, ACL Findings",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present a Linguistic Informed Multi-Task BERT (LIMIT-BERT)
for learning language representations across multiple linguistic tasks by
Multi-Task Learning (MTL). LIMIT-BERT includes five key linguistic syntax and
semantics tasks: Part-Of-Speech (POS) tags, constituent and dependency
syntactic parsing, span and dependency semantic role labeling (SRL). Besides,
LIMIT-BERT adopts linguistics mask strategy: Syntactic and Semantic Phrase
Masking which mask all of the tokens corresponding to a syntactic/semantic
phrase. Different from recent Multi-Task Deep Neural Networks (MT-DNN) (Liu et
al., 2019), our LIMIT-BERT is linguistically motivated and learning in a
semi-supervised method which provides large amounts of linguistic-task data as
same as BERT learning corpus. As a result, LIMIT-BERT not only improves
linguistic tasks performance but also benefits from a regularization effect and
linguistic information that leads to more general representations to help adapt
to new tasks and domains. LIMIT-BERT obtains new state-of-the-art or
competitive results on both span and dependency semantic parsing on Propbank
benchmarks and both dependency and constituent syntactic parsing on Penn
Treebank.
","[{'version': 'v1', 'created': 'Thu, 31 Oct 2019 08:14:51 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 03:11:55 GMT'}]",2020-10-07,"[['Zhou', 'Junru', ''], ['Zhang', 'Zhuosheng', ''], ['Zhao', 'Hai', ''], ['Zhang', 'Shuailiang', '']]"
1358716,2010.02386,Mo Yu,"Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu
  Chang","Interactive Fiction Game Playing as Multi-Paragraph Reading
  Comprehension with Reinforcement Learning",Accepted to EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Interactive Fiction (IF) games with real human-written natural language texts
provide a new natural evaluation for language understanding techniques. In
contrast to previous text games with mostly synthetic texts, IF games pose
language understanding challenges on the human-written textual descriptions of
diverse and sophisticated game worlds and language generation challenges on the
action command generation from less restricted combinatorial space. We take a
novel perspective of IF game solving and re-formulate it as Multi-Passage
Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query
attention mechanisms and the structured prediction in MPRC to efficiently
generate and evaluate action outputs and apply an object-centric historical
observation retrieval strategy to mitigate the partial observability of the
textual observations. Extensive experiments on the recent IF benchmark
(Jericho) demonstrate clear advantages of our approaches achieving high winning
rates and low data requirements compared to all previous approaches. Our source
code is available at: https://github.com/XiaoxiaoGuo/rcdqn.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 23:09:20 GMT'}]",2020-10-07,"[['Guo', 'Xiaoxiao', ''], ['Yu', 'Mo', ''], ['Gao', 'Yupeng', ''], ['Gan', 'Chuang', ''], ['Campbell', 'Murray', ''], ['Chang', 'Shiyu', '']]"
1358724,2010.02394,Lichao Sun,"Lichao Sun, Congying Xia, Wenpeng Yin, Tingting Liang, Philip S. Yu,
  Lifang He",Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks,Accepted by COLING 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Mixup is the latest data augmentation technique that linearly interpolates
input examples and the corresponding labels. It has shown strong effectiveness
in image classification by interpolating images at the pixel level. Inspired by
this line of research, in this paper, we explore i) how to apply mixup to
natural language processing tasks since text data can hardly be mixed in the
raw format; ii) if mixup is still effective in transformer-based learning
models, e.g., BERT. To achieve the goal, we incorporate mixup to
transformer-based pre-trained architecture, named ""mixup-transformer"", for a
wide range of NLP tasks while keeping the whole end-to-end training system. We
evaluate the proposed framework by running extensive experiments on the GLUE
benchmark. Furthermore, we also examine the performance of mixup-transformer in
low-resource scenarios by reducing the training data with a certain ratio. Our
studies show that mixup is a domain-independent data augmentation technique to
pre-trained language models, resulting in significant performance improvement
for transformer-based models.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 23:37:30 GMT'}]",2020-10-07,"[['Sun', 'Lichao', ''], ['Xia', 'Congying', ''], ['Yin', 'Wenpeng', ''], ['Liang', 'Tingting', ''], ['Yu', 'Philip S.', ''], ['He', 'Lifang', '']]"
1358725,2010.02395,Arbi Haza Nasution,"Arbi Haza Nasution, Yohei Murakami, Toru Ishida","A Generalized Constraint Approach to Bilingual Dictionary Induction for
  Low-Resource Language Families","30 pages, 13 figures, 14 tables, published in ACM TALLIP","ACM Trans. Asian Low-Resour. Lang. Inf. Process. 17, 2, Article 9
  (November 2017), 29 pages",10.1145/3138815,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The lack or absence of parallel and comparable corpora makes bilingual
lexicon extraction a difficult task for low-resource languages. The pivot
language and cognate recognition approaches have been proven useful for
inducing bilingual lexicons for such languages. We propose constraint-based
bilingual lexicon induction for closely-related languages by extending
constraints from the recent pivot-based induction technique and further
enabling multiple symmetry assumption cycles to reach many more cognates in the
transgraph. We further identify cognate synonyms to obtain many-to-many
translation pairs. This paper utilizes four datasets: one Austronesian
low-resource language and three Indo-European high-resource languages. We use
three constraint-based methods from our previous work, the Inverse Consultation
method and translation pairs generated from the Cartesian product of input
dictionaries as baselines. We evaluate our result using the metrics of
precision, recall and F-score. Our customizable approach allows the user to
conduct cross-validation to predict the optimal hyperparameters (cognate
threshold and cognate synonym threshold) with various combinations of
heuristics and the number of symmetry assumption cycles to gain the highest
F-score. Our proposed methods have statistically significant improvement of
precision and F-score compared to our previous constraint-based methods. The
results show that our method demonstrates the potential to complement other
bilingual dictionary creation methods like word alignment models using parallel
corpora for high-resource languages while well handling low-resource languages.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 23:41:04 GMT'}]",2020-10-07,"[['Nasution', 'Arbi Haza', ''], ['Murakami', 'Yohei', ''], ['Ishida', 'Toru', '']]"
1353041,2009.11538,Hai Ye,"Hai Ye, Qingyu Tan, Ruidan He, Juntao Li, Hwee Tou Ng, Lidong Bing","Feature Adaptation of Pre-Trained Language Models across Languages and
  Domains with Robust Self-Training",To appear at EMNLP 2020. 14 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has
gained much attention recently. Instead of fine-tuning PrLMs as done in most
previous work, we investigate how to adapt the features of PrLMs to new domains
without fine-tuning. We explore unsupervised domain adaptation (UDA) in this
paper. With the features from PrLMs, we adapt the models trained with labeled
data from the source domain to the unlabeled target domain. Self-training is
widely used for UDA which predicts pseudo labels on the target domain data for
training. However, the predicted pseudo labels inevitably include noise, which
will negatively affect training a robust model. To improve the robustness of
self-training, in this paper we present class-aware feature self-distillation
(CFd) to learn discriminative features from PrLMs, in which PrLM features are
self-distilled into a feature adaptation module and the features from the same
class are more tightly clustered. We further extend CFd to a cross-language
setting, in which language discrepancy is studied. Experiments on two
monolingual and multilingual Amazon review datasets show that CFd can
consistently improve the performance of self-training in cross-domain and
cross-language settings.
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 08:04:37 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 03:45:44 GMT'}]",2020-10-07,"[['Ye', 'Hai', ''], ['Tan', 'Qingyu', ''], ['He', 'Ruidan', ''], ['Li', 'Juntao', ''], ['Ng', 'Hwee Tou', ''], ['Bing', 'Lidong', '']]"
1358807,2010.02477,Youngmoon Jung,"Youngmoon Jung, Yeunju Choi, Hyungjun Lim, Hoirin Kim","A Unified Deep Learning Framework for Short-Duration Speaker
  Verification in Adverse Environments","19 pages, 10 figures, 13 tables","in IEEE Access, vol. 8, pp. 175448-175466, 2020",10.1109/ACCESS.2020.3025941,,eess.AS cs.CL cs.LG cs.SD stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Speaker verification (SV) has recently attracted considerable research
interest due to the growing popularity of virtual assistants. At the same time,
there is an increasing requirement for an SV system: it should be robust to
short speech segments, especially in noisy and reverberant environments. In
this paper, we consider one more important requirement for practical
applications: the system should be robust to an audio stream containing long
non-speech segments, where a voice activity detection (VAD) is not applied. To
meet these two requirements, we introduce feature pyramid module (FPM)-based
multi-scale aggregation (MSA) and self-adaptive soft VAD (SAS-VAD). We present
the FPM-based MSA to deal with short speech segments in noisy and reverberant
environments. Also, we use the SAS-VAD to increase the robustness to long
non-speech segments. To further improve the robustness to acoustic distortions
(i.e., noise and reverberation), we apply a masking-based speech enhancement
(SE) method. We combine SV, VAD, and SE models in a unified deep learning
framework and jointly train the entire network in an end-to-end manner. To the
best of our knowledge, this is the first work combining these three models in a
deep learning framework. We conduct experiments on Korean indoor (KID) and
VoxCeleb datasets, which are corrupted by noise and reverberation. The results
show that the proposed method is effective for SV in the challenging conditions
and performs better than the baseline i-vector and deep speaker embedding
systems.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 04:51:45 GMT'}]",2020-10-07,"[['Jung', 'Youngmoon', ''], ['Choi', 'Yeunju', ''], ['Lim', 'Hyungjun', ''], ['Kim', 'Hoirin', '']]"
1359137,2010.02807,Shubham Toshniwal,"Shubham Toshniwal, Sam Wiseman, Allyson Ettinger, Karen Livescu, Kevin
  Gimpel","Learning to Ignore: Long Document Coreference with Bounded Memory Neural
  Networks",EMNLP 2020 camera ready,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Long document coreference resolution remains a challenging task due to the
large memory and runtime requirements of current models. Recent work doing
incremental coreference resolution using just the global representation of
entities shows practical benefits but requires keeping all entities in memory,
which can be impractical for long documents. We argue that keeping all entities
in memory is unnecessary, and we propose a memory-augmented neural network that
tracks only a small bounded number of entities at a time, thus guaranteeing a
linear runtime in length of document. We show that (a) the model remains
competitive with models with high memory and computational requirements on
OntoNotes and LitBank, and (b) the model learns an efficient memory management
strategy easily outperforming a rule-based strategy.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 15:16:31 GMT'}]",2020-10-07,"[['Toshniwal', 'Shubham', ''], ['Wiseman', 'Sam', ''], ['Ettinger', 'Allyson', ''], ['Livescu', 'Karen', ''], ['Gimpel', 'Kevin', '']]"
1359140,2010.02810,Michel Pluess,"Michel Pl\""uss and Lukas Neukom and Manfred Vogel","Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech
  to Standard German Text Corpus","5 pages, 0 figures",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a forced sentence alignment procedure for Swiss German speech and
Standard German text. It is able to create a speech-to-text corpus in a fully
automatic fashion, given an audio recording and the corresponding unaligned
transcript. Compared to a manual alignment, it achieves a mean IoU of 0.8401
with a sentence recall of 0.9491. When applying our IoU estimate filter, the
mean IoU can be further improved to 0.9271 at the cost of a lower sentence
recall of 0.4881. Using this procedure, we created the Swiss Parliaments
Corpus, an automatically aligned Swiss German speech to Standard German text
corpus. 65 % of the raw data could be transformed to sentence-level
audio-text-pairs, resulting in 293 hours of training data. We have made the
corpus freely available for download.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 15:18:21 GMT'}]",2020-10-07,"[['Plüss', 'Michel', ''], ['Neukom', 'Lukas', ''], ['Vogel', 'Manfred', '']]"
1358928,2010.02598,Maksim Riabinin,"Max Ryabinin, Sergei Popov, Liudmila Prokhorenkova, Elena Voita",Embedding Words in Non-Vector Space with Unsupervised Graph Learning,"Accepted as a long paper for EMNLP 2020. 15 pages, 6 figures",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It has become a de-facto standard to represent words as elements of a vector
space (word2vec, GloVe). While this approach is convenient, it is unnatural for
language: words form a graph with a latent hierarchical structure, and this
structure has to be revealed and encoded by word embeddings. We introduce
GraphGlove: unsupervised graph word representations which are learned
end-to-end. In our setting, each word is a node in a weighted graph and the
distance between words is the shortest path distance between the corresponding
nodes. We adopt a recent method learning a representation of data in the form
of a differentiable weighted graph and use it to modify the GloVe training
algorithm. We show that our graph-based representations substantially
outperform vector-based methods on word similarity and analogy tasks. Our
analysis reveals that the structure of the learned graphs is hierarchical and
similar to that of WordNet, the geometry is highly non-trivial and contains
subgraphs with different local topology.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 10:17:49 GMT'}]",2020-10-07,"[['Ryabinin', 'Max', ''], ['Popov', 'Sergei', ''], ['Prokhorenkova', 'Liudmila', ''], ['Voita', 'Elena', '']]"
1359074,2010.02744,Shashi Narayan,"Shashi Narayan and Joshua Maynez and Jakub Adamek and Daniele Pighin
  and Bla\v{z} Bratani\v{c} and Ryan McDonald","Stepwise Extractive Summarization and Planning with Structured
  Transformers","17 pages, EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose encoder-centric stepwise models for extractive summarization using
structured transformers -- HiBERT and Extended Transformers. We enable stepwise
summarization by injecting the previously generated summary into the structured
transformer as an auxiliary sub-structure. Our models are not only efficient in
modeling the structure of long inputs, but they also do not rely on
task-specific redundancy-aware modeling, making them a general purpose
extractive content planner for different tasks. When evaluated on CNN/DailyMail
extractive summarization, stepwise models achieve state-of-the-art performance
in terms of Rouge without any redundancy aware modeling or sentence filtering.
This also holds true for Rotowire table-to-text generation, where our models
surpass previously reported metrics for content selection, planning and
ordering, highlighting the strength of stepwise modeling. Amongst the two
structured transformers we test, stepwise Extended Transformers provides the
best performance across both datasets and sets a new standard for these
challenges.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 14:12:58 GMT'}]",2020-10-07,"[['Narayan', 'Shashi', ''], ['Maynez', 'Joshua', ''], ['Adamek', 'Jakub', ''], ['Pighin', 'Daniele', ''], ['Bratanič', 'Blaž', ''], ['McDonald', 'Ryan', '']]"
1349313,2009.07810,Tara Safavi,"Tara Safavi, Danai Koutra",CoDEx: A Comprehensive Knowledge Graph Completion Benchmark,EMNLP 2020,,,,cs.CL cs.AI cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present CoDEx, a set of knowledge graph completion datasets extracted from
Wikidata and Wikipedia that improve upon existing knowledge graph completion
benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises
three knowledge graphs varying in size and structure, multilingual descriptions
of entities and relations, and tens of thousands of hard negative triples that
are plausible but verified to be false. To characterize CoDEx, we contribute
thorough empirical analyses and benchmarking experiments. First, we analyze
each CoDEx dataset in terms of logical relation patterns. Next, we report
baseline link prediction and triple classification results on CoDEx for five
extensively tuned embedding models. Finally, we differentiate CoDEx from the
popular FB15K-237 knowledge graph completion dataset by showing that CoDEx
covers more diverse and interpretable content, and is a more difficult link
prediction benchmark. Data, code, and pretrained models are available at
https://bit.ly/2EPbrJs.
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 17:08:23 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 09:10:10 GMT'}]",2020-10-07,"[['Safavi', 'Tara', ''], ['Koutra', 'Danai', '']]"
1358932,2010.02602,Guanglin Niu,"Guanglin Niu, Bo Li, Yongfei Zhang, Yongpan Sheng, Chuan Shi, Jingyang
  Li, Shiliang Pu","Joint Semantics and Data-Driven Path Representation for Knowledge Graph
  Inference","12 pages, 6 tables, 4 figures",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inference on a large-scale knowledge graph (KG) is of great importance for KG
applications like question answering. The path-based reasoning models can
leverage much information over paths other than pure triples in the KG, which
face several challenges: all the existing path-based methods are data-driven,
lacking explainability for path representation. Besides, some methods either
consider only relational paths or ignore the heterogeneity between entities and
relations both contained in paths, which cannot capture the rich semantics of
paths well. To address the above challenges, in this work, we propose a novel
joint semantics and data-driven path representation that balances
explainability and generalization in the framework of KG embedding. More
specifically, we inject horn rules to obtain the condensed paths by the
transparent and explainable path composition procedure. The entity converter is
designed to transform the entities along paths into the representations in the
semantic level similar to relations for reducing the heterogeneity between
entities and relations, in which the KGs both with and without type information
are considered. Our proposed model is evaluated on two classes of tasks: link
prediction and path query answering task. The experimental results show that it
has a significant performance gain over several different state-of-the-art
baselines.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 10:24:45 GMT'}]",2020-10-07,"[['Niu', 'Guanglin', ''], ['Li', 'Bo', ''], ['Zhang', 'Yongfei', ''], ['Sheng', 'Yongpan', ''], ['Shi', 'Chuan', ''], ['Li', 'Jingyang', ''], ['Pu', 'Shiliang', '']]"
1359063,2010.02733,"Jo\""el Doat","Jo\""el A. Doat",Towards Coalgebras in Stylometry,"9 pages, 12 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The syntactic behaviour of texts can highly vary depending on their contexts
(e.g. author, genre, etc.). From the standpoint of stylometry, it can be
helpful to objectively measure this behaviour. In this paper, we discuss how
coalgebras are used to formalise the notion of behaviour by embedding syntactic
features of a given text into probabilistic transition systems. By introducing
the behavioural distance, we are then able to quantitatively measure
differences between points in these systems and thus, comparing features of
different texts. Furthermore, the behavioural distance of points can be
approximated by a polynomial-time algorithm.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 13:55:11 GMT'}]",2020-10-07,"[['Doat', 'Joël A.', '']]"
1358939,2010.02609,Lu Xu,"Lu Xu, Hao Li, Wei Lu, and Lidong Bing",Position-Aware Tagging for Aspect Sentiment Triplet Extraction,"15 pages, 10 figures, accepted by EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the
triplets of target entities, their associated sentiment, and opinion spans
explaining the reason for the sentiment. Existing research efforts mostly solve
this problem using pipeline approaches, which break the triplet extraction
process into several stages. Our observation is that the three elements within
a triplet are highly related to each other, and this motivates us to build a
joint model to extract such triplets using a sequence tagging approach.
However, how to effectively design a tagging approach to extract the triplets
that can capture the rich interactions among the elements is a challenging
research question. In this work, we propose the first end-to-end model with a
novel position-aware tagging scheme that is capable of jointly extracting the
triplets. Our experimental results on several existing datasets show that
jointly capturing elements in the triplet using our approach leads to improved
performance over the existing approaches. We also conducted extensive
experiments to investigate the model effectiveness and robustness.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 10:40:34 GMT'}]",2020-10-07,"[['Xu', 'Lu', ''], ['Li', 'Hao', ''], ['Lu', 'Wei', ''], ['Bing', 'Lidong', '']]"
1358946,2010.02616,Marius Mosbach,"Marius Mosbach, Anna Khokhlova, Michael A. Hedderich, Dietrich Klakow","On the Interplay Between Fine-tuning and Sentence-level Probing for
  Linguistic Knowledge in Pre-trained Transformers",Accepted at Findings of EMNLP 2020 and BlackboxNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fine-tuning pre-trained contextualized embedding models has become an
integral part of the NLP pipeline. At the same time, probing has emerged as a
way to investigate the linguistic knowledge captured by pre-trained models.
Very little is, however, understood about how fine-tuning affects the
representations of pre-trained models and thereby the linguistic knowledge they
encode. This paper contributes towards closing this gap. We study three
different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate
through sentence-level probing how fine-tuning affects their representations.
We find that for some probing tasks fine-tuning leads to substantial changes in
accuracy, possibly suggesting that fine-tuning introduces or even removes
linguistic knowledge from a pre-trained model. These changes, however, vary
greatly across different models, fine-tuning and probing tasks. Our analysis
reveals that while fine-tuning indeed changes the representations of a
pre-trained model and these changes are typically larger for higher layers,
only in very few cases, fine-tuning has a positive effect on probing accuracy
that is larger than just using the pre-trained model with a strong pooling
method. Based on our findings, we argue that both positive and negative effects
of fine-tuning on probing require a careful interpretation.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 10:54:00 GMT'}]",2020-10-07,"[['Mosbach', 'Marius', ''], ['Khokhlova', 'Anna', ''], ['Hedderich', 'Michael A.', ''], ['Klakow', 'Dietrich', '']]"
1359035,2010.02705,Minki Kang,"Minki Kang, Moonsu Han, Sung Ju Hwang","Neural Mask Generator: Learning to Generate Adaptive Word Maskings for
  Language Model Adaptation","19 pages, 9 figures, EMNLP 2020",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a method to automatically generate a domain- and task-adaptive
maskings of the given text for self-supervised pre-training, such that we can
effectively adapt the language model to a particular target task (e.g. question
answering). Specifically, we present a novel reinforcement learning-based
framework which learns the masking policy, such that using the generated masks
for further pre-training of the target language model helps improve task
performance on unseen texts. We use off-policy actor-critic with entropy
regularization and experience replay for reinforcement learning, and propose a
Transformer-based policy network that can consider the relative importance of
words in a given text. We validate our Neural Mask Generator (NMG) on several
question answering and text classification datasets using BERT and DistilBERT
as the language models, on which it outperforms rule-based masking strategies,
by automatically learning optimal adaptive maskings.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 13:27:01 GMT'}]",2020-10-07,"[['Kang', 'Minki', ''], ['Han', 'Moonsu', ''], ['Hwang', 'Sung Ju', '']]"
1358966,2010.02636,Mark Fishel,"Liisa R\""atsep, Liisi Piits, Hille Pajupuu, Indrek Hein, Mark
  Fi\v{s}el",Neural Speech Synthesis for Estonian,9 pages in Estonian,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This technical report describes the results of a collaboration between the
NLP research group at the University of Tartu and the Institute of Estonian
Language on improving neural speech synthesis for Estonian. The report (written
in Estonian) describes the project results, the summary of which is: (1) Speech
synthesis data from 6 speakers for a total of 92.4 hours is collected and
openly released (CC-BY-4.0). Data available at https://konekorpus.tartunlp.ai
and https://www.eki.ee/litsents/. (2) software and models for neural speech
synthesis is released open-source (MIT license). Available at
https://koodivaramu.eesti.ee/tartunlp/text-to-speech . (3) We ran evaluations
of the new models and compared them to other existing solutions (HMM-based HTS
models from EKI, http://www.eki.ee/heli/, and Google's speech synthesis for
Estonian, accessed via https://translate.google.com). Evaluation includes voice
acceptability MOS scores for sentence-level and longer excerpts, detailed error
analysis and evaluation of the pre-processing module.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 11:37:46 GMT'}]",2020-10-07,"[['Rätsep', 'Liisa', ''], ['Piits', 'Liisi', ''], ['Pajupuu', 'Hille', ''], ['Hein', 'Indrek', ''], ['Fišel', 'Mark', '']]"
1358976,2010.02646,Yong Wang,"Yong Wang, Longyue Wang, Victor O.K. Li, Zhaopeng Tu",On the Sparsity of Neural Machine Translation Models,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern neural machine translation (NMT) models employ a large number of
parameters, which leads to serious over-parameterization and typically causes
the underutilization of computational resources. In response to this problem,
we empirically investigate whether the redundant parameters can be reused to
achieve better performance. Experiments and analyses are systematically
conducted on different datasets and NMT architectures. We show that: 1) the
pruned parameters can be rejuvenated to improve the baseline model by up to
+0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the
ability of modeling low-level lexical information.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 11:47:20 GMT'}]",2020-10-07,"[['Wang', 'Yong', ''], ['Wang', 'Longyue', ''], ['Li', 'Victor O. K.', ''], ['Tu', 'Zhaopeng', '']]"
1358978,2010.02648,Yilin Yang,"Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee
  and Zhaopeng Tu",On the Sub-Layer Functionalities of Transformer Decoder,"Findings of the 2020 Conference on Empirical Methods in Natural
  Language Processing (Long)",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There have been significant efforts to interpret the encoder of
Transformer-based encoder-decoder architectures for neural machine translation
(NMT); meanwhile, the decoder remains largely unexamined despite its critical
role. During translation, the decoder must predict output tokens by considering
both the source-language text from the encoder and the target-language prefix
produced in previous steps. In this work, we study how Transformer-based
decoders leverage information from the source and target languages --
developing a universal probe task to assess how information is propagated
through each module of each decoder layer. We perform extensive experiments on
three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis
provides insight on when and where decoders leverage different sources. Based
on these insights, we demonstrate that the residual feed-forward module in each
Transformer decoder layer can be dropped with minimal loss of performance -- a
significant reduction in computation and number of parameters, and consequently
a significant boost to both training and inference speed.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 11:50:54 GMT'}]",2020-10-07,"[['Yang', 'Yilin', ''], ['Wang', 'Longyue', ''], ['Shi', 'Shuming', ''], ['Tadepalli', 'Prasad', ''], ['Lee', 'Stefan', ''], ['Tu', 'Zhaopeng', '']]"
1358922,2010.02592,Eyal Ben-David,"Eyal Ben-David, Orgad Keller, Eric Malmi, Idan Szpektor, Roi Reichart",Semantically Driven Sentence Fusion: Modeling and Evaluation,This paper was accepted to Findings of EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Sentence fusion is the task of joining related sentences into coherent text.
Current training and evaluation schemes for this task are based on single
reference ground-truths and do not account for valid fusion variants. We show
that this hinders models from robustly capturing the semantic relationship
between input sentences. To alleviate this, we present an approach in which
ground-truth solutions are automatically expanded into multiple references via
curated equivalence classes of connective phrases. We apply this method to a
large-scale dataset and use the augmented dataset for both model training and
evaluation. To improve the learning of semantic representation using multiple
references, we enrich the model with auxiliary discourse classification tasks
under a multi-tasking framework. Our experiments highlight the improvements of
our approach over state-of-the-art models.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 10:06:01 GMT'}]",2020-10-07,"[['Ben-David', 'Eyal', ''], ['Keller', 'Orgad', ''], ['Malmi', 'Eric', ''], ['Szpektor', 'Idan', ''], ['Reichart', 'Roi', '']]"
1358979,2010.02649,Hao Zhang,"Sicheng Yu, Hao Zhang, Wei Jing, Jing Jiang","Context Modeling with Evidence Filter for Multiple Choice Question
  Answering","6 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multiple-Choice Question Answering (MCQA) is a challenging task in machine
reading comprehension. The main challenge in MCQA is to extract ""evidence"" from
the given context that supports the correct answer. In the OpenbookQA dataset,
the requirement of extracting ""evidence"" is particularly important due to the
mutual independence of sentences in the context. Existing work tackles this
problem by annotated evidence or distant supervision with rules which overly
rely on human efforts. To address the challenge, we propose a simple yet
effective approach termed evidence filtering to model the relationships between
the encoded contexts with respect to different options collectively and to
potentially highlight the evidence sentences and filter out unrelated
sentences. In addition to the effective reduction of human efforts of our
approach compared, through extensive experiments on OpenbookQA, we show that
the proposed approach outperforms the models that use the same backbone and
more training data; and our parameter analysis also demonstrates the
interpretability of our approach.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 11:53:23 GMT'}]",2020-10-07,"[['Yu', 'Sicheng', ''], ['Zhang', 'Hao', ''], ['Jing', 'Wei', ''], ['Jiang', 'Jing', '']]"
1358980,2010.02650,Clara Meister,"Clara Meister, Tim Vieira, Ryan Cotterell","If beam search is the answer, what was the question?",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural
language generators frequently leads to low-quality results. Rather, most
state-of-the-art results on language generation tasks are attained using beam
search despite its overwhelmingly high search error rate. This implies that the
MAP objective alone does not express the properties we desire in text, which
merits the question: if beam search is the answer, what was the question? We
frame beam search as the exact solution to a different decoding objective in
order to gain insights into why high probability under a model alone may not
indicate adequacy. We find that beam search enforces uniform information
density in text, a property motivated by cognitive science. We suggest a set of
decoding objectives that explicitly enforce this property and find that exact
decoding with these objectives alleviates the problems encountered when
decoding poorly calibrated language generation models. Additionally, we analyze
the text produced using various decoding strategies and see that, in our neural
machine translation experiments, the extent to which this property is adhered
to strongly correlates with BLEU.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 11:57:03 GMT'}]",2020-10-07,"[['Meister', 'Clara', ''], ['Vieira', 'Tim', ''], ['Cotterell', 'Ryan', '']]"
1358984,2010.02654,Yohan Jo,"Yohan Jo, Jacky Visser, Chris Reed, Eduard Hovy",Extracting Implicitly Asserted Propositions in Argumentation,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Argumentation accommodates various rhetorical devices, such as questions,
reported speech, and imperatives. These rhetorical tools usually assert
argumentatively relevant propositions rather implicitly, so understanding their
true meaning is key to understanding certain arguments properly. However, most
argument mining systems and computational linguistics research have paid little
attention to implicitly asserted propositions in argumentation. In this paper,
we examine a wide range of computational methods for extracting propositions
that are implicitly asserted in questions, reported speech, and imperatives in
argumentation. By evaluating the models on a corpus of 2016 U.S. presidential
debates and online commentary, we demonstrate the effectiveness and limitations
of the computational models. Our study may inform future research on argument
mining and the semantics of these rhetorical devices in argumentation.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 12:03:47 GMT'}]",2020-10-07,"[['Jo', 'Yohan', ''], ['Visser', 'Jacky', ''], ['Reed', 'Chris', ''], ['Hovy', 'Eduard', '']]"
1358986,2010.02656,Yuncong Li,"Yuncong Li, Cunxiang Yin, Sheng-hua Zhong and Xu Pan","Multi-Instance Multi-Label Learning Networks for Aspect-Category
  Sentiment Analysis",Long paper accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect-category sentiment analysis (ACSA) aims to predict sentiment
polarities of sentences with respect to given aspect categories. To detect the
sentiment toward a particular aspect category in a sentence, most previous
methods first generate an aspect category-specific sentence representation for
the aspect category, then predict the sentiment polarity based on the
representation. These methods ignore the fact that the sentiment of an aspect
category mentioned in a sentence is an aggregation of the sentiments of the
words indicating the aspect category in the sentence, which leads to suboptimal
performance. In this paper, we propose a Multi-Instance Multi-Label Learning
Network for Aspect-Category sentiment analysis (AC-MIMLLN), which treats
sentences as bags, words as instances, and the words indicating an aspect
category as the key instances of the aspect category. Given a sentence and the
aspect categories mentioned in the sentence, AC-MIMLLN first predicts the
sentiments of the instances, then finds the key instances for the aspect
categories, finally obtains the sentiments of the sentence toward the aspect
categories by aggregating the key instance sentiments. Experimental results on
three public datasets demonstrate the effectiveness of AC-MIMLLN.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 12:07:54 GMT'}]",2020-10-07,"[['Li', 'Yuncong', ''], ['Yin', 'Cunxiang', ''], ['Zhong', 'Sheng-hua', ''], ['Pan', 'Xu', '']]"
1358990,2010.02660,Yohan Jo,"Yohan Jo, Seojin Bang, Emaad Manzoor, Eduard Hovy, Chris Reed",Detecting Attackable Sentences in Arguments,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Finding attackable sentences in an argument is the first step toward
successful refutation in argumentation. We present a first large-scale analysis
of sentence attackability in online arguments. We analyze driving reasons for
attacks in argumentation and identify relevant characteristics of sentences. We
demonstrate that a sentence's attackability is associated with many of these
characteristics regarding the sentence's content, proposition types, and tone,
and that an external knowledge source can provide useful information about
attackability. Building on these findings, we demonstrate that machine learning
models can automatically detect attackable sentences in arguments,
significantly better than several baselines and comparably well to laypeople.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 12:13:00 GMT'}]",2020-10-07,"[['Jo', 'Yohan', ''], ['Bang', 'Seojin', ''], ['Manzoor', 'Emaad', ''], ['Hovy', 'Eduard', ''], ['Reed', 'Chris', '']]"
1279097,2004.14327,"Ahmet \""Ust\""un","Ahmet \""Ust\""un, Arianna Bisazza, Gosse Bouma, Gertjan van Noord",UDapter: Language Adaptation for Truly Universal Dependency Parsing,In EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in multilingual dependency parsing have brought the idea of a
truly universal parser closer to reality. However, cross-language interference
and restrained model capacity remain major obstacles. To address this, we
propose a novel multilingual task adaptation approach based on contextual
parameter generation and adapter modules. This approach enables to learn
adapters via language embeddings while sharing model parameters across
languages. It also allows for an easy but effective integration of existing
linguistic typology features into the parsing network. The resulting parser,
UDapter, outperforms strong monolingual and multilingual baselines on the
majority of both high-resource and low-resource (zero-shot) languages, showing
the success of the proposed adaptation approach. Our in-depth analyses show
that soft parameter sharing via typological features is key to this success.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 16:52:50 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 15:46:41 GMT'}]",2020-10-07,"[['Üstün', 'Ahmet', ''], ['Bisazza', 'Arianna', ''], ['Bouma', 'Gosse', ''], ['van Noord', 'Gertjan', '']]"
1358995,2010.02665,Nachum Dershowitz,"Kfir Bar, Nachum Dershowitz, Lena Dankin",Automatic Metaphor Interpretation Using Word Embeddings,"Presented at 19th International Conference on Computational
  Linguistics and Intelligent Text Processing (CICLing), 2018",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We suggest a model for metaphor interpretation using word embeddings trained
over a relatively large corpus. Our system handles nominal metaphors, like
""time is money"". It generates a ranked list of potential interpretations of
given metaphors. Candidate meanings are drawn from collocations of the topic
(""time"") and vehicle (""money"") components, automatically extracted from a
dependency-parsed corpus. We explore adding candidates derived from word
association norms (common human responses to cues). Our ranking procedure
considers similarity between candidate interpretations and metaphor components,
measured in a semantic vector space. Lastly, a clustering algorithm removes
semantically related duplicates, thereby allowing other candidate
interpretations to attain higher rank. We evaluate using a set of annotated
metaphors.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 12:35:13 GMT'}]",2020-10-07,"[['Bar', 'Kfir', ''], ['Dershowitz', 'Nachum', ''], ['Dankin', 'Lena', '']]"
1349113,2009.07610,Alexandra Chronopoulou,"Alexandra Chronopoulou, Dario Stojanovski, Alexander Fraser","Reusing a Pretrained Language Model on Languages with Limited Corpora
  for Unsupervised NMT","EMNLP 2020, main conference",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Using a language model (LM) pretrained on two languages with large
monolingual data in order to initialize an unsupervised neural machine
translation (UNMT) system yields state-of-the-art results. When limited data is
available for one language, however, this method leads to poor translations. We
present an effective approach that reuses an LM that is pretrained only on the
high-resource language. The monolingual LM is fine-tuned on both languages and
is then used to initialize a UNMT model. To reuse the pretrained LM, we have to
modify its predefined vocabulary, to account for the new language. We therefore
propose a novel vocabulary extension method. Our approach, RE-LM, outperforms a
competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk)
and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four
translation directions.
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 11:37:10 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 09:41:20 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 13:54:47 GMT'}]",2020-10-07,"[['Chronopoulou', 'Alexandra', ''], ['Stojanovski', 'Dario', ''], ['Fraser', 'Alexander', '']]"
1358997,2010.02667,Ruey-Cheng Chen,"Ruey-Cheng Chen, Chia-Jung Lee",Incorporating Behavioral Hypotheses for Query Generation,"EMNLP 2020 short paper, 6 pages",,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative neural networks have been shown effective on query suggestion.
Commonly posed as a conditional generation problem, the task aims to leverage
earlier inputs from users in a search session to predict queries that they will
likely issue at a later time. User inputs come in various forms such as
querying and clicking, each of which can imply different semantic signals
channeled through the corresponding behavioral patterns. This paper induces
these behavioral biases as hypotheses for query generation, where a generic
encoder-decoder Transformer framework is presented to aggregate arbitrary
hypotheses of choice. Our experimental results show that the proposed approach
leads to significant improvements on top-$k$ word error rate and Bert F1 Score
compared to a recent BART model.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 12:38:02 GMT'}]",2020-10-07,"[['Chen', 'Ruey-Cheng', ''], ['Lee', 'Chia-Jung', '']]"
1359014,2010.02684,Alvin Chan,"Alvin Chan, Yi Tay, Yew-Soon Ong, Aston Zhang","Poison Attacks against Text Datasets with Conditional Adversarially
  Regularized Autoencoder","Accepted in EMNLP-Findings 2020, Camera Ready Version",,,,cs.CL cs.AI cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper demonstrates a fatal vulnerability in natural language inference
(NLI) and text classification systems. More concretely, we present a 'backdoor
poisoning' attack on NLP models. Our poisoning attack utilizes conditional
adversarially regularized autoencoder (CARA) to generate poisoned training
samples by poison injection in latent space. Just by adding 1% poisoned data,
our experiments show that a victim BERT finetuned classifier's predictions can
be steered to the poison target class with success rates of >80% when the input
hypothesis is injected with the poison signature, demonstrating that NLI and
text classification systems face a huge security risk.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 13:03:49 GMT'}]",2020-10-07,"[['Chan', 'Alvin', ''], ['Tay', 'Yi', ''], ['Ong', 'Yew-Soon', ''], ['Zhang', 'Aston', '']]"
1349006,2009.07503,Ranran Haoran Zhang,"Ranran Haoran Zhang, Qianying Liu, Aysa Xuemo Fan, Heng Ji, Daojian
  Zeng, Fei Cheng, Daisuke Kawahara and Sadao Kurohashi","Minimize Exposure Bias of Seq2Seq Models in Joint Entity and Relation
  Extraction",EMNLP 2020 Findings,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Joint entity and relation extraction aims to extract relation triplets from
plain text directly. Prior work leverages Sequence-to-Sequence (Seq2Seq) models
for triplet sequence generation. However, Seq2Seq enforces an unnecessary order
on the unordered triplets and involves a large decoding length associated with
error accumulation. These introduce exposure bias, which may cause the models
overfit to the frequent label combination, thus deteriorating the
generalization. We propose a novel Sequence-to-Unordered-Multi-Tree
(Seq2UMTree) model to minimize the effects of exposure bias by limiting the
decoding length to three within a triplet and removing the order among
triplets. We evaluate our model on two datasets, DuIE and NYT, and
systematically study how exposure bias alters the performance of Seq2Seq
models. Experiments show that the state-of-the-art Seq2Seq model overfits to
both datasets while Seq2UMTree shows significantly better generalization. Our
code is available at https://github.com/WindChimeRan/OpenJERE .
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 06:53:34 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 08:56:20 GMT'}]",2020-10-07,"[['Zhang', 'Ranran Haoran', ''], ['Liu', 'Qianying', ''], ['Fan', 'Aysa Xuemo', ''], ['Ji', 'Heng', ''], ['Zeng', 'Daojian', ''], ['Cheng', 'Fei', ''], ['Kawahara', 'Daisuke', ''], ['Kurohashi', 'Sadao', '']]"
1359016,2010.02686,Aina Gar\'i Soler,"Aina Gar\'i Soler, Marianna Apidianaki","BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking
  Scalar Adjectives with Contextualised Representations",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adjectives like pretty, beautiful and gorgeous describe positive properties
of the nouns they modify but with different intensity. These differences are
important for natural language understanding and reasoning. We propose a novel
BERT-based approach to intensity detection for scalar adjectives. We model
intensity by vectors directly derived from contextualised representations and
show they can successfully rank scalar adjectives. We evaluate our models both
intrinsically, on gold standard datasets, and on an Indirect Question Answering
task. Our results demonstrate that BERT encodes rich knowledge about the
semantics of scalar adjectives, and is able to provide better quality intensity
rankings than static embeddings and previous models with access to dedicated
resources.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 13:05:47 GMT'}]",2020-10-07,"[['Soler', 'Aina Garí', ''], ['Apidianaki', 'Marianna', '']]"
1359026,2010.02696,Lu Xu,"Lu Xu, Lidong Bing, Wei Lu and Fei Huang",Aspect Sentiment Classification with Aspect-Specific Opinion Spans,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect based sentiment analysis, predicting sentiment polarity of given
aspects, has drawn extensive attention. Previous attention-based models
emphasize using aspect semantics to help extract opinion features for
classification. However, these works are either not able to capture opinion
spans as a whole, or not able to capture variable-length opinion spans. In this
paper, we present a neat and effective structured attention model by
aggregating multiple linear-chain CRFs. Such a design allows the model to
extract aspect-specific opinion spans and then evaluate sentiment polarity by
exploiting the extracted opinion features. The experimental results on four
datasets demonstrate the effectiveness of the proposed model, and our analysis
demonstrates that our model can capture aspect-specific opinion spans.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 13:18:35 GMT'}]",2020-10-07,"[['Xu', 'Lu', ''], ['Bing', 'Lidong', ''], ['Lu', 'Wei', ''], ['Huang', 'Fei', '']]"
1358238,2010.01908,Wenxiang Jiao,"Wenxiang Jiao, Michael R. Lyu, Irwin King",Exploiting Unsupervised Data for Emotion Recognition in Conversations,"Accepted to the Findings of EMNLP 2020, 8 pages (published version of
  arXiv:1910.08916)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Emotion Recognition in Conversations (ERC) aims to predict the emotional
state of speakers in conversations, which is essentially a text classification
task. Unlike the sentence-level text classification problem, the available
supervised data for the ERC task is limited, which potentially prevents the
models from playing their maximum effect. In this paper, we propose a novel
approach to leverage unsupervised conversation data, which is more accessible.
Specifically, we propose the Conversation Completion (ConvCom) task, which
attempts to select the correct answer from candidate answers to fill a masked
utterance in a conversation. Then, we Pre-train a basic COntext- Dependent
Encoder (Pre-CODE) on the ConvCom task. Finally, we fine-tune the Pre-CODE on
the datasets of ERC. Experimental results demonstrate that pre-training on
unsupervised data achieves significant improvement of performance on the ERC
datasets, particularly on the minority emotion classes.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 13:28:47 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 09:32:31 GMT'}]",2020-10-07,"[['Jiao', 'Wenxiang', ''], ['Lyu', 'Michael R.', ''], ['King', 'Irwin', '']]"
1358921,2010.02591,Xuanli He,"Xuanli He, Quan Hung Tran, Gholamreza Haffari, Walter Chang, Trung
  Bui, Zhe Lin, Franck Dernoncourt, Nhan Dam",Scene Graph Modification Based on Natural Language Commands,Accepted to the Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Structured representations like graphs and parse trees play a crucial role in
many Natural Language Processing systems. In recent years, the advancements in
multi-turn user interfaces necessitate the need for controlling and updating
these structured representations given new sources of information. Although
there have been many efforts focusing on improving the performance of the
parsers that map text to graphs or parse trees, very few have explored the
problem of directly manipulating these representations. In this paper, we
explore the novel problem of graph modification, where the systems need to
learn how to update an existing scene graph given a new user's command. Our
novel models based on graph-based sparse transformer and cross attention
information fusion outperform previous systems adapted from the machine
translation and graph generation literature. We further contribute our large
graph modification datasets to the research community to encourage future
research for this new problem.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 10:01:19 GMT'}]",2020-10-07,"[['He', 'Xuanli', ''], ['Tran', 'Quan Hung', ''], ['Haffari', 'Gholamreza', ''], ['Chang', 'Walter', ''], ['Bui', 'Trung', ''], ['Lin', 'Zhe', ''], ['Dernoncourt', 'Franck', ''], ['Dam', 'Nhan', '']]"
1358917,2010.02587,Roman Klinger,Sean Papay and Roman Klinger and Sebastian Pad\'o,Dissecting Span Identification Tasks with Performance Prediction,accepted at EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Span identification (in short, span ID) tasks such as chunking, NER, or
code-switching detection, ask models to identify and classify relevant spans in
a text. Despite being a staple of NLP, and sharing a common structure, there is
little insight on how these tasks' properties influence their difficulty, and
thus little guidance on what model families work well on span ID tasks, and
why. We analyze span ID tasks via performance prediction, estimating how well
neural architectures do on different tasks. Our contributions are: (a) we
identify key properties of span ID tasks that can inform performance
prediction; (b) we carry out a large-scale experiment on English data, building
a model to predict performance for unseen span ID tasks that can support
architecture choices; (c), we investigate the parameters of the meta model,
yielding new insights on how model and task properties interact to affect span
ID performance. We find, e.g., that span frequency is especially important for
LSTMs, and that CRFs help when spans are infrequent and boundaries
non-distinctive.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:55:00 GMT'}]",2020-10-07,"[['Papay', 'Sean', ''], ['Klinger', 'Roman', ''], ['Padó', 'Sebastian', '']]"
1358824,2010.02494,Venkata Subrahmanyan Govindarajan,"Venkata Subrahmanyan Govindarajan, Benjamin T Chen, Rebecca Warholic,
  Katrin Erk, Junyi Jessy Li",Help! Need Advice on Identifying Advice,EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Humans use language to accomplish a wide variety of tasks - asking for and
giving advice being one of them. In online advice forums, advice is mixed in
with non-advice, like emotional support, and is sometimes stated explicitly,
sometimes implicitly. Understanding the language of advice would equip systems
with a better grasp of language pragmatics; practically, the ability to
identify advice would drastically increase the efficiency of advice-seeking
online, as well as advice-giving in natural language generation systems.
  We present a dataset in English from two Reddit advice forums - r/AskParents
and r/needadvice - annotated for whether sentences in posts contain advice or
not. Our analysis reveals rich linguistic phenomena in advice discourse. We
present preliminary models showing that while pre-trained language models are
able to capture advice better than rule-based systems, advice identification is
challenging, and we identify directions for future research.
  Comments: To be presented at EMNLP 2020.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 05:49:03 GMT'}]",2020-10-07,"[['Govindarajan', 'Venkata Subrahmanyan', ''], ['Chen', 'Benjamin T', ''], ['Warholic', 'Rebecca', ''], ['Erk', 'Katrin', ''], ['Li', 'Junyi Jessy', '']]"
1359197,2010.02867,Jieyu Zhao,Jieyu Zhao and Kai-Wei Chang,LOGAN: Local Group Bias Detection by Clustering,EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning techniques have been widely used in natural language
processing (NLP). However, as revealed by many recent studies, machine learning
models often inherit and amplify the societal biases in data. Various metrics
have been proposed to quantify biases in model predictions. In particular,
several of them evaluate disparity in model performance between protected
groups and advantaged groups in the test corpus. However, we argue that
evaluating bias at the corpus level is not enough for understanding how biases
are embedded in a model. In fact, a model with similar aggregated performance
between different groups on the entire data may behave differently on instances
in a local region. To analyze and detect such local bias, we propose LOGAN, a
new bias detection technique based on clustering. Experiments on toxicity
classification and object classification tasks show that LOGAN identifies bias
in a local region and allows us to better analyze the biases in model
predictions.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 16:42:51 GMT'}]",2020-10-07,"[['Zhao', 'Jieyu', ''], ['Chang', 'Kai-Wei', '']]"
1358828,2010.02498,Wanzheng Zhu,"Wanzheng Zhu, Suma Bhat",GRUEN for Evaluating Linguistic Quality of Generated Text,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic evaluation metrics are indispensable for evaluating generated text.
To date, these metrics have focused almost exclusively on the content selection
aspect of the system output, ignoring the linguistic quality aspect altogether.
We bridge this gap by proposing GRUEN for evaluating Grammaticality,
non-Redundancy, focUs, structure and coherENce of generated text. GRUEN
utilizes a BERT-based model and a class of syntactic, semantic, and contextual
features to examine the system output. Unlike most existing evaluation metrics
which require human references as an input, GRUEN is reference-less and
requires only the system output. Besides, it has the advantage of being
unsupervised, deterministic, and adaptable to various tasks. Experiments on
seven datasets over four language generation tasks show that the proposed
metric correlates highly with human judgments.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 05:59:25 GMT'}]",2020-10-07,"[['Zhu', 'Wanzheng', ''], ['Bhat', 'Suma', '']]"
1358830,2010.02500,Zirui Wang,"Zirui Wang, Sanket Vaibhav Mehta, Barnab\'as P\'oczos and Jaime
  Carbonell",Efficient Meta Lifelong-Learning with Limited Memory,Published as a main conference paper at EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current natural language processing models work well on a single task, yet
they often fail to continuously learn new tasks without forgetting previous
ones as they are re-trained throughout their lifetime, a challenge known as
lifelong learning. State-of-the-art lifelong language learning methods store
past examples in episodic memory and replay them at both training and inference
time. However, as we show later in our experiments, there are three significant
impediments: (1) needing unrealistically large memory module to achieve good
performance, (2) suffering from negative transfer, (3) requiring multiple local
adaptation steps for each test example that significantly slows down the
inference speed. In this paper, we identify three common principles of lifelong
learning methods and propose an efficient meta-lifelong framework that combines
them in a synergistic fashion. To achieve sample efficiency, our method trains
the model in a manner that it learns a better initialization for local
adaptation. Extensive experiments on text classification and question answering
benchmarks demonstrate the effectiveness of our framework by achieving
state-of-the-art performance using merely 1% memory size and narrowing the gap
with multi-task learning. We further show that our method alleviates both
catastrophic forgetting and negative transfer at the same time.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 06:08:07 GMT'}]",2020-10-07,"[['Wang', 'Zirui', ''], ['Mehta', 'Sanket Vaibhav', ''], ['Póczos', 'Barnabás', ''], ['Carbonell', 'Jaime', '']]"
1358853,2010.02523,Yiren Wang,"Yiren Wang, ChengXiang Zhai, Hany Hassan Awadalla",Multi-task Learning for Multilingual Neural Machine Translation,EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While monolingual data has been shown to be useful in improving bilingual
neural machine translation (NMT), effectively and efficiently leveraging
monolingual data for Multilingual NMT (MNMT) systems is a less explored area.
In this work, we propose a multi-task learning (MTL) framework that jointly
trains the model with the translation task on bitext data and two denoising
tasks on the monolingual data. We conduct extensive empirical studies on MNMT
systems with 10 language pairs from WMT datasets. We show that the proposed
approach can effectively improve the translation quality for both high-resource
and low-resource languages with large margin, achieving significantly better
results than the individual bilingual models. We also demonstrate the efficacy
of the proposed approach in the zero-shot setup for language pairs without
bitext training data. Furthermore, we show the effectiveness of MTL over
pre-training approaches for both NMT and cross-lingual transfer learning NLU
tasks; the proposed approach outperforms massive scale models trained on single
task.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 06:54:12 GMT'}]",2020-10-07,"[['Wang', 'Yiren', ''], ['Zhai', 'ChengXiang', ''], ['Awadalla', 'Hany Hassan', '']]"
1358864,2010.02534,Joohong Lee,"Kyubyong Park, Joohong Lee, Seongbo Jang, Dawoon Jung","An Empirical Study of Tokenization Strategies for Various Korean NLP
  Tasks",Accepted to AACL-IJCNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Typically, tokenization is the very first step in most text processing works.
As a token serves as an atomic unit that embeds the contextual information of
text, how to define a token plays a decisive role in the performance of a
model.Even though Byte Pair Encoding (BPE) has been considered the de facto
standard tokenization method due to its simplicity and universality, it still
remains unclear whether BPE works best across all languages and tasks. In this
paper, we test several tokenization strategies in order to answer our primary
research question, that is, ""What is the best tokenization strategy for Korean
NLP tasks?"" Experimental results demonstrate that a hybrid approach of
morphological segmentation followed by BPE works best in Korean to/from English
machine translation and natural language understanding tasks such as KorNLI,
KorSTS, NSMC, and PAWS-X. As an exception, for KorQuAD, the Korean extension of
SQuAD, BPE segmentation turns out to be the most effective.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 07:20:41 GMT'}]",2020-10-07,"[['Park', 'Kyubyong', ''], ['Lee', 'Joohong', ''], ['Jang', 'Seongbo', ''], ['Jung', 'Dawoon', '']]"
1359134,2010.02804,Manuel Mager,"Manuel Mager, \""Ozlem \c{C}etino\u{g}lu and Katharina Kann",Tackling the Low-resource Challenge for Canonical Segmentation,Accepted to EMNLP 2020,,,,cs.CL cs.AI stat.ML,http://creativecommons.org/licenses/by-sa/4.0/,"  Canonical morphological segmentation consists of dividing words into their
standardized morphemes. Here, we are interested in approaches for the task when
training data is limited. We compare model performance in a simulated
low-resource setting for the high-resource languages German, English, and
Indonesian to experiments on new datasets for the truly low-resource languages
Popoluca and Tepehua. We explore two new models for the task, borrowing from
the closely related area of morphological generation: an LSTM pointer-generator
and a sequence-to-sequence model with hard monotonic attention trained with
imitation learning. We find that, in the low-resource setting, the novel
approaches outperform existing ones on all languages by up to 11.4% accuracy.
However, while accuracy in emulated low-resource scenarios is over 50% for all
languages, for the truly low-resource languages Popoluca and Tepehua, our best
model only obtains 37.4% and 28.4% accuracy, respectively. Thus, we conclude
that canonical segmentation is still a challenging task for low-resource
languages.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 15:15:05 GMT'}]",2020-10-07,"[['Mager', 'Manuel', ''], ['Çetinoğlu', 'Özlem', ''], ['Kann', 'Katharina', '']]"
1358867,2010.02537,Shijie Wu,"Shijie Wu, Mark Dredze",Do Explicit Alignments Robustly Improve Multilingual Encoders?,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised
multilingual encoders can effectively learn cross-lingual representation.
Explicit alignment objectives based on bitexts like Europarl or MultiUN have
been shown to further improve these representations. However, word-level
alignments are often suboptimal and such bitexts are unavailable for many
languages. In this paper, we propose a new contrastive alignment objective that
can better utilize such signal, and examine whether these previous alignment
methods can be adapted to noisier sources of aligned data: a randomly sampled 1
million pair subset of the OPUS collection. Additionally, rather than report
results on a single dataset with a single model run, we report the mean and
standard derivation of multiple runs with different seeds, on four datasets and
tasks. Our more extensive analysis finds that, while our new objective
outperforms previous work, overall these methods do not improve performance
with a more robust evaluation framework. Furthermore, the gains from using a
better underlying model eclipse any benefits from alignment training. These
negative results dictate more care in evaluating these methods and suggest
limitations in applying explicit alignment objectives.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 07:43:17 GMT'}]",2020-10-07,"[['Wu', 'Shijie', ''], ['Dredze', 'Mark', '']]"
1359125,2010.02795,Soujanya Poria,"Deepanway Ghosal, Navonil Majumder, Alexander Gelbukh, Rada Mihalcea,
  Soujanya Poria","COSMIC: COmmonSense knowledge for eMotion Identification in
  Conversations",,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  In this paper, we address the task of utterance level emotion recognition in
conversations using commonsense knowledge. We propose COSMIC, a new framework
that incorporates different elements of commonsense such as mental states,
events, and causal relations, and build upon them to learn interactions between
interlocutors participating in a conversation. Current state-of-the-art methods
often encounter difficulties in context propagation, emotion shift detection,
and differentiating between related emotion classes. By learning distinct
commonsense representations, COSMIC addresses these challenges and achieves new
state-of-the-art results for emotion recognition on four different benchmark
conversational datasets. Our code is available at
https://github.com/declare-lab/conv-emotion.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 15:09:38 GMT'}]",2020-10-07,"[['Ghosal', 'Deepanway', ''], ['Majumder', 'Navonil', ''], ['Gelbukh', 'Alexander', ''], ['Mihalcea', 'Rada', ''], ['Poria', 'Soujanya', '']]"
1358882,2010.02552,Wenxiang Jiao,"Wenxiang Jiao, Xing Wang, Shilin He, Irwin King, Michael R. Lyu,
  Zhaopeng Tu","Data Rejuvenation: Exploiting Inactive Training Examples for Neural
  Machine Translation","Accepted to EMNLP 2020 main conference, 12 pages",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale training datasets lie at the core of the recent success of neural
machine translation (NMT) models. However, the complex patterns and potential
noises in the large-scale data make training NMT models difficult. In this
work, we explore to identify the inactive training examples which contribute
less to the model performance, and show that the existence of inactive examples
depends on the data distribution. We further introduce data rejuvenation to
improve the training of NMT models on large-scale datasets by exploiting
inactive examples. The proposed framework consists of three phases. First, we
train an identification model on the original training data, and use it to
distinguish inactive examples and active examples by their sentence-level
output probabilities. Then, we train a rejuvenation model on the active
examples, which is used to re-label the inactive examples with
forward-translation. Finally, the rejuvenated examples and the active examples
are combined to train the final NMT model. Experimental results on WMT14
English-German and English-French datasets show that the proposed data
rejuvenation consistently and significantly improves performance for several
strong NMT models. Extensive analyses reveal that our approach stabilizes and
accelerates the training process of NMT models, resulting in final models with
better generalization capability.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 08:57:31 GMT'}]",2020-10-07,"[['Jiao', 'Wenxiang', ''], ['Wang', 'Xing', ''], ['He', 'Shilin', ''], ['King', 'Irwin', ''], ['Lyu', 'Michael R.', ''], ['Tu', 'Zhaopeng', '']]"
1358886,2010.02556,Sumegh Roychowdhury,"Sumegh Roychowdhury, Sumedh A. Sontakke, Nikaash Puri, Mausoom Sarkar,
  Milan Aggarwal, Pinkesh Badjatiya, Balaji Krishnamurthy, Laurent Itti",Unsupervised Hierarchical Concept Learning,,,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Discovering concepts (or temporal abstractions) in an unsupervised manner
from demonstration data in the absence of an environment is an important
problem. Organizing these discovered concepts hierarchically at different
levels of abstraction is useful in discovering patterns, building ontologies,
and generating tutorials from demonstration data. However, recent work to
discover such concepts without access to any environment does not discover
relationships (or a hierarchy) between these discovered concepts. In this
paper, we present a Transformer-based concept abstraction architecture UNHCLE
(pronounced uncle) that extracts a hierarchy of concepts in an unsupervised way
from demonstration data. We empirically demonstrate how UNHCLE discovers
meaningful hierarchies using datasets from Chess and Cooking domains. Finally,
we show how UNHCLE learns meaningful language labels for concepts by using
demonstration data augmented with natural language for cooking and chess. All
of our code is available at https://github.com/UNHCLE/UNHCLE
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:04:01 GMT'}]",2020-10-07,"[['Roychowdhury', 'Sumegh', ''], ['Sontakke', 'Sumedh A.', ''], ['Puri', 'Nikaash', ''], ['Sarkar', 'Mausoom', ''], ['Aggarwal', 'Milan', ''], ['Badjatiya', 'Pinkesh', ''], ['Krishnamurthy', 'Balaji', ''], ['Itti', 'Laurent', '']]"
1358918,2010.02588,Arie Cattan,"Aaron Bornstein, Arie Cattan, Ido Dagan",CoRefi: A Crowd Sourcing Suite for Coreference Annotation,EMNLP 2020 system demonstration paper,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Coreference annotation is an important, yet expensive and time consuming,
task, which often involved expert annotators trained on complex decision
guidelines. To enable cheaper and more efficient annotation, we present CoRefi,
a web-based coreference annotation suite, oriented for crowdsourcing. Beyond
the core coreference annotation tool, CoRefi provides guided onboarding for the
task as well as a novel algorithm for a reviewing phase. CoRefi is open source
and directly embeds into any website, including popular crowdsourcing
platforms.
  CoRefi Demo: aka.ms/corefi Video Tour: aka.ms/corefivideo Github Repo:
https://github.com/aribornstein/corefi
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:55:36 GMT'}]",2020-10-07,"[['Bornstein', 'Aaron', ''], ['Cattan', 'Arie', ''], ['Dagan', 'Ido', '']]"
1246333,2002.08910,Colin Raffel,"Adam Roberts, Colin Raffel, and Noam Shazeer",How Much Knowledge Can You Pack Into the Parameters of a Language Model?,Camera-ready version for EMNLP,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It has recently been observed that neural language models trained on
unstructured text can implicitly store and retrieve knowledge using natural
language queries. In this short paper, we measure the practical utility of this
approach by fine-tuning pre-trained models to answer questions without access
to any external context or knowledge. We show that this approach scales with
model size and performs competitively with open-domain systems that explicitly
retrieve answers from an external knowledge source when answering questions. To
facilitate reproducibility and future work, we release our code and trained
models at https://goo.gle/t5-cbqa.
","[{'version': 'v1', 'created': 'Mon, 10 Feb 2020 18:55:58 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2020 04:54:34 GMT'}, {'version': 'v3', 'created': 'Tue, 28 Apr 2020 16:04:06 GMT'}, {'version': 'v4', 'created': 'Mon, 5 Oct 2020 21:26:45 GMT'}]",2020-10-07,"[['Roberts', 'Adam', ''], ['Raffel', 'Colin', ''], ['Shazeer', 'Noam', '']]"
1358889,2010.02559,Ilias Chalkidis,"Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos
  Aletras and Ion Androutsopoulos",LEGAL-BERT: The Muppets straight out of Law School,"5 pages, short paper in Findings of EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  BERT has achieved impressive performance in several NLP tasks. However, there
has been limited investigation on its adaptation guidelines in specialised
domains. Here we focus on the legal domain, where we explore several approaches
for applying BERT models to downstream legal tasks, evaluating on multiple
datasets. Our findings indicate that the previous guidelines for pre-training
and fine-tuning, often blindly followed, do not always generalize well in the
legal domain. Thus we propose a systematic investigation of the available
strategies when applying BERT in specialised domains. These are: (a) use the
original BERT out of the box, (b) adapt BERT by additional pre-training on
domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific
corpora. We also propose a broader hyper-parameter search space when
fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT
models intended to assist legal NLP research, computational law, and legal
technology applications.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:06:07 GMT'}]",2020-10-07,"[['Chalkidis', 'Ilias', ''], ['Fergadiotis', 'Manos', ''], ['Malakasiotis', 'Prodromos', ''], ['Aletras', 'Nikolaos', ''], ['Androutsopoulos', 'Ion', '']]"
1359119,2010.02789,Lifu Tu,"Lifu Tu, Tianyu Liu, Kevin Gimpel","An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based
  Inference Networks",EMNLP 2020. The first two authors contributed equally,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Many tasks in natural language processing involve predicting structured
outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine
translation. Researchers are increasingly applying deep representation learning
to these problems, but the structured component of these approaches is usually
quite simplistic. In this work, we propose several high-order energy terms to
capture complex dependencies among labels in sequence labeling, including
several that consider the entire label sequence. We use neural
parameterizations for these energy terms, drawing from convolutional,
recurrent, and self-attention networks. We use the framework of learning
energy-based inference networks (Tu and Gimpel, 2018) for dealing with the
difficulties of training and inference with such models. We empirically
demonstrate that this approach achieves substantial improvement using a variety
of high-order energy terms on four sequence labeling tasks, while having the
same decoding speed as simple, local classifiers. We also find high-order
energies to help in noisy data conditions.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 14:59:16 GMT'}]",2020-10-07,"[['Tu', 'Lifu', ''], ['Liu', 'Tianyu', ''], ['Gimpel', 'Kevin', '']]"
1358892,2010.02562,Giannis Karamanolakis,"Giannis Karamanolakis, Daniel Hsu, Luis Gravano","Cross-Lingual Text Classification with Minimal Resources by Transferring
  a Sparse Teacher",Accepted to Findings of EMNLP 2020 (Long Paper),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cross-lingual text classification alleviates the need for manually labeled
documents in a target language by leveraging labeled documents from other
languages. Existing approaches for transferring supervision across languages
require expensive cross-lingual resources, such as parallel corpora, while less
expensive cross-lingual representation learning approaches train classifiers
without target labeled documents. In this work, we propose a cross-lingual
teacher-student method, CLTS, that generates ""weak"" supervision in the target
language using minimal cross-lingual resources, in the form of a small number
of word translations. Given a limited translation budget, CLTS extracts and
transfers only the most important task-specific seed words across languages and
initializes a teacher classifier based on the translated seed words. Then, CLTS
iteratively trains a more powerful student that also exploits the context of
the seed words in unlabeled target documents and outperforms the teacher. CLTS
is simple and surprisingly effective in 18 diverse languages: by transferring
just 20 seed words, even a bag-of-words logistic regression student outperforms
state-of-the-art cross-lingual methods (e.g., based on multilingual BERT).
Moreover, CLTS can accommodate any type of student classifier: leveraging a
monolingual BERT student leads to further improvements and outperforms even
more expensive approaches by up to 12% in accuracy. Finally, CLTS addresses
emerging tasks in low-resource languages using just a small number of word
translations.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:11:02 GMT'}]",2020-10-07,"[['Karamanolakis', 'Giannis', ''], ['Hsu', 'Daniel', ''], ['Gravano', 'Luis', '']]"
1358898,2010.02568,Umanga Bista,"Umanga Bista, Alexander Patrick Mathews, Aditya Krishna Menon, Lexing
  Xie","SupMMD: A Sentence Importance Model for Extractive Summarization using
  Maximum Mean Discrepancy",15 pages,EMNLP 2020,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most work on multi-document summarization has focused on generic
summarization of information present in each individual document set. However,
the under-explored setting of update summarization, where the goal is to
identify the new information present in each set, is of equal practical
interest (e.g., presenting readers with updates on an evolving news topic). In
this work, we present SupMMD, a novel technique for generic and update
summarization based on the maximum mean discrepancy from kernel two-sample
testing. SupMMD combines both supervised learning for salience and unsupervised
learning for coverage and diversity. Further, we adapt multiple kernel learning
to make use of similarity across multiple information sources (e.g., text
features and knowledge based concepts). We show the efficacy of SupMMD in both
generic and update summarization tasks by meeting or exceeding the current
state-of-the-art on the DUC-2004 and TAC-2009 datasets.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:26:55 GMT'}]",2020-10-07,"[['Bista', 'Umanga', ''], ['Mathews', 'Alexander Patrick', ''], ['Menon', 'Aditya Krishna', ''], ['Xie', 'Lexing', '']]"
1358899,2010.02569,Ze Yang,"Ze Yang, Wei Wu, Can Xu, Xinnian Liang, Jiaqi Bai, Liran Wang, Wei
  Wang, Zhoujun Li",StyleDGPT: Stylized Response Generation with Pre-trained Language Models,Findings of EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating responses following a desired style has great potentials to extend
applications of open-domain dialogue systems, yet is refrained by lacking of
parallel data for training. In this work, we explore the challenging task with
pre-trained language models that have brought breakthrough to various natural
language tasks. To this end, we introduce a KL loss and a style classifier to
the fine-tuning step in order to steer response generation towards the target
style in both a word-level and a sentence-level. Comprehensive empirical
studies with two public datasets indicate that our model can significantly
outperform state-of-the-art methods in terms of both style consistency and
contextual coherence.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:29:50 GMT'}]",2020-10-07,"[['Yang', 'Ze', ''], ['Wu', 'Wei', ''], ['Xu', 'Can', ''], ['Liang', 'Xinnian', ''], ['Bai', 'Jiaqi', ''], ['Wang', 'Liran', ''], ['Wang', 'Wei', ''], ['Li', 'Zhoujun', '']]"
1358900,2010.02570,Yordan Yordanov,"Yordan Yordanov, Oana-Maria Camburu, Vid Kocijan, Thomas Lukasiewicz","Does the Objective Matter? Comparing Training Objectives for Pronoun
  Resolution",Accepted to the EMNLP 2020 conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Hard cases of pronoun resolution have been used as a long-standing benchmark
for commonsense reasoning. In the recent literature, pre-trained language
models have been used to obtain state-of-the-art results on pronoun resolution.
Overall, four categories of training and evaluation objectives have been
introduced. The variety of training datasets and pre-trained language models
used in these works makes it unclear whether the choice of training objective
is critical. In this work, we make a fair comparison of the performance and
seed-wise stability of four models that represent the four categories of
objectives. Our experiments show that the objective of sequence ranking
performs the best in-domain, while the objective of semantic similarity between
candidates and pronoun performs the best out-of-domain. We also observe a
seed-wise instability of the model using sequence ranking, which is not the
case when the other objectives are used.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:29:51 GMT'}]",2020-10-07,"[['Yordanov', 'Yordan', ''], ['Camburu', 'Oana-Maria', ''], ['Kocijan', 'Vid', ''], ['Lukasiewicz', 'Thomas', '']]"
1358903,2010.02573,Phillip Keung,"Phillip Keung, Yichao Lu, Gy\""orgy Szarvas, Noah A. Smith",The Multilingual Amazon Reviews Corpus,To appear in EMNLP 2020,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale
collection of Amazon reviews for multilingual text classification. The corpus
contains reviews in English, Japanese, German, French, Spanish, and Chinese,
which were collected between 2015 and 2019. Each record in the dataset contains
the review text, the review title, the star rating, an anonymized reviewer ID,
an anonymized product ID, and the coarse-grained product category (e.g.,
'books', 'appliances', etc.) The corpus is balanced across the 5 possible star
ratings, so each rating constitutes 20% of the reviews in each language. For
each language, there are 200,000, 5,000, and 5,000 reviews in the training,
development, and test sets, respectively. We report baseline results for
supervised text classification and zero-shot cross-lingual transfer learning by
fine-tuning a multilingual BERT model on reviews data. We propose the use of
mean absolute error (MAE) instead of classification accuracy for this task,
since MAE accounts for the ordinal nature of the ratings.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:34:01 GMT'}]",2020-10-07,"[['Keung', 'Phillip', ''], ['Lu', 'Yichao', ''], ['Szarvas', 'György', ''], ['Smith', 'Noah A.', '']]"
1359114,2010.02784,Zehui Dai,"Zehui Dai, Cheng Peng, Huajie Chen, and Yadong Ding","A Multi-Task Incremental Learning Framework with Category Name Embedding
  for Aspect-Category Sentiment Analysis",EMNLP 2020 camera ready,,,,cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  (T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and
targeted aspect-category sentiment analysis (TACSA), aims at identifying
sentiment polarity on predefined categories. Incremental learning on new
categories is necessary for (T)ACSA real applications. Though current
multi-task learning models achieve good performance in (T)ACSA tasks, they
suffer from catastrophic forgetting problems in (T)ACSA incremental learning
tasks. In this paper, to make multi-task learning feasible for incremental
learning, we proposed Category Name Embedding network (CNE-net). We set both
encoder and decoder shared among all categories to weaken the catastrophic
forgetting problem. Besides the origin input sentence, we applied another input
feature, i.e., category name, for task discrimination. Our model achieved
state-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a
dataset for (T)ACSA incremental learning and achieved the best performance
compared with other strong baselines.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 14:52:54 GMT'}]",2020-10-07,"[['Dai', 'Zehui', ''], ['Peng', 'Cheng', ''], ['Chen', 'Huajie', ''], ['Ding', 'Yadong', '']]"
1358912,2010.02582,Wei Han,Wei Han and Hantao Huang and Tao Han,"Finding the Evidence: Localization-aware Answer Prediction for Text
  Visual Question Answering",Accepted in COLING2020,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Image text carries essential information to understand the scene and perform
reasoning. Text-based visual question answering (text VQA) task focuses on
visual questions that require reading text in images. Existing text VQA systems
generate an answer by selecting from optical character recognition (OCR) texts
or a fixed vocabulary. Positional information of text is underused and there is
a lack of evidence for the generated answer. As such, this paper proposes a
localization-aware answer prediction network (LaAP-Net) to address this
challenge. Our LaAP-Net not only generates the answer to the question but also
predicts a bounding box as evidence of the generated answer. Moreover, a
context-enriched OCR representation (COR) for multimodal fusion is proposed to
facilitate the localization task. Our proposed LaAP-Net outperforms existing
approaches on three benchmark datasets for the text VQA task by a noticeable
margin.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:46:20 GMT'}]",2020-10-07,"[['Han', 'Wei', ''], ['Huang', 'Hantao', ''], ['Han', 'Tao', '']]"
1358914,2010.02584,Wenpeng Yin,"Wenpeng Yin, Nazneen Fatema Rajani, Dragomir Radev, Richard Socher,
  Caiming Xiong","Universal Natural Language Processing with Limited Annotations: Try
  Few-shot Textual Entailment as a Start","EMNLP2020 Long, camera-ready",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A standard way to address different NLP problems is by first constructing a
problem-specific dataset, then building a model to fit this dataset. To build
the ultimate artificial intelligence, we desire a single machine that can
handle diverse new problems, for which task-specific annotations are limited.
We bring up textual entailment as a unified solver for such NLP problems.
However, current research of textual entailment has not spilled much ink on the
following questions: (i) How well does a pretrained textual entailment system
generalize across domains with only a handful of domain-specific examples? and
(ii) When is it worth transforming an NLP task into textual entailment? We
argue that the transforming is unnecessary if we can obtain rich annotations
for this task. Textual entailment really matters particularly when the target
NLP task has insufficient annotations.
  Universal NLP can be probably achieved through different routines. In this
work, we introduce Universal Few-shot textual Entailment (UFO-Entail). We
demonstrate that this framework enables a pretrained entailment model to work
well on new entailment domains in a few-shot setting, and show its
effectiveness as a unified solver for several downstream NLP tasks such as
question answering and coreference resolution when the end-task annotations are
limited. Code: https://github.com/salesforce/UniversalFewShotNLP
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:50:25 GMT'}]",2020-10-07,"[['Yin', 'Wenpeng', ''], ['Rajani', 'Nazneen Fatema', ''], ['Radev', 'Dragomir', ''], ['Socher', 'Richard', ''], ['Xiong', 'Caiming', '']]"
1358916,2010.02586,Carel Van Niekerk,"Carel van Niekerk, Michael Heck, Christian Geishauser, Hsien-Chin Lin,
  Nurul Lubis, Marco Moresi, Milica Ga\v{s}i\'c","Knowing What You Know: Calibrating Dialogue Belief State Distributions
  via Ensembles","7 pages, 9 figures, to be published in Findings of EMNLP 2020, code
  available at:
  https://gitlab.cs.uni-duesseldorf.de/general/dsml/calibrating-dialogue-belief-state-distributions",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The ability to accurately track what happens during a conversation is
essential for the performance of a dialogue system. Current state-of-the-art
multi-domain dialogue state trackers achieve just over 55% accuracy on the
current go-to benchmark, which means that in almost every second dialogue turn
they place full confidence in an incorrect dialogue state. Belief trackers, on
the other hand, maintain a distribution over possible dialogue states. However,
they lack in performance compared to dialogue state trackers, and do not
produce well calibrated distributions. In this work we present state-of-the-art
performance in calibration for multi-domain dialogue belief trackers using a
calibrated ensemble of models. Our resulting dialogue belief tracker also
outperforms previous dialogue belief tracking models in terms of accuracy.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:51:04 GMT'}]",2020-10-07,"[['van Niekerk', 'Carel', ''], ['Heck', 'Michael', ''], ['Geishauser', 'Christian', ''], ['Lin', 'Hsien-Chin', ''], ['Lubis', 'Nurul', ''], ['Moresi', 'Marco', ''], ['Gašić', 'Milica', '']]"
1358887,2010.02557,Wasi Ahmad,Wasi Uddin Ahmad and Jianfeng Chi and Yuan Tian and Kai-Wei Chang,PolicyQA: A Reading Comprehension Dataset for Privacy Policies,EMNLP Findings 2020 (short paper),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Privacy policy documents are long and verbose. A question answering (QA)
system can assist users in finding the information that is relevant and
important to them. Prior studies in this domain frame the QA task as retrieving
the most relevant text segment or a list of sentences from the policy document
given a question. On the contrary, we argue that providing users with a short
text span from policy documents reduces the burden of searching the target
information from a lengthy text segment. In this paper, we present PolicyQA, a
dataset that contains 25,017 reading comprehension style examples curated from
an existing corpus of 115 website privacy policies. PolicyQA provides 714
human-annotated questions written for a wide range of privacy practices. We
evaluate two existing neural QA models and perform rigorous analysis to reveal
the advantages and challenges offered by PolicyQA.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 09:04:58 GMT'}]",2020-10-07,"[['Ahmad', 'Wasi Uddin', ''], ['Chi', 'Jianfeng', ''], ['Tian', 'Yuan', ''], ['Chang', 'Kai-Wei', '']]"
1358023,2010.01693,Oluwatobi Olabiyi,"Oluwatobi O. Olabiyi, Prarthana Bhattarai, C. Bayan Bruss, Zachary
  Kulis","DLGNet-Task: An End-to-end Neural Network Framework for Modeling
  Multi-turn Multi-domain Task-Oriented Dialogue",,,,,cs.CL cs.AI cs.HC cs.LG cs.NE,http://creativecommons.org/publicdomain/zero/1.0/,"  Task oriented dialogue (TOD) requires the complex interleaving of a number of
individually controllable components with strong guarantees for explainability
and verifiability. This has made it difficult to adopt the multi-turn
multi-domain dialogue generation capabilities of streamlined end-to-end
open-domain dialogue systems. In this paper, we present a new framework,
DLGNet-Task, a unified task-oriented dialogue system which employs
autoregressive transformer networks such as DLGNet and GPT-2/3 to complete user
tasks in multi-turn multi-domain conversations. Our framework enjoys the
controllable, verifiable, and explainable outputs of modular approaches, and
the low development, deployment and maintenance cost of end-to-end systems.
Treating open-domain system components as additional TOD system modules allows
DLGNet-Task to learn the joint distribution of the inputs and outputs of all
the functional blocks of existing modular approaches such as, natural language
understanding (NLU), state tracking, action policy, as well as natural language
generation (NLG). Rather than training the modules individually, as is common
in real-world systems, we trained them jointly with appropriate module
separations. When evaluated on the MultiWOZ2.1 dataset, DLGNet-Task shows
comparable performance to the existing state-of-the-art approaches.
Furthermore, using DLGNet-Task in conversational AI systems reduces the level
of effort required for developing, deploying, and maintaining intelligent
assistants at scale.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 21:43:17 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 16:31:06 GMT'}]",2020-10-07,"[['Olabiyi', 'Oluwatobi O.', ''], ['Bhattarai', 'Prarthana', ''], ['Bruss', 'C. Bayan', ''], ['Kulis', 'Zachary', '']]"
1359025,2010.02695,Nadir Durrani Dr,Nadir Durrani and Hassan Sajjad and Fahim Dalvi and Yonatan Belinkov,Analyzing Individual Neurons in Pre-trained Language Models,Accepted in EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While a lot of analysis has been carried to demonstrate linguistic knowledge
captured by the representations learned within deep NLP models, very little
attention has been paid towards individual neurons.We carry outa neuron-level
analysis using core linguistic tasks of predicting morphology, syntax and
semantics, on pre-trained language models, with questions like: i) do
individual neurons in pre-trained models capture linguistic information? ii)
which parts of the network learn more about certain linguistic phenomena? iii)
how distributed or focused is the information? and iv) how do various
architectures differ in learning these properties? We found small subsets of
neurons to predict linguistic tasks, with lower level tasks (such as
morphology) localized in fewer neurons, compared to higher level task of
predicting syntax. Our study also reveals interesting cross architectural
comparisons. For example, we found neurons in XLNet to be more localized and
disjoint when predicting properties compared to BERT and others, where they are
more distributed and coupled.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 13:17:38 GMT'}]",2020-10-07,"[['Durrani', 'Nadir', ''], ['Sajjad', 'Hassan', ''], ['Dalvi', 'Fahim', ''], ['Belinkov', 'Yonatan', '']]"
1279221,2004.14451,Allen Nie,"Allen Nie, Reuben Cohn-Gordon, and Christopher Potts",Pragmatic Issue-Sensitive Image Captioning,"15 pages, 7 figures. EMNLP 2020 Findings Accepted",,,,cs.CL cs.CV,http://creativecommons.org/licenses/by-sa/4.0/,"  Image captioning systems have recently improved dramatically, but they still
tend to produce captions that are insensitive to the communicative goals that
captions should meet. To address this, we propose Issue-Sensitive Image
Captioning (ISIC). In ISIC, a captioning system is given a target image and an
issue, which is a set of images partitioned in a way that specifies what
information is relevant. The goal of the captioner is to produce a caption that
resolves this issue. To model this task, we use an extension of the Rational
Speech Acts model of pragmatic language use. Our extension is built on top of
state-of-the-art pretrained neural image captioners and explicitly reasons
about issues in our sense. We establish experimentally that these models
generate captions that are both highly descriptive and issue-sensitive, and we
show how ISIC can complement and enrich the related task of Visual Question
Answering.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 20:00:53 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 23:24:41 GMT'}]",2020-10-07,"[['Nie', 'Allen', ''], ['Cohn-Gordon', 'Reuben', ''], ['Potts', 'Christopher', '']]"
1278551,2004.13781,Shu Cheng Li,"Shucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu, Fengyuan Xu and Sheng
  Zhong","Graph-to-Tree Neural Networks for Learning Structured Input-Output
  Translation with Applications to Semantic Parsing and Math Word Problem",Long Paper in EMNLP 2020. 12 pages including references,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The celebrated Seq2Seq technique and its numerous variants achieve excellent
performance on many tasks such as neural machine translation, semantic parsing,
and math word problem solving. However, these models either only consider input
objects as sequences while ignoring the important structural information for
encoding, or they simply treat output objects as sequence outputs instead of
structural objects for decoding. In this paper, we present a novel
Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder
and a hierarchical tree decoder, that encodes an augmented graph-structured
input and decodes a tree-structured output. In particular, we investigated our
model for solving two problems, neural semantic parsing and math word problem.
Our extensive experiments demonstrate that our Graph2Tree model outperforms or
matches the performance of other state-of-the-art models on these tasks.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 17:36:38 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 09:07:57 GMT'}]",2020-10-07,"[['Li', 'Shucheng', ''], ['Wu', 'Lingfei', ''], ['Feng', 'Shiwei', ''], ['Xu', 'Fangli', ''], ['Xu', 'Fengyuan', ''], ['Zhong', 'Sheng', '']]"
1198456,1910.14537,Sufeng Duan,"Sufeng Duan, Hai Zhao",Attention Is All You Need for Chinese Word Segmentation,"11 pages, to appear in EMNLP 2020 as a long paper",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Taking greedy decoding algorithm as it should be, this work focuses on
further strengthening the model itself for Chinese word segmentation (CWS),
which results in an even more fast and more accurate CWS model. Our model
consists of an attention only stacked encoder and a light enough decoder for
the greedy segmentation plus two highway connections for smoother training, in
which the encoder is composed of a newly proposed Transformer variant,
Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer.
With the effective encoder design, our model only needs to take unigram
features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark
datasets. The experimental results show that with the highest segmentation
speed, the proposed model achieves new state-of-the-art or comparable
performance against strong baselines in terms of strict closed test setting.
","[{'version': 'v1', 'created': 'Thu, 31 Oct 2019 15:32:19 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Apr 2020 13:17:01 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 06:38:42 GMT'}]",2020-10-07,"[['Duan', 'Sufeng', ''], ['Zhao', 'Hai', '']]"
1357812,2010.01482,HaiYing Wang,Haim Bar and HaiYing Wang,Reproducible Science with LaTeX,,,,,cs.SE cs.CL stat.CO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper proposes a procedure to execute external source codes from a LaTeX
document and include the calculation outputs in the resulting Portable Document
Format (pdf) file automatically. It integrates programming tools into the LaTeX
writing tool to facilitate the production of reproducible research. In our
proposed approach to a LaTeX-based scientific notebook the user can easily
invoke any programming language or a command-line program when compiling the
LaTeX document, while using their favorite LaTeX editor in the writing process.
The required LaTeX setup, a new Python package, and the defined preamble are
discussed in detail, and working examples using R, Julia, and MatLab to
reproduce existing research are provided to illustrate the proposed procedure.
We also demonstrate how to include system setting information in a paper by
invoking shell scripts when compiling the document.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 04:04:07 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 01:54:58 GMT'}]",2020-10-07,"[['Bar', 'Haim', ''], ['Wang', 'HaiYing', '']]"
1274921,2004.10151,Jesse Thomason,"Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua
  Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May,
  Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian",Experience Grounds Language,"Empirical Methods in Natural Language Processing (EMNLP), 2020",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language understanding research is held back by a failure to relate language
to the physical world it describes and to the social interactions it
facilitates. Despite the incredible effectiveness of language processing models
to tackle tasks after being trained on text alone, successful linguistic
communication relies on a shared experience of the world. It is this shared
experience that makes utterances meaningful.
  Natural language processing is a diverse field, and progress throughout its
development has come from new representational theories, modeling techniques,
data collection paradigms, and tasks. We posit that the present success of
representation learning approaches trained on large, text-only corpora requires
the parallel tradition of research on the broader physical and social context
of language to address the deeper questions of communication.
","[{'version': 'v1', 'created': 'Tue, 21 Apr 2020 16:56:27 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 02:03:56 GMT'}]",2020-10-07,"[['Bisk', 'Yonatan', ''], ['Holtzman', 'Ari', ''], ['Thomason', 'Jesse', ''], ['Andreas', 'Jacob', ''], ['Bengio', 'Yoshua', ''], ['Chai', 'Joyce', ''], ['Lapata', 'Mirella', ''], ['Lazaridou', 'Angeliki', ''], ['May', 'Jonathan', ''], ['Nisnevich', 'Aleksandr', ''], ['Pinto', 'Nicolas', ''], ['Turian', 'Joseph', '']]"
1274872,2004.10102,Goro Kobayashi,"Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui",Attention is Not Only a Weight: Analyzing Transformers with Vector Norms,"19 pages, accepted by EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Attention is a key component of Transformers, which have recently achieved
considerable success in natural language processing. Hence, attention is being
extensively studied to investigate various linguistic capabilities of
Transformers, focusing on analyzing the parallels between attention weights and
specific linguistic phenomena. This paper shows that attention weights alone
are only one of the two factors that determine the output of attention and
proposes a norm-based analysis that incorporates the second factor, the norm of
the transformed input vectors. The findings of our norm-based analyses of BERT
and a Transformer-based neural machine translation system include the
following: (i) contrary to previous studies, BERT pays poor attention to
special tokens, and (ii) reasonable word alignment can be extracted from
attention mechanisms of Transformer. These findings provide insights into the
inner workings of Transformers.
","[{'version': 'v1', 'created': 'Tue, 21 Apr 2020 15:22:27 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 15:15:38 GMT'}]",2020-10-07,"[['Kobayashi', 'Goro', ''], ['Kuribayashi', 'Tatsuki', ''], ['Yokoi', 'Sho', ''], ['Inui', 'Kentaro', '']]"
1280523,2005.00728,Jesse Thomason,"Homero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz,
  Jianfeng Gao",RMM: A Recursive Mental Model for Dialog Navigation,"Findings of Empirical Methods in Natural Language Processing (EMNLP
  Findings), 2020",,,,cs.CL cs.AI cs.CV cs.LG cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language-guided robots must be able to both ask humans questions and
understand answers. Much existing work focuses only on the latter. In this
paper, we go beyond instruction following and introduce a two-agent task where
one agent navigates and asks questions that a second, guiding agent answers.
Inspired by theory of mind, we propose the Recursive Mental Model (RMM). The
navigating agent models the guiding agent to simulate answers given candidate
generated questions. The guiding agent in turn models the navigating agent to
simulate navigation steps it would take to generate answers. We use the
progress agents make towards the goal as a reinforcement learning reward signal
to directly inform not only navigation actions, but also both question and
answer generation. We demonstrate that RMM enables better generalization to
novel environments. Interlocutor modelling may be a way forward for human-agent
dialogue where robots need to both ask and answer questions.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 06:57:14 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 02:16:27 GMT'}]",2020-10-07,"[['Roman', 'Homero Roman', ''], ['Bisk', 'Yonatan', ''], ['Thomason', 'Jesse', ''], ['Celikyilmaz', 'Asli', ''], ['Gao', 'Jianfeng', '']]"
1268780,2004.04010,Fahim Dalvi,"Fahim Dalvi, Hassan Sajjad, Nadir Durrani and Yonatan Belinkov",Analyzing Redundancy in Pretrained Transformer Models,"19 Pages, 14 figures, EMNLP 2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer-based deep NLP models are trained using hundreds of millions of
parameters, limiting their applicability in computationally constrained
environments. In this paper, we study the cause of these limitations by
defining a notion of Redundancy, which we categorize into two classes: General
Redundancy and Task-specific Redundancy. We dissect two popular pretrained
models, BERT and XLNet, studying how much redundancy they exhibit at a
representation-level and at a more fine-grained neuron-level. Our analysis
reveals interesting insights, such as: i) 85% of the neurons across the network
are redundant and ii) at least 92% of them can be removed when optimizing
towards a downstream task. Based on our analysis, we present an efficient
feature-based transfer learning procedure, which maintains 97% performance
while using at-most 10% of the original neurons.
","[{'version': 'v1', 'created': 'Wed, 8 Apr 2020 14:29:23 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 11:45:07 GMT'}]",2020-10-07,"[['Dalvi', 'Fahim', ''], ['Sajjad', 'Hassan', ''], ['Durrani', 'Nadir', ''], ['Belinkov', 'Yonatan', '']]"
1271640,2004.06870,Deming Ye,"Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun,
  Zhiyuan Liu",Coreferential Reasoning Learning for Language Representation,Accepted by EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language representation models such as BERT could effectively capture
contextual semantic information from plain text, and have been proved to
achieve promising results in lots of downstream NLP tasks with appropriate
fine-tuning. However, most existing language representation models cannot
explicitly handle coreference, which is essential to the coherent understanding
of the whole discourse. To address this issue, we present CorefBERT, a novel
language representation model that can capture the coreferential relations in
context. The experimental results show that, compared with existing baseline
models, CorefBERT can achieve significant improvements consistently on various
downstream NLP tasks that require coreferential reasoning, while maintaining
comparable performance to previous models on other common NLP tasks. The source
code and experiment details of this paper can be obtained from
https://github.com/thunlp/CorefBERT.
","[{'version': 'v1', 'created': 'Wed, 15 Apr 2020 03:57:45 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 12:37:53 GMT'}]",2020-10-07,"[['Ye', 'Deming', ''], ['Lin', 'Yankai', ''], ['Du', 'Jiaju', ''], ['Liu', 'Zhenghao', ''], ['Li', 'Peng', ''], ['Sun', 'Maosong', ''], ['Liu', 'Zhiyuan', '']]"
1358646,2010.02316,Ameet Deshpande,"Ameet Deshpande, Eve Fleisig",Sentiment Analysis for Reinforcement Learning,Work in progress,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While reinforcement learning (RL) has been successful in natural language
processing (NLP) domains such as dialogue generation and text-based games, it
typically faces the problem of sparse rewards that leads to slow or no
convergence. Traditional methods that use text descriptions to extract only a
state representation ignore the feedback inherently present in them. In
text-based games, for example, descriptions like ""Good Job! You ate the food}""
indicate progress, and descriptions like ""You entered a new room"" indicate
exploration. Positive and negative cues like these can be converted to rewards
through sentiment analysis. This technique converts the sparse reward problem
into a dense one, which is easier to solve. Furthermore, this can enable
reinforcement learning without rewards, in which the agent learns entirely from
these intrinsic sentiment rewards. This framework is similar to intrinsic
motivation, where the environment does not necessarily provide the rewards, but
the agent analyzes and realizes them by itself. We find that providing dense
rewards in text-based games using sentiment analysis improves performance under
some conditions.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 20:15:51 GMT'}]",2020-10-07,"[['Deshpande', 'Ameet', ''], ['Fleisig', 'Eve', '']]"
1278778,2004.14008,Reina Akama,"Reina Akama, Sho Yokoi, Jun Suzuki, Kentaro Inui",Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness,"18 pages, Accepted at The 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2020)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale dialogue datasets have recently become available for training
neural dialogue agents. However, these datasets have been reported to contain a
non-negligible number of unacceptable utterance pairs. In this paper, we
propose a method for scoring the quality of utterance pairs in terms of their
connectivity and relatedness. The proposed scoring method is designed based on
findings widely shared in the dialogue and linguistics research communities. We
demonstrate that it has a relatively good correlation with the human judgment
of dialogue quality. Furthermore, the method is applied to filter out
potentially unacceptable utterance pairs from a large-scale noisy dialogue
corpus to ensure its quality. We experimentally confirm that training data
filtered by the proposed method improves the quality of neural dialogue agents
in response generation.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 08:08:32 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 16:19:58 GMT'}]",2020-10-07,"[['Akama', 'Reina', ''], ['Yokoi', 'Sho', ''], ['Suzuki', 'Jun', ''], ['Inui', 'Kentaro', '']]"
1222271,1912.10375,Boxin Wang,"Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang, Bo Li","T3: Tree-Autoencoder Constrained Adversarial Text Generation for
  Targeted Attack","Accepted to EMNLP 2020 as a long paper. 17 pages, 4 figures",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversarial attacks against natural language processing systems, which
perform seemingly innocuous modifications to inputs, can induce arbitrary
mistakes to the target models. Though raised great concerns, such adversarial
attacks can be leveraged to estimate the robustness of NLP models. Compared
with the adversarial example generation in continuous data domain (e.g.,
image), generating adversarial text that preserves the original meaning is
challenging since the text space is discrete and non-differentiable. To handle
these challenges, we propose a target-controllable adversarial attack framework
T3, which is applicable to a range of NLP tasks. In particular, we propose a
tree-based autoencoder to embed the discrete text data into a continuous
representation space, upon which we optimize the adversarial perturbation. A
novel tree-based decoder is then applied to regularize the syntactic
correctness of the generated text and manipulate it on either sentence
(T3(Sent)) or word (T3(Word)) level. We consider two most representative NLP
tasks: sentiment analysis and question answering (QA). Extensive experimental
results and human studies show that T3 generated adversarial texts can
successfully manipulate the NLP models to output the targeted incorrect answer
without misleading the human. Moreover, we show that the generated adversarial
texts have high transferability which enables the black-box attacks in
practice. Our work sheds light on an effective and general way to examine the
robustness of NLP models. Our code is publicly available at
https://github.com/AI-secure/T3/.
","[{'version': 'v1', 'created': 'Sun, 22 Dec 2019 03:02:42 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 02:29:20 GMT'}]",2020-10-07,"[['Wang', 'Boxin', ''], ['Pei', 'Hengzhi', ''], ['Pan', 'Boyuan', ''], ['Chen', 'Qian', ''], ['Wang', 'Shuohang', ''], ['Li', 'Bo', '']]"
1359023,2010.02693,Liang Ding,"Di Wu, Liang Ding, Fan Lu and Jian Xie","SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection
  and Slot Filling",To appear in main conference of EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Slot filling and intent detection are two main tasks in spoken language
understanding (SLU) system. In this paper, we propose a novel
non-autoregressive model named SlotRefine for joint intent detection and slot
filling. Besides, we design a novel two-pass iteration mechanism to handle the
uncoordinated slots problem caused by conditional independence of
non-autoregressive model. Experiments demonstrate that our model significantly
outperforms previous models in slot filling task, while considerably speeding
up the decoding (up to X 10.77). In-depth analyses show that 1) pretraining
schemes could further enhance our model; 2) two-pass mechanism indeed remedy
the uncoordinated slots.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 13:16:53 GMT'}]",2020-10-07,"[['Wu', 'Di', ''], ['Ding', 'Liang', ''], ['Lu', 'Fan', ''], ['Xie', 'Jian', '']]"
1358635,2010.02305,Jatin Ganhotra,"Jatin Ganhotra, Haggai Roitman, Doron Cohen, Nathaniel Mills, Chulaka
  Gunasekara, Yosi Mass, Sachindra Joshi, Luis Lastras and David Konopnicki",Conversational Document Prediction to Assist Customer Care Agents,"EMNLP 2020. The released Twitter dataset is available at:
  https://github.com/IBM/twitter-customer-care-document-prediction",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  A frequent pattern in customer care conversations is the agents responding
with appropriate webpage URLs that address users' needs. We study the task of
predicting the documents that customer care agents can use to facilitate users'
needs. We also introduce a new public dataset which supports the aforementioned
problem. Using this dataset and two others, we investigate state-of-the art
deep learning (DL) and information retrieval (IR) models for the task.
Additionally, we analyze the practicality of such systems in terms of inference
time complexity. Our show that an hybrid IR+DL approach provides the best of
both worlds.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 19:53:41 GMT'}]",2020-10-07,"[['Ganhotra', 'Jatin', ''], ['Roitman', 'Haggai', ''], ['Cohen', 'Doron', ''], ['Mills', 'Nathaniel', ''], ['Gunasekara', 'Chulaka', ''], ['Mass', 'Yosi', ''], ['Joshi', 'Sachindra', ''], ['Lastras', 'Luis', ''], ['Konopnicki', 'David', '']]"
1351950,2009.10447,Hwaran Lee,"Hwaran Lee, Seokhwan Jo, HyungJun Kim, Sangkeun Jung, Tae-Yoon Kim","SUMBT+LaRL: End-to-end Neural Task-oriented Dialog System with
  Reinforcement Learning","13 pages, 5 figures. This work has been submitted to the IEEE/ACM
  Transactions on Audio Speech and Language Processing for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recent advent of neural approaches for developing each dialog component
in task-oriented dialog systems has remarkably improved, yet optimizing the
overall system performance remains a challenge. In this paper, we propose an
end-to-end trainable neural dialog system with reinforcement learning, named
SUMBT+LaRL. The SUMBT+ estimates user-acts as well as dialog belief states, and
the LaRL models latent system action spaces and generates responses given the
estimated contexts. We experimentally demonstrate that the training framework
in which the SUMBT+ and LaRL are separately pretrained and then the entire
system is fine-tuned significantly increases dialog success rates. We propose
new success criteria for reinforcement learning to the end-to-end dialog system
as well as provide experimental analysis on a different result aspect depending
on the success criteria and evaluation methods. Consequently, our model
achieved the new state-of-the-art success rate of 85.4% on corpus-based
evaluation, and a comparable success rate of 81.40% on simulator-based
evaluation provided by the DSTC8 challenge.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 11:02:21 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 02:17:27 GMT'}]",2020-10-07,"[['Lee', 'Hwaran', ''], ['Jo', 'Seokhwan', ''], ['Kim', 'HyungJun', ''], ['Jung', 'Sangkeun', ''], ['Kim', 'Tae-Yoon', '']]"
1171173,1909.01135,Chidimma Opara,"Chidimma Opara, Bo Wei, and Yingke Chen","HTMLPhish: Enabling Phishing Web Page Detection by Applying Deep
  Learning Techniques on HTML Analysis",,,10.1109/IJCNN48605.2020.9207707,,cs.CR cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, the development and implementation of phishing attacks require
little technical skills and costs. This uprising has led to an ever-growing
number of phishing attacks on the World Wide Web. Consequently, proactive
techniques to fight phishing attacks have become extremely necessary. In this
paper, we propose HTMLPhish, a deep learning based data-driven end-to-end
automatic phishing web page classification approach. Specifically, HTMLPhish
receives the content of the HTML document of a web page and employs
Convolutional Neural Networks (CNNs) to learn the semantic dependencies in the
textual contents of the HTML. The CNNs learn appropriate feature
representations from the HTML document embeddings without extensive manual
feature engineering. Furthermore, our proposed approach of the concatenation of
the word and character embeddings allows our model to manage new features and
ensure easy extrapolation to test data. We conduct comprehensive experiments on
a dataset of more than 50,000 HTML documents that provides a distribution of
phishing to benign web pages obtainable in the real-world that yields over 93
percent Accuracy and True Positive Rate. Also, HTMLPhish is a completely
language-independent and client-side strategy which can, therefore, conduct web
page phishing detection regardless of the textual language.
","[{'version': 'v1', 'created': 'Wed, 28 Aug 2019 23:58:50 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Feb 2020 17:10:56 GMT'}, {'version': 'v3', 'created': 'Fri, 15 May 2020 10:30:32 GMT'}]",2020-10-07,"[['Opara', 'Chidimma', ''], ['Wei', 'Bo', ''], ['Chen', 'Yingke', '']]"
1265938,2004.01168,Tara Safavi,"Tara Safavi, Danai Koutra, Edgar Meij","Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy
  Link Prediction",EMNLP 2020,,,,cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Little is known about the trustworthiness of predictions made by knowledge
graph embedding (KGE) models. In this paper we take initial steps toward this
direction by investigating the calibration of KGE models, or the extent to
which they output confidence scores that reflect the expected correctness of
predicted knowledge graph triples. We first conduct an evaluation under the
standard closed-world assumption (CWA), in which predicted triples not already
in the knowledge graph are considered false, and show that existing calibration
techniques are effective for KGE under this common but narrow assumption. Next,
we introduce the more realistic but challenging open-world assumption (OWA), in
which unobserved predictions are not considered true or false until
ground-truth labels are obtained. Here, we show that existing calibration
techniques are much less effective under the OWA than the CWA, and provide
explanations for this discrepancy. Finally, to motivate the utility of
calibration for KGE from a practitioner's perspective, we conduct a unique case
study of human-AI collaboration, showing that calibrated predictions can
improve human performance in a knowledge graph completion task.
","[{'version': 'v1', 'created': 'Thu, 2 Apr 2020 17:46:47 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Sep 2020 16:02:54 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 09:31:15 GMT'}]",2020-10-07,"[['Safavi', 'Tara', ''], ['Koutra', 'Danai', ''], ['Meij', 'Edgar', '']]"
1358559,2010.02229,Xusen Yin,"Xusen Yin, Ralph Weischedel, Jonathan May",Learning to Generalize for Sequential Decision Making,"Findings of EMNLP2020, 18 pages",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider problems of making sequences of decisions to accomplish tasks,
interacting via the medium of language. These problems are often tackled with
reinforcement learning approaches. We find that these models do not generalize
well when applied to novel task domains. However, the large amount of
computation necessary to adequately train and explore the search space of
sequential decision making, under a reinforcement learning paradigm, precludes
the inclusion of large contextualized language models, which might otherwise
enable the desired generalization ability. We introduce a teacher-student
imitation learning methodology and a means of converting a reinforcement
learning model into a natural language understanding model. Together, these
methodologies enable the introduction of contextualized language models into
the sequential decision making problem space. We show that models can learn
faster and generalize more, leveraging both the imitation learning and the
reformulation. Our models exceed teacher performance on various held-out
decision problems, by up to 7% on in-domain problems and 24% on out-of-domain
problems.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 18:00:03 GMT'}]",2020-10-07,"[['Yin', 'Xusen', ''], ['Weischedel', 'Ralph', ''], ['May', 'Jonathan', '']]"
1358223,2010.01893,Shaoxiong Feng,"Shaoxiong Feng, Xuancheng Ren, Hongshen Chen, Bin Sun, Kan Li, Xu Sun",Regularizing Dialogue Generation by Imitating Implicit Scenarios,Accepted by EMNLP 2020 (long paper),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human dialogues are scenario-based and appropriate responses generally relate
to the latent context knowledge entailed by the specific scenario. To enable
responses that are more meaningful and context-specific, we propose to improve
generative dialogue systems from the scenario perspective, where both dialogue
history and future conversation are taken into account to implicitly
reconstruct the scenario knowledge. More importantly, the conversation
scenarios are further internalized using imitation learning framework, where
the conventional dialogue model that has no access to future conversations is
effectively regularized by transferring the scenario knowledge contained in
hierarchical supervising signals from the scenario-based dialogue model, so
that the future conversation is not required in actual inference. Extensive
evaluations show that our approach significantly outperforms state-of-the-art
baselines on diversity and relevance, and expresses scenario-specific
knowledge.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 10:10:19 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 05:51:09 GMT'}]",2020-10-07,"[['Feng', 'Shaoxiong', ''], ['Ren', 'Xuancheng', ''], ['Chen', 'Hongshen', ''], ['Sun', 'Bin', ''], ['Li', 'Kan', ''], ['Sun', 'Xu', '']]"
1273131,2004.08361,Anjalie Field,"Anjalie Field, Yulia Tsvetkov",Unsupervised Discovery of Implicit Gender Bias,Accepted to EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite their prevalence in society, social biases are difficult to identify,
primarily because human judgements in this domain can be unreliable. We take an
unsupervised approach to identifying gender bias against women at a comment
level and present a model that can surface text likely to contain bias. Our
main challenge is forcing the model to focus on signs of implicit bias, rather
than other artifacts in the data. Thus, our methodology involves reducing the
influence of confounds through propensity matching and adversarial learning.
Our analysis shows how biased comments directed towards female politicians
contain mixed criticisms, while comments directed towards other female public
figures focus on appearance and sexualization. Ultimately, our work offers a
way to capture subtle biases in various domains without relying on subjective
human judgements.
","[{'version': 'v1', 'created': 'Fri, 17 Apr 2020 17:36:20 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 16:43:42 GMT'}]",2020-10-07,"[['Field', 'Anjalie', ''], ['Tsvetkov', 'Yulia', '']]"
1319293,2007.07779,Jonas Pfeiffer,"Jonas Pfeiffer, Andreas R\""uckl\'e, Clifton Poth, Aishwarya Kamath,
  Ivan Vuli\'c, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych",AdapterHub: A Framework for Adapting Transformers,EMNLP 2020: Systems Demonstrations,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The current modus operandi in NLP involves downloading and fine-tuning
pre-trained models consisting of millions or billions of parameters. Storing
and sharing such large trained models is expensive, slow, and time-consuming,
which impedes progress towards more general and versatile NLP methods that
learn from and for many tasks. Adapters -- small learnt bottleneck layers
inserted within each layer of a pre-trained model -- ameliorate this issue by
avoiding full fine-tuning of the entire model. However, sharing and integrating
adapter layers is not straightforward. We propose AdapterHub, a framework that
allows dynamic ""stitching-in"" of pre-trained adapters for different tasks and
languages. The framework, built on top of the popular HuggingFace Transformers
library, enables extremely easy and quick adaptations of state-of-the-art
pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages.
Downloading, sharing, and training adapters is as seamless as possible using
minimal changes to the training scripts and a specialized infrastructure. Our
framework enables scalable and easy access to sharing of task-specific models,
particularly in low-resource scenarios. AdapterHub includes all recent adapter
architectures and can be found at https://AdapterHub.ml.
","[{'version': 'v1', 'created': 'Wed, 15 Jul 2020 15:56:05 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 15:22:21 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 10:16:39 GMT'}]",2020-10-07,"[['Pfeiffer', 'Jonas', ''], ['Rücklé', 'Andreas', ''], ['Poth', 'Clifton', ''], ['Kamath', 'Aishwarya', ''], ['Vulić', 'Ivan', ''], ['Ruder', 'Sebastian', ''], ['Cho', 'Kyunghyun', ''], ['Gurevych', 'Iryna', '']]"
1280366,2005.00571,Deren Lei,"Deren Lei and Gangrong Jiang and Xiaotao Gu and Kexuan Sun and Yuning
  Mao and Xiang Ren","Learning Collaborative Agents with Rule Guidance for Knowledge Graph
  Reasoning",EMNLP 2020,,,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Walk-based models have shown their advantages in knowledge graph (KG)
reasoning by achieving decent performance while providing interpretable
decisions. However, the sparse reward signals offered by the KG during
traversal are often insufficient to guide a sophisticated walk-based
reinforcement learning (RL) model. An alternate approach is to use traditional
symbolic methods (e.g., rule induction), which achieve good performance but can
be hard to generalize due to the limitation of symbolic representation. In this
paper, we propose RuleGuider, which leverages high-quality rules generated by
symbolic-based methods to provide reward supervision for walk-based agents.
Experiments on benchmark datasets show that RuleGuider improves the performance
of walk-based models without losing interpretability.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 18:57:14 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 10:13:30 GMT'}]",2020-10-07,"[['Lei', 'Deren', ''], ['Jiang', 'Gangrong', ''], ['Gu', 'Xiaotao', ''], ['Sun', 'Kexuan', ''], ['Mao', 'Yuning', ''], ['Ren', 'Xiang', '']]"
1358631,2010.02301,Xinyu Hua,Xinyu Hua and Lu Wang,"PAIR: Planning and Iterative Refinement in Pre-trained Transformers for
  Long Text Generation",Accepted at EMNLP 2020 as a long paper,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained Transformers have enabled impressive breakthroughs in generating
long and fluent text, yet their outputs are often ""rambling"" without coherently
arranged content. In this work, we present a novel content-controlled text
generation framework, PAIR, with planning and iterative refinement, which is
built upon a large model, BART. We first adapt the BERT model to automatically
construct the content plans, consisting of keyphrase assignments and their
corresponding sentence-level positions. The BART model is employed for
generation without modifying its structure. We then propose a refinement
algorithm to gradually enhance the generation quality within the
sequence-to-sequence framework. Evaluation with automatic metrics shows that
adding planning consistently improves the generation quality on three distinct
domains, with an average of 20 BLEU points and 12 METEOR points improvements.
In addition, human judges rate our system outputs to be more relevant and
coherent than comparisons without planning.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 19:45:03 GMT'}]",2020-10-07,"[['Hua', 'Xinyu', ''], ['Wang', 'Lu', '']]"
1272117,2004.07347,Wenhu Chen,"Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William
  Wang","HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and
  Textual Data",Accepted to Proceedings of EMNLP 2020 (Findings),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing question answering datasets focus on dealing with homogeneous
information, based either only on text or KB/Table information alone. However,
as human knowledge is distributed over heterogeneous forms, using homogeneous
information alone might lead to severe coverage problems. To fill in the gap,
we present HybridQA https://github.com/wenhuchen/HybridQA, a new large-scale
question-answering dataset that requires reasoning on heterogeneous
information. Each question is aligned with a Wikipedia table and multiple
free-form corpora linked with the entities in the table. The questions are
designed to aggregate both tabular information and text information, i.e., lack
of either form would render the question unanswerable. We test with three
different models: 1) a table-only model. 2) text-only model. 3) a hybrid model
that combines heterogeneous information to find the answer. The experimental
results show that the EM scores obtained by two baselines are below 20\%, while
the hybrid model can achieve an EM over 40\%. This gap suggests the necessity
to aggregate heterogeneous information in HybridQA. However, the hybrid model's
score is still far behind human performance. Hence, HybridQA can serve as a
challenging benchmark to study question answering with heterogeneous
information.
","[{'version': 'v1', 'created': 'Wed, 15 Apr 2020 21:18:15 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 06:52:11 GMT'}]",2020-10-07,"[['Chen', 'Wenhu', ''], ['Zha', 'Hanwen', ''], ['Chen', 'Zhiyu', ''], ['Xiong', 'Wenhan', ''], ['Wang', 'Hong', ''], ['Wang', 'William', '']]"
1280538,2005.00743,Yi Tay,"Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng",Synthesizer: Rethinking Self-Attention in Transformer Models,,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The dot product self-attention is known to be central and indispensable to
state-of-the-art Transformer models. But is it really required? This paper
investigates the true importance and contribution of the dot product-based
self-attention mechanism on the performance of Transformer models. Via
extensive experiments, we find that (1) random alignment matrices surprisingly
perform quite competitively and (2) learning attention weights from token-token
(query-key) interactions is useful but not that important after all. To this
end, we propose \textsc{Synthesizer}, a model that learns synthetic attention
weights without token-token interactions. In our experiments, we first show
that simple Synthesizers achieve highly competitive performance when compared
against vanilla Transformer models across a range of tasks, including machine
translation, language modeling, text generation and GLUE/SuperGLUE benchmarks.
When composed with dot product attention, we find that Synthesizers
consistently outperform Transformers. Moreover, we conduct additional
comparisons of Synthesizers against Dynamic Convolutions, showing that simple
Random Synthesizer is not only $60\%$ faster but also improves perplexity by a
relative $3.5\%$. Finally, we show that simple factorized Synthesizers can
outperform Linformers on encoding only tasks.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 08:16:19 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 12:16:06 GMT'}]",2020-10-07,"[['Tay', 'Yi', ''], ['Bahri', 'Dara', ''], ['Metzler', 'Donald', ''], ['Juan', 'Da-Cheng', ''], ['Zhao', 'Zhe', ''], ['Zheng', 'Che', '']]"
1269491,2004.04721,Mikel Artetxe,"Mikel Artetxe, Gorka Labaka, Eneko Agirre",Translation Artifacts in Cross-lingual Transfer Learning,EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Both human and machine translation play a central role in cross-lingual
transfer learning: many multilingual datasets have been created through
professional translation services, and using machine translation to translate
either the test set or the training set is a widely used transfer technique. In
this paper, we show that such translation process can introduce subtle
artifacts that have a notable impact in existing cross-lingual models. For
instance, in natural language inference, translating the premise and the
hypothesis independently can reduce the lexical overlap between them, which
current models are highly sensitive to. We show that some previous findings in
cross-lingual transfer learning need to be reconsidered in the light of this
phenomenon. Based on the gained insights, we also improve the state-of-the-art
in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points,
respectively.
","[{'version': 'v1', 'created': 'Thu, 9 Apr 2020 17:54:30 GMT'}, {'version': 'v2', 'created': 'Tue, 14 Apr 2020 16:06:53 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 00:42:41 GMT'}]",2020-10-07,"[['Artetxe', 'Mikel', ''], ['Labaka', 'Gorka', ''], ['Agirre', 'Eneko', '']]"
1359212,2010.02882,Nathaniel Weir,"Nathaniel Weir, Jo\~ao Sedoc, and Benjamin Van Durme",COD3S: Diverse Generation with Discrete Semantic Signatures,EMNLP2020 preprint,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present COD3S, a novel method for generating semantically diverse
sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an
input, seq2seq models typically produce semantically and syntactically
homogeneous sets of sentences and thus perform poorly on one-to-many sequence
generation tasks. Our two-stage approach improves output diversity by
conditioning generation on locality-sensitive hash (LSH)-based semantic
sentence codes whose Hamming distances highly correlate with human judgments of
semantic textual similarity. Though it is generally applicable, we apply COD3S
to causal generation, the task of predicting a proposition's plausible causes
or effects. We demonstrate through automatic and human evaluation that
responses produced using our method exhibit improved diversity without
degrading task performance.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 17:06:50 GMT'}]",2020-10-07,"[['Weir', 'Nathaniel', ''], ['Sedoc', 'João', ''], ['Van Durme', 'Benjamin', '']]"
1279390,2004.14620,Tomasz Limisiewicz,Tomasz Limisiewicz and Rudolf Rosa and David Mare\v{c}ek,"Universal Dependencies according to BERT: both more specific and more
  general",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This work focuses on analyzing the form and extent of syntactic abstraction
captured by BERT by extracting labeled dependency trees from self-attentions.
  Previous work showed that individual BERT heads tend to encode particular
dependency relation types. We extend these findings by explicitly comparing
BERT relations to Universal Dependencies (UD) annotations, showing that they
often do not match one-to-one.
  We suggest a method for relation identification and syntactic tree
construction. Our approach produces significantly more consistent dependency
trees than previous work, showing that it better explains the syntactic
abstractions in BERT. At the same time, it can be successfully applied with
only a minimal amount of supervision and generalizes well across languages.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 07:48:07 GMT'}, {'version': 'v2', 'created': 'Fri, 1 May 2020 00:34:10 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 10:22:33 GMT'}]",2020-10-07,"[['Limisiewicz', 'Tomasz', ''], ['Rosa', 'Rudolf', ''], ['Mareček', 'David', '']]"
1358683,2010.02353,Julia Kreutzer,"Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa,
  Tajudeen Kolawole, Taiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddee
  Hassan Muhammad, Salomon Kabongo, Salomey Osei, Sackey Freshia, Rubungo Andre
  Niyongabo, Ricky Macharm, Perez Ogayo, Orevaoghene Ahia, Musie Meressa, Mofe
  Adeyemi, Masabata Mokgesi-Selinga, Lawrence Okegbemi, Laura Jane Martinus,
  Kolawole Tajudeen, Kevin Degila, Kelechi Ogueji, Kathleen Siminyu, Julia
  Kreutzer, Jason Webster, Jamiil Toure Ali, Jade Abbott, Iroro Orife, Ignatius
  Ezeani, Idris Abdulkabir Dangana, Herman Kamper, Hady Elsahar, Goodness Duru,
  Ghollah Kioko, Espoir Murhabazi, Elan van Biljon, Daniel Whitenack,
  Christopher Onyefuluchi, Chris Emezue, Bonaventure Dossou, Blessing Sibanda,
  Blessing Itoro Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp \""Oktem,
  Adewale Akinfaderin, Abdallah Bashir","Participatory Research for Low-resourced Machine Translation: A Case
  Study in African Languages",Findings of EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Research in NLP lacks geographic diversity, and the question of how NLP can
be scaled to low-resourced languages has not yet been adequately solved.
""Low-resourced""-ness is a complex problem going beyond data availability and
reflects systemic problems in society. In this paper, we focus on the task of
Machine Translation (MT), that plays a crucial role for information
accessibility and communication worldwide. Despite immense improvements in MT
over the past decade, MT is centered around a few high-resourced languages. As
MT researchers cannot solve the problem of low-resourcedness alone, we propose
participatory research as a means to involve all necessary agents required in
the MT development process. We demonstrate the feasibility and scalability of
participatory research with a case study on MT for African languages. Its
implementation leads to a collection of novel translation datasets, MT
benchmarks for over 30 languages, with human evaluations for a third of them,
and enables participants without formal training to make a unique scientific
contribution. Benchmarks, models, data, code, and evaluation results are
released under https://github.com/masakhane-io/masakhane-mt.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 21:50:38 GMT'}]",2020-10-07,"[['Nekoto', 'Wilhelmina', ''], ['Marivate', 'Vukosi', ''], ['Matsila', 'Tshinondiwa', ''], ['Fasubaa', 'Timi', ''], ['Kolawole', 'Tajudeen', ''], ['Fagbohungbe', 'Taiwo', ''], ['Akinola', 'Solomon Oluwole', ''], ['Muhammad', 'Shamsuddee Hassan', ''], ['Kabongo', 'Salomon', ''], ['Osei', 'Salomey', ''], ['Freshia', 'Sackey', ''], ['Niyongabo', 'Rubungo Andre', ''], ['Macharm', 'Ricky', ''], ['Ogayo', 'Perez', ''], ['Ahia', 'Orevaoghene', ''], ['Meressa', 'Musie', ''], ['Adeyemi', 'Mofe', ''], ['Mokgesi-Selinga', 'Masabata', ''], ['Okegbemi', 'Lawrence', ''], ['Martinus', 'Laura Jane', ''], ['Tajudeen', 'Kolawole', ''], ['Degila', 'Kevin', ''], ['Ogueji', 'Kelechi', ''], ['Siminyu', 'Kathleen', ''], ['Kreutzer', 'Julia', ''], ['Webster', 'Jason', ''], ['Ali', 'Jamiil Toure', ''], ['Abbott', 'Jade', ''], ['Orife', 'Iroro', ''], ['Ezeani', 'Ignatius', ''], ['Dangana', 'Idris Abdulkabir', ''], ['Kamper', 'Herman', ''], ['Elsahar', 'Hady', ''], ['Duru', 'Goodness', ''], ['Kioko', 'Ghollah', ''], ['Murhabazi', 'Espoir', ''], ['van Biljon', 'Elan', ''], ['Whitenack', 'Daniel', ''], ['Onyefuluchi', 'Christopher', ''], ['Emezue', 'Chris', ''], ['Dossou', 'Bonaventure', ''], ['Sibanda', 'Blessing', ''], ['Bassey', 'Blessing Itoro', ''], ['Olabiyi', 'Ayodele', ''], ['Ramkilowan', 'Arshath', ''], ['Öktem', 'Alp', ''], ['Akinfaderin', 'Adewale', ''], ['Bashir', 'Abdallah', '']]"
1358569,2010.02239,Katharina Kann,Rajat Agarwal and Katharina Kann,Acrostic Poem Generation,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a new task in the area of computational creativity: acrostic poem
generation in English. Acrostic poems are poems that contain a hidden message;
typically, the first letter of each line spells out a word or short phrase. We
define the task as a generation task with multiple constraints: given an input
word, 1) the initial letters of each line should spell out the provided word,
2) the poem's semantics should also relate to it, and 3) the poem should
conform to a rhyming scheme. We further provide a baseline model for the task,
which consists of a conditional neural language model in combination with a
neural rhyming model. Since no dedicated datasets for acrostic poem generation
exist, we create training data for our task by first training a separate topic
prediction model on a small set of topic-annotated poems and then predicting
topics for additional poems. Our experiments show that the acrostic poems
generated by our baseline are received well by humans and do not lose much
quality due to the additional constraints. Last, we confirm that poems
generated by our model are indeed closely related to the provided prompts, and
that pretraining on Wikipedia can boost performance.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 18:00:15 GMT'}]",2020-10-07,"[['Agarwal', 'Rajat', ''], ['Kann', 'Katharina', '']]"
1279771,2004.15001,Phillip Keung,"Phillip Keung, Yichao Lu, Julian Salazar, Vikas Bhardwaj","Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of
  Contextual Embeddings",To appear in EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual contextual embeddings have demonstrated state-of-the-art
performance in zero-shot cross-lingual transfer learning, where multilingual
BERT is fine-tuned on one source language and evaluated on a different target
language. However, published results for mBERT zero-shot accuracy vary as much
as 17 points on the MLDoc classification task across four papers. We show that
the standard practice of using English dev accuracy for model selection in the
zero-shot setting makes it difficult to obtain reproducible results on the
MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even
anti-correlated) with target language accuracy, and zero-shot performance
varies greatly at different points in the same fine-tuning run and between
different fine-tuning runs. These reproducibility issues are also present for
other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We
recommend providing oracle scores alongside zero-shot results: still fine-tune
using English data, but choose a checkpoint with the target dev set. Reporting
this upper bound makes results more consistent by avoiding arbitrarily bad
checkpoints.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:47:17 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 09:50:52 GMT'}]",2020-10-07,"[['Keung', 'Phillip', ''], ['Lu', 'Yichao', ''], ['Salazar', 'Julian', ''], ['Bhardwaj', 'Vikas', '']]"
1279053,2004.14283,Johannes Bjerva,"Johannes Bjerva, Nikita Bhutani, Behzad Golshan, Wang-Chiew Tan, and
  Isabelle Augenstein",SubjQA: A Dataset for Subjectivity and Review Comprehension,EMNLP 2020 Long Paper - Camera Ready,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Subjectivity is the expression of internal opinions or beliefs which cannot
be objectively observed or verified, and has been shown to be important for
sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is
an important aspect of user-generated data. In spite of this, subjectivity has
not been investigated in contexts where such data is widespread, such as in
question answering (QA). We therefore investigate the relationship between
subjectivity and QA, while developing a new dataset. We compare and contrast
with analyses from previous work, and verify that findings regarding
subjectivity still hold when using recently developed NLP architectures. We
find that subjectivity is also an important feature in the case of QA, albeit
with more intricate interactions between subjectivity and QA performance. For
instance, a subjective question may or may not be associated with a subjective
answer. We release an English QA dataset (SubjQA) based on customer reviews,
containing subjectivity annotations for questions and answer spans across 6
distinct domains.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 15:59:30 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 13:36:44 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 06:04:27 GMT'}]",2020-10-07,"[['Bjerva', 'Johannes', ''], ['Bhutani', 'Nikita', ''], ['Golshan', 'Behzad', ''], ['Tan', 'Wang-Chiew', ''], ['Augenstein', 'Isabelle', '']]"
1270586,2004.05816,Hyunwoo Kim,"Hyunwoo Kim, Byeongchang Kim, Gunhee Kim","Will I Sound Like Me? Improving Persona Consistency in Dialogues through
  Pragmatic Self-Consciousness",Accepted paper at EMNLP 2020 and ICLR 2020 BAICS workshop (Oral),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We explore the task of improving persona consistency of dialogue agents.
Recent models tackling consistency often train with additional Natural Language
Inference (NLI) labels or attach trained extra modules to the generative agent
for maintaining consistency. However, such additional labels and training can
be demanding. Also, we find even the best-performing persona-based agents are
insensitive to contradictory words. Inspired by social cognition and
pragmatics, we endow existing dialogue agents with public self-consciousness on
the fly through an imaginary listener. Our approach, based on the Rational
Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to
refrain from uttering contradiction. We further extend the framework by
learning the distractor selection, which has been usually done manually or
randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang
et al., 2018) dataset show that our approach reduces contradiction and improves
consistency of existing dialogue models. Moreover, we show that it can be
generalized to improve context-consistency beyond persona in dialogues.
","[{'version': 'v1', 'created': 'Mon, 13 Apr 2020 08:16:16 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 08:20:22 GMT'}]",2020-10-07,"[['Kim', 'Hyunwoo', ''], ['Kim', 'Byeongchang', ''], ['Kim', 'Gunhee', '']]"
1267921,2004.03151,Dana Ruiter,"Dana Ruiter, Josef van Genabith and Cristina Espa\~na-Bonet","Self-Induced Curriculum Learning in Self-Supervised Neural Machine
  Translation","12 pages, 5 images, to be published at EMNLP2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Self-supervised neural machine translation (SSNMT) jointly learns to identify
and select suitable training data from comparable (rather than parallel)
corpora and to translate, in a way that the two tasks support each other in a
virtuous circle. In this study, we provide an in-depth analysis of the sampling
choices the SSNMT model makes during training. We show how, without it having
been told to do so, the model self-selects samples of increasing (i) complexity
and (ii) task-relevance in combination with (iii) performing a denoising
curriculum. We observe that the dynamics of the mutual-supervision signals of
both system internal representation types are vital for the extraction and
translation performance. We show that in terms of the Gunning-Fog Readability
index, SSNMT starts extracting and learning from Wikipedia data suitable for
high school students and quickly moves towards content suitable for first year
undergraduate students.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 06:45:45 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 14:13:45 GMT'}]",2020-10-07,"[['Ruiter', 'Dana', ''], ['van Genabith', 'Josef', ''], ['España-Bonet', 'Cristina', '']]"
1286397,2005.06602,"Martin P\""omsl","Martin P\""omsl (Osnabr\""uck University) and Roman Lyapin (Cogent Labs
  Inc.)","CIRCE at SemEval-2020 Task 1: Ensembling Context-Free and
  Context-Dependent Word Representations","Accepted at SemEval-2020 Task 1 @ COLING 2020. Code available at
  https://github.com/mpoemsl/circe",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  This paper describes the winning contribution to SemEval-2020 Task 1:
Unsupervised Lexical Semantic Change Detection (Subtask 2) handed in by team UG
Student Intern. We present an ensemble model that makes predictions based on
context-free and context-dependent word representations. The key findings are
that (1) context-free word representations are a powerful and robust baseline,
(2) a sentence classification objective can be used to obtain useful
context-dependent word representations, and (3) combining those representations
increases performance on some datasets while decreasing performance on others.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 13:18:29 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jul 2020 10:10:05 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 13:50:47 GMT'}]",2020-10-07,"[['Pömsl', 'Martin', '', 'Osnabrück University'], ['Lyapin', 'Roman', '', 'Cogent Labs\n  Inc.']]"
1359233,2010.02903,Shunyu Yao,"Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan","Keep CALM and Explore: Language Models for Action Generation in
  Text-based Games",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-based games present a unique challenge for autonomous agents to operate
in natural language and handle enormous action spaces. In this paper, we
propose the Contextual Action Language Model (CALM) to generate a compact set
of action candidates at each game state. Our key insight is to train language
models on human gameplay, where people demonstrate linguistic priors and a
general game sense for promising actions conditioned on game history. We
combine CALM with a reinforcement learning agent which re-ranks the generated
action candidates to maximize in-game rewards. We evaluate our approach using
the Jericho benchmark, on games unseen by CALM during training. Our method
obtains a 69% relative improvement in average game score over the previous
state-of-the-art model. Surprisingly, on half of these games, CALM is
competitive with or better than other models that have access to ground truth
admissible actions. Code and data are available at
https://github.com/princeton-nlp/calm-textgame.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 17:36:29 GMT'}]",2020-10-07,"[['Yao', 'Shunyu', ''], ['Rao', 'Rohan', ''], ['Hausknecht', 'Matthew', ''], ['Narasimhan', 'Karthik', '']]"
1358668,2010.02338,Tianlu Wang,"Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li, Jilin Chen,
  Alex Beutel, Ed Chi","CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial
  Text Generation","6 pages, accepted to EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  NLP models are shown to suffer from robustness issues, i.e., a model's
prediction can be easily changed under small perturbations to the input. In
this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model
that, given an input text, generates adversarial texts through controllable
attributes that are known to be invariant to task labels. For example, in order
to attack a model for sentiment classification over product reviews, we can use
the product categories as the controllable attribute which would not change the
sentiment of the reviews. Experiments on real-world NLP datasets demonstrate
that our method can generate more diverse and fluent adversarial texts,
compared to many existing adversarial text generation approaches. We further
use our generated adversarial examples to improve models through adversarial
training, and we demonstrate that our generated attacks are more robust against
model re-training and different model architectures.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 21:07:45 GMT'}]",2020-10-07,"[['Wang', 'Tianlu', ''], ['Wang', 'Xuezhi', ''], ['Qin', 'Yao', ''], ['Packer', 'Ben', ''], ['Li', 'Kang', ''], ['Chen', 'Jilin', ''], ['Beutel', 'Alex', ''], ['Chi', 'Ed', '']]"
1267208,2004.02438,Xuming Hu,"Xuming Hu, Chenwei Zhang, Yusong Xu, Lijie Wen, Philip S. Yu","SelfORE: Self-supervised Relational Feature Learning for Open Relation
  Extraction","In EMNLP 2020 as a long paper. Code and data are available at
  https://github.com/THU-BPM/SelfORE",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open relation extraction is the task of extracting open-domain relation facts
from natural language sentences. Existing works either utilize heuristics or
distant-supervised annotations to train a supervised classifier over
pre-defined relations, or adopt unsupervised methods with additional
assumptions that have less discriminative power. In this work, we proposed a
self-supervised framework named SelfORE, which exploits weak, self-supervised
signals by leveraging large pretrained language model for adaptive clustering
on contextualized relational features, and bootstraps the self-supervised
signals by improving contextualized features in relation classification.
Experimental results on three datasets show the effectiveness and robustness of
SelfORE on open-domain Relation Extraction when comparing with competitive
baselines.
","[{'version': 'v1', 'created': 'Mon, 6 Apr 2020 07:23:17 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 12:32:20 GMT'}]",2020-10-07,"[['Hu', 'Xuming', ''], ['Zhang', 'Chenwei', ''], ['Xu', 'Yusong', ''], ['Wen', 'Lijie', ''], ['Yu', 'Philip S.', '']]"
1077710,1901.08163,Joohong Lee,"Joohong Lee, Sangwoo Seo and Yong Suk Choi","Semantic Relation Classification via Bidirectional LSTM Networks with
  Entity-aware Attention using Latent Entity Typing",,"Symmetry 2019, 11 (6), 785",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Classifying semantic relations between entity pairs in sentences is an
important task in Natural Language Processing (NLP). Most previous models for
relation classification rely on the high-level lexical and syntactic features
obtained by NLP tools such as WordNet, dependency parser, part-of-speech (POS)
tagger, and named entity recognizers (NER). In addition, state-of-the-art
neural models based on attention mechanisms do not fully utilize information of
entity that may be the most crucial features for relation classification. To
address these issues, we propose a novel end-to-end recurrent neural model
which incorporates an entity-aware attention mechanism with a latent entity
typing (LET) method. Our model not only utilizes entities and their latent
types as features effectively but also is more interpretable by visualizing
attention mechanisms applied to our model and results of LET. Experimental
results on the SemEval-2010 Task 8, one of the most popular relation
classification task, demonstrate that our model outperforms existing
state-of-the-art models without any high-level features.
","[{'version': 'v1', 'created': 'Wed, 23 Jan 2019 23:19:45 GMT'}]",2020-10-07,"[['Lee', 'Joohong', ''], ['Seo', 'Sangwoo', ''], ['Choi', 'Yong Suk', '']]"
1279847,2005.00052,Jonas Pfeiffer,"Jonas Pfeiffer, Ivan Vuli\'c, Iryna Gurevych, Sebastian Ruder",MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The main goal behind state-of-the-art pre-trained multilingual models such as
multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in
low-resource languages through zero-shot or few-shot cross-lingual transfer.
However, due to limited model capacity, their transfer performance is the
weakest exactly on such low-resource languages and languages unseen during
pre-training. We propose MAD-X, an adapter-based framework that enables high
portability and parameter-efficient transfer to arbitrary tasks and languages
by learning modular language and task representations. In addition, we
introduce a novel invertible adapter architecture and a strong baseline method
for adapting a pre-trained multilingual model to a new language. MAD-X
outperforms the state of the art in cross-lingual transfer across a
representative set of typologically diverse languages on named entity
recognition and causal commonsense reasoning, and achieves competitive results
on question answering. Our code and adapters are available at AdapterHub.ml
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 18:54:43 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 15:28:42 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 10:17:45 GMT'}]",2020-10-07,"[['Pfeiffer', 'Jonas', ''], ['Vulić', 'Ivan', ''], ['Gurevych', 'Iryna', ''], ['Ruder', 'Sebastian', '']]"
1358590,2010.02260,Jatin Ganhotra,"Jatin Ganhotra, Robert Moore, Sachindra Joshi and Kahini Wadhawan",Effects of Naturalistic Variation in Goal-Oriented Dialog,"Findings of EMNLP 2020. The updated datasets are available at:
  https://github.com/IBM/naturalistic-variation-goal-oriented-dialog-datasets",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Existing benchmarks used to evaluate the performance of end-to-end neural
dialog systems lack a key component: natural variation present in human
conversations. Most datasets are constructed through crowdsourcing, where the
crowd workers follow a fixed template of instructions while enacting the role
of a user/agent. This results in straight-forward, somewhat routine, and mostly
trouble-free conversations, as crowd workers do not think to represent the full
range of actions that occur naturally with real users. In this work, we
investigate the impact of naturalistic variation on two goal-oriented datasets:
bAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new
and more effective testbeds for both datasets, by introducing naturalistic
variation by the user. We observe that there is a significant drop in
performance (more than 60% in Ent. F1 on SMD and 85% in per-dialog accuracy on
bAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet
and GLMP on both datasets.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 18:13:45 GMT'}]",2020-10-07,"[['Ganhotra', 'Jatin', ''], ['Moore', 'Robert', ''], ['Joshi', 'Sachindra', ''], ['Wadhawan', 'Kahini', '']]"
1357490,2010.01160,Aditi Chaudhary,"Aditi Chaudhary, Antonios Anastasopoulos, Adithya Pratapa, David R.
  Mortensen, Zaid Sheikh, Yulia Tsvetkov, Graham Neubig",Automatic Extraction of Rules Governing Morphological Agreement,Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Creating a descriptive grammar of a language is an indispensable step for
language documentation and preservation. However, at the same time it is a
tedious, time-consuming task. In this paper, we take steps towards automating
this process by devising an automated framework for extracting a first-pass
grammatical specification from raw text in a concise, human- and
machine-readable format. We focus on extracting rules describing agreement, a
morphosyntactic phenomenon at the core of the grammars of many of the world's
languages. We apply our framework to all languages included in the Universal
Dependencies project, with promising results. Using cross-lingual transfer,
even with no expert annotations in the language of interest, our framework
extracts a grammatical specification which is nearly equivalent to those
created with large amounts of gold-standard annotated data. We confirm this
finding with human expert evaluations of the rules that our framework produces,
which have an average accuracy of 78%. We release an interface demonstrating
the extracted rules at https://neulab.github.io/lase/.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 18:31:45 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 03:30:27 GMT'}]",2020-10-07,"[['Chaudhary', 'Aditi', ''], ['Anastasopoulos', 'Antonios', ''], ['Pratapa', 'Adithya', ''], ['Mortensen', 'David R.', ''], ['Sheikh', 'Zaid', ''], ['Tsvetkov', 'Yulia', ''], ['Neubig', 'Graham', '']]"
1269619,2004.04849,Daniel Khashabi Mr.,"Daniel Khashabi, Tushar Khot, Ashish Sabharwal","More Bang for Your Buck: Natural Perturbation for Robust Question
  Answering",EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While recent models have achieved human-level scores on many NLP datasets, we
observe that they are considerably sensitive to small changes in input. As an
alternative to the standard approach of addressing this issue by constructing
training sets of completely new examples, we propose doing so via minimal
perturbation of examples. Specifically, our approach involves first collecting
a set of seed examples and then applying human-driven natural perturbations (as
opposed to rule-based machine perturbations), which often change the gold label
as well. Local perturbations have the advantage of being relatively easier (and
hence cheaper) to create than writing out completely new examples. To evaluate
the impact of this phenomenon, we consider a recent question-answering dataset
(BoolQ) and study the benefit of our approach as a function of the perturbation
cost ratio, the relative cost of perturbing an existing question vs. creating a
new one from scratch. We find that when natural perturbations are moderately
cheaper to create, it is more effective to train models using them: such models
exhibit higher robustness and better generalization, while retaining
performance on the original BoolQ dataset.
","[{'version': 'v1', 'created': 'Thu, 9 Apr 2020 23:12:39 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 07:10:00 GMT'}]",2020-10-07,"[['Khashabi', 'Daniel', ''], ['Khot', 'Tushar', ''], ['Sabharwal', 'Ashish', '']]"
1276316,2004.11546,Yiben Yang,"Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta,
  Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, Doug Downey",Generative Data Augmentation for Commonsense Reasoning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in commonsense reasoning depend on large-scale
human-annotated training data to achieve peak performance. However, manual
curation of training examples is expensive and has been shown to introduce
annotation artifacts that neural models can readily exploit and overfit on. We
investigate G-DAUG^C, a novel generative data augmentation method that aims to
achieve more accurate and robust learning in the low-resource setting. Our
approach generates synthetic examples using pretrained language models, and
selects the most informative and diverse set of examples for data augmentation.
In experiments with multiple commonsense reasoning benchmarks, G-DAUG^C
consistently outperforms existing data augmentation methods based on
back-translation, and establishes a new state-of-the-art on WinoGrande, CODAH,
and CommonsenseQA. Further, in addition to improvements in in-distribution
accuracy, G-DAUG^C-augmented training also enhances out-of-distribution
generalization, showing greater robustness against adversarial or perturbed
examples. Our analysis demonstrates that G-DAUG^C produces a diverse set of
fluent training examples, and that its selection and training approaches are
important for performance. Our findings encourage future research toward
generative data augmentation to enhance both in-distribution learning and
out-of-distribution generalization.
","[{'version': 'v1', 'created': 'Fri, 24 Apr 2020 06:12:10 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 06:45:52 GMT'}]",2020-10-07,"[['Yang', 'Yiben', ''], ['Malaviya', 'Chaitanya', ''], ['Fernandez', 'Jared', ''], ['Swayamdipta', 'Swabha', ''], ['Bras', 'Ronan Le', ''], ['Wang', 'Ji-Ping', ''], ['Bhagavatula', 'Chandra', ''], ['Choi', 'Yejin', ''], ['Downey', 'Doug', '']]"
1279143,2004.14373,Ankur Parikh,"Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui,
  Bhuwan Dhingra, Diyi Yang, Dipanjan Das",ToTTo: A Controlled Table-To-Text Generation Dataset,Accepted to EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present ToTTo, an open-domain English table-to-text dataset with over
120,000 training examples that proposes a controlled generation task: given a
Wikipedia table and a set of highlighted table cells, produce a one-sentence
description. To obtain generated targets that are natural but also faithful to
the source table, we introduce a dataset construction process where annotators
directly revise existing candidate sentences from Wikipedia. We present
systematic analyses of our dataset and annotation process as well as results
achieved by several state-of-the-art baselines. While usually fluent, existing
methods often hallucinate phrases that are not supported by the table,
suggesting that this dataset can serve as a useful research benchmark for
high-precision conditional text generation.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 17:53:45 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Apr 2020 05:18:35 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 06:07:06 GMT'}]",2020-10-07,"[['Parikh', 'Ankur P.', ''], ['Wang', 'Xuezhi', ''], ['Gehrmann', 'Sebastian', ''], ['Faruqui', 'Manaal', ''], ['Dhingra', 'Bhuwan', ''], ['Yang', 'Diyi', ''], ['Das', 'Dipanjan', '']]"
1358625,2010.02295,Chenguang Zhu,"Yu-An Chung, Chenguang Zhu, Michael Zeng","Semi-Supervised Speech-Language Joint Pre-Training for Spoken Language
  Understanding",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Spoken language understanding (SLU) requires a model to analyze input
acoustic signals to understand its linguistic content and make predictions. To
boost the models' performance, various pre-training methods have been proposed
to utilize large-scale unlabeled text and speech data. However, the inherent
disparities between the two modalities necessitate a mutual analysis. In this
paper, we propose a novel semi-supervised learning method, AlignNet, to jointly
pre-train the speech and language modules. Besides a self-supervised masked
language modeling of the two individual modules, AlignNet aligns
representations from paired speech and transcripts in a shared latent semantic
space. Thus, during fine-tuning, the speech module alone can produce
representations carrying both acoustic information and contextual semantic
knowledge. Experimental results verify the effectiveness of our approach on
various SLU tasks. For example, AlignNet improves the previous state-of-the-art
accuracy on the Spoken SQuAD dataset by 6.2%.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 19:29:49 GMT'}]",2020-10-07,"[['Chung', 'Yu-An', ''], ['Zhu', 'Chenguang', ''], ['Zeng', 'Michael', '']]"
1043536,1810.12349,James Gibson,"James Gibson, David C. Atkins, Torrey Creed, Zac Imel, Panayiotis
  Georgiou, and Shrikanth Narayanan",Multi-label Multi-task Deep Learning for Behavioral Coding,,,10.1109/TAFFC.2019.2952113,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a methodology for estimating human behaviors in psychotherapy
sessions using mutli-label and multi-task learning paradigms. We discuss the
problem of behavioral coding in which data of human interactions is the
annotated with labels to describe relevant human behaviors of interest. We
describe two related, yet distinct, corpora consisting of therapist client
interactions in psychotherapy sessions. We experimentally compare the proposed
learning approaches for estimating behaviors of interest in these datasets.
Specifically, we compare single and multiple label learning approaches, single
and multiple task learning approaches, and evaluate the performance of these
approaches when incorporating turn context. We demonstrate the prediction
performance gains which can be achieved by using the proposed paradigms and
discuss the insights these models provide into these complex interactions.
","[{'version': 'v1', 'created': 'Mon, 29 Oct 2018 18:57:30 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Nov 2018 23:25:33 GMT'}]",2020-10-07,"[['Gibson', 'James', ''], ['Atkins', 'David C.', ''], ['Creed', 'Torrey', ''], ['Imel', 'Zac', ''], ['Georgiou', 'Panayiotis', ''], ['Narayanan', 'Shrikanth', '']]"
1358652,2010.02322,Yue Yu,"Rongzhi Zhang, Yue Yu and Chao Zhang",SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup,EMNLP 2020 Long Paper,EMNLP 2020,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Active learning is an important technique for low-resource sequence labeling
tasks. However, current active sequence labeling methods use the queried
samples alone in each iteration, which is an inefficient way of leveraging
human annotations. We propose a simple but effective data augmentation method
to improve the label efficiency of active sequence labeling. Our method,
SeqMix, simply augments the queried samples by generating extra labeled
sequences in each iteration. The key difficulty is to generate plausible
sequences along with token-level labels. In SeqMix, we address this challenge
by performing mixup for both sequences and token-level labels of the queried
samples. Furthermore, we design a discriminator during sequence mixup, which
judges whether the generated sequences are plausible or not. Our experiments on
Named Entity Recognition and Event Detection tasks show that SeqMix can improve
the standard active sequence labeling method by $2.27\%$--$3.75\%$ in terms of
$F_1$ scores. The code and data for SeqMix can be found at
https://github.com/rz-zhang/SeqMix
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 20:27:14 GMT'}]",2020-10-07,"[['Zhang', 'Rongzhi', ''], ['Yu', 'Yue', ''], ['Zhang', 'Chao', '']]"
1358444,2010.02114,Divyansh Kaushik,"Divyansh Kaushik, Amrith Setlur, Eduard Hovy, Zachary C. Lipton",Explaining The Efficacy of Counterfactually-Augmented Data,,,,,cs.CL cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In attempts to produce machine learning models less reliant on spurious
patterns in training data, researchers have recently proposed a
human-in-the-loop process for generating counterfactually augmented datasets.
As applied in NLP, given some documents and their (initial) labels, humans are
tasked with revising the text to make a (given) counterfactual label
applicable. Importantly, the instructions prohibit edits that are not necessary
to flip the applicable label. Models trained on the augmented (original and
revised) data have been shown to rely less on semantically irrelevant words and
to generalize better out-of-domain. While this work draws on causal thinking,
casting edits as interventions and relying on human understanding to assess
outcomes, the underlying causal model is not clear nor are the principles
underlying the observed improvements in out-of-domain evaluation. In this
paper, we explore a toy analog, using linear Gaussian models. Our analysis
reveals interesting relationships between causal models, measurement noise,
out-of-domain generalization, and reliance on spurious signals. Interestingly
our analysis suggests that data corrupted by adding noise to causal features
will degrade out-of-domain performance, while noise added to non-causal
features may make models more robust out-of-domain. This analysis yields
interesting insights that help to explain the efficacy of counterfactually
augmented data. Finally, we present a large-scale empirical study that supports
this hypothesis.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 15:57:07 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 02:21:13 GMT'}]",2020-10-07,"[['Kaushik', 'Divyansh', ''], ['Setlur', 'Amrith', ''], ['Hovy', 'Eduard', ''], ['Lipton', 'Zachary C.', '']]"
1278753,2004.13983,Zhengyuan Liu,"Zhengyuan Liu, Ke Shi, Nancy F. Chen","Conditional Neural Generation using Sub-Aspect Functions for Extractive
  News Summarization",Accepted to Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Much progress has been made in text summarization, fueled by neural
architectures using large-scale training corpora. However, in the news domain,
neural models easily overfit by leveraging position-related features due to the
prevalence of the inverted pyramid writing style. In addition, there is an
unmet need to generate a variety of summaries for different users. In this
paper, we propose a neural framework that can flexibly control summary
generation by introducing a set of sub-aspect functions (i.e. importance,
diversity, position). These sub-aspect functions are regulated by a set of
control codes to decide which sub-aspect to focus on during summary generation.
We demonstrate that extracted summaries with minimal position bias is
comparable with those generated by standard models that take advantage of
position preference. We also show that news summaries generated with a focus on
diversity can be more preferred by human raters. These results suggest that a
more flexible neural summarization framework providing more control options
could be desirable in tailoring to different user preferences, which is useful
since it is often impractical to articulate such preferences for different
applications a priori.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 06:52:15 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Apr 2020 02:57:55 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 04:57:16 GMT'}]",2020-10-07,"[['Liu', 'Zhengyuan', ''], ['Shi', 'Ke', ''], ['Chen', 'Nancy F.', '']]"
1280037,2005.00242,Qiang Ning,"Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt Gardner, Dan Roth",TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions,15 pages (incl. 4 pages in the appendix); accepted to EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A critical part of reading is being able to understand the temporal
relationships between events described in a passage of text, even when those
relationships are not explicitly stated. However, current machine reading
comprehension benchmarks have practically no questions that test temporal
phenomena, so systems trained on these benchmarks have no capacity to answer
questions such as ""what happened before/after [some event]?"" We introduce
TORQUE, a new English reading comprehension benchmark built on 3.2k news
snippets with 21k human-generated questions querying temporal relationships.
Results show that RoBERTa-large achieves an exact-match score of 51% on the
test set of TORQUE, about 30% behind human performance.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 06:29:56 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 03:57:19 GMT'}]",2020-10-07,"[['Ning', 'Qiang', ''], ['Wu', 'Hao', ''], ['Han', 'Rujun', ''], ['Peng', 'Nanyun', ''], ['Gardner', 'Matt', ''], ['Roth', 'Dan', '']]"
1359426,2010.03096,Sheng Bi,Xiya Cheng and Sheng Bi and Guilin Qi and Yongzhen Wang,Knowledge-aware Method for Confusing Charge Prediction,Accepted by NLPCC 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic charge prediction task aims to determine the final charges based on
fact descriptions of criminal cases, which is a vital application of legal
assistant systems. Conventional works usually depend on fact descriptions to
predict charges while ignoring the legal schematic knowledge, which makes it
difficult to distinguish confusing charges. In this paper, we propose a
knowledge-attentive neural network model, which introduces legal schematic
knowledge about charges and exploit the knowledge hierarchical representation
as the discriminative features to differentiate confusing charges. Our model
takes the textual fact description as the input and learns fact representation
through a graph convolutional network. A legal schematic knowledge transformer
is utilized to generate crucial knowledge representations oriented to the legal
schematic knowledge at both the schema and charge levels. We apply a knowledge
matching network for effectively incorporating charge information into the fact
to learn knowledge-aware fact representation. Finally, we use the
knowledge-aware fact representation for charge prediction. We create two
real-world datasets and experimental results show that our proposed model can
outperform other state-of-the-art baselines on accuracy and F1 score,
especially on dealing with confusing charges.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 00:58:10 GMT'}]",2020-10-08,"[['Cheng', 'Xiya', ''], ['Bi', 'Sheng', ''], ['Qi', 'Guilin', ''], ['Wang', 'Yongzhen', '']]"
1359554,2010.03224,Jingxuan Yang,"Jingxuan Yang, Kerui Xu, Jun Xu, Si Li, Sheng Gao, Jun Guo, Ji-Rong
  Wen, Nianwen Xue","Transformer-GCRF: Recovering Chinese Dropped Pronouns with General
  Conditional Random Fields",Accept as EMNLP-findings 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pronouns are often dropped in Chinese conversations and recovering the
dropped pronouns is important for NLP applications such as Machine Translation.
Existing approaches usually formulate this as a sequence labeling task of
predicting whether there is a dropped pronoun before each token and its type.
Each utterance is considered to be a sequence and labeled independently.
Although these approaches have shown promise, labeling each utterance
independently ignores the dependencies between pronouns in neighboring
utterances. Modeling these dependencies is critical to improving the
performance of dropped pronoun recovery. In this paper, we present a novel
framework that combines the strength of Transformer network with General
Conditional Random Fields (GCRF) to model the dependencies between pronouns in
neighboring utterances. Results on three Chinese conversation datasets show
that the Transformer-GCRF model outperforms the state-of-the-art dropped
pronoun recovery models. Exploratory analysis also demonstrates that the GCRF
did help to capture the dependencies between pronouns in neighboring
utterances, thus contributes to the performance improvements.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 07:06:09 GMT'}]",2020-10-08,"[['Yang', 'Jingxuan', ''], ['Xu', 'Kerui', ''], ['Xu', 'Jun', ''], ['Li', 'Si', ''], ['Gao', 'Sheng', ''], ['Guo', 'Jun', ''], ['Wen', 'Ji-Rong', ''], ['Xue', 'Nianwen', '']]"
1359423,2010.03093,Faisal Ladhak,"Faisal Ladhak, Esin Durmus, Claire Cardie, Kathleen McKeown","WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive
  Summarization",Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce WikiLingua, a large-scale, multilingual dataset for the
evaluation of crosslingual abstractive summarization systems. We extract
article and summary pairs in 18 languages from WikiHow, a high quality,
collaborative resource of how-to guides on a diverse set of topics written by
human authors. We create gold-standard article-summary alignments across
languages by aligning the images that are used to describe each how-to step in
an article. As a set of baselines for further studies, we evaluate the
performance of existing cross-lingual abstractive summarization methods on our
dataset. We further propose a method for direct crosslingual summarization
(i.e., without requiring translation at inference time) by leveraging synthetic
data and Neural Machine Translation as a pre-training step. Our method
significantly outperforms the baseline approaches, while being more cost
efficient during inference.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 00:28:05 GMT'}]",2020-10-08,"[['Ladhak', 'Faisal', ''], ['Durmus', 'Esin', ''], ['Cardie', 'Claire', ''], ['McKeown', 'Kathleen', '']]"
1359776,2010.03446,Ewan Dunbar,"Louis Fournier, Emmanuel Dupoux, Ewan Dunbar",Analogies minus analogy test: measuring regularities in word embeddings,,Proceedings of CoNLL 2020,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Vector space models of words have long been claimed to capture linguistic
regularities as simple vector translations, but problems have been raised with
this claim. We decompose and empirically analyze the classic arithmetic word
analogy test, to motivate two new metrics that address the issues with the
standard test, and which distinguish between class-wise offset concentration
(similar directions between pairs of words drawn from different broad classes,
such as France--London, China--Ottawa, ...) and pairing consistency (the
existence of a regular transformation between correctly-matched pairs such as
France:Paris::China:Beijing). We show that, while the standard analogy test is
flawed, several popular word embeddings do nevertheless encode linguistic
regularities.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 14:38:35 GMT'}]",2020-10-08,"[['Fournier', 'Louis', ''], ['Dupoux', 'Emmanuel', ''], ['Dunbar', 'Ewan', '']]"
1359454,2010.03124,Machel Reid,"Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo","VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word
  Representations for Improved Definition Modeling","EMNLP 2020, 10 Pages",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we tackle the task of definition modeling, where the goal is
to learn to generate definitions of words and phrases. Existing approaches for
this task are discriminative, combining distributional and lexical semantics in
an implicit rather than direct way. To tackle this issue we propose a
generative model for the task, introducing a continuous latent variable to
explicitly model the underlying relationship between a phrase used within a
context and its definition. We rely on variational inference for estimation and
leverage contextualized word embeddings for improved performance. Our approach
is evaluated on four existing challenging benchmarks with the addition of two
new datasets, ""Cambridge"" and the first non-English corpus ""Robert"", which we
release to complement our empirical study. Our Variational Contextual
Definition Modeler (VCDM) achieves state-of-the-art performance in terms of
automatic and human evaluation metrics, demonstrating the effectiveness of our
approach.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 02:48:44 GMT'}]",2020-10-08,"[['Reid', 'Machel', ''], ['Marrese-Taylor', 'Edison', ''], ['Matsuo', 'Yutaka', '']]"
1359535,2010.03205,Bodhisattwa Prasad Majumder,"Bodhisattwa Prasad Majumder, Harsh Jhamtani, Taylor Berg-Kirkpatrick,
  Julian McAuley","Like hiking? You probably enjoy nature: Persona-grounded Dialog with
  Commonsense Expansions",Accepted in EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing persona-grounded dialog models often fail to capture simple
implications of given persona descriptions, something which humans are able to
do seamlessly. For example, state-of-the-art models cannot infer that interest
in hiking might imply love for nature or longing for a break. In this paper, we
propose to expand available persona sentences using existing commonsense
knowledge bases and paraphrasing resources to imbue dialog models with access
to an expanded and richer set of persona descriptions. Additionally, we
introduce fine-grained grounding on personas by encouraging the model to make a
discrete choice among persona sentences while synthesizing a dialog response.
Since such a choice is not observed in the data, we model it using a discrete
latent random variable and use variational learning to sample from hundreds of
persona expansions. Our model outperforms competitive baselines on the
PersonaChat dataset in terms of dialog quality and diversity while achieving
persona-consistent and controllable dialog generation.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 06:25:39 GMT'}]",2020-10-08,"[['Majumder', 'Bodhisattwa Prasad', ''], ['Jhamtani', 'Harsh', ''], ['Berg-Kirkpatrick', 'Taylor', ''], ['McAuley', 'Julian', '']]"
1359523,2010.03193,Urmish Thakker,"Urmish Thakker, Jesse Beu, Dibakar Gope, Ganesh Dasika, Matthew
  Mattina",Rank and run-time aware compression of NLP Applications,"Published at SustaiNLP@EMNLP 2020. arXiv admin note: text overlap
  with arXiv:1906.04886",,,,cs.CL cs.LG cs.PF,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sequence model based NLP applications can be large. Yet, many applications
that benefit from them run on small devices with very limited compute and
storage capabilities, while still having run-time constraints. As a result,
there is a need for a compression technique that can achieve significant
compression without negatively impacting inference run-time and task accuracy.
This paper proposes a new compression technique called Hybrid Matrix
Factorization that achieves this dual objective. HMF improves low-rank matrix
factorization (LMF) techniques by doubling the rank of the matrix using an
intelligent hybrid-structure leading to better accuracy than LMF. Further, by
preserving dense matrices, it leads to faster inference run-time than pruning
or structure matrix based compression technique. We evaluate the impact of this
technique on 5 NLP benchmarks across multiple tasks (Translation, Intent
Detection, Language Modeling) and show that for similar accuracy values and
compression factors, HMF can achieve more than 2.32x faster inference run-time
than pruning and 16.77% better accuracy than LMF.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 16:03:15 GMT'}]",2020-10-08,"[['Thakker', 'Urmish', ''], ['Beu', 'Jesse', ''], ['Gope', 'Dibakar', ''], ['Dasika', 'Ganesh', ''], ['Mattina', 'Matthew', '']]"
1359509,2010.03179,Michael A. Hedderich,"Michael A. Hedderich, David Adelani, Dawei Zhu, Jesujoba Alabi, Udia
  Markus, Dietrich Klakow","Transfer Learning and Distant Supervision for Multilingual Transformer
  Models: A Study on African Languages",Accepted at EMNLP'20,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual transformer models like mBERT and XLM-RoBERTa have obtained
great improvements for many NLP tasks on a variety of languages. However,
recent works also showed that results from high-resource languages could not be
easily transferred to realistic, low-resource scenarios. In this work, we study
trends in performance for different amounts of available resources for the
three African languages Hausa, isiXhosa and Yor\`ub\'a on both NER and topic
classification. We show that in combination with transfer learning or distant
supervision, these models can achieve with as little as 10 or 100 labeled
sentences the same performance as baselines with much more supervised training
data. However, we also find settings where this does not hold. Our discussions
and additional experiments on assumptions such as time and hardware
restrictions highlight challenges and opportunities in low-resource learning.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 05:23:27 GMT'}]",2020-10-08,"[['Hedderich', 'Michael A.', ''], ['Adelani', 'David', ''], ['Zhu', 'Dawei', ''], ['Alabi', 'Jesujoba', ''], ['Markus', 'Udia', ''], ['Klakow', 'Dietrich', '']]"
1359485,2010.03155,Masato Mita,"Masato Mita, Shun Kiyono, Masahiro Kaneko, Jun Suzuki and Kentaro Inui","A Self-Refinement Strategy for Noise Reduction in Grammatical Error
  Correction",accepted by EMNLP 2020 (Findings),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing approaches for grammatical error correction (GEC) largely rely on
supervised learning with manually created GEC datasets. However, there has been
little focus on verifying and ensuring the quality of the datasets, and on how
lower-quality data might affect GEC performance. We indeed found that there is
a non-negligible amount of ""noise"" where errors were inappropriately edited or
left uncorrected. To address this, we designed a self-refinement method where
the key idea is to denoise these datasets by leveraging the prediction
consistency of existing models, and outperformed strong denoising baseline
methods. We further applied task-specific techniques and achieved
state-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks.
We then analyzed the effect of the proposed denoising method, and found that
our approach leads to improved coverage of corrections and facilitated fluency
edits which are reflected in higher recall and overall performance.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 04:45:09 GMT'}]",2020-10-08,"[['Mita', 'Masato', ''], ['Kiyono', 'Shun', ''], ['Kaneko', 'Masahiro', ''], ['Suzuki', 'Jun', ''], ['Inui', 'Kentaro', '']]"
1359484,2010.03154,Xiaochuang Han,"Xiaochuang Han, Yulia Tsvetkov",Fortifying Toxic Speech Detectors Against Veiled Toxicity,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern toxic speech detectors are incompetent in recognizing disguised
offensive language, such as adversarial attacks that deliberately avoid known
toxic lexicons, or manifestations of implicit bias. Building a large annotated
dataset for such veiled toxicity can be very expensive. In this work, we
propose a framework aimed at fortifying existing toxic speech detectors without
a large labeled corpus of veiled toxicity. Just a handful of probing examples
are used to surface orders of magnitude more disguised offenses. We augment the
toxic speech detector's training data with these discovered offensive examples,
thereby making it more robust to veiled toxicity while preserving its utility
in detecting overt toxicity.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 04:43:48 GMT'}]",2020-10-08,"[['Han', 'Xiaochuang', ''], ['Tsvetkov', 'Yulia', '']]"
1359477,2010.03147,Keshav Kolluru,"Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Mausam, and Soumen
  Chakrabarti","OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open
  Information Extraction",EMNLP 2020 (Long),,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  A recent state-of-the-art neural open information extraction (OpenIE) system
generates extractions iteratively, requiring repeated encoding of partial
outputs. This comes at a significant computational cost. On the other hand,
sequence labeling approaches for OpenIE are much faster, but worse in
extraction quality. In this paper, we bridge this trade-off by presenting an
iterative labeling-based system that establishes a new state of the art for
OpenIE, while extracting 10x faster. This is achieved through a novel Iterative
Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling
task. We improve its performance further by applying coverage (soft)
constraints on the grid at training time.
  Moreover, on observing that the best OpenIE systems falter at handling
coordination structures, our OpenIE system also incorporates a new coordination
analyzer built with the same IGL architecture. This IGL based coordination
analyzer helps our OpenIE system handle complicated coordination structures,
while also establishing a new state of the art on the task of coordination
analysis, with a 12.3 pts improvement in F1 over previous analyzers. Our OpenIE
system, OpenIE6, beats the previous systems by as much as 4 pts in F1, while
being much faster.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 04:05:37 GMT'}]",2020-10-08,"[['Kolluru', 'Keshav', ''], ['Adlakha', 'Vaibhav', ''], ['Aggarwal', 'Samarth', ''], ['Mausam', '', ''], ['Chakrabarti', 'Soumen', '']]"
1359429,2010.03099,Jiecao Chen,"Jiecao Chen, Liu Yang, Karthik Raman, Michael Bendersky, Jung-Jung
  Yeh, Yun Zhou, Marc Najork, Danyang Cai, Ehsan Emadzadeh","DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching
  and Pair Modeling",13 pages. Accepted to Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained models like BERT (Devlin et al., 2018) have dominated NLP / IR
applications such as single sentence classification, text pair classification,
and question answering. However, deploying these models in real systems is
highly non-trivial due to their exorbitant computational costs. A common remedy
to this is knowledge distillation (Hinton et al., 2015), leading to faster
inference. However -- as we show here -- existing works are not optimized for
dealing with pairs (or tuples) of texts. Consequently, they are either not
scalable or demonstrate subpar performance. In this work, we propose DiPair --
a novel framework for distilling fast and accurate models on text pair tasks.
Coupled with an end-to-end training strategy, DiPair is both highly scalable
and offers improved quality-speed tradeoffs. Empirical studies conducted on
both academic and real-world e-commerce benchmarks demonstrate the efficacy of
the proposed approach with speedups of over 350x and minimal quality drop
relative to the cross-attention teacher BERT model.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 01:19:23 GMT'}]",2020-10-08,"[['Chen', 'Jiecao', ''], ['Yang', 'Liu', ''], ['Raman', 'Karthik', ''], ['Bendersky', 'Michael', ''], ['Yeh', 'Jung-Jung', ''], ['Zhou', 'Yun', ''], ['Najork', 'Marc', ''], ['Cai', 'Danyang', ''], ['Emadzadeh', 'Ehsan', '']]"
1359476,2010.03146,Steven Cao,"Steven Cao, Nikita Kitaev, Dan Klein",Unsupervised Parsing via Constituency Tests,EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a method for unsupervised parsing based on the linguistic notion
of a constituency test. One type of constituency test involves modifying the
sentence via some transformation (e.g. replacing the span with a pronoun) and
then judging the result (e.g. checking if it is grammatical). Motivated by this
idea, we design an unsupervised parser by specifying a set of transformations
and using an unsupervised neural acceptability model to make grammaticality
decisions. To produce a tree given a sentence, we score each span by
aggregating its constituency test judgments, and we choose the binary tree with
the highest total score. While this approach already achieves performance in
the range of current methods, we further improve accuracy by fine-tuning the
grammaticality model through a refinement procedure, where we alternate between
improving the estimated trees and improving the grammaticality model. The
refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute
improvement of 7.6 points over the previous best published result.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 04:05:01 GMT'}]",2020-10-08,"[['Cao', 'Steven', ''], ['Kitaev', 'Nikita', ''], ['Klein', 'Dan', '']]"
1359552,2010.03222,Lukas Muttenthaler,"Lukas Muttenthaler, Isabelle Augenstein, Johannes Bjerva",Unsupervised Evaluation for Question Answering with Transformers,"8 pages, to be published in the Proceedings of the 2020 EMNLP
  Workshop BlackboxNLP: Analysing and Interpreting Neural Networks for NLP",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is challenging to automatically evaluate the answer of a QA model at
inference time. Although many models provide confidence scores, and simple
heuristics can go a long way towards indicating answer correctness, such
measures are heavily dataset-dependent and are unlikely to generalize. In this
work, we begin by investigating the hidden representations of questions,
answers, and contexts in transformer-based QA architectures. We observe a
consistent pattern in the answer representations, which we show can be used to
automatically evaluate whether or not a predicted answer span is correct. Our
method does not require any labeled data and outperforms strong heuristic
baselines, across 2 datasets and 7 domains. We are able to predict whether or
not a model's answer is correct with 91.37% accuracy on SQuAD, and 80.7%
accuracy on SubjQA. We expect that this method will have broad applications,
e.g., in the semi-automatic development of QA datasets
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 07:03:30 GMT'}]",2020-10-08,"[['Muttenthaler', 'Lukas', ''], ['Augenstein', 'Isabelle', ''], ['Bjerva', 'Johannes', '']]"
1202155,1911.03561,Alireza Mohammadshahi,Alireza Mohammadshahi and James Henderson,Graph-to-Graph Transformer for Transition-based Dependency Parsing,"20 pages, 4 figures, accepted to Findings of EMNLP 2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose the Graph2Graph Transformer architecture for conditioning on and
predicting arbitrary graphs, and apply it to the challenging task of
transition-based dependency parsing. After proposing two novel Transformer
models of transition-based dependency parsing as strong baselines, we show that
adding the proposed mechanisms for conditioning on and predicting graphs of
Graph2Graph Transformer results in significant improvements, both with and
without BERT pre-training. The novel baselines and their integration with
Graph2Graph Transformer significantly outperform the state-of-the-art in
traditional transition-based dependency parsing on both English Penn Treebank,
and 13 languages of Universal Dependencies Treebanks. Graph2Graph Transformer
can be integrated with many previous structured prediction methods, making it
easy to apply to a wide range of NLP tasks.
","[{'version': 'v1', 'created': 'Fri, 8 Nov 2019 22:14:35 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Apr 2020 09:11:16 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 10:22:09 GMT'}]",2020-10-08,"[['Mohammadshahi', 'Alireza', ''], ['Henderson', 'James', '']]"
1359468,2010.03138,Linzi Xing,"Linzi Xing, Brad Hackinen, Giuseppe Carenini, Francesco Trebbi",Improving Context Modeling in Neural Topic Segmentation,Accepted at AACL-IJCNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Topic segmentation is critical in key NLP tasks and recent works favor highly
effective neural supervised approaches. However, current neural solutions are
arguably limited in how they model context. In this paper, we enhance a
segmenter based on a hierarchical attention BiLSTM network to better model
context, by adding a coherence-related auxiliary task and restricted
self-attention. Our optimized segmenter outperforms SOTA approaches when
trained and tested on three datasets. We also the robustness of our proposed
model in domain transfer setting by training a model on a large-scale dataset
and testing it on four challenging real-world benchmarks. Furthermore, we apply
our proposed strategy to two other languages (German and Chinese), and show its
effectiveness in multilingual scenarios.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 03:40:49 GMT'}]",2020-10-08,"[['Xing', 'Linzi', ''], ['Hackinen', 'Brad', ''], ['Carenini', 'Giuseppe', ''], ['Trebbi', 'Francesco', '']]"
1359418,2010.03088,Piotr Szyma\'nski,"Piotr Szyma\'nski, Kyle Gorman","Is the Best Better? Bayesian Statistical Model Comparison for Natural
  Language Processing",Accepted to EMNLP2020,,,,cs.CL cs.LG stat.ME,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work raises concerns about the use of standard splits to compare
natural language processing models. We propose a Bayesian statistical model
comparison technique which uses k-fold cross-validation across multiple data
sets to estimate the likelihood that one model will outperform the other, or
that the two will produce practically equivalent results. We use this technique
to rank six English part-of-speech taggers across two data sets and three
evaluation metrics.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 23:37:28 GMT'}]",2020-10-08,"[['Szymański', 'Piotr', ''], ['Gorman', 'Kyle', '']]"
1359457,2010.03127,Takuma Udagawa,"Takuma Udagawa, Takato Yamazaki, Akiko Aizawa","A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial
  Expressions","16 pages, Findings of EMNLP 2020",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent models achieve promising results in visually grounded dialogues.
However, existing datasets often contain undesirable biases and lack
sophisticated linguistic analyses, which make it difficult to understand how
well current models recognize their precise linguistic structures. To address
this problem, we make two design choices: first, we focus on OneCommon Corpus
\citep{udagawa2019natural,udagawa2020annotated}, a simple yet challenging
common grounding dataset which contains minimal bias by design. Second, we
analyze their linguistic structures based on \textit{spatial expressions} and
provide comprehensive and reliable annotation for 600 dialogues. We show that
our annotation captures important linguistic structures including
predicate-argument structure, modification and ellipsis. In our experiments, we
assess the model's understanding of these structures through reference
resolution. We demonstrate that our annotation can reveal both the strengths
and weaknesses of baseline models in essential levels of detail. Overall, we
propose a novel framework and resource for investigating fine-grained language
understanding in visually grounded dialogues.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 02:50:38 GMT'}]",2020-10-08,"[['Udagawa', 'Takuma', ''], ['Yamazaki', 'Takato', ''], ['Aizawa', 'Akiko', '']]"
1359699,2010.03369,Marco Guerini,"Thomas Scialom, Serra Sinem Tekiroglu, Jacopo Staiano, Marco Guerini",Toward Stance-based Personas for Opinionated Dialogues,Accepted at Findings of EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the context of chit-chat dialogues it has been shown that endowing systems
with a persona profile is important to produce more coherent and meaningful
conversations. Still, the representation of such personas has thus far been
limited to a fact-based representation (e.g. ""I have two cats.""). We argue that
these representations remain superficial w.r.t. the complexity of human
personality. In this work, we propose to make a step forward and investigate
stance-based persona, trying to grasp more profound characteristics, such as
opinions, values, and beliefs to drive language generation. To this end, we
introduce a novel dataset allowing to explore different stance-based persona
representations and their impact on claim generation, showing that they are
able to grasp abstract and profound aspects of the author persona.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 12:30:30 GMT'}]",2020-10-08,"[['Scialom', 'Thomas', ''], ['Tekiroglu', 'Serra Sinem', ''], ['Staiano', 'Jacopo', ''], ['Guerini', 'Marco', '']]"
1359472,2010.03142,Zehui Lin,"Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Jiangtao Feng, Hao
  Zhou and Lei Li","Pre-training Multilingual Neural Machine Translation by Leveraging
  Alignment Information",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate the following question for machine translation (MT): can we
develop a single universal MT model to serve as the common seed and obtain
derivative and improved models on arbitrary language pairs? We propose mRASP,
an approach to pre-train a universal multilingual neural machine translation
model. Our key idea in mRASP is its novel technique of random aligned
substitution, which brings words and phrases with similar meanings across
multiple languages closer in the representation space. We pre-train a mRASP
model on 32 language pairs jointly with only public datasets. The model is then
fine-tuned on downstream language pairs to obtain specialized MT models. We
carry out extensive experiments on 42 translation directions across a diverse
settings, including low, medium, rich resource, and as well as transferring to
exotic language pairs. Experimental results demonstrate that mRASP achieves
significant performance improvement compared to directly training on those
target pairs. It is the first time to verify that multiple low-resource
language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP
is even able to improve the translation quality on exotic languages that never
occur in the pre-training corpus. Code, data, and pre-trained models are
available at https://github.com/linzehui/mRASP.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 03:57:54 GMT'}]",2020-10-08,"[['Lin', 'Zehui', ''], ['Pan', 'Xiao', ''], ['Wang', 'Mingxuan', ''], ['Qiu', 'Xipeng', ''], ['Feng', 'Jiangtao', ''], ['Zhou', 'Hao', ''], ['Li', 'Lei', '']]"
1359579,2010.03249,Zhiyuan Liu,"Zhiyuan Liu, Yixin Cao, Liangming Pan, Juanzi Li, Zhiyuan Liu,
  Tat-Seng Chua","Exploring and Evaluating Attributes, Values, and Structures for Entity
  Alignment","10 pages, EMNLP2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich
content by linking the equivalent entities from various KGs. GNN-based EA
methods present promising performances by modeling the KG structure defined by
relation triples. However, attribute triples can also provide crucial alignment
signal but have not been well explored yet. In this paper, we propose to
utilize an attributed value encoder and partition the KG into subgraphs to
model the various types of attribute triples efficiently. Besides, the
performances of current EA methods are overestimated because of the name-bias
of existing EA datasets. To make an objective evaluation, we propose a hard
experimental setting where we select equivalent entity pairs with very
different names as the test set. Under both the regular and hard settings, our
method achieves significant improvements ($5.10\%$ on average Hits@$1$ in
DBP$15$k) over $12$ baselines in cross-lingual and monolingual datasets.
Ablation studies on different subgraphs and a case study about attribute types
further demonstrate the effectiveness of our method. Source code and data can
be found at https://github.com/thunlp/explore-and-evaluate.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 08:03:58 GMT'}]",2020-10-08,"[['Liu', 'Zhiyuan', ''], ['Cao', 'Yixin', ''], ['Pan', 'Liangming', ''], ['Li', 'Juanzi', ''], ['Liu', 'Zhiyuan', ''], ['Chua', 'Tat-Seng', '']]"
1359364,2010.03034,Peyman Passban,"Yimeng Wu, Peyman Passban, Mehdi Rezagholizade, Qun Liu","Why Skip If You Can Combine: A Simple Knowledge Distillation Technique
  for Intermediate Layers",The first two authors contributed equally,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the growth of computing power neural machine translation (NMT) models
also grow accordingly and become better. However, they also become harder to
deploy on edge devices due to memory constraints. To cope with this problem, a
common practice is to distill knowledge from a large and accurately-trained
teacher network (T) into a compact student network (S). Although knowledge
distillation (KD) is useful in most cases, our study shows that existing KD
techniques might not be suitable enough for deep NMT engines, so we propose a
novel alternative. In our model, besides matching T and S predictions we have a
combinatorial mechanism to inject layer-level supervision from T to S. In this
paper, we target low-resource settings and evaluate our translation engines for
Portuguese--English, Turkish--English, and English--German directions. Students
trained using our technique have 50% fewer parameters and can still deliver
comparable results to those of 12-layer teachers.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 21:08:16 GMT'}]",2020-10-08,"[['Wu', 'Yimeng', ''], ['Passban', 'Peyman', ''], ['Rezagholizade', 'Mehdi', ''], ['Liu', 'Qun', '']]"
1359403,2010.03073,Cicero Nogueira dos Santos,"Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng
  Huang, Bing Xiang",Beyond [CLS] through Ranking by Generation,EMNLP 2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative models for Information Retrieval, where ranking of documents is
viewed as the task of generating a query from a document's language model, were
very successful in various IR tasks in the past. However, with the advent of
modern deep neural networks, attention has shifted to discriminative ranking
functions that model the semantic similarity of documents and queries instead.
Recently, deep generative models such as GPT2 and BART have been shown to be
excellent text generators, but their effectiveness as rankers have not been
demonstrated yet. In this work, we revisit the generative framework for
information retrieval and show that our generative approaches are as effective
as state-of-the-art semantic similarity-based discriminative models for the
answer selection task. Additionally, we demonstrate the effectiveness of
unlikelihood losses for IR.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 22:56:31 GMT'}]",2020-10-08,"[['Santos', 'Cicero Nogueira dos', ''], ['Ma', 'Xiaofei', ''], ['Nallapati', 'Ramesh', ''], ['Huang', 'Zhiheng', ''], ['Xiang', 'Bing', '']]"
1359762,2010.03432,Piotr Szyma\'nski,"Piotr Szyma\'nski, Piotr \.Zelasko, Mikolaj Morzy, Adrian Szymczak,
  Marzena \.Zy{\l}a-Hoppe, Joanna Banaszczak, Lukasz Augustyniak, Jan Mizgajski
  and Yishay Carmiel",WER we are and WER we think we are,Accepted to EMNLP Findings,,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language processing of conversational speech requires the
availability of high-quality transcripts. In this paper, we express our
skepticism towards the recent reports of very low Word Error Rates (WERs)
achieved by modern Automatic Speech Recognition (ASR) systems on benchmark
datasets. We outline several problems with popular benchmarks and compare three
state-of-the-art commercial ASR systems on an internal dataset of real-life
spontaneous human conversations and HUB'05 public benchmark. We show that WERs
are significantly higher than the best reported results. We formulate a set of
guidelines which may aid in the creation of real-life, multi-domain datasets
with high quality annotations for training and testing of robust ASR systems.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 14:20:31 GMT'}]",2020-10-08,"[['Szymański', 'Piotr', ''], ['Żelasko', 'Piotr', ''], ['Morzy', 'Mikolaj', ''], ['Szymczak', 'Adrian', ''], ['Żyła-Hoppe', 'Marzena', ''], ['Banaszczak', 'Joanna', ''], ['Augustyniak', 'Lukasz', ''], ['Mizgajski', 'Jan', ''], ['Carmiel', 'Yishay', '']]"
1201344,1911.02750,Mingbo Ma,"Mingbo Ma, Baigong Zheng, Kaibo Liu, Renjie Zheng, Hairong Liu, Kainan
  Peng, Kenneth Church, Liang Huang",Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework,Findings of EMNLP 2020,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-to-speech synthesis (TTS) has witnessed rapid progress in recent years,
where neural methods became capable of producing audios with high naturalness.
However, these efforts still suffer from two types of latencies: (a) the {\em
computational latency} (synthesizing time), which grows linearly with the
sentence length even with parallel approaches, and (b) the {\em input latency}
in scenarios where the input text is incrementally generated (such as in
simultaneous translation, dialog generation, and assistive technologies). To
reduce these latencies, we devise the first neural incremental TTS approach
based on the recently proposed prefix-to-prefix framework. We synthesize speech
in an online fashion, playing a segment of audio while generating the next,
resulting in an $O(1)$ rather than $O(n)$ latency.
","[{'version': 'v1', 'created': 'Thu, 7 Nov 2019 04:22:54 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Apr 2020 02:14:34 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 01:58:17 GMT'}]",2020-10-08,"[['Ma', 'Mingbo', ''], ['Zheng', 'Baigong', ''], ['Liu', 'Kaibo', ''], ['Zheng', 'Renjie', ''], ['Liu', 'Hairong', ''], ['Peng', 'Kainan', ''], ['Church', 'Kenneth', ''], ['Huang', 'Liang', '']]"
1359742,2010.03412,Weijia Xu,"Weijia Xu, Xing Niu, Marine Carpuat","Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural
  Machine Translation",Accepted at Findings of EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While Iterative Back-Translation and Dual Learning effectively incorporate
monolingual training data in neural machine translation, they use different
objectives and heuristic gradient approximation strategies, and have not been
extensively compared. We introduce a novel dual reconstruction objective that
provides a unified view of Iterative Back-Translation and Dual Learning. It
motivates a theoretical analysis and controlled empirical study on
German-English and Turkish-English tasks, which both suggest that Iterative
Back-Translation is more effective than Dual Learning despite its relative
simplicity.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 13:40:32 GMT'}]",2020-10-08,"[['Xu', 'Weijia', ''], ['Niu', 'Xing', ''], ['Carpuat', 'Marine', '']]"
1359806,2010.03476,Bernhard Kratzwald,"Bernhard Kratzwald, Stefan Feuerriegel, Huan Sun",Learning a Cost-Effective Annotation Policy for Question Answering,Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State-of-the-art question answering (QA) relies upon large amounts of
training data for which labeling is time consuming and thus expensive. For this
reason, customizing QA systems is challenging. As a remedy, we propose a novel
framework for annotating QA datasets that entails learning a cost-effective
annotation policy and a semi-supervised annotation scheme. The latter reduces
the human effort: it leverages the underlying QA system to suggest potential
candidate annotations. Human annotators then simply provide binary feedback on
these candidates. Our system is designed such that past annotations
continuously improve the future performance and thus overall annotation cost.
To the best of our knowledge, this is the first paper to address the problem of
annotating questions with minimal annotation cost. We compare our framework
against traditional manual annotations in an extensive set of experiments. We
find that our approach can reduce up to 21.1% of the annotation cost.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 15:25:41 GMT'}]",2020-10-08,"[['Kratzwald', 'Bernhard', ''], ['Feuerriegel', 'Stefan', ''], ['Sun', 'Huan', '']]"
1359811,2010.03481,Andrey Kutuzov,"Julia Rodina, Yuliya Trofimova, Andrey Kutuzov, Ekaterina Artemova",ELMo and BERT in semantic change detection for Russian,"The 9th International Conference on Analysis of Images, Social
  Networks and Texts (AIST 2020)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the effectiveness of contextualized embeddings for the task of
diachronic semantic change detection for Russian language data. Evaluation test
sets consist of Russian nouns and adjectives annotated based on their
occurrences in texts created in pre-Soviet, Soviet and post-Soviet time
periods. ELMo and BERT architectures are compared on the task of ranking
Russian words according to the degree of their semantic change over time. We
use several methods for aggregation of contextualized embeddings from these
architectures and evaluate their performance. Finally, we compare unsupervised
and supervised techniques in this task.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 15:34:00 GMT'}]",2020-10-08,"[['Rodina', 'Julia', ''], ['Trofimova', 'Yuliya', ''], ['Kutuzov', 'Andrey', ''], ['Artemova', 'Ekaterina', '']]"
1359279,2010.02949,Bowen Zhang,"Bowen Zhang, Hexiang Hu, Vihan Jain, Eugene Ie, Fei Sha",Learning to Represent Image and Text with Denotation Graph,to appear at EMNLP 2020,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning to fuse vision and language information and representing them is an
important research problem with many applications. Recent progresses have
leveraged the ideas of pre-training (from language modeling) and attention
layers in Transformers to learn representation from datasets containing images
aligned with linguistic expressions that describe the images. In this paper, we
propose learning representations from a set of implied, visually grounded
expressions between image and text, automatically mined from those datasets. In
particular, we use denotation graphs to represent how specific concepts (such
as sentences describing images) can be linked to abstract and generic concepts
(such as short phrases) that are also visually grounded. This type of
generic-to-specific relations can be discovered using linguistic analysis
tools. We propose methods to incorporate such relations into learning
representation. We show that state-of-the-art multimodal learning models can be
further improved by leveraging automatically harvested structural relations.
The representations lead to stronger empirical results on downstream tasks of
cross-modal image retrieval, referring expression, and compositional
attribute-object recognition. Both our codes and the extracted denotation
graphs on the Flickr30K and the COCO datasets are publically available on
https://sha-lab.github.io/DG.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 18:00:58 GMT'}]",2020-10-08,"[['Zhang', 'Bowen', ''], ['Hu', 'Hexiang', ''], ['Jain', 'Vihan', ''], ['Ie', 'Eugene', ''], ['Sha', 'Fei', '']]"
1359816,2010.03486,Valentin Barriere,Valentin Barriere and Alexandra Balahur,"Improving Sentiment Analysis over non-English Tweets using Multilingual
  Transformers and Automatic Translation for Data-Augmentation",Accepted to COLING2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Tweets are specific text data when compared to general text. Although
sentiment analysis over tweets has become very popular in the last decade for
English, it is still difficult to find huge annotated corpora for non-English
languages. The recent rise of the transformer models in Natural Language
Processing allows to achieve unparalleled performances in many tasks, but these
models need a consequent quantity of text to adapt to the tweet domain. We
propose the use of a multilingual transformer model, that we pre-train over
English tweets and apply data-augmentation using automatic translation to adapt
the model to non-English languages. Our experiments in French, Spanish, German
and Italian suggest that the proposed technique is an efficient way to improve
the results of the transformers over small corpora of tweets in a non-English
language.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 15:44:55 GMT'}]",2020-10-08,"[['Barriere', 'Valentin', ''], ['Balahur', 'Alexandra', '']]"
1359826,2010.03496,Daniel Daza,"Daniel Daza, Michael Cochez, Paul Groth",Inductive Entity Representations from Text via Link Prediction,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a method for learning representations of entities, that uses a
Transformer-based architecture as an entity encoder, and link prediction
training on a knowledge graph with textual entity descriptions. We demonstrate
that our approach can be applied effectively for link prediction in different
inductive settings involving entities not seen during training, outperforming
related state-of-the-art methods (22% MRR improvement on average). We provide
evidence that the learned representations transfer to other tasks that do not
require fine-tuning the entity encoder. In an entity classification task we
obtain an average improvement of 16% accuracy compared with baselines that also
employ pre-trained models. For an information retrieval task, significant
improvements of up to 8.8% in NDCG@10 were obtained for natural language
queries.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 16:04:06 GMT'}]",2020-10-08,"[['Daza', 'Daniel', ''], ['Cochez', 'Michael', ''], ['Groth', 'Paul', '']]"
1359290,2010.02960,David Gaddy,David Gaddy and Dan Klein,Digital Voicing of Silent Speech,EMNLP 2020,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we consider the task of digitally voicing silent speech, where
silently mouthed words are converted to audible speech based on
electromyography (EMG) sensor measurements that capture muscle impulses. While
prior work has focused on training speech synthesis models from EMG collected
during vocalized speech, we are the first to train from EMG collected during
silently articulated speech. We introduce a method of training on silent EMG by
transferring audio targets from vocalized to silent signals. Our method greatly
improves intelligibility of audio generated from silent EMG compared to a
baseline that only trains with vocalized data, decreasing transcription word
error rate from 64% to 4% in one data condition and 88% to 68% in another. To
spur further development on this task, we share our new dataset of silent and
vocalized facial EMG measurements.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 18:23:35 GMT'}]",2020-10-08,"[['Gaddy', 'David', ''], ['Klein', 'Dan', '']]"
1359305,2010.02975,Yuchen Lu,"Yuchen Lu, Soumye Singhal, Florian Strub, Olivier Pietquin, Aaron
  Courville",Supervised Seeded Iterated Learning for Interactive Language Learning,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Language drift has been one of the major obstacles to train language models
through interaction. When word-based conversational agents are trained towards
completing a task, they tend to invent their language rather than leveraging
natural language. In recent literature, two general methods partially counter
this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL).
While S2P jointly trains interactive and supervised losses to counter the
drift, SIL changes the training dynamics to prevent language drift from
occurring. In this paper, we first highlight their respective weaknesses, i.e.,
late-stage training collapses and higher negative likelihood when evaluated on
human corpus. Given these observations, we introduce Supervised Seeded Iterated
Learning to combine both methods to minimize their respective weaknesses. We
then show the effectiveness of \algo in the language-drift translation game.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 19:09:02 GMT'}]",2020-10-08,"[['Lu', 'Yuchen', ''], ['Singhal', 'Soumye', ''], ['Strub', 'Florian', ''], ['Pietquin', 'Olivier', ''], ['Courville', 'Aaron', '']]"
1359714,2010.03384,Max Glockner,"Max Glockner, Ivan Habernal, Iryna Gurevych","Why do you think that? Exploring Faithful Sentence-Level Rationales
  Without Supervision",EMNLP Findings 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Evaluating the trustworthiness of a model's prediction is essential for
differentiating between `right for the right reasons' and `right for the wrong
reasons'. Identifying textual spans that determine the target label, known as
faithful rationales, usually relies on pipeline approaches or reinforcement
learning. However, such methods either require supervision and thus costly
annotation of the rationales or employ non-differentiable models. We propose a
differentiable training-framework to create models which output faithful
rationales on a sentence level, by solely applying supervision on the target
task. To achieve this, our model solves the task based on each rationale
individually and learns to assign high scores to those which solved the task
best. Our evaluation on three different datasets shows competitive results
compared to a standard BERT blackbox while exceeding a pipeline counterpart's
performance in two cases. We further exploit the transparent decision-making
process of these models to prefer selecting the correct rationales by applying
direct supervision, thereby boosting the performance on the rationale-level.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 12:54:28 GMT'}]",2020-10-08,"[['Glockner', 'Max', ''], ['Habernal', 'Ivan', ''], ['Gurevych', 'Iryna', '']]"
1359868,2010.03538,Esin Durmus,"Jialu Li, Esin Durmus and Claire Cardie",Exploring the Role of Argument Structure in Online Debate Persuasion,Accepted to EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Online debate forums provide users a platform to express their opinions on
controversial topics while being exposed to opinions from diverse set of
viewpoints. Existing work in Natural Language Processing (NLP) has shown that
linguistic features extracted from the debate text and features encoding the
characteristics of the audience are both critical in persuasion studies. In
this paper, we aim to further investigate the role of discourse structure of
the arguments from online debates in their persuasiveness. In particular, we
use the factor graph model to obtain features for the argument structure of
debates from an online debating platform and incorporate these features to an
LSTM-based model to predict the debater that makes the most convincing
arguments. We find that incorporating argument structure features play an
essential role in achieving the better predictive performance in assessing the
persuasiveness of the arguments in online debates.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 17:34:50 GMT'}]",2020-10-08,"[['Li', 'Jialu', ''], ['Durmus', 'Esin', ''], ['Cardie', 'Claire', '']]"
1359872,2010.03542,Shuohuan Wang,"Shuohuan Wang, Jiaxiang Liu, Xuan Ouyang, Yu Sun","Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive
  Language Identification using Pre-trained Language Models","8 pages, 2 figures, 6 tables. Accepted at Proceedings of 14th
  International Workshop on Semantic Evaluation (SemEval-2020)",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper describes Galileo's performance in SemEval-2020 Task 12 on
detecting and categorizing offensive language in social media. For Offensive
Language Identification, we proposed a multi-lingual method using Pre-trained
Language Models, ERNIE and XLM-R. For offensive language categorization, we
proposed a knowledge distillation method trained on soft labels generated by
several supervised models. Our team participated in all three sub-tasks. In
Sub-task A - Offensive Language Identification, we ranked first in terms of
average F1 scores in all languages. We are also the only team which ranked
among the top three across all languages. We also took the first place in
Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence
Target Identification.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 17:40:19 GMT'}]",2020-10-08,"[['Wang', 'Shuohuan', ''], ['Liu', 'Jiaxiang', ''], ['Ouyang', 'Xuan', ''], ['Sun', 'Yu', '']]"
1359874,2010.03544,Nima Ebadi,"Nima Ebadi, Peyman Najafirad","A Self-supervised Approach for Semantic Indexing in the Context of
  COVID-19 Pandemic",,,,,cs.IR cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The pandemic has accelerated the pace at which COVID-19 scientific papers are
published. In addition, the process of manually assigning semantic indexes to
these papers by experts is even more time-consuming and overwhelming in the
current health crisis. Therefore, there is an urgent need for automatic
semantic indexing models which can effectively scale-up to newly introduced
concepts and rapidly evolving distributions of the hyperfocused related
literature. In this research, we present a novel semantic indexing approach
based on the state-of-the-art self-supervised representation learning and
transformer encoding exclusively suitable for pandemic crises. We present a
case study on a novel dataset that is based on COVID-19 papers published and
manually indexed in PubMed. Our study shows that our self-supervised model
outperforms the best performing models of BioASQ Task 8a by micro-F1 score of
0.1 and LCA-F score of 0.08 on average. Our model also shows superior
performance on detecting the supplementary concepts which is quite important
when the focus of the literature has drastically shifted towards specific
concepts related to the pandemic. Our study sheds light on the main challenges
confronting semantic indexing models during a pandemic, namely new domains and
drastic changes of their distributions, and as a superior alternative for such
situations, propose a model founded on approaches which have shown auspicious
performance in improving generalization and data efficiency in various NLP
tasks. We also show the joint indexing of major Medical Subject Headings (MeSH)
and supplementary concepts improves the overall performance.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 17:43:55 GMT'}]",2020-10-08,"[['Ebadi', 'Nima', ''], ['Najafirad', 'Peyman', '']]"
1359316,2010.02986,Charlie Welch,"Charles Welch, Jonathan K. Kummerfeld, Ver\'onica P\'erez-Rosas, Rada
  Mihalcea",Compositional Demographic Word Embeddings,To appear at EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Word embeddings are usually derived from corpora containing text from many
individuals, thus leading to general purpose representations rather than
individually personalized representations. While personalized embeddings can be
useful to improve language model performance and other language processing
tasks, they can only be computed for people with a large amount of longitudinal
data, which is not the case for new users. We propose a new form of
personalized word embeddings that use demographic-specific word representations
derived compositionally from full or partial demographic information for a user
(i.e., gender, age, location, religion). We show that the resulting
demographic-aware word representations outperform generic word representations
on two tasks for English: language modeling and word associations. We further
explore the trade-off between the number of available attributes and their
relative effectiveness and discuss the ethical implications of using them.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 19:23:46 GMT'}]",2020-10-08,"[['Welch', 'Charles', ''], ['Kummerfeld', 'Jonathan K.', ''], ['Pérez-Rosas', 'Verónica', ''], ['Mihalcea', 'Rada', '']]"
1359606,2010.03276,Tzuf Paz-Argaman,"Tzuf Paz-Argaman, Yuval Atzmon, Gal Chechik, Reut Tsarfaty","ZEST: Zero-shot Learning from Text Descriptions using Textual Similarity
  and Visual Summarization","11 pages, Findings of EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the problem of recognizing visual entities from the textual
descriptions of their classes. Specifically, given birds' images with free-text
descriptions of their species, we learn to classify images of previously-unseen
species based on specie descriptions. This setup has been studied in the vision
community under the name zero-shot learning from text, focusing on learning to
transfer knowledge about visual aspects of birds from seen classes to
previously-unseen ones. Here, we suggest focusing on the textual description
and distilling from the description the most relevant information to
effectively match visual features to the parts of the text that discuss them.
Specifically, (1) we propose to leverage the similarity between species,
reflected in the similarity between text descriptions of the species. (2) we
derive visual summaries of the texts, i.e., extractive summaries that focus on
the visual features that tend to be reflected in images. We propose a simple
attention-based model augmented with the similarity and visual summaries
components. Our empirical results consistently and significantly outperform the
state-of-the-art on the largest benchmarks for text-based zero-shot learning,
illustrating the critical importance of texts for zero-shot image-recognition.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 08:57:34 GMT'}]",2020-10-08,"[['Paz-Argaman', 'Tzuf', ''], ['Atzmon', 'Yuval', ''], ['Chechik', 'Gal', ''], ['Tsarfaty', 'Reut', '']]"
1359339,2010.03009,Wasi Ahmad,Wasi Uddin Ahmad and Nanyun Peng and Kai-Wei Chang,"GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and
  Event Extraction","9 pages, 4 pages of references, 7 pages of supplementary material",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prevalent approaches in cross-lingual relation and event extraction use graph
convolutional networks (GCNs) with universal dependency parses to learn
language-agnostic representations such that models trained on one language can
be applied to other languages. However, GCNs lack in modeling long-range
dependencies or disconnected words in the dependency tree. To address this
challenge, we propose to utilize the self-attention mechanism where we
explicitly fuse structural information to learn the dependencies between words
at different syntactic distances. We introduce GATE, a {\bf G}raph {\bf
A}ttention {\bf T}ransformer {\bf E}ncoder, and test its cross-lingual
transferability on relation and event extraction tasks. We perform rigorous
experiments on the widely used ACE05 dataset that includes three typologically
different languages: English, Chinese, and Arabic. The evaluation results show
that GATE outperforms three recently proposed methods by a large margin. Our
detailed analysis reveals that due to the reliance on syntactic dependencies,
GATE produces robust representations that facilitate transfer across languages.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 20:30:35 GMT'}]",2020-10-08,"[['Ahmad', 'Wasi Uddin', ''], ['Peng', 'Nanyun', ''], ['Chang', 'Kai-Wei', '']]"
1359400,2010.03070,Liam Dugan,"Liam Dugan, Daphne Ippolito, Arun Kirubarajan and Chris Callison-Burch",RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text,"To be published in Annual Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2020)",,,,cs.CL cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, large neural networks for natural language generation (NLG)
have made leaps and bounds in their ability to generate fluent text. However,
the tasks of evaluating quality differences between NLG systems and
understanding how humans perceive the generated text remain both crucial and
difficult. In this system demonstration, we present Real or Fake Text (RoFT), a
website that tackles both of these challenges by inviting users to try their
hand at detecting machine-generated text in a variety of domains. We introduce
a novel evaluation task based on detecting the boundary at which a text passage
that starts off human-written transitions to being machine-generated. We show
preliminary results of using RoFT to evaluate detection of machine-generated
news articles.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 22:47:43 GMT'}]",2020-10-08,"[['Dugan', 'Liam', ''], ['Ippolito', 'Daphne', ''], ['Kirubarajan', 'Arun', ''], ['Callison-Burch', 'Chris', '']]"
1359395,2010.03065,Aditya Pal,Aditya Pal and Bhaskar Karn,"Anubhuti -- An annotated dataset for emotional analysis of Bengali short
  stories","4 pages, 6 figures",,,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  Thousands of short stories and articles are being written in many different
languages all around the world today. Bengali, or Bangla, is the second highest
spoken language in India after Hindi and is the national language of the
country of Bangladesh. This work reports in detail the creation of Anubhuti --
the first and largest text corpus for analyzing emotions expressed by writers
of Bengali short stories. We explain the data collection methods, the manual
annotation process and the resulting high inter-annotator agreement of the
dataset due to the linguistic expertise of the annotators and the clear
methodology of labelling followed. We also address some of the challenges faced
in the collection of raw data and annotation process of a low resource language
like Bengali. We have verified the performance of our dataset with baseline
Machine Learning as well as a Deep Learning model for emotion classification
and have found that these standard models have a high accuracy and relevant
feature selection on Anubhuti. In addition, we also explain how this dataset
can be of interest to linguists and data analysts to study the flow of emotions
as expressed by writers of Bengali literature.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 22:33:58 GMT'}]",2020-10-08,"[['Pal', 'Aditya', ''], ['Karn', 'Bhaskar', '']]"
1359391,2010.03061,Adam Poliak,Adam Poliak,A Survey on Recognizing Textual Entailment as an NLP Evaluation,"1st Workshop on Evaluation and Comparison for NLP systems (Eval4NLP)
  at EMNLP 2020; 18 pages",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recognizing Textual Entailment (RTE) was proposed as a unified evaluation
framework to compare semantic understanding of different NLP systems. In this
survey paper, we provide an overview of different approaches for evaluating and
understanding the reasoning capabilities of NLP systems. We then focus our
discussion on RTE by highlighting prominent RTE datasets as well as advances in
RTE dataset that focus on specific linguistic phenomena that can be used to
evaluate NLP systems on a fine-grained level. We conclude by arguing that when
evaluating NLP systems, the community should utilize newly introduced RTE
datasets that focus on specific linguistic phenomena.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 22:23:00 GMT'}]",2020-10-08,"[['Poliak', 'Adam', '']]"
1359602,2010.03272,Harsh Jhamtani,Harsh Jhamtani and Taylor Berg-Kirkpatrick,Narrative Text Generation with a Latent Discrete Plan,Findings of EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Past work on story generation has demonstrated the usefulness of conditioning
on a generation plan to generate coherent stories. However, these approaches
have used heuristics or off-the-shelf models to first tag training stories with
the desired type of plan, and then train generation models in a supervised
fashion. In this paper, we propose a deep latent variable model that first
samples a sequence of anchor words, one per sentence in the story, as part of
its generative process. During training, our model treats the sequence of
anchor words as a latent variable and attempts to induce anchoring sequences
that help guide generation in an unsupervised fashion. We conduct experiments
with several types of sentence decoder distributions: left-to-right and
non-monotonic, with different degrees of restriction. Further, since we use
amortized variational inference to train our model, we introduce two
corresponding types of inference network for predicting the posterior on anchor
words. We conduct human evaluations which demonstrate that the stories produced
by our model are rated better in comparison with baselines which do not
consider story plans, and are similar or better in quality relative to
baselines which use external supervision for plans. Additionally, the proposed
model gets favorable scores when evaluated on perplexity, diversity, and
control of story via discrete plan.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 08:45:37 GMT'}]",2020-10-08,"[['Jhamtani', 'Harsh', ''], ['Berg-Kirkpatrick', 'Taylor', '']]"
1359390,2010.03060,Gongbo Liang,"Gongbo Liang, Connor Greenwell, Yu Zhang, Xiaoqin Wang, Ramakanth
  Kavuluru, Nathan Jacobs",Weakly-Supervised Feature Learning via Text and Image Matching,,,,,cs.CV cs.CL cs.LG eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When training deep neural networks for medical image classification,
obtaining a sufficient number of manually annotated images is often a
significant challenge. We propose to use textual findings, which are routinely
written by clinicians during manual image analysis, to help overcome this
problem. The key idea is to use a contrastive loss to train image and text
feature extractors to recognize if a given image-finding pair is a true match.
The learned image feature extractor is then fine-tuned, in a transfer learning
setting, for a supervised classification task. This approach makes it possible
to train using large datasets because pairs of images and textual findings are
widely available in medical records. We evaluate our method on three datasets
and find consistent performance improvements. The biggest gains are realized
when fewer manually labeled examples are available. In some cases, our method
achieves the same performance as the baseline even when using 70\%--98\% fewer
labeled examples.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 22:20:29 GMT'}]",2020-10-08,"[['Liang', 'Gongbo', ''], ['Greenwell', 'Connor', ''], ['Zhang', 'Yu', ''], ['Wang', 'Xiaoqin', ''], ['Kavuluru', 'Ramakanth', ''], ['Jacobs', 'Nathan', '']]"
1359136,2010.02806,Bertrand Higy,"Bertrand Higy, Desmond Elliott, Grzegorz Chrupa{\l}a",Textual Supervision for Visually Grounded Spoken Language Understanding,Findings of EMNLP 2020,,,,cs.CL cs.LG cs.SD eess.AS,http://creativecommons.org/licenses/by/4.0/,"  Visually-grounded models of spoken language understanding extract semantic
information directly from speech, without relying on transcriptions. This is
useful for low-resource languages, where transcriptions can be expensive or
impossible to obtain. Recent work showed that these models can be improved if
transcriptions are available at training time. However, it is not clear how an
end-to-end approach compares to a traditional pipeline-based approach when one
has access to transcriptions. Comparing different strategies, we find that the
pipeline approach works better when enough text is available. With low-resource
languages in mind, we also show that translations can be effectively used in
place of transcriptions but more data is needed to obtain similar results.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 15:16:23 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 07:48:12 GMT'}]",2020-10-08,"[['Higy', 'Bertrand', ''], ['Elliott', 'Desmond', ''], ['Chrupała', 'Grzegorz', '']]"
1359590,2010.03260,Tao Ge,"Mengyun Chen, Tao Ge, Xingxing Zhang, Furu Wei, Ming Zhou","Improving the Efficiency of Grammatical Error Correction with Erroneous
  Span Detection and Correction",Accepted by EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We propose a novel language-independent approach to improve the efficiency
for Grammatical Error Correction (GEC) by dividing the task into two subtasks:
Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD
identifies grammatically incorrect text spans with an efficient sequence
tagging model. Then, ESC leverages a seq2seq model to take the sentence with
annotated erroneous spans as input and only outputs the corrected text for
these spans. Experiments show our approach performs comparably to conventional
seq2seq approaches in both English and Chinese GEC benchmarks with less than
50% time cost for inference.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 08:29:11 GMT'}]",2020-10-08,"[['Chen', 'Mengyun', ''], ['Ge', 'Tao', ''], ['Zhang', 'Xingxing', ''], ['Wei', 'Furu', ''], ['Zhou', 'Ming', '']]"
1352858,2009.11355,Yasmin Salehi,"Kian Ahrabian, Aarash Feizi, Yasmin Salehi, William L. Hamilton and
  Avishek Joey Bose",Structure Aware Negative Sampling in Knowledge Graphs,Accepted to EMNLP 2020. Camera-ready submission,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning low-dimensional representations for entities and relations in
knowledge graphs using contrastive estimation represents a scalable and
effective method for inferring connectivity patterns. A crucial aspect of
contrastive learning approaches is the choice of corruption distribution that
generates hard negative samples, which force the embedding model to learn
discriminative representations and find critical characteristics of observed
data. While earlier methods either employ too simple corruption distributions,
i.e. uniform, yielding easy uninformative negatives or sophisticated
adversarial distributions with challenging optimization schemes, they do not
explicitly incorporate known graph structure resulting in suboptimal negatives.
In this paper, we propose Structure Aware Negative Sampling (SANS), an
inexpensive negative sampling strategy that utilizes the rich graph structure
by selecting negative samples from a node's k-hop neighborhood. Empirically, we
demonstrate that SANS finds semantically meaningful negatives and is
competitive with SOTA approaches while requires no additional parameters nor
difficult adversarial optimization.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 19:57:00 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 02:23:58 GMT'}]",2020-10-08,"[['Ahrabian', 'Kian', ''], ['Feizi', 'Aarash', ''], ['Salehi', 'Yasmin', ''], ['Hamilton', 'William L.', ''], ['Bose', 'Avishek Joey', '']]"
1359604,2010.03274,Harsh Jhamtani,Harsh Jhamtani and Peter Clark,"Learning to Explain: Datasets and Models for Identifying Valid Reasoning
  Chains in Multihop Question-Answering",EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the rapid progress in multihop question-answering (QA), models still
have trouble explaining why an answer is correct, with limited explanation
training data available to learn from. To address this, we introduce three
explanation datasets in which explanations formed from corpus facts are
annotated. Our first dataset, eQASC, contains over 98K explanation annotations
for the multihop question answering dataset QASC, and is the first that
annotates multiple candidate explanations for each answer. The second dataset
eQASC-perturbed is constructed by crowd-sourcing perturbations (while
preserving their validity) of a subset of explanations in QASC, to test
consistency and generalization of explanation prediction models. The third
dataset eOBQA is constructed by adding explanation annotations to the OBQA
dataset to test generalization of models trained on eQASC. We show that this
data can be used to significantly improve explanation quality (+14% absolute F1
over a strong retrieval baseline) using a BERT-based classifier, but still
behind the upper bound, offering a new challenge for future research. We also
explore a delexicalized chain representation in which repeated noun phrases are
replaced by variables, thus turning them into generalized reasoning chains (for
example: ""X is a Y"" AND ""Y has Z"" IMPLIES ""X has Z""). We find that generalized
chains maintain performance while also being more robust to certain
perturbations.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 08:46:02 GMT'}]",2020-10-08,"[['Jhamtani', 'Harsh', ''], ['Clark', 'Peter', '']]"
1359347,2010.03017,Zirui Wang,"Zirui Wang, Zachary C. Lipton, Yulia Tsvetkov","On Negative Interference in Multilingual Models: Findings and A
  Meta-Learning Treatment",Published as a main conference paper at EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern multilingual models are trained on concatenated text from multiple
languages in hopes of conferring benefits to each (positive transfer), with the
most pronounced benefits accruing to low-resource languages. However, recent
work has shown that this approach can degrade performance on high-resource
languages, a phenomenon known as negative interference. In this paper, we
present the first systematic study of negative interference. We show that,
contrary to previous belief, negative interference also impacts low-resource
languages. While parameters are maximally shared to learn language-universal
structures, we demonstrate that language-specific parameters do exist in
multilingual models and they are a potential cause of negative interference.
Motivated by these observations, we also present a meta-learning algorithm that
obtains better cross-lingual transferability and alleviates negative
interference, by adding language-specific layers as meta-parameters and
training them in a manner that explicitly improves shared layers'
generalization on all languages. Overall, our results show that negative
interference is more common than previously known, suggesting new directions
for improving multilingual representations.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 20:48:58 GMT'}]",2020-10-08,"[['Wang', 'Zirui', ''], ['Lipton', 'Zachary C.', ''], ['Tsvetkov', 'Yulia', '']]"
1201664,1911.03070,Mozhi Zhang,"Michelle Yuan, Mozhi Zhang, Benjamin Van Durme, Leah Findlater, Jordan
  Boyd-Graber",Interactive Refinement of Cross-Lingual Word Embeddings,EMNLP 2020; first two authors contribute equally,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cross-lingual word embeddings transfer knowledge between languages: models
trained on high-resource languages can predict in low-resource languages. We
introduce CLIME, an interactive system to quickly refine cross-lingual word
embeddings for a given classification problem. First, CLIME ranks words by
their salience to the downstream task. Then, users mark similarity between
keywords and their nearest neighbors in the embedding space. Finally, CLIME
updates the embeddings using the annotations. We evaluate CLIME on identifying
health-related text in four low-resource languages: Ilocano, Sinhalese,
Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word
semantics and have higher test accuracy than the original embeddings. CLIME
often improves accuracy faster than an active learning baseline and can be
easily combined with active learning to improve results.
","[{'version': 'v1', 'created': 'Fri, 8 Nov 2019 06:07:25 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Apr 2020 17:58:45 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 17:49:50 GMT'}]",2020-10-08,"[['Yuan', 'Michelle', ''], ['Zhang', 'Mozhi', ''], ['Van Durme', 'Benjamin', ''], ['Findlater', 'Leah', ''], ['Boyd-Graber', 'Jordan', '']]"
809257,1701.03849,Pavel Kral,"Ladislav Lenc, Pavel Kr\'al",Deep Neural Networks for Czech Multi-label Document Classification,"Presented at 17th International Conference on Intelligent Text
  Processing and Computational Linguistics (CICLing 2016), Konya, Turkey, 3-9
  April 2016, pp. 460-471, Springer, ISBN: 978-3-319-75487-1",,10.1007/978-3-319-75487-1_36,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper is focused on automatic multi-label document classification of
Czech text documents. The current approaches usually use some pre-processing
which can have negative impact (loss of information, additional implementation
work, etc). Therefore, we would like to omit it and use deep neural networks
that learn from simple features. This choice was motivated by their successful
usage in many other machine learning fields. Two different networks are
compared: the first one is a standard multi-layer perceptron, while the second
one is a popular convolutional network. The experiments on a Czech newspaper
corpus show that both networks significantly outperform baseline method which
uses a rich set of features with maximum entropy classifier. We have also shown
that convolutional network gives the best results.
","[{'version': 'v1', 'created': 'Fri, 13 Jan 2017 23:23:12 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Jan 2017 23:17:30 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 20:07:14 GMT'}]",2020-10-08,"[['Lenc', 'Ladislav', ''], ['Král', 'Pavel', '']]"
1359876,2010.03546,Xilun Chen,"Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke Zettlemoyer and Sonal
  Gupta","Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic
  Parsing",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task-oriented semantic parsing is a critical component of virtual assistants,
which is responsible for understanding the user's intents (set reminder, play
music, etc.). Recent advances in deep learning have enabled several approaches
to successfully parse more complex queries (Gupta et al., 2018; Rongali et
al.,2020), but these models require a large amount of annotated training data
to parse queries on new domains (e.g. reminder, music).
  In this paper, we focus on adapting task-oriented semantic parsers to
low-resource domains, and propose a novel method that outperforms a supervised
neural model at a 10-fold data reduction. In particular, we identify two
fundamental factors for low-resource domain adaptation: better representation
learning and better training techniques. Our representation learning uses BART
(Lewis et al., 2019) to initialize our model which outperforms encoder-only
pre-trained representations used in previous work. Furthermore, we train with
optimization-based meta-learning (Finn et al., 2017) to improve generalization
to low-resource domains. This approach significantly outperforms all baseline
methods in the experiments on a newly collected multi-domain task-oriented
semantic parsing dataset (TOPv2), which we release to the public.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 17:47:53 GMT'}]",2020-10-08,"[['Chen', 'Xilun', ''], ['Ghoshal', 'Asish', ''], ['Mehdad', 'Yashar', ''], ['Zettlemoyer', 'Luke', ''], ['Gupta', 'Sonal', '']]"
1359340,2010.03010,Kanishka Misra,"Kanishka Misra, Allyson Ettinger, Julia Taylor Rayz","Exploring BERT's Sensitivity to Lexical Cues using Tests from Semantic
  Priming",Accepted for publication in Findings of ACL: EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Models trained to estimate word probabilities in context have become
ubiquitous in natural language processing. How do these models use lexical cues
in context to inform their word probabilities? To answer this question, we
present a case study analyzing the pre-trained BERT model with tests informed
by semantic priming. Using English lexical stimuli that show priming in humans,
we find that BERT too shows ""priming,"" predicting a word with greater
probability when the context includes a related word versus an unrelated one.
This effect decreases as the amount of information provided by the context
increases. Follow-up analysis shows BERT to be increasingly distracted by
related prime words as context becomes more informative, assigning lower
probabilities to related words. Our findings highlight the importance of
considering contextual constraint effects when studying word prediction in
these models, and highlight possible parallels with human processing.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 20:30:59 GMT'}]",2020-10-08,"[['Misra', 'Kanishka', ''], ['Ettinger', 'Allyson', ''], ['Rayz', 'Julia Taylor', '']]"
1359352,2010.03022,Jie Ma,"Jie Ma, Shuai Wang, Rishita Anubhai, Miguel Ballesteros, Yaser
  Al-Onaizan",Resource-Enhanced Neural Model for Event Argument Extraction,Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Event argument extraction (EAE) aims to identify the arguments of an event
and classify the roles that those arguments play. Despite great efforts made in
prior work, there remain many challenges: (1) Data scarcity. (2) Capturing the
long-range dependency, specifically, the connection between an event trigger
and a distant event argument. (3) Integrating event trigger information into
candidate argument representation. For (1), we explore using unlabeled data in
different ways. For (2), we propose to use a syntax-attending Transformer that
can utilize dependency parses to guide the attention mechanism. For (3), we
propose a trigger-aware sequence encoder with several types of
trigger-dependent sequence representations. We also support argument extraction
either from text annotated with gold entities or from plain text. Experiments
on the English ACE2005 benchmark show that our approach achieves a new
state-of-the-art.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 21:00:54 GMT'}]",2020-10-08,"[['Ma', 'Jie', ''], ['Wang', 'Shuai', ''], ['Anubhai', 'Rishita', ''], ['Ballesteros', 'Miguel', ''], ['Al-Onaizan', 'Yaser', '']]"
1359780,2010.03450,Annie Louis,"Annie Louis, Dan Roth, and Filip Radlinski","""I'd rather just go to bed"": Understanding Indirect Answers","15 pages, 3 figures. To appear at the 2020 Conference on Empirical
  Methods in Natural Language Processing (EMNLP), 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We revisit a pragmatic inference problem in dialog: understanding indirect
responses to questions. Humans can interpret 'I'm starving.' in response to
'Hungry?', even without direct cue words such as 'yes' and 'no'. In dialog
systems, allowing natural responses rather than closed vocabularies would be
similarly beneficial. However, today's systems are only as sensitive to these
pragmatic moves as their language model allows. We create and release the first
large-scale English language corpus 'Circa' with 34,268 (polar question,
indirect answer) pairs to enable progress on this task. The data was collected
via elaborate crowdsourcing, and contains utterances with yes/no meaning, as
well as uncertain, middle-ground, and conditional responses. We also present
BERT-based neural models to predict such categories for a question-answer pair.
We find that while transfer learning from entailment works reasonably,
performance is not yet sufficient for robust dialog. Our models reach 82-88%
accuracy for a 4-class distinction, and 74-85% for 6 classes.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 14:41:40 GMT'}]",2020-10-08,"[['Louis', 'Annie', ''], ['Roth', 'Dan', ''], ['Radlinski', 'Filip', '']]"
1212238,1912.00342,Won Ik Cho,"Won Ik Cho, Young Ki Moon, Sangwhan Moon, Seok Min Kim, Nam Soo Kim","Machines Getting with the Program: Understanding Intent Arguments of
  Non-Canonical Directives",Findings of ACL: EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern dialog managers face the challenge of having to fulfill human-level
conversational skills as part of common user expectations, including but not
limited to discourse with no clear objective. Along with these requirements,
agents are expected to extrapolate intent from the user's dialogue even when
subjected to non-canonical forms of speech. This depends on the agent's
comprehension of paraphrased forms of such utterances. Especially in
low-resource languages, the lack of data is a bottleneck that prevents
advancements of the comprehension performance for these types of agents. In
this regard, here we demonstrate the necessity of extracting the intent
argument of non-canonical directives in a natural language format, which may
yield more accurate parsing, and suggest guidelines for building a parallel
corpus for this purpose. Following the guidelines, we construct a Korean corpus
of 50K instances of question/command-intent pairs, including the labels for
classification of the utterance type. We also propose a method for mitigating
class imbalance, demonstrating the potential applications of the corpus
generation method and its multilingual extensibility.
","[{'version': 'v1', 'created': 'Sun, 1 Dec 2019 07:08:19 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 08:55:30 GMT'}]",2020-10-08,"[['Cho', 'Won Ik', ''], ['Moon', 'Young Ki', ''], ['Moon', 'Sangwhan', ''], ['Kim', 'Seok Min', ''], ['Kim', 'Nam Soo', '']]"
1359668,2010.03338,Mingzhu Wu,"Mingzhu Wu, Nafise Sadat Moosavi, Andreas R\""uckl\'e and Iryna
  Gurevych",Improving QA Generalization by Concurrent Modeling of Multiple Biases,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing NLP datasets contain various biases that models can easily exploit
to achieve high performances on the corresponding evaluation sets. However,
focusing on dataset-specific biases limits their ability to learn more
generalizable knowledge about the task from more general data patterns. In this
paper, we investigate the impact of debiasing methods for improving
generalization and propose a general framework for improving the performance on
both in-domain and out-of-domain datasets by concurrent modeling of multiple
biases in the training data. Our framework weights each example based on the
biases it contains and the strength of those biases in the training data. It
then uses these weights in the training objective so that the model relies less
on examples with high bias weights. We extensively evaluate our framework on
extractive question answering with training data from various domains with
multiple biases of different strengths. We perform the evaluations in two
different settings, in which the model is trained on a single domain or
multiple domains simultaneously, and show its effectiveness in both settings
compared to state-of-the-art debiasing methods.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 11:18:49 GMT'}]",2020-10-08,"[['Wu', 'Mingzhu', ''], ['Moosavi', 'Nafise Sadat', ''], ['Rücklé', 'Andreas', ''], ['Gurevych', 'Iryna', '']]"
1350938,2009.09435,Francisco Vargas,Francisco Vargas and Ryan Cotterell,Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation,,"Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing",,,cs.LG cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Bolukbasi et al. (2016) presents one of the first gender bias mitigation
techniques for word embeddings. Their method takes pre-trained word embeddings
as input and attempts to isolate a linear subspace that captures most of the
gender bias in the embeddings. As judged by an analogical evaluation task,
their method virtually eliminates gender bias in the embeddings. However, an
implicit and untested assumption of their method is that the bias sub-space is
actually linear. In this work, we generalize their method to a kernelized,
non-linear version. We take inspiration from kernel principal component
analysis and derive a non-linear bias isolation technique. We discuss and
overcome some of the practical drawbacks of our method for non-linear gender
bias mitigation in word embeddings and analyze empirically whether the bias
subspace is actually linear. Our analysis shows that gender bias is in fact
well captured by a linear subspace, justifying the assumption of Bolukbasi et
al. (2016).
","[{'version': 'v1', 'created': 'Sun, 20 Sep 2020 14:13:45 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 12:11:40 GMT'}]",2020-10-08,"[['Vargas', 'Francisco', ''], ['Cotterell', 'Ryan', '']]"
1280467,2005.00672,Valentin Hofmann,"Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\""utze","DagoBERT: Generating Derivational Morphology with a Pretrained Language
  Model",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Can pretrained language models (PLMs) generate derivationally complex words?
We present the first study investigating this question, taking BERT as the
example PLM. We examine BERT's derivational capabilities in different settings,
ranging from using the unmodified pretrained model to full finetuning. Our best
model, DagoBERT (Derivationally and generatively optimized BERT), clearly
outperforms the previous state of the art in derivation generation (DG).
Furthermore, our experiments show that the input segmentation crucially impacts
BERT's derivational knowledge, suggesting that the performance of PLMs could be
further improved if a morphologically informed vocabulary of units were used.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 01:26:46 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 16:57:31 GMT'}]",2020-10-08,"[['Hofmann', 'Valentin', ''], ['Pierrehumbert', 'Janet B.', ''], ['Schütze', 'Hinrich', '']]"
1280487,2005.00692,Xingyu Fu,"Xingyu Fu, Weijia Shi, Xiaodong Yu, Zian Zhao, Dan Roth",Design Challenges in Low-resource Cross-lingual Entity Linking,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cross-lingual Entity Linking (XEL), the problem of grounding mentions of
entities in a foreign language text into an English knowledge base such as
Wikipedia, has seen a lot of research in recent years, with a range of
promising techniques. However, current techniques do not rise to the challenges
introduced by text in low-resource languages (LRL) and, surprisingly, fail to
generalize to text not taken from Wikipedia, on which they are usually trained.
  This paper provides a thorough analysis of low-resource XEL techniques,
focusing on the key step of identifying candidate English Wikipedia titles that
correspond to a given foreign language mention. Our analysis indicates that
current methods are limited by their reliance on Wikipedia's interlanguage
links and thus suffer when the foreign language's Wikipedia is small. We
conclude that the LRL setting requires the use of outside-Wikipedia
cross-lingual resources and present a simple yet effective zero-shot XEL
system, QuEL, that utilizes search engines query logs. With experiments on 25
languages, QuEL~shows an average increase of 25\% in gold candidate recall and
of 13\% in end-to-end linking accuracy over state-of-the-art baselines.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 04:00:26 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 06:27:49 GMT'}]",2020-10-08,"[['Fu', 'Xingyu', ''], ['Shi', 'Weijia', ''], ['Yu', 'Xiaodong', ''], ['Zhao', 'Zian', ''], ['Roth', 'Dan', '']]"
1278795,2004.14025,Sungjin Park,"Sungjin Park, Taesun Whang, Yeochan Yoon, Heuiseok Lim",Multi-View Attention Network for Visual Dialog,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Visual dialog is a challenging vision-language task in which a series of
questions visually grounded by a given image are answered. To resolve the
visual dialog task, a high-level understanding of various multimodal inputs
(e.g., question, dialog history, and image) is required. Specifically, it is
necessary for an agent to 1) determine the semantic intent of question and 2)
align question-relevant textual and visual contents among heterogeneous
modality inputs. In this paper, we propose Multi-View Attention Network (MVAN),
which leverages multiple views about heterogeneous inputs based on attention
mechanisms. MVAN effectively captures the question-relevant information from
the dialog history with two complementary modules (i.e., Topic Aggregation and
Context Matching), and builds multimodal representations through sequential
alignment processes (i.e., Modality Alignment). Experimental results on VisDial
v1.0 dataset show the effectiveness of our proposed model, which outperforms
the previous state-of-the-art methods with respect to all evaluation metrics.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 08:46:38 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 11:28:57 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 00:51:40 GMT'}]",2020-10-08,"[['Park', 'Sungjin', ''], ['Whang', 'Taesun', ''], ['Yoon', 'Yeochan', ''], ['Lim', 'Heuiseok', '']]"
1311731,2007.00217,Minbyul Jeong,"Minbyul Jeong, Mujeen Sung, Gangwoo Kim, Donghyeon Kim, Wonjin Yoon,
  Jaehyo Yoo, Jaewoo Kang","Transferability of Natural Language Inference to Biomedical Question
  Answering",submit for the 8th BioASQ workshop 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Biomedical question answering (QA) is a challenging task due to the scarcity
of data and the requirement of domain expertise. Pre-trained language models
have been used to address these issues. Recently, learning relationships
between sentence pairs has been proved to improve performance in general QA. In
this paper, we focus on applying BioBERT to transfer the knowledge of natural
language inference (NLI) to biomedical QA. We observe that BioBERT trained on
the NLI dataset obtains better performance on Yes/No (+5.59%), Factoid
(+0.53%), List type (+13.58%) questions compared to performance obtained in a
previous challenge (BioASQ 7B Phase B). We present a sequential transfer
learning method that significantly performed well in the 8th BioASQ Challenge
(Phase B). In sequential transfer learning, the order in which tasks are
fine-tuned is important. We measure an unanswerable rate of the extractive QA
setting when the formats of factoid and list type questions are converted to
the format of the Stanford Question Answering Dataset (SQuAD).
","[{'version': 'v1', 'created': 'Wed, 1 Jul 2020 04:05:48 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Aug 2020 08:21:55 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 07:08:36 GMT'}]",2020-10-08,"[['Jeong', 'Minbyul', ''], ['Sung', 'Mujeen', ''], ['Kim', 'Gangwoo', ''], ['Kim', 'Donghyeon', ''], ['Yoon', 'Wonjin', ''], ['Yoo', 'Jaehyo', ''], ['Kang', 'Jaewoo', '']]"
1274503,2004.09733,Yuxian Gu,"Yuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, Maosong Sun",Train No Evil: Selective Masking for Task-Guided Pre-Training,Accepted by EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, pre-trained language models mostly follow the
pre-train-then-fine-tuning paradigm and have achieved great performance on
various downstream tasks. However, since the pre-training stage is typically
task-agnostic and the fine-tuning stage usually suffers from insufficient
supervised data, the models cannot always well capture the domain-specific and
task-specific patterns. In this paper, we propose a three-stage framework by
adding a task-guided pre-training stage with selective masking between general
pre-training and fine-tuning. In this stage, the model is trained by masked
language modeling on in-domain unsupervised data to learn domain-specific
patterns and we propose a novel selective masking strategy to learn
task-specific patterns. Specifically, we design a method to measure the
importance of each token in sequences and selectively mask the important
tokens. Experimental results on two sentiment analysis tasks show that our
method can achieve comparable or even better performance with less than 50% of
computation cost, which indicates our method is both effective and efficient.
The source code of this paper can be obtained from
https://github.com/thunlp/SelectiveMasking.
","[{'version': 'v1', 'created': 'Tue, 21 Apr 2020 03:14:22 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 09:47:41 GMT'}]",2020-10-08,"[['Gu', 'Yuxian', ''], ['Zhang', 'Zhengyan', ''], ['Wang', 'Xiaozhi', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', '']]"
1110570,1904.05530,Xiang Ren,"Woojeong Jin, Meng Qu, Xisen Jin, Xiang Ren","Recurrent Event Network: Autoregressive Structure Inference over
  Temporal Knowledge Graphs","15 pages, 8 figures, accepted at as full paper in EMNLP 2020",,,,cs.LG cs.AI cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graph reasoning is a critical task in natural language processing.
The task becomes more challenging on temporal knowledge graphs, where each fact
is associated with a timestamp. Most existing methods focus on reasoning at
past timestamps and they are not able to predict facts happening in the future.
This paper proposes Recurrent Event Network (RE-NET), a novel autoregressive
architecture for predicting future interactions. The occurrence of a fact
(event) is modeled as a probability distribution conditioned on temporal
sequences of past knowledge graphs. Specifically, our RE-NET employs a
recurrent event encoder to encode past facts and uses a neighborhood aggregator
to model the connection of facts at the same timestamp. Future facts can then
be inferred in a sequential manner based on the two modules. We evaluate our
proposed method via link prediction at future times on five public datasets.
Through extensive experiments, we demonstrate the strength of RENET, especially
on multi-step inference over future timestamps, and achieve state-of-the-art
performance on all five datasets. Code and data can be found at
https://github.com/INK-USC/RE-Net.
","[{'version': 'v1', 'created': 'Thu, 11 Apr 2019 04:45:42 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Jun 2019 19:06:37 GMT'}, {'version': 'v3', 'created': 'Tue, 8 Oct 2019 03:32:40 GMT'}, {'version': 'v4', 'created': 'Tue, 6 Oct 2020 18:40:59 GMT'}]",2020-10-08,"[['Jin', 'Woojeong', ''], ['Qu', 'Meng', ''], ['Jin', 'Xisen', ''], ['Ren', 'Xiang', '']]"
1169692,1908.11522,Junru Zhou,"Junru Zhou, Zuchao Li, Hai Zhao","Parsing All: Syntax and Semantics, Dependencies and Spans","EMNLP 2020, ACL Findings. arXiv admin note: text overlap with
  arXiv:1907.02684",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Both syntactic and semantic structures are key linguistic contextual clues,
in which parsing the latter has been well shown beneficial from parsing the
former. However, few works ever made an attempt to let semantic parsing help
syntactic parsing. As linguistic representation formalisms, both syntax and
semantics may be represented in either span (constituent/phrase) or dependency,
on both of which joint learning was also seldom explored. In this paper, we
propose a novel joint model of syntactic and semantic parsing on both span and
dependency representations, which incorporates syntactic information
effectively in the encoder of neural network and benefits from two
representation formalisms in a uniform way. The experiments show that semantics
and syntax can benefit each other by optimizing joint objectives. Our single
model achieves new state-of-the-art or competitive results on both span and
dependency semantic parsing on Propbank benchmarks and both dependency and
constituent syntactic parsing on Penn Treebank.
","[{'version': 'v1', 'created': 'Fri, 30 Aug 2019 03:49:19 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Apr 2020 05:52:11 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 03:30:01 GMT'}]",2020-10-08,"[['Zhou', 'Junru', ''], ['Li', 'Zuchao', ''], ['Zhao', 'Hai', '']]"
1271972,2004.07202,Livio Baldini Soares,"Thibault F\'evry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol
  Choi, Tom Kwiatkowski",Entities as Experts: Sparse Memory Access with Entity Supervision,,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We focus on the problem of capturing declarative knowledge about entities in
the learned parameters of a language model. We introduce a new model - Entities
as Experts (EAE) - that can access distinct memories of the entities mentioned
in a piece of text. Unlike previous efforts to integrate entity knowledge into
sequence models, EAE's entity representations are learned directly from text.
We show that EAE's learned representations capture sufficient knowledge to
answer TriviaQA questions such as ""Which Dr. Who villain has been played by
Roger Delgado, Anthony Ainley, Eric Roberts?"", outperforming an
encoder-generator Transformer model with 10x the parameters. According to the
LAMA knowledge probes, EAE contains more factual knowledge than a similarly
sized BERT, as well as previous approaches that integrate external sources of
entity knowledge. Because EAE associates parameters with specific entities, it
only needs to access a fraction of its parameters at inference time, and we
show that the correct identification and representation of entities is
essential to EAE's performance.
","[{'version': 'v1', 'created': 'Wed, 15 Apr 2020 17:00:05 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 19:00:27 GMT'}]",2020-10-08,"[['Févry', 'Thibault', ''], ['Soares', 'Livio Baldini', ''], ['FitzGerald', 'Nicholas', ''], ['Choi', 'Eunsol', ''], ['Kwiatkowski', 'Tom', '']]"
1358334,2010.02004,Emanuele La Malfa,"Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony
  Hartshorn, Marta Kwiatkowska","Assessing Robustness of Text Classification through Maximal Safe Radius
  Computation",12 pages + appendix,EMNLP-Findings2020,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural network NLP models are vulnerable to small modifications of the input
that maintain the original meaning but result in a different prediction. In
this paper, we focus on robustness of text classification against word
substitutions, aiming to provide guarantees that the model prediction does not
change if a word is replaced with a plausible alternative, such as a synonym.
As a measure of robustness, we adopt the notion of the maximal safe radius for
a given input text, which is the minimum distance in the embedding space to the
decision boundary. Since computing the exact maximal safe radius is not
feasible in practice, we instead approximate it by computing a lower and upper
bound. For the upper bound computation, we employ Monte Carlo Tree Search in
conjunction with syntactic filtering to analyse the effect of single and
multiple word substitutions. The lower bound computation is achieved through an
adaptation of the linear bounding techniques implemented in tools CNN-Cert and
POPQORN, respectively for convolutional and recurrent network models. We
evaluate the methods on sentiment analysis and news classification models for
four datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and
provide an analysis of robustness trends. We also apply our framework to
interpretability analysis and compare it with LIME.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 09:46:32 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 08:50:10 GMT'}]",2020-10-08,"[['La Malfa', 'Emanuele', ''], ['Wu', 'Min', ''], ['Laurenti', 'Luca', ''], ['Wang', 'Benjie', ''], ['Hartshorn', 'Anthony', ''], ['Kwiatkowska', 'Marta', '']]"
1280063,2005.00268,Emily Sheng,"Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng",Towards Controllable Biases in Language Generation,"16 pages, Findings of EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a general approach towards controllable societal biases in natural
language generation (NLG). Building upon the idea of adversarial triggers, we
develop a method to induce societal biases in generated text when input prompts
contain mentions of specific demographic groups. We then analyze two scenarios:
1) inducing negative biases for one demographic and positive biases for another
demographic, and 2) equalizing biases between demographics. The former scenario
enables us to detect the types of biases present in the model. Specifically, we
show the effectiveness of our approach at facilitating bias analysis by finding
topics that correspond to demographic inequalities in generated text and
comparing the relative effectiveness of inducing biases for different
demographics. The second scenario is useful for mitigating biases in downstream
applications such as dialogue generation. In our experiments, the mitigation
technique proves to be effective at equalizing the amount of biases across
demographics while simultaneously generating less negatively biased text
overall.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 08:25:11 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 05:17:16 GMT'}]",2020-10-08,"[['Sheng', 'Emily', ''], ['Chang', 'Kai-Wei', ''], ['Natarajan', 'Premkumar', ''], ['Peng', 'Nanyun', '']]"
1280495,2005.00700,Daniel Khashabi Mr.,"Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind
  Tafjord, Peter Clark, Hannaneh Hajishirzi",UnifiedQA: Crossing Format Boundaries With a Single QA System,EMNLP 2020 (Findings),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Question answering (QA) tasks have been posed using a variety of formats,
such as extractive span selection, multiple choice, etc. This has led to
format-specialized models, and even to an implicit division in the QA
community. We argue that such boundaries are artificial and perhaps
unnecessary, given the reasoning abilities we seek to teach are not governed by
the format. As evidence, we use the latest advances in language modeling to
build a single pre-trained QA model, UnifiedQA, that performs surprisingly well
across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par
with 9 different models that were trained on individual datasets themselves.
Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs
surprisingly well, showing strong generalization from its out-of-format
training data. Finally, simply fine-tuning this pre-trained QA model into
specialized models results in a new state of the art on 6 datasets,
establishing UnifiedQA as a strong starting point for building QA systems.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 04:42:14 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 07:46:48 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 03:12:45 GMT'}]",2020-10-08,"[['Khashabi', 'Daniel', ''], ['Min', 'Sewon', ''], ['Khot', 'Tushar', ''], ['Sabharwal', 'Ashish', ''], ['Tafjord', 'Oyvind', ''], ['Clark', 'Peter', ''], ['Hajishirzi', 'Hannaneh', '']]"
1268156,2004.03386,Su Zhu,"Su Zhu, Jieyu Li, Lu Chen, and Kai Yu","Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue
  State Tracking","16 pages, 4 figures, 11 tables. Accepted to EMNLP 2020 Findings",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialogue state tracking (DST) aims at estimating the current dialogue state
given all the preceding conversation. For multi-domain DST, the data sparsity
problem is a major obstacle due to increased numbers of state candidates and
dialogue lengths. To encode the dialogue context efficiently, we utilize the
previous dialogue state (predicted) and the current dialogue utterance as the
input for DST. To consider relations among different domain-slots, the schema
graph involving prior knowledge is exploited. In this paper, a novel context
and schema fusion network is proposed to encode the dialogue context and schema
graph by using internal and external attention mechanisms. Experiment results
show that our approach can obtain new state-of-the-art performance of the
open-vocabulary DST on both MultiWOZ 2.0 and MultiWOZ 2.1 benchmarks.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 13:46:39 GMT'}, {'version': 'v2', 'created': 'Thu, 9 Apr 2020 13:08:17 GMT'}, {'version': 'v3', 'created': 'Fri, 10 Apr 2020 10:31:51 GMT'}, {'version': 'v4', 'created': 'Wed, 7 Oct 2020 11:19:57 GMT'}]",2020-10-08,"[['Zhu', 'Su', ''], ['Li', 'Jieyu', ''], ['Chen', 'Lu', ''], ['Yu', 'Kai', '']]"
1279684,2004.14914,Sabrina Mielke,"Suzanna Sia, Ayush Dalmia, Sabrina J. Mielke","Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for
  Fast and Good Topics too!",Published as a short paper at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Topic models are a useful analysis tool to uncover the underlying themes
within document collections. The dominant approach is to use probabilistic
topic models that posit a generative story, but in this paper we propose an
alternative way to obtain topics: clustering pre-trained word embeddings while
incorporating document information for weighted clustering and reranking top
words. We provide benchmarks for the combination of different word embeddings
and clustering algorithms, and analyse their performance under dimensionality
reduction with PCA. The best performing combination for our approach performs
as well as classical topic models, but with lower runtime and computational
complexity.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 16:18:18 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 19:23:46 GMT'}]",2020-10-08,"[['Sia', 'Suzanna', ''], ['Dalmia', 'Ayush', ''], ['Mielke', 'Sabrina J.', '']]"
1290799,2005.11004,Hieu-Thi Luong,"Hieu-Thi Luong, Junichi Yamagishi",NAUTILUS: a Versatile Voice Cloning System,"Submitted to The IEEE/ACM Transactions on Audio, Speech, and Language
  Processing",,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a novel speech synthesis system, called NAUTILUS, that can
generate speech with a target voice either from a text input or a reference
utterance of an arbitrary source speaker. By using a multi-speaker speech
corpus to train all requisite encoders and decoders in the initial training
stage, our system can clone unseen voices using untranscribed speech of target
speakers on the basis of the backpropagation algorithm. Moreover, depending on
the data circumstance of the target speaker, the cloning strategy can be
adjusted to take advantage of additional data and modify the behaviors of
text-to-speech (TTS) and/or voice conversion (VC) systems to accommodate the
situation. We test the performance of the proposed framework by using deep
convolution layers to model the encoders, decoders and WaveNet vocoder.
Evaluations show that it achieves comparable quality with state-of-the-art TTS
and VC systems when cloning with just five minutes of untranscribed speech.
Moreover, it is demonstrated that the proposed framework has the ability to
switch between TTS and VC with high speaker consistency, which will be useful
for many applications.
","[{'version': 'v1', 'created': 'Fri, 22 May 2020 05:00:20 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 01:12:22 GMT'}]",2020-10-08,"[['Luong', 'Hieu-Thi', ''], ['Yamagishi', 'Junichi', '']]"
1280565,2005.00770,Tu Vu,"Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam
  Trischler, Andrew Mattarella-Micke, Subhransu Maji, Mohit Iyyer",Exploring and Predicting Transferability across NLP Tasks,"Accepted as a conference paper at EMNLP 2020, 45 pages, 3 figures, 34
  tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in NLP demonstrate the effectiveness of training large-scale
language models and transferring them to downstream tasks. Can fine-tuning
these models on tasks other than language modeling further improve performance?
In this paper, we conduct an extensive study of the transferability between 33
NLP tasks across three broad classes of problems (text classification, question
answering, and sequence labeling). Our results show that transfer learning is
more beneficial than previously thought, especially when target task data is
scarce, and can improve performance even when the source task is small or
differs substantially from the target task (e.g., part-of-speech tagging
transfers well to the DROP QA dataset). We also develop task embeddings that
can be used to predict the most transferable source tasks for a given target
task, and we validate their effectiveness in experiments controlled for source
and target data size. Overall, our experiments reveal that factors such as
source data size, task and domain similarity, and task complexity all play a
role in determining transferability.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 09:39:36 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 18:49:48 GMT'}]",2020-10-08,"[['Vu', 'Tu', ''], ['Wang', 'Tong', ''], ['Munkhdalai', 'Tsendsuren', ''], ['Sordoni', 'Alessandro', ''], ['Trischler', 'Adam', ''], ['Mattarella-Micke', 'Andrew', ''], ['Maji', 'Subhransu', ''], ['Iyyer', 'Mohit', '']]"
1279372,2004.14602,Miyoung Ko,"Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, Jaewoo Kang",Look at the First Sentence: Position Bias in Question Answering,"13 pages, EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many extractive question answering models are trained to predict start and
end positions of answers. The choice of predicting answers as positions is
mainly due to its simplicity and effectiveness. In this study, we hypothesize
that when the distribution of the answer positions is highly skewed in the
training set (e.g., answers lie only in the k-th sentence of each passage), QA
models predicting answers as positions can learn spurious positional cues and
fail to give answers in different positions. We first illustrate this position
bias in popular extractive QA models such as BiDAF and BERT and thoroughly
examine how position bias propagates through each layer of BERT. To safely
deliver position information without position bias, we train models with
various de-biasing methods including entropy regularization and bias
ensembling. Among them, we found that using the prior distribution of answer
positions as a bias model is very effective at reducing position bias,
recovering the performance of BERT from 37.48% to 81.64% when trained on a
biased SQuAD dataset.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 06:25:16 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 09:59:16 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 02:48:40 GMT'}]",2020-10-08,"[['Ko', 'Miyoung', ''], ['Lee', 'Jinhyuk', ''], ['Kim', 'Hyunjae', ''], ['Kim', 'Gangwoo', ''], ['Kang', 'Jaewoo', '']]"
1290185,2005.10390,Yusuke Yasuda,"Yusuke Yasuda, Xin Wang, Junichi Yamagishi","Investigation of learning abilities on linguistic features in
  sequence-to-sequence text-to-speech synthesis",,,,,eess.AS cs.CL cs.SD stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural sequence-to-sequence text-to-speech synthesis (TTS) can produce
high-quality speech directly from text or simple linguistic features such as
phonemes. Unlike traditional pipeline TTS, the neural sequence-to-sequence TTS
does not require manually annotated and complicated linguistic features such as
part-of-speech tags and syntactic structures for system training. However, it
must be carefully designed and well optimized so that it can implicitly extract
useful linguistic features from the input features. In this paper we
investigate under what conditions the neural sequence-to-sequence TTS can work
well in Japanese and English along with comparisons with deep neural network
(DNN) based pipeline TTS systems. Unlike past comparative studies, the pipeline
systems also use autoregressive probabilistic modeling and a neural vocoder. We
investigated systems from three aspects: a) model architecture, b) model
parameter size, and c) language. For the model architecture aspect, we adopt
modified Tacotron systems that we previously proposed and their variants using
an encoder from Tacotron or Tacotron2. For the model parameter size aspect, we
investigate two model parameter sizes. For the language aspect, we conduct
listening tests in both Japanese and English to see if our findings can be
generalized across languages. Our experiments suggest that a) a neural
sequence-to-sequence TTS system should have a sufficient number of model
parameters to produce high quality speech, b) it should also use a powerful
encoder when it takes characters as inputs, and c) the encoder still has a room
for improvement and needs to have an improved architecture to learn
supra-segmental features more appropriately.
","[{'version': 'v1', 'created': 'Wed, 20 May 2020 23:26:14 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 04:18:56 GMT'}]",2020-10-08,"[['Yasuda', 'Yusuke', ''], ['Wang', 'Xin', ''], ['Yamagishi', 'Junichi', '']]"
985431,1805.12386,Daniel Hershcovich,"Daniel Hershcovich, Leshem Choshen, Elior Sulem, Zohar Aizenbud, Ari
  Rappoport and Omri Abend","SemEval 2019 Shared Task: Cross-lingual Semantic Parsing with UCCA -
  Call for Participation","Superseded by the actual shared task description paper at
  arXiv:1903.02953",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We announce a shared task on UCCA parsing in English, German and French, and
call for participants to submit their systems. UCCA is a cross-linguistically
applicable framework for semantic representation, which builds on extensive
typological work and supports rapid annotation. UCCA poses a challenge for
existing parsing techniques, as it exhibits reentrancy (resulting in DAG
structures), discontinuous structures and non-terminal nodes corresponding to
complex semantic units. Given the success of recent semantic parsing shared
tasks (on SDP and AMR), we expect the task to have a significant contribution
to the advancement of UCCA parsing in particular, and semantic parsing in
general. Furthermore, existing applications for semantic evaluation that are
based on UCCA will greatly benefit from better automatic methods for UCCA
parsing. The competition website is
https://competitions.codalab.org/competitions/19160
","[{'version': 'v1', 'created': 'Thu, 31 May 2018 09:11:16 GMT'}, {'version': 'v2', 'created': 'Sun, 19 Aug 2018 11:25:34 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 12:58:22 GMT'}]",2020-10-08,"[['Hershcovich', 'Daniel', ''], ['Choshen', 'Leshem', ''], ['Sulem', 'Elior', ''], ['Aizenbud', 'Zohar', ''], ['Rappoport', 'Ari', ''], ['Abend', 'Omri', '']]"
1351132,2009.09629,Wenliang Dai,"Wenliang Dai, Zihan Liu, Tiezheng Yu and Pascale Fung","Modality-Transferable Emotion Embeddings for Low-Resource Multimodal
  Emotion Recognition","12 pages, 5 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Despite the recent achievements made in the multi-modal emotion recognition
task, two problems still exist and have not been well investigated: 1) the
relationship between different emotion categories are not utilized, which leads
to sub-optimal performance; and 2) current models fail to cope well with
low-resource emotions, especially for unseen emotions. In this paper, we
propose a modality-transferable model with emotion embeddings to tackle the
aforementioned issues. We use pre-trained word embeddings to represent emotion
categories for textual data. Then, two mapping functions are learned to
transfer these embeddings into visual and acoustic spaces. For each modality,
the model calculates the representation distance between the input sequence and
target emotions and makes predictions based on the distances. By doing so, our
model can directly adapt to the unseen emotions in any modality since we have
their pre-trained embeddings and modality mapping functions. Experiments show
that our model achieves state-of-the-art performance on most of the emotion
categories. In addition, our model also outperforms existing baselines in the
zero-shot and few-shot scenarios for unseen emotions.
","[{'version': 'v1', 'created': 'Mon, 21 Sep 2020 06:10:39 GMT'}, {'version': 'v2', 'created': 'Sat, 26 Sep 2020 07:29:25 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 05:09:31 GMT'}]",2020-10-08,"[['Dai', 'Wenliang', ''], ['Liu', 'Zihan', ''], ['Yu', 'Tiezheng', ''], ['Fung', 'Pascale', '']]"
1277920,2004.13150,Zhe Zhang,"Zhe Zhang, Chung-Wei Hang, Munindar P. Singh",Octa: Omissions and Conflicts in Target-Aspect Sentiment Analysis,Accepted by Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentiments in opinionated text are often determined by both aspects and
target words (or targets). We observe that targets and aspects interrelate in
subtle ways, often yielding conflicting sentiments. Thus, a naive aggregation
of sentiments from aspects and targets treated separately, as in existing
sentiment analysis models, impairs performance.
  We propose Octa, an approach that jointly considers aspects and targets when
inferring sentiments. To capture and quantify relationships between targets and
context words, Octa uses a selective self-attention mechanism that handles
implicit or missing targets. Specifically, Octa involves two layers of
attention mechanisms for, respectively, selective attention between targets and
context words and attention over words based on aspects. On benchmark datasets,
Octa outperforms leading models by a large margin, yielding (absolute) gains in
accuracy of 1.6% to 4.3%.
","[{'version': 'v1', 'created': 'Mon, 27 Apr 2020 20:11:50 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 20:37:33 GMT'}]",2020-10-08,"[['Zhang', 'Zhe', ''], ['Hang', 'Chung-Wei', ''], ['Singh', 'Munindar P.', '']]"
1247181,2002.09758,Ethan Perez,"Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, Douwe Kiela",Unsupervised Question Decomposition for Question Answering,"EMNLP 2020 Camera-Ready. Code available at
  https://github.com/facebookresearch/UnsupervisedDecomposition",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We aim to improve question answering (QA) by decomposing hard questions into
simpler sub-questions that existing QA systems are capable of answering. Since
labeling questions with decompositions is cumbersome, we take an unsupervised
approach to produce sub-questions, also enabling us to leverage millions of
questions from the internet. Specifically, we propose an algorithm for One-to-N
Unsupervised Sequence transduction (ONUS) that learns to map one hard,
multi-hop question to many simpler, single-hop sub-questions. We answer
sub-questions with an off-the-shelf QA model and give the resulting answers to
a recomposition model that combines them into a final answer. We show large QA
improvements on HotpotQA over a strong baseline on the original, out-of-domain,
and multi-hop dev sets. ONUS automatically learns to decompose different kinds
of questions, while matching the utility of supervised and heuristic
decomposition methods for QA and exceeding those methods in fluency.
Qualitatively, we find that using sub-questions is promising for shedding light
on why a QA system makes a prediction.
","[{'version': 'v1', 'created': 'Sat, 22 Feb 2020 19:40:35 GMT'}, {'version': 'v2', 'created': 'Fri, 27 Mar 2020 18:49:59 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 18:47:48 GMT'}]",2020-10-08,"[['Perez', 'Ethan', ''], ['Lewis', 'Patrick', ''], ['Yih', 'Wen-tau', ''], ['Cho', 'Kyunghyun', ''], ['Kiela', 'Douwe', '']]"
1349631,2009.08128,Youngbin Ro,"Youngbin Ro, Yukyung Lee, Pilsung Kang","Multi$^2$OIE: Multilingual Open Information Extraction Based on
  Multi-Head Attention with BERT","11 pages, Findings of EMNLP 2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose Multi$^2$OIE, which performs open information
extraction (open IE) by combining BERT with multi-head attention. Our model is
a sequence-labeling system with an efficient and effective argument extraction
method. We use a query, key, and value setting inspired by the Multimodal
Transformer to replace the previously used bidirectional long short-term memory
architecture with multi-head attention. Multi$^2$OIE outperforms existing
sequence-labeling systems with high computational efficiency on two benchmark
evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed
method to multilingual open IE using multilingual BERT. Experimental results on
new benchmark datasets introduced for two languages (Spanish and Portuguese)
demonstrate that our model outperforms other multilingual systems without
training data for the target languages.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 08:03:42 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 06:41:03 GMT'}]",2020-10-08,"[['Ro', 'Youngbin', ''], ['Lee', 'Yukyung', ''], ['Kang', 'Pilsung', '']]"
1358880,2010.02550,Ran Zmigrod,"Ran Zmigrod, Tim Vieira, Ryan Cotterell",Please Mind the Root: Decoding Arborescences for Dependency Parsing,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The connection between dependency trees and spanning trees is exploited by
the NLP community to train and to decode graph-based dependency parsers.
However, the NLP literature has missed an important difference between the two
structures: only one edge may emanate from the root in a dependency tree. We
analyzed the output of state-of-the-art parsers on many languages from the
Universal Dependency Treebank: although these parsers are often able to learn
that trees which violate the constraint should be assigned lower probabilities,
their ability to do so unsurprisingly de-grades as the size of the training set
decreases. In fact, the worst constraint-violation rate we observe is 24%.
Prior work has proposed an inefficient algorithm to enforce the constraint,
which adds a factor of n to the decoding runtime. We adapt an algorithm due to
Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint
without compromising the original runtime.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 08:31:14 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 08:12:11 GMT'}]",2020-10-08,"[['Zmigrod', 'Ran', ''], ['Vieira', 'Tim', ''], ['Cotterell', 'Ryan', '']]"
1279785,2004.15015,Eric Wallace,"Eric Wallace, Mitchell Stern, Dawn Song",Imitation Attacks and Defenses for Black-box Machine Translation Systems,EMNLP 2020,,,,cs.CL cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversaries may look to steal or attack black-box NLP systems, either for
financial gain or to exploit model errors. One setting of particular interest
is machine translation (MT), where models have high commercial value and errors
can be costly. We investigate possible exploitations of black-box MT systems
and explore a preliminary defense against such threats. We first show that MT
systems can be stolen by querying them with monolingual sentences and training
models to imitate their outputs. Using simulated experiments, we demonstrate
that MT model stealing is possible even when imitation models have different
input data or architectures than their target models. Applying these ideas, we
train imitation models that reach within 0.6 BLEU of three production MT
systems on both high-resource and low-resource language pairs. We then leverage
the similarity of our imitation models to transfer adversarial examples to the
production systems. We use gradient-based attacks that expose inputs which lead
to semantically-incorrect translations, dropped content, and vulgar model
outputs. To mitigate these vulnerabilities, we propose a defense that modifies
translation outputs in order to misdirect the optimization of imitation models.
This defense degrades the adversary's BLEU score and attack success rate at
some cost in the defender's BLEU and inference speed.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:56:49 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 22:05:02 GMT'}]",2020-10-08,"[['Wallace', 'Eric', ''], ['Stern', 'Mitchell', ''], ['Song', 'Dawn', '']]"
1350862,2009.09359,Rifat Shahriyar,"Tahmid Hasan, Abhik Bhattacharjee, Kazi Samin, Masum Hasan, Madhusudan
  Basak, M. Sohel Rahman, Rifat Shahriyar","Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New
  Datasets for Bengali-English Machine Translation",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite being the seventh most widely spoken language in the world, Bengali
has received much less attention in machine translation literature due to being
low in resources. Most publicly available parallel corpora for Bengali are not
large enough; and have rather poor quality, mostly because of incorrect
sentence alignments resulting from erroneous sentence segmentation, and also
because of a high volume of noise present in them. In this work, we build a
customized sentence segmenter for Bengali and propose two novel methods for
parallel corpus creation on low-resource setups: aligner ensembling and batch
filtering. With the segmenter and the two methods combined, we compile a
high-quality Bengali-English parallel corpus comprising of 2.75 million
sentence pairs, more than 2 million of which were not available before.
Training on neural models, we achieve an improvement of more than 9 BLEU score
over previous approaches to Bengali-English machine translation. We also
evaluate on a new test set of 1000 pairs made with extensive quality control.
We release the segmenter, parallel corpus, and the evaluation set, thus
elevating Bengali from its low-resource status. To the best of our knowledge,
this is the first ever large scale study on Bengali-English machine
translation. We believe our study will pave the way for future research on
Bengali-English machine translation as well as other low-resource languages.
Our data and code are available at https://github.com/csebuetnlp/banglanmt.
","[{'version': 'v1', 'created': 'Sun, 20 Sep 2020 06:06:27 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 05:33:13 GMT'}]",2020-10-08,"[['Hasan', 'Tahmid', ''], ['Bhattacharjee', 'Abhik', ''], ['Samin', 'Kazi', ''], ['Hasan', 'Masum', ''], ['Basak', 'Madhusudan', ''], ['Rahman', 'M. Sohel', ''], ['Shahriyar', 'Rifat', '']]"
1359673,2010.03343,Gustavo Penha,Gustavo Penha and Claudia Hauff,Slice-Aware Neural Ranking,Paper accepted to EMNLP workshop SCAI 2020,,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding when and why neural ranking models fail for an IR task via
error analysis is an important part of the research cycle. Here we focus on the
challenges of (i) identifying categories of difficult instances (a pair of
question and response candidates) for which a neural ranker is ineffective and
(ii) improving neural ranking for such instances. To address both challenges we
resort to slice-based learning for which the goal is to improve effectiveness
of neural models for slices (subsets) of data. We address challenge (i) by
proposing different slicing functions (SFs) that select slices of the
dataset---based on prior work we heuristically capture different failures of
neural rankers. Then, for challenge (ii) we adapt a neural ranking model to
learn slice-aware representations, i.e. the adapted model learns to represent
the question and responses differently based on the model's prediction of which
slices they belong to. Our experimental results (the source code and data are
available at https://github.com/Guzpenha/slice_based_learning) across three
different ranking tasks and four corpora show that slice-based learning
improves the effectiveness by an average of 2% over a neural ranker that is not
slice-aware.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 11:40:49 GMT'}]",2020-10-08,"[['Penha', 'Gustavo', ''], ['Hauff', 'Claudia', '']]"
1234017,2001.08604,Kang Min Yoo,"Kang Min Yoo, Hanbit Lee, Franck Dernoncourt, Trung Bui, Walter Chang,
  Sang-goo Lee","Variational Hierarchical Dialog Autoencoder for Dialog State Tracking
  Data Augmentation","11 pages (main) + 9 pages (appendix), 1 figure, 6 tables, accepted to
  EMNLP 2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent works have shown that generative data augmentation, where synthetic
samples generated from deep generative models complement the training dataset,
benefit NLP tasks. In this work, we extend this approach to the task of dialog
state tracking for goal-oriented dialogs. Due to the inherent hierarchical
structure of goal-oriented dialogs over utterances and related annotations, the
deep generative model must be capable of capturing the coherence among
different hierarchies and types of dialog features. We propose the Variational
Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of
goal-oriented dialogs, including linguistic features and underlying structured
annotations, namely speaker information, dialog acts, and goals. The proposed
architecture is designed to model each aspect of goal-oriented dialogs using
inter-connected latent variables and learns to generate coherent goal-oriented
dialogs from the latent spaces. To overcome training issues that arise from
training complex variational models, we propose appropriate training
strategies. Experiments on various dialog datasets show that our model improves
the downstream dialog trackers' robustness via generative data augmentation. We
also discover additional benefits of our unified approach to modeling
goal-oriented dialogs: dialog response generation and user simulation, where
our model outperforms previous strong baselines.
","[{'version': 'v1', 'created': 'Thu, 23 Jan 2020 15:34:56 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Feb 2020 12:15:35 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 01:39:34 GMT'}]",2020-10-08,"[['Yoo', 'Kang Min', ''], ['Lee', 'Hanbit', ''], ['Dernoncourt', 'Franck', ''], ['Bui', 'Trung', ''], ['Chang', 'Walter', ''], ['Lee', 'Sang-goo', '']]"
1280491,2005.00696,Tsung-Yen Yang,Tsung-Yen Yang and Andrew S. Lan and Karthik Narasimhan,"Robust and Interpretable Grounding of Spatial References with Relation
  Networks","Findings of Empirical Methods in Natural Language Processing (EMNLP)
  2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning representations of spatial references in natural language is a key
challenge in tasks like autonomous navigation and robotic manipulation. Recent
work has investigated various neural architectures for learning multi-modal
representations for spatial concepts. However, the lack of explicit reasoning
over entities makes such approaches vulnerable to noise in input text or state
observations. In this paper, we develop effective models for understanding
spatial references in text that are robust and interpretable, without
sacrificing performance. We design a text-conditioned \textit{relation network}
whose parameters are dynamically computed with a cross-modal attention module
to capture fine-grained spatial relations between entities. This design choice
provides interpretability of learned intermediate outputs. Experiments across
three tasks demonstrate that our model achieves superior performance, with a
17\% improvement in predicting goal locations and a 15\% improvement in
robustness compared to state-of-the-art systems.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 04:11:33 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 04:05:00 GMT'}]",2020-10-08,"[['Yang', 'Tsung-Yen', ''], ['Lan', 'Andrew S.', ''], ['Narasimhan', 'Karthik', '']]"
1358494,2010.02164,Kevin Yang,"Kevin Yang, Violet Yao, John DeNero, Dan Klein",A Streaming Approach For Efficient Batched Beam Search,EMNLP 2020,,,,cs.CL cs.AI cs.DC cs.LG cs.PF,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose an efficient batching strategy for variable-length decoding on GPU
architectures. During decoding, when candidates terminate or are pruned
according to heuristics, our streaming approach periodically ""refills"" the
batch before proceeding with a selected subset of candidates. We apply our
method to variable-width beam search on a state-of-the-art machine translation
model. Our method decreases runtime by up to 71% compared to a fixed-width beam
search baseline and 17% compared to a variable-width baseline, while matching
baselines' BLEU. Finally, experiments show that our method can speed up
decoding in other domains, such as semantic and syntactic parsing.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 17:13:34 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 22:22:27 GMT'}]",2020-10-08,"[['Yang', 'Kevin', ''], ['Yao', 'Violet', ''], ['DeNero', 'John', ''], ['Klein', 'Dan', '']]"
1278750,2004.13980,Matthew Sims,"Matthew Sims, David Bamman",Measuring Information Propagation in Literary Social Networks,EMNLP 2020 long paper,,,,cs.CL cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present the task of modeling information propagation in literature, in
which we seek to identify pieces of information passing from character A to
character B to character C, only given a description of their activity in text.
We describe a new pipeline for measuring information propagation in this domain
and publish a new dataset for speaker attribution, enabling the evaluation of
an important component of this pipeline on a wider range of literary texts than
previously studied. Using this pipeline, we analyze the dynamics of information
propagation in over 5,000 works of fiction, finding that information flows
through characters that fill structural holes connecting different communities,
and that characters who are women are depicted as filling this role much more
frequently than characters who are men.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 06:41:26 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 19:09:34 GMT'}]",2020-10-08,"[['Sims', 'Matthew', ''], ['Bamman', 'David', '']]"
1358753,2010.02423,Haoyue Shi,"Haoyue Shi, Karen Livescu, Kevin Gimpel",On the Role of Supervision in Unsupervised Constituency Parsing,"EMNLP 2020. Project page:
  https://ttic.uchicago.edu/~freda/project/rsucp/",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We analyze several recent unsupervised constituency parsing models, which are
tuned with respect to the parsing $F_1$ score on the Wall Street Journal (WSJ)
development set (1,700 sentences). We introduce strong baselines for them, by
training an existing supervised parsing model (Kitaev and Klein, 2018) on the
same labeled examples they access. When training on the 1,700 examples, or even
when using only 50 examples for training and 5 for development, such a few-shot
parsing approach can outperform all the unsupervised parsing methods by a
significant margin. Few-shot parsing can be further improved by a simple data
augmentation method and self-training. This suggests that, in order to arrive
at fair conclusions, we should carefully consider the amount of labeled data
used for model development. We propose two protocols for future work on
unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter
tuning and model selection; (ii) use as few labeled examples as possible for
model development, and compare to few-shot parsing trained on the same labeled
examples.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 01:34:58 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 01:38:38 GMT'}]",2020-10-08,"[['Shi', 'Haoyue', ''], ['Livescu', 'Karen', ''], ['Gimpel', 'Kevin', '']]"
1358746,2010.02416,Yi-Te Hsu,"Yi-Te Hsu, Sarthak Garg, Yi-Hsiu Liao, Ilya Chatsviorkin",Efficient Inference For Neural Machine Translation,Accepted SustaiNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Transformer models have achieved state-of-the-art results in neural
machine translation and have become standard in the field. In this work, we
look for the optimal combination of known techniques to optimize inference
speed without sacrificing translation quality. We conduct an empirical study
that stacks various approaches and demonstrates that combination of replacing
decoder self-attention with simplified recurrent units, adopting a deep encoder
and a shallow decoder architecture and multi-head attention pruning can achieve
up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of
parameters by 25% while maintaining the same translation quality in terms of
BLEU.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 01:21:11 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 13:48:02 GMT'}]",2020-10-08,"[['Hsu', 'Yi-Te', ''], ['Garg', 'Sarthak', ''], ['Liao', 'Yi-Hsiu', ''], ['Chatsviorkin', 'Ilya', '']]"
1302788,2006.08274,Aleksandr Laptev,"Andrei Andrusenko, Aleksandr Laptev, Ivan Medennikov","Exploration of End-to-End ASR for OpenSTT -- Russian Open Speech-to-Text
  Dataset",Accepted by SPECOM 2020,,10.1007/978-3-030-60276-5_4,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents an exploration of end-to-end automatic speech recognition
systems (ASR) for the largest open-source Russian language data set -- OpenSTT.
We evaluate different existing end-to-end approaches such as joint
CTC/Attention, RNN-Transducer, and Transformer. All of them are compared with
the strong hybrid ASR system based on LF-MMI TDNN-F acoustic model. For the
three available validation sets (phone calls, YouTube, and books), our best
end-to-end model achieves word error rate (WER) of 34.8%, 19.1%, and 18.1%,
respectively. Under the same conditions, the hybridASR system demonstrates
33.5%, 20.9%, and 18.6% WER.
","[{'version': 'v1', 'created': 'Mon, 15 Jun 2020 10:35:31 GMT'}, {'version': 'v2', 'created': 'Sun, 26 Jul 2020 20:21:09 GMT'}]",2020-10-08,"[['Andrusenko', 'Andrei', ''], ['Laptev', 'Aleksandr', ''], ['Medennikov', 'Ivan', '']]"
1350587,2009.09084,Irene Y. Chen,"Irene Y. Chen, Emily Alsentzer, Hyesun Park, Richard Thomas, Babina
  Gosangi, Rahul Gujrathi, Bharti Khurana",Intimate Partner Violence and Injury Prediction From Radiology Reports,,,,,cs.CY cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Intimate partner violence (IPV) is an urgent, prevalent, and under-detected
public health issue. We present machine learning models to assess patients for
IPV and injury. We train the predictive algorithms on radiology reports with 1)
IPV labels based on entry to a violence prevention program and 2) injury labels
provided by emergency radiology fellowship-trained physicians. Our dataset
includes 34,642 radiology reports and 1479 patients of IPV victims and control
patients. Our best model predicts IPV a median of 3.08 years before violence
prevention program entry with a sensitivity of 64% and a specificity of 95%. We
conduct error analysis to determine for which patients our model has especially
high or low performance and discuss next steps for a deployed clinical risk
model.
","[{'version': 'v1', 'created': 'Fri, 28 Aug 2020 17:20:37 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 16:26:58 GMT'}]",2020-10-08,"[['Chen', 'Irene Y.', ''], ['Alsentzer', 'Emily', ''], ['Park', 'Hyesun', ''], ['Thomas', 'Richard', ''], ['Gosangi', 'Babina', ''], ['Gujrathi', 'Rahul', ''], ['Khurana', 'Bharti', '']]"
1360018,2010.03688,Amrit Nagarajan,"Amrit Nagarajan, Sanchari Sen, Jacob R. Stevens, Anand Raghunathan","Optimizing Transformers with Approximate Computing for Faster, Smaller
  and more Accurate NLP Models",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer models have garnered a lot of interest in recent years by
delivering state-of-the-art performance in a range of Natural Language
Processing (NLP) tasks. However, these models can have over a hundred billion
parameters, presenting very high computational and memory requirements. We
address this challenge through Approximate Computing, specifically targeting
the use of Transformers in NLP tasks. Transformers are typically pre-trained
and subsequently specialized for specific tasks through transfer learning.
Based on the observation that pre-trained Transformers are often
over-parameterized for several downstream NLP tasks, we propose a framework to
create smaller, faster and in some cases more accurate models. The key
cornerstones of the framework are a Significance Analysis (SA) method that
identifies components in a pre-trained Transformer that are less significant
for a given task, and techniques to approximate the less significant
components. Our approximations include pruning of blocks, attention heads and
weight groups, quantization of less significant weights and a low-complexity
sign-matching based attention mechanism. Our framework can be adapted to
produce models that are faster, smaller and/or more accurate, depending on the
user's constraints. We apply our framework to seven Transformer models,
including optimized models like DistilBERT and Q8BERT, and three downstream
tasks. We demonstrate that our framework produces models that are up to 4x
faster and up to 14x smaller (with less than 0.5% relative accuracy
degradation), or up to 5.5% more accurate with simultaneous improvements of up
to 9.83x in model size or 2.94x in speed.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 23:29:34 GMT'}]",2020-10-09,"[['Nagarajan', 'Amrit', ''], ['Sen', 'Sanchari', ''], ['Stevens', 'Jacob R.', ''], ['Raghunathan', 'Anand', '']]"
1360036,2010.03706,"Ekin Aky\""urek","Ekin Aky\""urek, Afra Feyza Aky\""urek, Jacob Andreas",Learning to Recombine and Resample Data for Compositional Generalization,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Flexible neural models outperform grammar- and automaton-based counterparts
on a variety of sequence modeling tasks. However, neural models perform poorly
in settings requiring compositional generalization beyond the training data --
particularly to rare or unseen subsequences. Past work has found symbolic
scaffolding (e.g. grammars or automata) essential in these settings. Here we
present a family of learned data augmentation schemes that support a large
category of compositional generalizations without appeal to latent symbolic
structure. Our approach to data augmentation has two components: recombination
of original training examples via a prototype-based generative model and
resampling of generated examples to encourage extrapolation. Training an
ordinary neural sequence model on a dataset augmented with recombined and
resampled examples significantly improves generalization in two language
processing problems---instruction following (SCAN) and morphological analysis
(Sigmorphon 2018)---where our approach enables learning of new constructions
and tenses from as few as eight initial examples.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 00:36:33 GMT'}]",2020-10-09,"[['Akyürek', 'Ekin', ''], ['Akyürek', 'Afra Feyza', ''], ['Andreas', 'Jacob', '']]"
1360090,2010.03760,Xiaoan Ding,"Xiaoan Ding, Tianyu Liu, Baobao Chang, Zhifang Sui, Kevin Gimpel","Discriminatively-Tuned Generative Classifiers for Robust Natural
  Language Inference","14 pages, EMNLP 2020, the first two authors contributed equally",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While discriminative neural network classifiers are generally preferred,
recent work has shown advantages of generative classifiers in term of data
efficiency and robustness. In this paper, we focus on natural language
inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and
empirically characterize its performance by comparing it to five baselines,
including discriminative models and large-scale pretrained language
representation models like BERT. We explore training objectives for
discriminative fine-tuning of our generative classifiers, showing improvements
over log loss fine-tuning from prior work . In particular, we find strong
results with a simple unbounded modification to log loss, which we call the
""infinilog loss"". Our experiments show that GenNLI outperforms both
discriminative and pretrained baselines across several challenging NLI
experimental settings, including small training sets, imbalanced label
distributions, and label noise.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 04:44:00 GMT'}]",2020-10-09,"[['Ding', 'Xiaoan', ''], ['Liu', 'Tianyu', ''], ['Chang', 'Baobao', ''], ['Sui', 'Zhifang', ''], ['Gimpel', 'Kevin', '']]"
1360085,2010.03755,Xinting Huang,"Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang","Generalizable and Explainable Dialogue Generation via Explicit Action
  Learning",Accepted to Proceedings of EMNLP 2020 (Findings),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Response generation for task-oriented dialogues implicitly optimizes two
objectives at the same time: task completion and language quality. Conditioned
response generation serves as an effective approach to separately and better
optimize these two objectives. Such an approach relies on system action
annotations which are expensive to obtain. To alleviate the need of action
annotations, latent action learning is introduced to map each utterance to a
latent representation. However, this approach is prone to over-dependence on
the training data, and the generalization capability is thus restricted. To
address this issue, we propose to learn natural language actions that represent
utterances as a span of words. This explicit action representation promotes
generalization via the compositional structure of language. It also enables an
explainable generation process. Our proposed unsupervised approach learns a
memory component to summarize system utterances into a short span of words. To
further promote a compact action representation, we propose an auxiliary task
that restores state annotations as the summarized dialogue context using the
memory component. Our proposed approach outperforms latent action baselines on
MultiWOZ, a benchmark multi-domain dataset.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 04:37:22 GMT'}]",2020-10-09,"[['Huang', 'Xinting', ''], ['Qi', 'Jianzhong', ''], ['Sun', 'Yu', ''], ['Zhang', 'Rui', '']]"
1360044,2010.03714,Haidar Khan,"Qile Zhu, Haidar Khan, Saleh Soltan, Stephen Rawls, Wael Hamza","Don't Parse, Insert: Multilingual Semantic Parsing with Insertion Based
  Decoding",Presented at CoNLL 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semantic parsing is one of the key components of natural language
understanding systems. A successful parse transforms an input utterance to an
action that is easily understood by the system. Many algorithms have been
proposed to solve this problem, from conventional rulebased or statistical
slot-filling systems to shiftreduce based neural parsers. For complex parsing
tasks, the state-of-the-art method is based on autoregressive sequence to
sequence models to generate the parse directly. This model is slow at inference
time, generating parses in O(n) decoding steps (n is the length of the target
sequence). In addition, we demonstrate that this method performs poorly in
zero-shot cross-lingual transfer learning settings. In this paper, we propose a
non-autoregressive parser which is based on the insertion transformer to
overcome these two issues. Our approach 1) speeds up decoding by 3x while
outperforming the autoregressive model and 2) significantly improves
cross-lingual transfer in the low-resource setting by 37% compared to
autoregressive baseline. We test our approach on three well-known monolingual
datasets: ATIS, SNIPS and TOP. For cross lingual semantic parsing, we use the
MultiATIS++ and the multilingual TOP datasets.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 01:18:42 GMT'}]",2020-10-09,"[['Zhu', 'Qile', ''], ['Khan', 'Haidar', ''], ['Soltan', 'Saleh', ''], ['Rawls', 'Stephen', ''], ['Hamza', 'Wael', '']]"
1360047,2010.03717,Hieu-Thi Luong,"Hieu-Thi Luong, Junichi Yamagishi","Latent linguistic embedding for cross-lingual text-to-speech and voice
  conversion",Accepted to Voice Conversion Challenge 2020 Online Workshop,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As the recently proposed voice cloning system, NAUTILUS, is capable of
cloning unseen voices using untranscribed speech, we investigate the
feasibility of using it to develop a unified cross-lingual TTS/VC system.
Cross-lingual speech generation is the scenario in which speech utterances are
generated with the voices of target speakers in a language not spoken by them
originally. This type of system is not simply cloning the voice of the target
speaker, but essentially creating a new voice that can be considered better
than the original under a specific framing. By using a well-trained English
latent linguistic embedding to create a cross-lingual TTS and VC system for
several German, Finnish, and Mandarin speakers included in the Voice Conversion
Challenge 2020, we show that our method not only creates cross-lingual VC with
high speaker similarity but also can be seamlessly used for cross-lingual TTS
without having to perform any extra steps. However, the subjective evaluations
of perceived naturalness seemed to vary between target speakers, which is one
aspect for future improvement.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 01:25:07 GMT'}]",2020-10-09,"[['Luong', 'Hieu-Thi', ''], ['Yamagishi', 'Junichi', '']]"
1360055,2010.03725,Yun He,"Yun He, Zhuoer Wang, Yin Zhang, Ruihong Huang and James Caverlee","PARADE: A New Dataset for Paraphrase Identification Requiring Computer
  Science Domain Knowledge",Accepted by EMNLP 2020 as a regular long paper,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a new benchmark dataset called PARADE for paraphrase
identification that requires specialized domain knowledge. PARADE contains
paraphrases that overlap very little at the lexical and syntactic level but are
semantically equivalent based on computer science domain knowledge, as well as
non-paraphrases that overlap greatly at the lexical and syntactic level but are
not semantically equivalent based on this domain knowledge. Experiments show
that both state-of-the-art neural models and non-expert human annotators have
poor performance on PARADE. For example, BERT after fine-tuning achieves an F1
score of 0.709, which is much lower than its performance on other paraphrase
identification datasets. PARADE can serve as a resource for researchers
interested in testing models that incorporate domain knowledge. We make our
data and code freely available.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 02:01:31 GMT'}]",2020-10-09,"[['He', 'Yun', ''], ['Wang', 'Zhuoer', ''], ['Zhang', 'Yin', ''], ['Huang', 'Ruihong', ''], ['Caverlee', 'James', '']]"
1360076,2010.03746,Yun He,"Yun He, Ziwei Zhu, Yin Zhang, Qin Chen, James Caverlee","Infusing Disease Knowledge into BERT for Health Question Answering,
  Medical Inference and Disease Name Recognition",Accepted by EMNLP 2020 as a regular long paper,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge of a disease includes information of various aspects of the
disease, such as signs and symptoms, diagnosis and treatment. This disease
knowledge is critical for many health-related and biomedical tasks, including
consumer health question answering, medical language inference and disease name
recognition. While pre-trained language models like BERT have shown success in
capturing syntactic, semantic, and world knowledge from text, we find they can
be further complemented by specific information like knowledge of symptoms,
diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with
disease knowledge for improving these important tasks. Specifically, we propose
a new disease knowledge infusion training procedure and evaluate it on a suite
of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and
ALBERT. Experiments over the three tasks show that these models can be enhanced
in nearly all cases, demonstrating the viability of disease knowledge infusion.
For example, accuracy of BioBERT on consumer health question answering is
improved from 68.29% to 72.09%, while new SOTA results are observed in two
datasets. We make our data and code freely available.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 03:14:38 GMT'}]",2020-10-09,"[['He', 'Yun', ''], ['Zhu', 'Ziwei', ''], ['Zhang', 'Yin', ''], ['Chen', 'Qin', ''], ['Caverlee', 'James', '']]"
1360068,2010.03738,Yang Deng,"Yang Deng, Wenxuan Zhang, Wai Lam",Multi-hop Inference for Question-driven Summarization,"Accepted by EMNLP 2020 (main conference, long paper)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Question-driven summarization has been recently studied as an effective
approach to summarizing the source document to produce concise but informative
answers for non-factoid questions. In this work, we propose a novel
question-driven abstractive summarization method, Multi-hop Selective Generator
(MSG), to incorporate multi-hop reasoning into question-driven summarization
and, meanwhile, provide justifications for the generated summaries.
Specifically, we jointly model the relevance to the question and the
interrelation among different sentences via a human-like multi-hop inference
module, which captures important sentences for justifying the summarized
answer. A gated selective pointer generator network with a multi-view coverage
mechanism is designed to integrate diverse information from different
perspectives. Experimental results show that the proposed method consistently
outperforms state-of-the-art methods on two non-factoid QA datasets, namely
WikiHow and PubMedQA.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 02:36:39 GMT'}]",2020-10-09,"[['Deng', 'Yang', ''], ['Zhang', 'Wenxuan', ''], ['Lam', 'Wai', '']]"
1360067,2010.03737,Li Bei,"Bei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen
  Wang and Jingbo Zhu",Shallow-to-Deep Training for Neural Machine Translation,Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep encoders have been proven to be effective in improving neural machine
translation (NMT) systems, but training an extremely deep encoder is time
consuming. Moreover, why deep models help NMT is an open question. In this
paper, we investigate the behavior of a well-tuned deep Transformer system. We
find that stacking layers is helpful in improving the representation ability of
NMT models and adjacent layers perform similarly. This inspires us to develop a
shallow-to-deep training method that learns deep models by stacking shallow
models. In this way, we successfully train a Transformer system with a 54-layer
encoder. Experimental results on WMT'16 English-German and WMT'14
English-French translation tasks show that it is $1.4$ $\times$ faster than
training from scratch, and achieves a BLEU score of $30.33$ and $43.29$ on two
tasks. The code is publicly available at
https://github.com/libeineu/SDT-Training/.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 02:36:07 GMT'}]",2020-10-09,"[['Li', 'Bei', ''], ['Wang', 'Ziyang', ''], ['Liu', 'Hui', ''], ['Jiang', 'Yufan', ''], ['Du', 'Quan', ''], ['Xiao', 'Tong', ''], ['Wang', 'Huizhen', ''], ['Zhu', 'Jingbo', '']]"
1360052,2010.03722,Logan Lebanoff,"Logan Lebanoff, Franck Dernoncourt, Doo Soon Kim, Walter Chang, Fei
  Liu","A Cascade Approach to Neural Abstractive Summarization with Content
  Selection and Fusion",AACL-IJCNLP 2020 (Short Paper),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present an empirical study in favor of a cascade architecture to neural
text summarization. Summarization practices vary widely but few other than news
summarization can provide a sufficient amount of training data enough to meet
the requirement of end-to-end neural abstractive systems which perform content
selection and surface realization jointly to generate abstracts. Such systems
also pose a challenge to summarization evaluation, as they force content
selection to be evaluated along with text generation, yet evaluation of the
latter remains an unsolved problem. In this paper, we present empirical results
showing that the performance of a cascaded pipeline that separately identifies
important content pieces and stitches them together into a coherent text is
comparable to or outranks that of end-to-end systems, whereas a pipeline
architecture allows for flexible content selection. We finally discuss how we
can take advantage of a cascaded pipeline in neural text summarization and shed
light on important directions for future research.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 01:49:16 GMT'}]",2020-10-09,"[['Lebanoff', 'Logan', ''], ['Dernoncourt', 'Franck', ''], ['Kim', 'Doo Soon', ''], ['Chang', 'Walter', ''], ['Liu', 'Fei', '']]"
1201658,1911.03064,Po-Sen Huang,"Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl,
  Jack Rae, Vishal Maini, Dani Yogatama, Pushmeet Kohli",Reducing Sentiment Bias in Language Models via Counterfactual Evaluation,"Accepted in the Findings of EMNLP, 2020",,,,cs.CL cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Advances in language modeling architectures and the availability of large
text corpora have driven progress in automatic text generation. While this
results in models capable of generating coherent texts, it also prompts models
to internalize social biases present in the training corpus. This paper aims to
quantify and reduce a particular type of bias exhibited by language models:
bias in the sentiment of generated text. Given a conditioning context (e.g., a
writing prompt) and a language model, we analyze if (and how) the sentiment of
the generated text is affected by changes in values of sensitive attributes
(e.g., country names, occupations, genders) in the conditioning context using a
form of counterfactual evaluation. We quantify sentiment bias by adopting
individual and group fairness metrics from the fair machine learning
literature, and demonstrate that large-scale models trained on two different
corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We
then propose embedding and sentiment prediction-derived regularization on the
language model's latent representations. The regularizations improve fairness
metrics while retaining comparable levels of perplexity and semantic
similarity.
","[{'version': 'v1', 'created': 'Fri, 8 Nov 2019 05:56:01 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Apr 2020 17:51:20 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Oct 2020 17:58:35 GMT'}]",2020-10-09,"[['Huang', 'Po-Sen', ''], ['Zhang', 'Huan', ''], ['Jiang', 'Ray', ''], ['Stanforth', 'Robert', ''], ['Welbl', 'Johannes', ''], ['Rae', 'Jack', ''], ['Maini', 'Vishal', ''], ['Yogatama', 'Dani', ''], ['Kohli', 'Pushmeet', '']]"
1360056,2010.03726,Logan Lebanoff,"Logan Lebanoff, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter
  Chang, Fei Liu",Learning to Fuse Sentences with Transformers for Summarization,EMNLP 2020 (Short Paper),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The ability to fuse sentences is highly attractive for summarization systems
because it is an essential step to produce succinct abstracts. However, to
date, summarizers can fail on fusing sentences. They tend to produce few
summary sentences by fusion or generate incorrect fusions that lead the summary
to fail to retain the original meaning. In this paper, we explore the ability
of Transformers to fuse sentences and propose novel algorithms to enhance their
ability to perform sentence fusion by leveraging the knowledge of points of
correspondence between sentences. Through extensive experiments, we investigate
the effects of different design choices on Transformer's performance. Our
findings highlight the importance of modeling points of correspondence between
sentences for effective sentence fusion.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 02:01:35 GMT'}]",2020-10-09,"[['Lebanoff', 'Logan', ''], ['Dernoncourt', 'Franck', ''], ['Kim', 'Doo Soon', ''], ['Wang', 'Lidan', ''], ['Chang', 'Walter', ''], ['Liu', 'Fei', '']]"
1360193,2010.03863,Anna Rogers,"Anna Rogers, Isabelle Augenstein",What Can We Do to Improve Peer Review in NLP?,To appear at Findings of EMNLP,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Peer review is our best tool for judging the quality of conference
submissions, but it is becoming increasingly spurious. We argue that a part of
the problem is that the reviewers and area chairs face a poorly defined task
forcing apples-to-oranges comparisons. There are several potential ways
forward, but the key difficulty is creating the incentives and mechanisms for
their consistent implementation in the NLP community.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 09:32:21 GMT'}]",2020-10-09,"[['Rogers', 'Anna', ''], ['Augenstein', 'Isabelle', '']]"
1360098,2010.03768,Mohit Shridhar,"Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\^ot\'e, Yonatan Bisk,
  Adam Trischler, Matthew Hausknecht","ALFWorld: Aligning Text and Embodied Environments for Interactive
  Learning","Data, code, and videos are available at alfworld.github.io",,,,cs.CL cs.AI cs.CV cs.LG cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Given a simple request (e.g., Put a washed apple in the kitchen fridge),
humans can reason in purely abstract terms by imagining action sequences and
scoring their likelihood of success, prototypicality, and efficiency, all
without moving a muscle. Once we see the kitchen in question, we can update our
abstract plans to fit the scene. Embodied agents require the same abilities,
but existing work does not yet provide the infrastructure necessary for both
reasoning abstractly and executing concretely. We address this limitation by
introducing ALFWorld, a simulator that enables agents to learn abstract,
text-based policies in TextWorld (C\^ot\'e et al., 2018) and then execute goals
from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment.
ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge,
learned in TextWorld, corresponds directly to concrete, visually grounded
actions. In turn, as we demonstrate empirically, this fosters better agent
generalization than training only in the visually grounded environment.
BUTLER's simple, modular design factors the problem to allow researchers to
focus on models for improving every piece of the pipeline (language
understanding, planning, navigation, visual scene understanding, and so forth).
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 05:13:36 GMT'}]",2020-10-09,"[['Shridhar', 'Mohit', ''], ['Yuan', 'Xingdi', ''], ['Côté', 'Marc-Alexandre', ''], ['Bisk', 'Yonatan', ''], ['Trischler', 'Adam', ''], ['Hausknecht', 'Matthew', '']]"
1360449,2010.04119,Peter Hase,"Peter Hase, Shiyue Zhang, Harry Xie, Mohit Bansal","Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial
  Explanations of Their Behavior in Natural Language?",EMNLP 2020 Findings (17 pages),,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Data collection for natural language (NL) understanding tasks has
increasingly included human explanations alongside data points, allowing past
works to introduce models that both perform a task and generate NL explanations
for their outputs. Yet to date, model-generated explanations have been
evaluated on the basis of surface-level similarities to human explanations,
both through automatic metrics like BLEU and human evaluations. We argue that
these evaluations are insufficient, since they fail to indicate whether
explanations support actual model behavior (faithfulness), rather than simply
match what a human would say (plausibility). In this work, we address the
problem of evaluating explanations from the model simulatability perspective.
Our contributions are as follows: (1) We introduce a leakage-adjusted
simulatability (LAS) metric for evaluating NL explanations, which measures how
well explanations help an observer predict a model's output, while controlling
for how explanations can directly leak the output. We use a model as a proxy
for a human observer, and validate this choice with two human subject
experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing
generative graphical models and two new approaches; one rationalizing method we
introduce achieves roughly human-level LAS scores. (3) Lastly, we frame
explanation generation as a multi-agent game and optimize explanations for
simulatability while penalizing label leakage, which can improve LAS scores. We
provide code for the experiments in this paper at
https://github.com/peterbhase/LAS-NL-Explanations
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 16:59:07 GMT'}]",2020-10-09,"[['Hase', 'Peter', ''], ['Zhang', 'Shiyue', ''], ['Xie', 'Harry', ''], ['Bansal', 'Mohit', '']]"
1360455,2010.04125,Kun Zhou,"Kun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang and Ji-Rong Wen",Towards Topic-Guided Conversational Recommender System,"12 pages, Accepted by Coling2020",,,,cs.CL cs.HC cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conversational recommender systems (CRS) aim to recommend high-quality items
to users through interactive conversations. To develop an effective CRS, the
support of high-quality datasets is essential. Existing CRS datasets mainly
focus on immediate requests from users, while lack proactive guidance to the
recommendation scenario. In this paper, we contribute a new CRS dataset named
\textbf{TG-ReDial} (\textbf{Re}commendation through
\textbf{T}opic-\textbf{G}uided \textbf{Dial}og). Our dataset has two major
features. First, it incorporates topic threads to enforce natural semantic
transitions towards the recommendation scenario. Second, it is created in a
semi-automatic way, hence human annotation is more reasonable and controllable.
Based on TG-ReDial, we present the task of topic-guided conversational
recommendation, and propose an effective approach to this task. Extensive
experiments have demonstrated the effectiveness of our approach on three
sub-tasks, namely topic prediction, item recommendation and response
generation. TG-ReDial is available at https://github.com/RUCAIBox/TG-ReDial.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 17:04:30 GMT'}]",2020-10-09,"[['Zhou', 'Kun', ''], ['Zhou', 'Yuanhang', ''], ['Zhao', 'Wayne Xin', ''], ['Wang', 'Xiaoke', ''], ['Wen', 'Ji-Rong', '']]"
1360471,2010.04141,Ernie Chang,"Ernie Chang, Jeriah Caplinger, Alex Marin, Xiaoyu Shen, Vera Demberg",DART: A Lightweight Quality-Suggestive Data-to-Text Annotation Tool,Accepted to COLING 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We present a lightweight annotation tool, the Data AnnotatoR Tool (DART), for
the general task of labeling structured data with textual descriptions. The
tool is implemented as an interactive application that reduces human efforts in
annotating large quantities of structured data, e.g. in the format of a table
or tree structure. By using a backend sequence-to-sequence model, our system
iteratively analyzes the annotated labels in order to better sample unlabeled
data. In a simulation experiment performed on annotating large quantities of
structured data, DART has been shown to reduce the total number of annotations
needed with active learning and automatically suggesting relevant labels.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 17:36:34 GMT'}]",2020-10-09,"[['Chang', 'Ernie', ''], ['Caplinger', 'Jeriah', ''], ['Marin', 'Alex', ''], ['Shen', 'Xiaoyu', ''], ['Demberg', 'Vera', '']]"
1360324,2010.03994,Zheng Ye,"Lishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, Xiaodan Liang","GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating
  Open-Domain Dialogue Systems",Long paper; EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatically evaluating dialogue coherence is a challenging but high-demand
ability for developing high-quality open-domain dialogue systems. However,
current evaluation metrics consider only surface features or utterance-level
semantics, without explicitly considering the fine-grained topic transition
dynamics of dialogue flows. Here, we first consider that the graph structure
constituted with topics in a dialogue can accurately depict the underlying
communication logic, which is a more natural way to produce persuasive metrics.
Capitalized on the topic-level dialogue graph, we propose a new evaluation
metric GRADE, which stands for Graph-enhanced Representations for Automatic
Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained
utterance-level contextualized representations and fine-grained topic-level
graph representations to evaluate dialogue coherence. The graph representations
are obtained by reasoning over topic-level dialogue graphs enhanced with the
evidence from a commonsense graph, including k-hop neighboring representations
and hop-attention weights. Experimental results show that our GRADE
significantly outperforms other state-of-the-art metrics on measuring diverse
dialogue models in terms of the Pearson and Spearman correlations with human
judgements. Besides, we release a new large-scale human evaluation benchmark to
facilitate future research on automatic metrics.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 14:07:32 GMT'}]",2020-10-09,"[['Huang', 'Lishan', ''], ['Ye', 'Zheng', ''], ['Qin', 'Jinghui', ''], ['Lin', 'Liang', ''], ['Liang', 'Xiaodan', '']]"
1360312,2010.03982,"Arne K\""ohn","Arne K\""ohn and Julia Wichlacz and \'Alvaro Torralba and Daniel
  H\""oller and J\""org Hoffmann and Alexander Koller",Generating Instructions at Different Levels of Abstraction,Accepted COLING 2020 long paper,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  When generating technical instructions, it is often convenient to describe
complex objects in the world at different levels of abstraction. A novice user
might need an object explained piece by piece, while for an expert, talking
about the complex object (e.g. a wall or railing) directly may be more succinct
and efficient. We show how to generate building instructions at different
levels of abstraction in Minecraft. We introduce the use of hierarchical
planning to this end, a method from AI planning which can capture the structure
of complex objects neatly. A crowdsourcing evaluation shows that the choice of
abstraction level matters to users, and that an abstraction strategy which
balances low-level and high-level object descriptions compares favorably to
ones which don't.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 13:56:09 GMT'}]",2020-10-09,"[['Köhn', 'Arne', ''], ['Wichlacz', 'Julia', ''], ['Torralba', 'Álvaro', ''], ['Höller', 'Daniel', ''], ['Hoffmann', 'Jörg', ''], ['Koller', 'Alexander', '']]"
1360250,2010.03920,Daniel Zeman,"Martin Vastl, Daniel Zeman, Rudolf Rosa","Predicting Typological Features in WALS using Language Embeddings and
  Conditional Probabilities: \'UFAL Submission to the SIGTYP 2020 Shared Task",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  We present our submission to the SIGTYP 2020 Shared Task on the prediction of
typological features. We submit a constrained system, predicting typological
features only based on the WALS database. We investigate two approaches. The
simpler of the two is a system based on estimating correlation of feature
values within languages by computing conditional probabilities and mutual
information. The second approach is to train a neural predictor operating on
precomputed language embeddings based on WALS features. Our submitted system
combines the two approaches based on their self-estimated confidence scores. We
reach the accuracy of 70.7% on the test data and rank first in the shared task.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 12:05:48 GMT'}]",2020-10-09,"[['Vastl', 'Martin', ''], ['Zeman', 'Daniel', ''], ['Rosa', 'Rudolf', '']]"
1360233,2010.03903,Dechuan Teng,"Dechuang Teng, Libo Qin, Wanxiang Che, Sendong Zhao, Ting Liu","Injecting Word Information with Multi-Level Word Adapter for Chinese
  Spoken Language Understanding",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Intent detection and slot filling are two closely related tasks for building
a spoken language understanding (SLU) system. In this paper, we focus on
improving Chinese SLU by flexibly injecting word information, which has shown
effectiveness for various Chinese NLP tasks. Previous studies on Chinese SLU do
not consider the word information, failing to detect the word boundaries that
are beneficial for intent detection and slot filling. To address this issue, we
propose a multi-level word adapter to inject word information for Chinese SLU:
(1) sentence-level word adapter, which directly fuses the sentence
representations of the word information and character information to perform
intent detection; (2) character-level word adapter, which is applied at each
character for selectively controlling weights on word information as well as
character information. In addition, the proposed word adapter is applied at the
output layer, which can be utilized as a plugin and easily combined with other
pre-trained models (e.g., BERT). Experimental results on two Chinese SLU
datasets show that our model can capture useful word information and achieve
state-of-the-art performance. More importantly, our framework substantially
gains further improvements when we plug the word adapters into a BERT, which
again demonstrates the effectiveness and flexibility of our word adapter.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 11:11:05 GMT'}]",2020-10-09,"[['Teng', 'Dechuang', ''], ['Qin', 'Libo', ''], ['Che', 'Wanxiang', ''], ['Zhao', 'Sendong', ''], ['Liu', 'Ting', '']]"
1360229,2010.03899,Gabriel Synnaeve,"Daniel Haziza, J\'er\'emy Rapin, Gabriel Synnaeve","Population Based Training for Data Augmentation and Regularization in
  Speech Recognition",tech report from Dec. 2019,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Varying data augmentation policies and regularization over the course of
optimization has led to performance improvements over using fixed values. We
show that population based training is a useful tool to continuously search
those hyperparameters, within a fixed budget. This greatly simplifies the
experimental burden and computational cost of finding such optimal schedules.
We experiment in speech recognition by optimizing SpecAugment this way, as well
as dropout. It compares favorably to a baseline that does not change those
hyperparameters over the course of training, with an 8% relative WER
improvement. We obtain 5.18% word error rate on LibriSpeech's test-other.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 11:00:18 GMT'}]",2020-10-09,"[['Haziza', 'Daniel', ''], ['Rapin', 'Jérémy', ''], ['Synnaeve', 'Gabriel', '']]"
1360211,2010.03881,Gyuwan Kim,Gyuwan Kim and Tae-Hwan Jung,Large Product Key Memory for Pretrained Language Models,"Accepted to Findings of EMNLP 2020; 10 pages, 4 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Product key memory (PKM) proposed by Lample et al. (2019) enables to improve
prediction accuracy by increasing model capacity efficiently with insignificant
computational overhead. However, their empirical application is only limited to
causal language modeling. Motivated by the recent success of pretrained
language models (PLMs), we investigate how to incorporate large PKM into PLMs
that can be finetuned for a wide variety of downstream NLP tasks. We define a
new memory usage metric, and careful observation using this metric reveals that
most memory slots remain outdated during the training of PKM-augmented models.
To train better PLMs by tackling this issue, we propose simple but effective
solutions: (1) initialization from the model weights pretrained without memory
and (2) augmenting PKM by addition rather than replacing a feed-forward
network. We verify that both of them are crucial for the pretraining of
PKM-augmented PLMs, enhancing memory utilization and downstream performance.
Code and pretrained weights are available at
https://github.com/clovaai/pkm-transformers.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 10:19:50 GMT'}]",2020-10-09,"[['Kim', 'Gyuwan', ''], ['Jung', 'Tae-Hwan', '']]"
1346890,2009.05387,Genta Indra Winata,"Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel
  Cahyawijaya, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra,
  Pascale Fung, Syafri Bahar, Ayu Purwarianti","IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural
  Language Understanding","This paper will be presented in AACL-IJCNLP 2020 (with new results
  and acknowledgment)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although Indonesian is known to be the fourth most frequently used language
over the internet, the research progress on this language in the natural
language processing (NLP) is slow-moving due to a lack of available resources.
In response, we introduce the first-ever vast resource for the training,
evaluating, and benchmarking on Indonesian natural language understanding
(IndoNLU) tasks. IndoNLU includes twelve tasks, ranging from single sentence
classification to pair-sentences sequence labeling with different levels of
complexity. The datasets for the tasks lie in different domains and styles to
ensure task diversity. We also provide a set of Indonesian pre-trained models
(IndoBERT) trained from a large and clean Indonesian dataset Indo4B collected
from publicly available sources such as social media texts, blogs, news, and
websites. We release baseline models for all twelve tasks, as well as the
framework for benchmark evaluation, and thus it enables everyone to benchmark
their system performances.
","[{'version': 'v1', 'created': 'Fri, 11 Sep 2020 12:21:41 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 13:13:40 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Oct 2020 13:11:59 GMT'}]",2020-10-09,"[['Wilie', 'Bryan', ''], ['Vincentio', 'Karissa', ''], ['Winata', 'Genta Indra', ''], ['Cahyawijaya', 'Samuel', ''], ['Li', 'Xiaohong', ''], ['Lim', 'Zhi Yuan', ''], ['Soleman', 'Sidik', ''], ['Mahendra', 'Rahmad', ''], ['Fung', 'Pascale', ''], ['Bahar', 'Syafri', ''], ['Purwarianti', 'Ayu', '']]"
1360010,2010.03680,Yaqing Wang,"Yaqing Wang, Subhabrata Mukherjee, Haoda Chu, Yuancheng Tu, Ming Wu,
  Jing Gao, Ahmed Hassan Awadallah",Adaptive Self-training for Few-shot Neural Sequence Labeling,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural sequence labeling is an important technique employed for many Natural
Language Processing (NLP) tasks, such as Named Entity Recognition (NER), slot
tagging for dialog systems and semantic parsing. Large-scale pre-trained
language models obtain very good performance on these tasks when fine-tuned on
large amounts of task-specific labeled data. However, such large-scale labeled
datasets are difficult to obtain for several tasks and domains due to the high
cost of human annotation as well as privacy and data access constraints for
sensitive user applications. This is exacerbated for sequence labeling tasks
requiring such annotations at token-level. In this work, we develop techniques
to address the label scarcity challenge for neural sequence labeling models.
Specifically, we develop self-training and meta-learning techniques for
few-shot training of neural sequence taggers, namely MetaST. While
self-training serves as an effective mechanism to learn from large amounts of
unlabeled data -- meta-learning helps in adaptive sample re-weighting to
mitigate error propagation from noisy pseudo-labels. Extensive experiments on
six benchmark datasets including two massive multilingual NER datasets and four
slot tagging datasets for task-oriented dialog systems demonstrate the
effectiveness of our method with around 10% improvement over state-of-the-art
systems for the 10-shot setting.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 22:29:05 GMT'}]",2020-10-09,"[['Wang', 'Yaqing', ''], ['Mukherjee', 'Subhabrata', ''], ['Chu', 'Haoda', ''], ['Tu', 'Yuancheng', ''], ['Wu', 'Ming', ''], ['Gao', 'Jing', ''], ['Awadallah', 'Ahmed Hassan', '']]"
1280125,2005.00330,Shailaja Keyur Sampat,"Shailaja Keyur Sampat, Yezhou Yang and Chitta Baral",Visuo-Lingustic Question Answering (VLQA) Challenge,"Findings of EMNLP 2020 (22 pages, 13 figures)",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding images and text together is an important aspect of cognition
and building advanced Artificial Intelligence (AI) systems. As a community, we
have achieved good benchmarks over language and vision domains separately,
however joint reasoning is still a challenge for state-of-the-art computer
vision and natural language processing (NLP) systems. We propose a novel task
to derive joint inference about a given image-text modality and compile the
Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question
answering setting. Each dataset item consists of an image and a reading
passage, where questions are designed to combine both visual and textual
information i.e., ignoring either modality would make the question
unanswerable. We first explore the best existing vision-language architectures
to solve VLQA subsets and show that they are unable to reason well. We then
develop a modular method with slightly better baseline performance, but it is
still far behind human performance. We believe that VLQA will be a good
benchmark for reasoning over a visuo-linguistic context. The dataset, code and
leaderboard is available at https://shailaja183.github.io/vlqa/.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 12:18:55 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 01:06:30 GMT'}]",2020-10-09,"[['Sampat', 'Shailaja Keyur', ''], ['Yang', 'Yezhou', ''], ['Baral', 'Chitta', '']]"
1279762,2004.14992,Nicola De Cao,"Nicola De Cao, Michael Schlichtkrull, Wilker Aziz, Ivan Titov","How do Decisions Emerge across Layers in Neural Models? Interpretation
  with Differentiable Masking","Accepted at the 2020 Conference on Empirical Methods in Natural
  Language Processing (EMNLP). Source code available at
  https://github.com/nicola-decao/diffmask . 18 pages, 15 figures, 4 tables",,,,cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Attribution methods assess the contribution of inputs to the model
prediction. One way to do so is erasure: a subset of inputs is considered
irrelevant if it can be removed without affecting the prediction. Though
conceptually simple, erasure's objective is intractable and approximate search
remains expensive with modern deep NLP models. Erasure is also susceptible to
the hindsight bias: the fact that an input can be dropped does not mean that
the model `knows' it can be dropped. The resulting pruning is over-aggressive
and does not reflect how the model arrives at the prediction. To deal with
these challenges, we introduce Differentiable Masking. DiffMask learns to
mask-out subsets of the input while maintaining differentiability. The decision
to include or disregard an input token is made with a simple model based on
intermediate hidden layers of the analyzed model. First, this makes the
approach efficient because we predict rather than search. Second, as with
probing classifiers, this reveals what the network `knows' at the corresponding
layers. This lets us not only plot attribution heatmaps but also analyze how
decisions are formed across network layers. We use DiffMask to study BERT
models on sentiment classification and question answering.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:36:14 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 15:51:43 GMT'}]",2020-10-09,"[['De Cao', 'Nicola', ''], ['Schlichtkrull', 'Michael', ''], ['Aziz', 'Wilker', ''], ['Titov', 'Ivan', '']]"
1360181,2010.03851,Jue Wang,Jue Wang and Wei Lu,"Two are Better than One: Joint Entity and Relation Extraction with
  Table-Sequence Encoders",EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Named entity recognition and relation extraction are two important
fundamental problems. Joint learning algorithms have been proposed to solve
both tasks simultaneously, and many of them cast the joint task as a
table-filling problem. However, they typically focused on learning a single
encoder (usually learning representation in the form of a table) to capture
information required for both tasks within the same space. We argue that it can
be beneficial to design two distinct encoders to capture such two different
types of information in the learning process. In this work, we propose the
novel {\em table-sequence encoders} where two different encoders -- a table
encoder and a sequence encoder are designed to help each other in the
representation learning process. Our experiments confirm the advantages of
having {\em two} encoders over {\em one} encoder. On several standard datasets,
our model shows significant improvements over existing approaches.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 09:10:55 GMT'}]",2020-10-09,"[['Wang', 'Jue', ''], ['Lu', 'Wei', '']]"
1360154,2010.03824,Tom Hope,"Aida Amini, Tom Hope, David Wadden, Madeleine van Zuylen, Eric
  Horvitz, Roy Schwartz, Hannaneh Hajishirzi",Extracting a Knowledge Base of Mechanisms from COVID-19 Papers,"Tom Hope and Aida Amini made an equal contribution as joint first
  authors, listed in alphabetical order. Data and code: https://git.io/JUhv7",,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The urgency of mitigating COVID-19 has spawned a large and diverse body of
scientific literature that is challenging for researchers to navigate. This
explosion of information has stimulated interest in automated tools to help
identify useful knowledge. We have pursued the use of methods for extracting
diverse forms of mechanism relations from the natural language of scientific
papers. We seek to identify concepts in COVID-19 and related literature which
represent activities, functions, associations and causal relations, ranging
from cellular processes to economic impacts. We formulate a broad,
coarse-grained schema targeting mechanism relations between open, free-form
entities. Our approach strikes a balance between expressivity and breadth that
supports generalization across diverse concepts. We curate a dataset of
scientific papers annotated according to our novel schema. Using an information
extraction model trained on this new corpus, we construct a knowledge base (KB)
of 2M mechanism relations, which we make publicly available. Our model is able
to extract relations at an F1 at least twice that of baselines such as open IE
or related scientific IE systems. We conduct experiments examining the ability
of our system to retrieve relevant information on viral mechanisms of action,
and on applications of AI to COVID-19 research. In both cases, our system
identifies relevant information from our automatically-constructed knowledge
base with high precision.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 07:54:14 GMT'}]",2020-10-09,"[['Amini', 'Aida', ''], ['Hope', 'Tom', ''], ['Wadden', 'David', ''], ['van Zuylen', 'Madeleine', ''], ['Horvitz', 'Eric', ''], ['Schwartz', 'Roy', ''], ['Hajishirzi', 'Hannaneh', '']]"
1360132,2010.03802,Noah Constant,"Parker Riley, Noah Constant, Mandy Guo, Girish Kumar, David Uthus,
  Zarana Parekh","TextSETTR: Label-Free Text Style Extraction and Tunable Targeted
  Restyling",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a novel approach to the problem of text style transfer. Unlike
previous approaches that use parallel or non-parallel labeled data, our
technique removes the need for labels entirely, relying instead on the implicit
connection in style between adjacent sentences in unlabeled text. We show that
T5 (Raffel et al., 2019), a strong pretrained text-to-text model, can be
adapted to extract a style vector from arbitrary text and use this vector to
condition the decoder to perform style transfer. As the resulting learned style
vector space encodes many facets of textual style, we recast transfers as
""targeted restyling"" vector operations that adjust specific attributes of the
input text while preserving others. When trained over unlabeled Amazon reviews
data, our resulting TextSETTR model is competitive on sentiment transfer, even
when given only four exemplars of each class. Furthermore, we demonstrate that
a single model trained on unlabeled Common Crawl data is capable of
transferring along multiple dimensions including dialect, emotiveness,
formality, politeness, and sentiment.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 07:06:38 GMT'}]",2020-10-09,"[['Riley', 'Parker', ''], ['Constant', 'Noah', ''], ['Guo', 'Mandy', ''], ['Kumar', 'Girish', ''], ['Uthus', 'David', ''], ['Parekh', 'Zarana', '']]"
1360120,2010.03790,Keerthiram Murugesan,"Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar
  Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya
  Sachan, Murray Campbell","Text-based RL Agents with Commonsense Knowledge: New Challenges,
  Environments and Baselines",,,,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-based games have emerged as an important test-bed for Reinforcement
Learning (RL) research, requiring RL agents to combine grounded language
understanding with sequential decision making. In this paper, we examine the
problem of infusing RL agents with commonsense knowledge. Such knowledge would
allow agents to efficiently act in the world by pruning out implausible
actions, and to perform look-ahead planning to determine how current actions
might affect future world states. We design a new text-based gaming environment
called TextWorld Commonsense (TWC) for training and evaluating RL agents with a
specific kind of commonsense knowledge about objects, their attributes, and
affordances. We also introduce several baseline RL agents which track the
sequential context and dynamically retrieve the relevant commonsense knowledge
from ConceptNet. We show that agents which incorporate commonsense knowledge in
TWC perform better, while acting more efficiently. We conduct user-studies to
estimate human performance on TWC and show that there is ample room for future
improvement.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 06:20:00 GMT'}]",2020-10-09,"[['Murugesan', 'Keerthiram', ''], ['Atzeni', 'Mattia', ''], ['Kapanipathi', 'Pavan', ''], ['Shukla', 'Pushkar', ''], ['Kumaravel', 'Sadhana', ''], ['Tesauro', 'Gerald', ''], ['Talamadupula', 'Kartik', ''], ['Sachan', 'Mrinmaya', ''], ['Campbell', 'Murray', '']]"
1266740,2004.01970,Siddhant Garg,"Siddhant Garg, Goutham Ramakrishnan",BAE: BERT-based Adversarial Examples for Text Classification,Accepted at EMNLP 2020 Main Conference,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Modern text classification models are susceptible to adversarial examples,
perturbed versions of the original text indiscernible by humans which get
misclassified by the model. Recent works in NLP use rule-based synonym
replacement strategies to generate adversarial examples. These strategies can
lead to out-of-context and unnaturally complex token replacements, which are
easily identifiable by humans. We present BAE, a black box attack for
generating adversarial examples using contextual perturbations from a BERT
masked language model. BAE replaces and inserts tokens in the original text by
masking a portion of the text and leveraging the BERT-MLM to generate
alternatives for the masked tokens. Through automatic and human evaluations, we
show that BAE performs a stronger attack, in addition to generating adversarial
examples with improved grammaticality and semantic coherence as compared to
prior work.
","[{'version': 'v1', 'created': 'Sat, 4 Apr 2020 16:25:48 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 16:44:29 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Oct 2020 00:41:43 GMT'}]",2020-10-09,"[['Garg', 'Siddhant', ''], ['Ramakrishnan', 'Goutham', '']]"
1360103,2010.03773,Yang Li,"Yang Li, Tao Shen, Guodong Long, Jing Jiang, Tianyi Zhou, Chengqi
  Zhang","Improving Long-Tail Relation Extraction with Collaborating
  Relation-Augmented Attention",Accepted to appear at COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Wrong labeling problem and long-tail relations are two main challenges caused
by distant supervision in relation extraction. Recent works alleviate the wrong
labeling by selective attention via multi-instance learning, but cannot well
handle long-tail relations even if hierarchies of the relations are introduced
to share knowledge. In this work, we propose a novel neural network,
Collaborating Relation-augmented Attention (CoRA), to handle both the wrong
labeling and long-tail relations. Particularly, we first propose
relation-augmented attention network as base model. It operates on sentence bag
with a sentence-to-relation attention to minimize the effect of wrong labeling.
Then, facilitated by the proposed base model, we introduce collaborating
relation features shared among relations in the hierarchies to promote the
relation-augmenting process and balance the training data for long-tail
relations. Besides the main training objective to predict the relation of a
sentence bag, an auxiliary objective is utilized to guide the
relation-augmenting process for a more accurate bag-level representation. In
the experiments on the popular benchmark dataset NYT, the proposed CoRA
improves the prior state-of-the-art performance by a large margin in terms of
Precision@N, AUC and Hits@K. Further analyses verify its superior capability in
handling long-tail relations in contrast to the competitors.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 05:34:43 GMT'}]",2020-10-09,"[['Li', 'Yang', ''], ['Shen', 'Tao', ''], ['Long', 'Guodong', ''], ['Jiang', 'Jing', ''], ['Zhou', 'Tianyi', ''], ['Zhang', 'Chengqi', '']]"
1360096,2010.03766,Chuhan Wu,"Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang",Improving Attention Mechanism with Query-Value Interaction,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Attention mechanism has played critical roles in various state-of-the-art NLP
models such as Transformer and BERT. It can be formulated as a ternary function
that maps the input queries, keys and values into an output by using a
summation of values weighted by the attention weights derived from the
interactions between queries and keys. Similar with query-key interactions,
there is also inherent relatedness between queries and values, and
incorporating query-value interactions has the potential to enhance the output
by learning customized values according to the characteristics of queries.
However, the query-value interactions are ignored by existing attention
methods, which may be not optimal. In this paper, we propose to improve the
existing attention mechanism by incorporating query-value interactions. We
propose a query-value interaction function which can learn query-aware
attention values, and combine them with the original values and attention
weights to form the final output. Extensive experiments on four datasets for
different tasks show that our approach can consistently improve the performance
of many attention-based models by incorporating query-value interactions.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 05:12:52 GMT'}]",2020-10-09,"[['Wu', 'Chuhan', ''], ['Wu', 'Fangzhao', ''], ['Qi', 'Tao', ''], ['Huang', 'Yongfeng', '']]"
1359992,2010.03662,Eleftheria Briakou,Eleftheria Briakou and Marine Carpuat,"Detecting Fine-Grained Cross-Lingual Semantic Divergences without
  Supervision by Learning to Rank",EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Detecting fine-grained differences in content conveyed in different languages
matters for cross-lingual NLP and multilingual corpora analysis, but it is a
challenging machine learning problem since annotation is expensive and hard to
scale. This work improves the prediction and annotation of fine-grained
semantic divergences. We introduce a training strategy for multilingual BERT
models by learning to rank synthetic divergent examples of varying granularity.
We evaluate our models on the Rationalized English-French Semantic Divergences,
a new dataset released with this work, consisting of English-French
sentence-pairs annotated with semantic divergence classes and token-level
rationales. Learning to rank helps detect fine-grained sentence-level
divergences more accurately than a strong sentence-level similarity model,
while token-level predictions have the potential of further distinguishing
between coarse and fine-grained divergences.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 21:26:20 GMT'}]",2020-10-09,"[['Briakou', 'Eleftheria', ''], ['Carpuat', 'Marine', '']]"
1347878,2009.06375,Nickil Maveli,Nickil Maveli,"EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with
  Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets",5 pages + 1 Appendix draft (after review),,,,cs.CL cs.IR cs.LG cs.SI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Twitter has become an important communication channel in times of emergency.
The ubiquitousness of smartphones enables people to announce an emergency
they're observing in real-time. Because of this, more agencies are interested
in programatically monitoring Twitter (disaster relief organizations and news
agencies) and therefore recognizing the informativeness of a tweet can help
filter noise from large volumes of data. In this paper, we present our
submission for WNUT-2020 Task 2: Identification of informative COVID-19 English
Tweets. Our most successful model is an ensemble of transformers including
RoBERTa, XLNet, and BERTweet trained in a semi-supervised experimental setting.
The proposed system achieves a F1 score of 0.9011 on the test set (ranking 7th
on the leaderboard), and shows significant gains in performance compared to a
baseline system using fasttext embeddings.
","[{'version': 'v1', 'created': 'Sun, 6 Sep 2020 15:57:28 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 16:47:10 GMT'}]",2020-10-09,"[['Maveli', 'Nickil', '']]"
1359982,2010.03652,Shuohang Wang,"Shuohang Wang, Yuwei Fang, Siqi Sun, Zhe Gan, Yu Cheng, Jing Jiang,
  Jingjing Liu",Cross-Thought for Sentence Encoder Pre-training,Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose Cross-Thought, a novel approach to pre-training
sequence encoder, which is instrumental in building reusable sequence
embeddings for large-scale NLP tasks such as question answering. Instead of
using the original signals of full sentences, we train a Transformer-based
sequence encoder over a large set of short sequences, which allows the model to
automatically select the most useful information for predicting masked words.
Experiments on question answering and textual entailment tasks demonstrate that
our pre-trained encoder can outperform state-of-the-art encoders trained with
continuous sentence signals as well as traditional masked language modeling
baselines. Our proposed approach also achieves new state of the art on HotpotQA
(full-wiki setting) by improving intermediate information retrieval
performance.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 21:02:41 GMT'}]",2020-10-09,"[['Wang', 'Shuohang', ''], ['Fang', 'Yuwei', ''], ['Sun', 'Siqi', ''], ['Gan', 'Zhe', ''], ['Cheng', 'Yu', ''], ['Jiang', 'Jing', ''], ['Liu', 'Jingjing', '']]"
1279123,2004.14353,Weijia Xu,"Weijia Xu, Batool Haider, Saab Mansour",End-to-End Slot Alignment and Recognition for Cross-Lingual NLU,Accepted at EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language understanding (NLU) in the context of goal-oriented dialog
systems typically includes intent classification and slot labeling tasks.
Existing methods to expand an NLU system to new languages use machine
translation with slot label projection from source to the translated
utterances, and thus are sensitive to projection errors. In this work, we
propose a novel end-to-end model that learns to align and predict target slot
labels jointly for cross-lingual transfer. We introduce MultiATIS++, a new
multilingual NLU corpus that extends the Multilingual ATIS corpus to nine
languages across four language families, and evaluate our method using the
corpus. Results show that our method outperforms a simple label projection
method using fast-align on most languages, and achieves competitive performance
to the more complex, state-of-the-art projection method with only half of the
training time. We release our MultiATIS++ corpus to the community to continue
future research on cross-lingual NLU.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 17:31:11 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 04:36:04 GMT'}]",2020-10-09,"[['Xu', 'Weijia', ''], ['Haider', 'Batool', ''], ['Mansour', 'Saab', '']]"
1359986,2010.03656,Shachar Rosenman,"Shachar Rosenman, Alon Jacovi, Yoav Goldberg","Exposing Shallow Heuristics of Relation Extraction Models with Challenge
  Data",Accepted as a short paper in EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The process of collecting and annotating training data may introduce
distribution artifacts which may limit the ability of models to learn correct
generalization behavior. We identify failure modes of SOTA relation extraction
(RE) models trained on TACRED, which we attribute to limitations in the data
annotation process. We collect and annotate a challenge-set we call Challenging
RE (CRE), based on naturally occurring corpus examples, to benchmark this
behavior. Our experiments with four state-of-the-art RE models show that they
have indeed adopted shallow heuristics that do not generalize to the
challenge-set data. Further, we find that alternative question answering
modeling performs significantly better than the SOTA models on the
challenge-set, despite worse overall TACRED performance. By adding some of the
challenge data as training examples, the performance of the model improves.
Finally, we provide concrete suggestion on how to improve RE data collection to
alleviate this behavior.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 21:17:25 GMT'}]",2020-10-09,"[['Rosenman', 'Shachar', ''], ['Jacovi', 'Alon', ''], ['Goldberg', 'Yoav', '']]"
1358576,2010.02246,Sopan Khosla,"Sopan Khosla, Shikhar Vashishth, Jill Fain Lehman, Carolyn Rose","MedFilter: Improving Extraction of Task-relevant Utterances through
  Integration of Discourse Structure and Ontological Knowledge",Accepted as Long Paper to EMNLP 2020,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Information extraction from conversational data is particularly challenging
because the task-centric nature of conversation allows for effective
communication of implicit information by humans, but is challenging for
machines. The challenges may differ between utterances depending on the role of
the speaker within the conversation, especially when relevant expertise is
distributed asymmetrically across roles. Further, the challenges may also
increase over the conversation as more shared context is built up through
information communicated implicitly earlier in the dialogue. In this paper, we
propose the novel modeling approach MedFilter, which addresses these insights
in order to increase performance at identifying and categorizing task-relevant
utterances, and in so doing, positively impacts performance at a downstream
information extraction task. We evaluate this approach on a corpus of nearly
7,000 doctor-patient conversations where MedFilter is used to identify
medically relevant contributions to the discussion (achieving a 10% improvement
over SOTA baselines in terms of area under the PR curve). Identifying
task-relevant utterances benefits downstream medical processing, achieving
improvements of 15%, 105%, and 23% respectively for the extraction of symptoms,
medications, and complaints.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 18:01:38 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 18:28:49 GMT'}]",2020-10-09,"[['Khosla', 'Sopan', ''], ['Vashishth', 'Shikhar', ''], ['Lehman', 'Jill Fain', ''], ['Rose', 'Carolyn', '']]"
1278879,2004.14109,Jungsoo Park,"Jungsoo Park, Mujeen Sung, Jinhyuk Lee, Jaewoo Kang",Adversarial Subword Regularization for Robust Neural Machine Translation,"9 pages,1 figure, Findings of EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Exposing diverse subword segmentations to neural machine translation (NMT)
models often improves the robustness of machine translation as NMT models can
experience various subword candidates. However, the diversification of subword
segmentations mostly relies on the pre-trained subword language models from
which erroneous segmentations of unseen words are less likely to be sampled. In
this paper, we present adversarial subword regularization (ADVSR) to study
whether gradient signals during training can be a substitute criterion for
exposing diverse subword segmentations. We experimentally show that our
model-based adversarial samples effectively encourage NMT models to be less
sensitive to segmentation errors and improve the performance of NMT models in
low-resource and out-domain datasets.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 12:06:42 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 05:25:34 GMT'}]",2020-10-09,"[['Park', 'Jungsoo', ''], ['Sung', 'Mujeen', ''], ['Lee', 'Jinhyuk', ''], ['Kang', 'Jaewoo', '']]"
1359625,2010.03295,Marco Basaldella,"Marco Basaldella, Fangyu Liu, Ehsan Shareghi and Nigel Collier",COMETA: A Corpus for Medical Entity Linking in the Social Media,Accepted to EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Whilst there has been growing progress in Entity Linking (EL) for general
language, existing datasets fail to address the complex nature of health
terminology in layman's language. Meanwhile, there is a growing need for
applications that can understand the public's voice in the health domain. To
address this we introduce a new corpus called COMETA, consisting of 20k English
biomedical entity mentions from Reddit expert-annotated with links to SNOMED
CT, a widely-used medical knowledge graph. Our corpus satisfies a combination
of desirable properties, from scale and coverage to diversity and quality, that
to the best of our knowledge has not been met by any of the existing resources
in the field. Through benchmark experiments on 20 EL baselines from string- to
neural-based models we shed light on the ability of these systems to perform
complex inference on entities and concepts under 2 challenging evaluation
scenarios. Our experimental results on COMETA illustrate that no golden bullet
exists and even the best mainstream techniques still have a significant
performance gap to fill, while the best solution relies on combining different
views of data.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 09:16:45 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 12:01:55 GMT'}]",2020-10-09,"[['Basaldella', 'Marco', ''], ['Liu', 'Fangyu', ''], ['Shareghi', 'Ehsan', ''], ['Collier', 'Nigel', '']]"
1357097,2010.00767,Heng Yang,"Heng Yang, Biqing Zeng","Enhancing Fine-grained Sentiment Classification Exploiting Local Context
  Embedding",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Target-oriented sentiment classification is a fine-grained task of natural
language processing to analyze the sentiment polarity of the targets. To
improve the performance of sentiment classification, many approaches proposed
various attention mechanisms to capture the important context words of a
target. However, previous approaches ignored the significant relatedness of a
target's sentiment and its local context. This paper proposes a local
context-aware network (LCA-Net), equipped with the local context embedding and
local context prediction loss, to strengthen the model by emphasizing the
sentiment information of the local context. The experimental results on three
common datasets show that local context-aware network performs superior to
existing approaches in extracting local context features. Besides, the local
context-aware framework is easy to adapt to many models, with the potential to
improve other target-level tasks.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 03:54:37 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 02:21:01 GMT'}]",2020-10-09,"[['Yang', 'Heng', ''], ['Zeng', 'Biqing', '']]"
1355809,2009.14306,Shirley Anugrah Hayati,"Shirley Anugrah Hayati, Dongyeop Kang, Qingxiaoyang Zhu, Weiyan Shi,
  and Zhou Yu",INSPIRED: Toward Sociable Recommendation Dialog Systems,"Accepted as a long paper at EMNLP 2020, corrected typos",,,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  In recommendation dialogs, humans commonly disclose their preference and make
recommendations in a friendly manner. However, this is a challenge when
developing a sociable recommendation dialog system, due to the lack of dialog
dataset annotated with such sociable strategies. Therefore, we present
INSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation
with measures for successful recommendations. To better understand how humans
make recommendations in communication, we design an annotation scheme related
to recommendation strategies based on social science theories and annotate
these dialogs. Our analysis shows that sociable recommendation strategies, such
as sharing personal opinions or communicating with encouragement, more
frequently lead to successful recommendations. Based on our dataset, we train
end-to-end recommendation dialog systems with and without our strategy labels.
In both automatic and human evaluation, our model with strategy incorporation
outperforms the baseline model. This work is a first step for building sociable
recommendation dialog systems with a basis of social science theories.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 21:03:44 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 06:17:22 GMT'}]",2020-10-09,"[['Hayati', 'Shirley Anugrah', ''], ['Kang', 'Dongyeop', ''], ['Zhu', 'Qingxiaoyang', ''], ['Shi', 'Weiyan', ''], ['Yu', 'Zhou', '']]"
1359856,2010.03526,Jiapeng Wu,"Jiapeng Wu, Meng Cao, Jackie Chi Kit Cheung and William L. Hamilton",TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion,"17 pages, 9 figures. EMNLP 2020 Long Paper",,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental
and challenging task. Previous works have approached this problem by augmenting
methods for static knowledge graphs to leverage time-dependent representations.
However, these methods do not explicitly leverage multi-hop structural
information and temporal facts from recent time steps to enhance their
predictions. Additionally, prior work does not explicitly address the temporal
sparsity and variability of entity distributions in TKGs. We propose the
Temporal Message Passing (TeMP) framework to address these challenges by
combining graph neural networks, temporal dynamics models, data imputation and
frequency-based gating techniques. Experiments on standard TKG tasks show that
our approach provides substantial gains compared to the previous state of the
art, achieving a 10.7% average relative improvement in Hits@10 across three
standard benchmarks. Our analysis also reveals important sources of variability
both within and across TKG datasets, and we introduce several simple but strong
baselines that outperform the prior state of the art in certain settings.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 17:11:53 GMT'}]",2020-10-09,"[['Wu', 'Jiapeng', ''], ['Cao', 'Meng', ''], ['Cheung', 'Jackie Chi Kit', ''], ['Hamilton', 'William L.', '']]"
1271570,2004.06800,Lee James O'Riordan,"Lee J. O'Riordan, Myles Doyle, Fabio Baruffa, Venkatesh Kannan",A hybrid classical-quantum workflow for natural language processing,"For associated code, see https://github.com/ICHEC/QNLP",,10.1088/2632-2153/abbd2e,,quant-ph cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language processing (NLP) problems are ubiquitous in classical
computing, where they often require significant computational resources to
infer sentence meanings. With the appearance of quantum computing hardware and
simulators, it is worth developing methods to examine such problems on these
platforms. In this manuscript we demonstrate the use of quantum computing
models to perform NLP tasks, where we represent corpus meanings, and perform
comparisons between sentences of a given structure. We develop a hybrid
workflow for representing small and large scale corpus data sets to be encoded,
processed, and decoded using a quantum circuit model. In addition, we provide
our results showing the efficacy of the method, and release our developed
toolkit as an open software suite.
","[{'version': 'v1', 'created': 'Sun, 12 Apr 2020 12:19:17 GMT'}]",2020-10-09,"[[""O'Riordan"", 'Lee J.', ''], ['Doyle', 'Myles', ''], ['Baruffa', 'Fabio', ''], ['Kannan', 'Venkatesh', '']]"
1278360,2004.13590,Xiaozhi Wang,"Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu,
  Juanzi Li, Peng Li, Yankai Lin, Jie Zhou",MAVEN: A Massive General Domain Event Detection Dataset,Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Event detection (ED), which means identifying event trigger words and
classifying event types, is the first and most fundamental step for extracting
event knowledge from plain text. Most existing datasets exhibit the following
issues that limit further development of ED: (1) Data scarcity. Existing
small-scale datasets are not sufficient for training and stably benchmarking
increasingly sophisticated modern neural methods. (2) Low coverage. Limited
event types of existing datasets cannot well cover general-domain events, which
restricts the applications of ED models. To alleviate these problems, we
present a MAssive eVENt detection dataset (MAVEN), which contains 4,480
Wikipedia documents, 118,732 event mention instances, and 168 event types.
MAVEN alleviates the data scarcity problem and covers much more general event
types. We reproduce the recent state-of-the-art ED models and conduct a
thorough evaluation on MAVEN. The experimental results show that existing ED
methods cannot achieve promising results on MAVEN as on the small datasets,
which suggests that ED in the real world remains a challenging task and
requires further research efforts. We also discuss further directions for
general domain ED with empirical analyses. The source code and dataset can be
obtained from https://github.com/THU-KEG/MAVEN-dataset.
","[{'version': 'v1', 'created': 'Tue, 28 Apr 2020 15:25:19 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 09:19:13 GMT'}]",2020-10-09,"[['Wang', 'Xiaozhi', ''], ['Wang', 'Ziqi', ''], ['Han', 'Xu', ''], ['Jiang', 'Wangyi', ''], ['Han', 'Rong', ''], ['Liu', 'Zhiyuan', ''], ['Li', 'Juanzi', ''], ['Li', 'Peng', ''], ['Lin', 'Yankai', ''], ['Zhou', 'Jie', '']]"
1230685,2001.05272,Zhenyu Xuan,"Zhenyu Xuan, Rui Bao, Shengyi Jiang",FGN: Fusion Glyph Network for Chinese Named Entity Recognition,,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chinese NER is a challenging task. As pictographs, Chinese characters contain
latent glyph information, which is often overlooked. In this paper, we propose
the FGN, Fusion Glyph Network for Chinese NER. Except for adding glyph
information, this method may also add extra interactive information with the
fusion mechanism. The major innovations of FGN include: (1) a novel CNN
structure called CGS-CNN is proposed to capture both glyph information and
interactive information between glyphs from neighboring characters. (2) we
provide a method with sliding window and Slice-Attention to fuse the BERT
representation and glyph representation for a character, which may capture
potential interactive knowledge between context and glyph. Experiments are
conducted on four NER datasets, showing that FGN with LSTM-CRF as tagger
achieves new state-of-the-arts performance for Chinese NER. Further, more
experiments are conducted to investigate the influences of various components
and settings in FGN.
","[{'version': 'v1', 'created': 'Wed, 15 Jan 2020 12:39:20 GMT'}, {'version': 'v2', 'created': 'Fri, 14 Feb 2020 15:58:51 GMT'}, {'version': 'v3', 'created': 'Tue, 24 Mar 2020 05:05:45 GMT'}, {'version': 'v4', 'created': 'Sat, 27 Jun 2020 13:28:21 GMT'}, {'version': 'v5', 'created': 'Tue, 15 Sep 2020 07:54:43 GMT'}, {'version': 'v6', 'created': 'Thu, 8 Oct 2020 11:46:09 GMT'}]",2020-10-09,"[['Xuan', 'Zhenyu', ''], ['Bao', 'Rui', ''], ['Jiang', 'Shengyi', '']]"
1335661,2008.07939,Van-Hoang Nguyen,"Van-Hoang Nguyen and Kazunari Sugiyama and Preslav Nakov and Min-Yen
  Kan","FANG: Leveraging Social Context for Fake News Detection Using Graph
  Representation",To appear in CIKM 2020,,10.1145/3340531.3412046,,cs.SI cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose Factual News Graph (FANG), a novel graphical social context
representation and learning framework for fake news detection. Unlike previous
contextual models that have targeted performance, our focus is on
representation learning. Compared to transductive models, FANG is scalable in
training as it does not have to maintain all nodes, and it is efficient at
inference time, without the need to re-process the entire graph. Our
experimental results show that FANG is better at capturing the social context
into a high fidelity representation, compared to recent graphical and
non-graphical models. In particular, FANG yields significant improvements for
the task of fake news detection, and it is robust in the case of limited
training data. We further demonstrate that the representations learned by FANG
generalize to related tasks, such as predicting the factuality of reporting of
a news medium.
","[{'version': 'v1', 'created': 'Tue, 18 Aug 2020 14:05:16 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 11:45:23 GMT'}]",2020-10-09,"[['Nguyen', 'Van-Hoang', ''], ['Sugiyama', 'Kazunari', ''], ['Nakov', 'Preslav', ''], ['Kan', 'Min-Yen', '']]"
1352767,2009.11264,Satwik Bhattamishra,"Satwik Bhattamishra, Kabir Ahuja, Navin Goyal","On the Ability and Limitations of Transformers to Recognize Formal
  Languages",EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformers have supplanted recurrent models in a large number of NLP tasks.
However, the differences in their abilities to model different syntactic
properties remain largely unknown. Past works suggest that LSTMs generalize
very well on regular languages and have close connections with counter
languages. In this work, we systematically study the ability of Transformers to
model such languages as well as the role of its individual components in doing
so. We first provide a construction of Transformers for a subclass of counter
languages, including well-studied languages such as n-ary Boolean Expressions,
Dyck-1, and its generalizations. In experiments, we find that Transformers do
well on this subclass, and their learned mechanism strongly correlates with our
construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well
only on a subset of regular languages with degrading performance as we make
languages more complex according to a well-known measure of complexity. Our
analysis also provides insights on the role of self-attention mechanism in
modeling certain behaviors and the influence of positional encoding schemes on
the learning and generalization abilities of the model.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 17:21:33 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 12:55:37 GMT'}]",2020-10-09,"[['Bhattamishra', 'Satwik', ''], ['Ahuja', 'Kabir', ''], ['Goyal', 'Navin', '']]"
1349286,2009.07783,Shuhei Kurita,Shuhei Kurita and Kyunghyun Cho,"Generative Language-Grounded Policy in Vision-and-Language Navigation
  with Bayes' Rule","13 pages, 8 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Vision-and-language navigation (VLN) is a task in which an agent is embodied
in a realistic 3D environment and follows an instruction to reach the goal
node. While most of the previous studies have built and investigated a
discriminative approach, we notice that there are in fact two possible
approaches to building such a VLN agent: discriminative \textit{and}
generative. In this paper, we design and investigate a generative
language-grounded policy which uses a language model to compute the
distribution over all possible instructions i.e. all possible sequences of
vocabulary tokens given action and the transition history. In experiments, we
show that the proposed generative approach outperforms the discriminative
approach in the Room-2-Room (R2R) and Room-4-Room (R4R) datasets, especially in
the unseen environments. We further show that the combination of the generative
and discriminative policies achieves close to the state-of-the art results in
the R2R dataset, demonstrating that the generative and discriminative policies
capture the different aspects of VLN.
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 16:23:17 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 18:57:09 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Oct 2020 17:16:49 GMT'}]",2020-10-09,"[['Kurita', 'Shuhei', ''], ['Cho', 'Kyunghyun', '']]"
1202505,1911.03911,Lukasz Borchmann,"{\L}ukasz Borchmann and Dawid Wi\'sniewski and Andrzej Gretkowski and
  Izabela Kosmala and Dawid Jurkiewicz and {\L}ukasz Sza{\l}kiewicz and
  Gabriela Pa{\l}ka and Karol Kaczmarek and Agnieszka Kaliska and Filip
  Grali\'nski","Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge
  with Competitive Baselines",Submitted to Findings of EMNLP,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a new shared task of semantic retrieval from legal texts, in which
a so-called contract discovery is to be performed, where legal clauses are
extracted from documents, given a few examples of similar clauses from other
legal acts. The task differs substantially from conventional NLI and shared
tasks on legal information extraction (e.g., one has to identify text span
instead of a single document, page, or paragraph). The specification of the
proposed task is followed by an evaluation of multiple solutions within the
unified framework proposed for this branch of methods. It is shown that
state-of-the-art pretrained encoders fail to provide satisfactory results on
the task proposed. In contrast, Language Model-based solutions perform better,
especially when unsupervised fine-tuning is applied. Besides the ablation
studies, we addressed questions regarding detection accuracy for relevant text
fragments depending on the number of examples available. In addition to the
dataset and reference results, LMs specialized in the legal domain were made
publicly available.
","[{'version': 'v1', 'created': 'Sun, 10 Nov 2019 11:50:09 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 12:36:24 GMT'}]",2020-10-09,"[['Borchmann', 'Łukasz', ''], ['Wiśniewski', 'Dawid', ''], ['Gretkowski', 'Andrzej', ''], ['Kosmala', 'Izabela', ''], ['Jurkiewicz', 'Dawid', ''], ['Szałkiewicz', 'Łukasz', ''], ['Pałka', 'Gabriela', ''], ['Kaczmarek', 'Karol', ''], ['Kaliska', 'Agnieszka', ''], ['Graliński', 'Filip', '']]"
1281321,2005.01526,Dheeraj Rajagopal,"Dheeraj Rajagopal, Niket Tandon, Bhavana Dalvi, Peter Clark, Eduard
  Hovy","What-if I ask you to explain: Explaining the effects of perturbations in
  procedural text",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We address the task of explaining the effects of perturbations in procedural
text, an important test of process comprehension. Consider a passage describing
a rabbit's life-cycle: humans can easily explain the effect on the rabbit
population if a female rabbit becomes ill -- i.e., the female rabbit would not
become pregnant, and as a result not have babies leading to a decrease in
rabbit population. We present QUARTET, a system that constructs such
explanations from paragraphs, by modeling the explanation task as a multitask
learning problem. QUARTET provides better explanations (based on the sentences
in the procedural text) compared to several strong baselines on a recent
process comprehension benchmark. We also present a surprising secondary effect:
our model also achieves a new SOTA with a 7% absolute F1 improvement on a
downstream QA task. This illustrates that good explanations do not have to come
at the expense of end task performance.
","[{'version': 'v1', 'created': 'Mon, 4 May 2020 14:36:23 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 01:16:34 GMT'}]",2020-10-09,"[['Rajagopal', 'Dheeraj', ''], ['Tandon', 'Niket', ''], ['Dalvi', 'Bhavana', ''], ['Clark', 'Peter', ''], ['Hovy', 'Eduard', '']]"
1356368,2010.00038,Mohit Chandra,"Mohit Chandra, Ashwin Pathak, Eesha Dutta, Paryul Jain, Manish Gupta,
  Manish Shrivastava, Ponnurangam Kumaraguru","AbuseAnalyzer: Abuse Detection, Severity and Target Prediction for Gab
  Posts",Extended version for our paper accepted at COLING 2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While extensive popularity of online social media platforms has made
information dissemination faster, it has also resulted in widespread online
abuse of different types like hate speech, offensive language, sexist and
racist opinions, etc. Detection and curtailment of such abusive content is
critical for avoiding its psychological impact on victim communities, and
thereby preventing hate crimes. Previous works have focused on classifying user
posts into various forms of abusive behavior. But there has hardly been any
focus on estimating the severity of abuse and the target. In this paper, we
present a first of the kind dataset with 7601 posts from Gab which looks at
online abuse from the perspective of presence of abuse, severity and target of
abusive behavior. We also propose a system to address these tasks, obtaining an
accuracy of ~80% for abuse presence, ~82% for abuse target prediction, and ~65%
for abuse severity prediction.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 18:12:50 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 17:42:33 GMT'}]",2020-10-09,"[['Chandra', 'Mohit', ''], ['Pathak', 'Ashwin', ''], ['Dutta', 'Eesha', ''], ['Jain', 'Paryul', ''], ['Gupta', 'Manish', ''], ['Shrivastava', 'Manish', ''], ['Kumaraguru', 'Ponnurangam', '']]"
1268442,2004.03672,Zi-Yi Dou,"Zi-Yi Dou, Antonios Anastasopoulos, Graham Neubig",Dynamic Data Selection and Weighting for Iterative Back-Translation,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Back-translation has proven to be an effective method to utilize monolingual
data in neural machine translation (NMT), and iteratively conducting
back-translation can further improve the model performance. Selecting which
monolingual data to back-translate is crucial, as we require that the resulting
synthetic data are of high quality and reflect the target domain. To achieve
these two goals, data selection and weighting strategies have been proposed,
with a common practice being to select samples close to the target domain but
also dissimilar to the average general-domain text. In this paper, we provide
insights into this commonly used approach and generalize it to a dynamic
curriculum learning strategy, which is applied to iterative back-translation
models. In addition, we propose weighting strategies based on both the current
quality of the sentence and its improvement over the previous iteration. We
evaluate our models on domain adaptation, low-resource, and high-resource MT
settings and on two language pairs. Experimental results demonstrate that our
methods achieve improvements of up to 1.8 BLEU points over competitive
baselines.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 19:49:58 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 22:00:22 GMT'}]",2020-10-09,"[['Dou', 'Zi-Yi', ''], ['Anastasopoulos', 'Antonios', ''], ['Neubig', 'Graham', '']]"
1359978,2010.03648,Nikunj Saunshi,"Nikunj Saunshi, Sadhika Malladi, Sanjeev Arora","A Mathematical Exploration of Why Language Models Help Solve Downstream
  Tasks",29 pages,,,,cs.CL cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Autoregressive language models pretrained on large corpora have been
successful at solving downstream tasks, even with zero-shot usage. However,
there is little theoretical justification for their success. This paper
considers the following questions: (1) Why should learning the distribution of
natural language help with downstream classification tasks? (2) Why do features
learned using language modeling help solve downstream tasks with linear
classifiers? For (1), we hypothesize, and verify empirically, that
classification tasks of interest can be reformulated as next word prediction
tasks, thus making language modeling a meaningful pretraining task. For (2), we
analyze properties of the cross-entropy objective to show that
$\epsilon$-optimal language models in cross-entropy (log-perplexity) learn
features that are $\mathcal{O}(\sqrt{\epsilon})$-good on natural linear
classification tasks, thus demonstrating mathematically that doing well on
language modeling can be beneficial for downstream tasks. We perform
experiments to verify assumptions and validate theoretical results. Our
theoretical insights motivate a simple alternative to the cross-entropy
objective that performs well on some linear classification tasks.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 20:56:40 GMT'}]",2020-10-09,"[['Saunshi', 'Nikunj', ''], ['Malladi', 'Sadhika', ''], ['Arora', 'Sanjeev', '']]"
1359974,2010.03644,Wanrong Zhu,"Wanrong Zhu, Xin Eric Wang, Pradyumna Narayana, Kazoo Sone, Sugato
  Basu, William Yang Wang","Towards Understanding Sample Variance in Visually Grounded Language
  Generation: Evaluations and Observations",EMNLP 2020,,,,cs.CL cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A major challenge in visually grounded language generation is to build robust
benchmark datasets and models that can generalize well in real-world settings.
To do this, it is critical to ensure that our evaluation protocols are correct,
and benchmarks are reliable. In this work, we set forth to design a set of
experiments to understand an important but often ignored problem in visually
grounded language generation: given that humans have different utilities and
visual attention, how will the sample variance in multi-reference datasets
affect the models' performance? Empirically, we study several multi-reference
datasets and corresponding vision-and-language tasks. We show that it is of
paramount importance to report variance in experiments; that human-generated
references could vary drastically in different datasets/tasks, revealing the
nature of each task; that metric-wise, CIDEr has shown systematically larger
variances than others. Our evaluations on reference-per-instance shed light on
the design of reliable datasets in the future.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 20:45:14 GMT'}]",2020-10-09,"[['Zhu', 'Wanrong', ''], ['Wang', 'Xin Eric', ''], ['Narayana', 'Pradyumna', ''], ['Sone', 'Kazoo', ''], ['Basu', 'Sugato', ''], ['Wang', 'William Yang', '']]"
1359970,2010.03640,Emily Allaway,Emily Allaway and Kathleen McKeown,"Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic
  Representations",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Stance detection is an important component of understanding hidden influences
in everyday life. Since there are thousands of potential topics to take a
stance on, most with little to no training data, we focus on zero-shot stance
detection: classifying stance from no training examples. In this paper, we
present a new dataset for zero-shot stance detection that captures a wider
range of topics and lexical variation than in previous datasets. Additionally,
we propose a new model for stance detection that implicitly captures
relationships between topics using generalized topic representations and show
that this model improves performance on a number of challenging linguistic
phenomena.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 20:27:12 GMT'}]",2020-10-09,"[['Allaway', 'Emily', ''], ['McKeown', 'Kathleen', '']]"
1359953,2010.03623,Dominika Woszczyk,"Dominika Woszczyk, Stavros Petridis, David Millard",Domain Adversarial Neural Networks for Dysarthric Speech Recognition,"5 pages, to be published in Interspeech 2020",,,,cs.SD cs.CL cs.LG eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Speech recognition systems have improved dramatically over the last few
years, however, their performance is significantly degraded for the cases of
accented or impaired speech. This work explores domain adversarial neural
networks (DANN) for speaker-independent speech recognition on the UAS dataset
of dysarthric speech. The classification task on 10 spoken digits is performed
using an end-to-end CNN taking raw audio as input. The results are compared to
a speaker-adaptive (SA) model as well as speaker-dependent (SD) and multi-task
learning models (MTL). The experiments conducted in this paper show that DANN
achieves an absolute recognition rate of 74.91% and outperforms the baseline by
12.18%. Additionally, the DANN model achieves comparable results to the SA
model's recognition rate of 77.65%. We also observe that when labelled
dysarthric speech data is available DANN and MTL perform similarly, but when
they are not DANN performs better than MTL.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 19:51:41 GMT'}]",2020-10-09,"[['Woszczyk', 'Dominika', ''], ['Petridis', 'Stavros', ''], ['Millard', 'David', '']]"
1359947,2010.03617,Markus Leippold,Rahul Mishra and Piyush Yadav and Remi Calizzano and Markus Leippold,"MuSeM: Detecting Incongruent News Headlines using Mutual Attentive
  Semantic Matching","Accepted paper; IEEE 2020 International Conference on Machine
  Learning and Applications (ICMLA)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Measuring the congruence between two texts has several useful applications,
such as detecting the prevalent deceptive and misleading news headlines on the
web. Many works have proposed machine learning based solutions such as text
similarity between the headline and body text to detect the incongruence. Text
similarity based methods fail to perform well due to different inherent
challenges such as relative length mismatch between the news headline and its
body content and non-overlapping vocabulary. On the other hand, more recent
works that use headline guided attention to learn a headline derived contextual
representation of the news body also result in convoluting overall
representation due to the news body's lengthiness. This paper proposes a method
that uses inter-mutual attention-based semantic matching between the original
and synthetically generated headlines, which utilizes the difference between
all pairs of word embeddings of words involved. The paper also investigates two
more variations of our method, which use concatenation and dot-products of word
embeddings of the words of original and synthetic headlines. We observe that
the proposed method outperforms prior arts significantly for two publicly
available datasets.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 19:19:42 GMT'}]",2020-10-09,"[['Mishra', 'Rahul', ''], ['Yadav', 'Piyush', ''], ['Calizzano', 'Remi', ''], ['Leippold', 'Markus', '']]"
1280580,2005.00785,Xisen Jin,"Xisen Jin, Junyi Du, Arka Sadhu, Ram Nevatia, Xiang Ren",Visually Grounded Continual Learning of Compositional Phrases,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Humans acquire language continually with much more limited access to data
samples at a time, as compared to contemporary NLP systems. To study this
human-like language acquisition ability, we present VisCOLL, a visually
grounded language learning task, which simulates the continual acquisition of
compositional phrases from streaming visual scenes. In the task, models are
trained on a paired image-caption stream which has shifting object
distribution; while being constantly evaluated by a visually-grounded masked
language prediction task on held-out test sets. VisCOLL compounds the
challenges of continual learning (i.e., learning from continuously shifting
data distribution) and compositional generalization (i.e., generalizing to
novel compositions). To facilitate research on VisCOLL, we construct two
datasets, COCO-shift and Flickr-shift, and benchmark them using different
continual learning methods. Results reveal that SoTA continual learning
approaches provide little to no improvements on VisCOLL, since storing examples
of all possible compositions is infeasible. We conduct further ablations and
analysis to guide future work.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 10:45:30 GMT'}, {'version': 'v2', 'created': 'Tue, 15 Sep 2020 21:19:50 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 10:25:18 GMT'}, {'version': 'v4', 'created': 'Wed, 7 Oct 2020 18:31:37 GMT'}]",2020-10-09,"[['Jin', 'Xisen', ''], ['Du', 'Junyi', ''], ['Sadhu', 'Arka', ''], ['Nevatia', 'Ram', ''], ['Ren', 'Xiang', '']]"
1359944,2010.03614,Radu Tudor Ionescu,"Mihaela Gaman, Radu Tudor Ionescu","Combining Deep Learning and String Kernels for the Localization of Swiss
  German Tweets",Accepted at VarDial 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we introduce the methods proposed by the UnibucKernel team in
solving the Social Media Variety Geolocation task featured in the 2020 VarDial
Evaluation Campaign. We address only the second subtask, which targets a data
set composed of nearly 30 thousand Swiss German Jodels. The dialect
identification task is about accurately predicting the latitude and longitude
of test samples. We frame the task as a double regression problem, employing a
variety of machine learning approaches to predict both latitude and longitude.
From simple models for regression, such as Support Vector Regression, to deep
neural networks, such as Long Short-Term Memory networks and character-level
convolutional neural networks, and, finally, to ensemble models based on
meta-learners, such as XGBoost, our interest is focused on approaching the
problem from a few different perspectives, in an attempt to minimize the
prediction error. With the same goal in mind, we also considered many types of
features, from high-level features, such as BERT embeddings, to low-level
features, such as characters n-grams, which are known to provide good results
in dialect identification. Our empirical results indicate that the handcrafted
model based on string kernels outperforms the deep learning approaches.
Nevertheless, our best performance is given by the ensemble model that combines
both handcrafted and deep learning models.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 19:16:45 GMT'}]",2020-10-09,"[['Gaman', 'Mihaela', ''], ['Ionescu', 'Radu Tudor', '']]"
1359934,2010.03604,Chen Zheng,"Chen Zheng, Parisa Kordjamshidi",SRLGRN: Semantic Role Labeling Graph Reasoning Network,Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This work deals with the challenge of learning and reasoning over multi-hop
question answering (QA). We propose a graph reasoning network based on the
semantic structure of the sentences to learn cross paragraph reasoning paths
and find the supporting facts and the answer jointly. The proposed graph is a
heterogeneous document-level graph that contains nodes of type sentence
(question, title, and other sentences), and semantic role labeling sub-graphs
per sentence that contain arguments as nodes and predicates as edges.
Incorporating the argument types, the argument phrases, and the semantics of
the edges originated from SRL predicates into the graph encoder helps in
finding and also the explainability of the reasoning paths. Our proposed
approach shows competitive performance on the HotpotQA distractor setting
benchmark compared to the recent state-of-the-art models.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 18:51:17 GMT'}]",2020-10-09,"[['Zheng', 'Chen', ''], ['Kordjamshidi', 'Parisa', '']]"
1279923,2005.00128,Patrick Xia,"Patrick Xia, Jo\~ao Sedoc, Benjamin Van Durme",Incremental Neural Coreference Resolution in Constant Memory,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate modeling coreference resolution under a fixed memory
constraint by extending an incremental clustering algorithm to utilize
contextualized encoders and neural components. Given a new sentence, our
end-to-end algorithm proposes and scores each mention span against explicit
entity representations created from the earlier document context (if any).
These spans are then used to update the entity's representations before being
forgotten; we only retain a fixed set of salient entities throughout the
document. In this work, we successfully convert a high-performing model (Joshi
et al., 2020), asymptotically reducing its memory usage to constant space with
only a 0.3% relative loss in F1 on OntoNotes 5.0.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 22:31:20 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 19:49:31 GMT'}]",2020-10-09,"[['Xia', 'Patrick', ''], ['Sedoc', 'João', ''], ['Van Durme', 'Benjamin', '']]"
1360373,2010.04043,Haokun Liu,"Haokun Liu, William Huang, Dhara A. Mungra, Samuel R. Bowman",Precise Task Formalization Matters in Winograd Schema Evaluations,Accepted to the EMNLP 2020 conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Performance on the Winograd Schema Challenge (WSC), a respected English
commonsense reasoning benchmark, recently rocketed from chance accuracy to 89%
on the SuperGLUE leaderboard, with relatively little corroborating evidence of
a correspondingly large improvement in reasoning ability. We hypothesize that
much of this improvement comes from recent changes in task formalization---the
combination of input specification, loss function, and reuse of pretrained
parameters---by users of the dataset, rather than improvements in the
pretrained model's reasoning ability. We perform an ablation on two Winograd
Schema datasets that interpolates between the formalizations used before and
after this surge, and find (i) framing the task as multiple choice improves
performance by 2-6 points and (ii) several additional techniques, including the
reuse of a pretrained language modeling head, can mitigate the model's extreme
sensitivity to hyperparameters. We urge future benchmark creators to impose
additional structure to minimize the impact of formalization decisions on
reported results.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 15:10:47 GMT'}]",2020-10-09,"[['Liu', 'Haokun', ''], ['Huang', 'William', ''], ['Mungra', 'Dhara A.', ''], ['Bowman', 'Samuel R.', '']]"
1359488,2010.03158,Xuelu Chen,"Xuelu Chen, Muhao Chen, Changjun Fan, Ankith Uppunda, Yizhou Sun,
  Carlo Zaniolo",Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer,Findings of EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Predicting missing facts in a knowledge graph (KG) is a crucial task in
knowledge base construction and reasoning, and it has been the subject of much
research in recent works using KG embeddings. While existing KG embedding
approaches mainly learn and predict facts within a single KG, a more plausible
solution would benefit from the knowledge in multiple language-specific KGs,
considering that different KGs have their own strengths and limitations on data
quality and coverage. This is quite challenging, since the transfer of
knowledge among multiple independently maintained KGs is often hindered by the
insufficiency of alignment information and the inconsistency of described
facts. In this paper, we propose KEnS, a novel framework for embedding learning
and ensemble knowledge transfer across a number of language-specific KGs. KEnS
embeds all KGs in a shared embedding space, where the association of entities
is captured based on self-learning. Then, KEnS performs ensemble inference to
combine prediction results from embeddings of multiple language-specific KGs,
for which multiple ensemble techniques are investigated. Experiments on five
real-world language-specific KGs show that KEnS consistently improves
state-of-the-art methods on KG completion, via effectively identifying and
leveraging complementary knowledge.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 04:54:03 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 07:54:24 GMT'}]",2020-10-09,"[['Chen', 'Xuelu', ''], ['Chen', 'Muhao', ''], ['Fan', 'Changjun', ''], ['Uppunda', 'Ankith', ''], ['Sun', 'Yizhou', ''], ['Zaniolo', 'Carlo', '']]"
1359904,2010.03574,Chao-Chun Hsu,"Chao-Chun Hsu, Shantanu Karnwal, Sendhil Mullainathan, Ziad Obermeyer,
  Chenhao Tan",Characterizing the Value of Information in Medical Notes,"15 pages, 12 figures, Findings of EMNLP 2020, code is available at
  https://github.com/BoulderDS/value-of-medical-notes",,,,cs.CL cs.AI cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning models depend on the quality of input data. As electronic
health records are widely adopted, the amount of data in health care is
growing, along with complaints about the quality of medical notes. We use two
prediction tasks, readmission prediction and in-hospital mortality prediction,
to characterize the value of information in medical notes. We show that as a
whole, medical notes only provide additional predictive power over structured
information in readmission prediction. We further propose a probing framework
to select parts of notes that enable more accurate predictions than using all
notes, despite that the selected information leads to a distribution shift from
the training data (""all notes""). Finally, we demonstrate that models trained on
the selected valuable information achieve even better predictive performance,
with only 6.8% of all the tokens for readmission prediction.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 18:00:03 GMT'}]",2020-10-09,"[['Hsu', 'Chao-Chun', ''], ['Karnwal', 'Shantanu', ''], ['Mullainathan', 'Sendhil', ''], ['Obermeyer', 'Ziad', ''], ['Tan', 'Chenhao', '']]"
1359880,2010.03550,Benjamin Nye,"Benjamin E. Nye, Jay DeYoung, Eric Lehman, Ani Nenkova, Iain J.
  Marshall, Byron C. Wallace","Understanding Clinical Trial Reports: Extracting Medical Entities and
  Their Relations",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The best evidence concerning comparative treatment effectiveness comes from
clinical trials, the results of which are reported in unstructured articles.
Medical experts must manually extract information from articles to inform
decision-making, which is time-consuming and expensive. Here we consider the
end-to-end task of both (a) extracting treatments and outcomes from full-text
articles describing clinical trials (entity identification) and, (b) inferring
the reported results for the former with respect to the latter (relation
extraction). We introduce new data for this task, and evaluate models that have
recently achieved state-of-the-art results on similar tasks in Natural Language
Processing. We then propose a new method motivated by how trial results are
typically presented that outperforms these purely data-driven baselines.
Finally, we run a fielded evaluation of the model with a non-profit seeking to
identify existing drugs that might be re-purposed for cancer, showing the
potential utility of end-to-end evidence extraction systems.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 17:50:58 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 15:19:07 GMT'}]",2020-10-09,"[['Nye', 'Benjamin E.', ''], ['DeYoung', 'Jay', ''], ['Lehman', 'Eric', ''], ['Nenkova', 'Ani', ''], ['Marshall', 'Iain J.', ''], ['Wallace', 'Byron C.', '']]"
1280501,2005.00706,Frank F. Xu,"Frank F. Xu, Lei Ji, Botian Shi, Junyi Du, Graham Neubig, Yonatan
  Bisk, Nan Duan","A Benchmark for Structured Procedural Knowledge Extraction from Cooking
  Videos","Accepted by NLP Beyond Text - First International Workshop on Natural
  Language Processing Beyond Text @ EMNLP 2020",,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Watching instructional videos are often used to learn about procedures. Video
captioning is one way of automatically collecting such knowledge. However, it
provides only an indirect, overall evaluation of multimodal models with no
finer-grained quantitative measure of what they have learned. We propose
instead, a benchmark of structured procedural knowledge extracted from cooking
videos. This work is complementary to existing tasks, but requires models to
produce interpretable structured knowledge in the form of verb-argument tuples.
Our manually annotated open-vocabulary resource includes 356 instructional
cooking videos and 15,523 video clip/sentence-level annotations. Our analysis
shows that the proposed task is challenging and standard modeling approaches
like unsupervised segmentation, semantic role labeling, and visual action
detection perform poorly when forced to predict every action of a procedure in
a structured form.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 05:15:20 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 13:54:27 GMT'}]",2020-10-12,"[['Xu', 'Frank F.', ''], ['Ji', 'Lei', ''], ['Shi', 'Botian', ''], ['Du', 'Junyi', ''], ['Neubig', 'Graham', ''], ['Bisk', 'Yonatan', ''], ['Duan', 'Nan', '']]"
1353009,2009.11506,Liang Wang,"Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, Jingming Liu",Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems,"We decide to withdraw this paper, since the proposed Ape210K dataset
  is not going public, the experiments in this paper is meaningless and
  irreproducible without access to the dataset. Please contact
  wangliang01@fenbi.com if you have any questions",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Automatic math word problem solving has attracted growing attention in recent
years. The evaluation datasets used by previous works have serious limitations
in terms of scale and diversity. In this paper, we release a new large-scale
and template-rich math word problem dataset named Ape210K. It consists of 210K
Chinese elementary school-level math problems, which is 9 times the size of the
largest public dataset Math23K. Each problem contains both the gold answer and
the equations needed to derive the answer. Ape210K is also of greater diversity
with 56K templates, which is 25 times more than Math23K. Our analysis shows
that solving Ape210K requires not only natural language understanding but also
commonsense knowledge. We expect Ape210K to be a benchmark for math word
problem solving systems. Experiments indicate that state-of-the-art models on
the Math23K dataset perform poorly on Ape210K. We propose a copy-augmented and
feature-enriched sequence to sequence (seq2seq) model, which outperforms
existing models by 3.2% on the Math23K dataset and serves as a strong baseline
of the Ape210K dataset. The gap is still significant between human and our
baseline model, calling for further research efforts. We make Ape210K dataset
publicly available at https://github.com/yuantiku/ape210k
","[{'version': 'v1', 'created': 'Thu, 24 Sep 2020 06:17:10 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 01:36:35 GMT'}]",2020-10-12,"[['Zhao', 'Wei', ''], ['Shang', 'Mingyue', ''], ['Liu', 'Yang', ''], ['Wang', 'Liang', ''], ['Liu', 'Jingming', '']]"
1360106,2010.03776,Kunze Wang,"Kunze Wang, Dong Lu, Soyeon Caren Han, Siqu Long, Josiah Poon",Detect All Abuse! Toward Universal Abusive Language Detection Models,"11 pages, 3 figures. Accepted by COLING2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Online abusive language detection (ALD) has become a societal issue of
increasing importance in recent years. Several previous works in online ALD
focused on solving a single abusive language problem in a single domain, like
Twitter, and have not been successfully transferable to the general ALD task or
domain. In this paper, we introduce a new generic ALD framework, MACAS, which
is capable of addressing several types of ALD tasks across different domains.
Our generic framework covers multi-aspect abusive language embeddings that
represent the target and content aspects of abusive language and applies a
textual graph embedding that analyses the user's linguistic behaviour. Then, we
propose and use the cross-attention gate flow mechanism to embrace multiple
aspects of abusive language. Quantitative and qualitative evaluation results
show that our ALD algorithm rivals or exceeds the six state-of-the-art ALD
algorithms across seven ALD datasets covering multiple aspects of abusive
language and different online community domains.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 05:39:00 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 10:29:36 GMT'}]",2020-10-12,"[['Wang', 'Kunze', ''], ['Lu', 'Dong', ''], ['Han', 'Soyeon Caren', ''], ['Long', 'Siqu', ''], ['Poon', 'Josiah', '']]"
1359862,2010.03532,Yixin Nie,"Yixin Nie, Xiang Zhou, Mohit Bansal","What Can We Learn from Collective Human Opinions on Natural Language
  Inference Data?",EMNLP 2020 (13 pages),,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the subjective nature of many NLP tasks, most NLU evaluations have
focused on using the majority label with presumably high agreement as the
ground truth. Less attention has been paid to the distribution of human
opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to
study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset
is created by collecting 100 annotations per example for 3,113 examples in SNLI
and MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high
human disagreement exists in a noticeable amount of examples in these datasets;
(2) the state-of-the-art models lack the ability to recover the distribution
over human labels; (3) models achieve near-perfect accuracy on the subset of
data with a high level of human agreement, whereas they can barely beat a
random guess on the data with low levels of human agreement, which compose most
of the common errors made by state-of-the-art models on the evaluation sets.
This questions the validity of improving model performance on old metrics for
the low-agreement part of evaluation datasets. Hence, we argue for a detailed
examination of human agreement in future data collection efforts, and
evaluating model outputs against the distribution over collective human
opinions. The ChaosNLI dataset and experimental scripts are available at
https://github.com/easonnie/ChaosNLI
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 17:26:06 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 19:32:45 GMT'}]",2020-10-12,"[['Nie', 'Yixin', ''], ['Zhou', 'Xiang', ''], ['Bansal', 'Mohit', '']]"
1096576,1903.04190,Xingyi Cheng,"Weipeng Huang, Xingyi Cheng, Kunlong Chen, Taifeng Wang, Wei Chu","Toward Fast and Accurate Neural Chinese Word Segmentation with
  Multi-Criteria Learning",Accepted at COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The ambiguous annotation criteria lead to divergence of Chinese Word
Segmentation (CWS) datasets in various granularities. Multi-criteria Chinese
word segmentation aims to capture various annotation criteria among datasets
and leverage their common underlying knowledge. In this paper, we propose a
domain adaptive segmenter to exploit diverse criteria of various datasets. Our
model is based on Bidirectional Encoder Representations from Transformers
(BERT), which is responsible for introducing open-domain knowledge. Private and
shared projection layers are proposed to capture domain-specific knowledge and
common knowledge, respectively. We also optimize computational efficiency via
distillation, quantization, and compiler optimization. Experiments show that
our segmenter outperforms the previous state of the art (SOTA) models on 10 CWS
datasets with superior efficiency.
","[{'version': 'v1', 'created': 'Mon, 11 Mar 2019 09:48:39 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 07:58:36 GMT'}]",2020-10-12,"[['Huang', 'Weipeng', ''], ['Cheng', 'Xingyi', ''], ['Chen', 'Kunlong', ''], ['Wang', 'Taifeng', ''], ['Chu', 'Wei', '']]"
1353822,2009.12319,Son T. Luu,"Son T. Luu, Kiet Van Nguyen and Ngan Luu-Thuy Nguyen",Empirical Study of Text Augmentation on Social Media Text in Vietnamese,"Accepted by The 34th Pacific Asia Conference on Language, Information
  and Computation",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In the text classification problem, the imbalance of labels in datasets
affect the performance of the text-classification models. Practically, the data
about user comments on social networking sites not altogether appeared - the
administrators often only allow positive comments and hide negative comments.
Thus, when collecting the data about user comments on the social network, the
data is usually skewed about one label, which leads the dataset to become
imbalanced and deteriorate the model's ability. The data augmentation
techniques are applied to solve the imbalance problem between classes of the
dataset, increasing the prediction model's accuracy. In this paper, we
performed augmentation techniques on the VLSP2019 Hate Speech Detection on
Vietnamese social texts and the UIT - VSFC: Vietnamese Students' Feedback
Corpus for Sentiment Analysis. The result of augmentation increases by about
1.5% in the F1-macro score on both corpora.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 16:18:52 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 09:40:30 GMT'}]",2020-10-12,"[['Luu', 'Son T.', ''], ['Van Nguyen', 'Kiet', ''], ['Nguyen', 'Ngan Luu-Thuy', '']]"
1358399,2010.02069,Oskar van der Wal MSc,"Oskar van der Wal, Silvan de Boer, Elia Bruni and Dieuwke Hupkes",The Grammar of Emergent Languages,Accepted at EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we consider the syntactic properties of languages emerged in
referential games, using unsupervised grammar induction (UGI) techniques
originally designed to analyse natural language. We show that the considered
UGI techniques are appropriate to analyse emergent languages and we then study
if the languages that emerge in a typical referential game setup exhibit
syntactic structure, and to what extent this depends on the maximum message
length and number of symbols that the agents are allowed to use. Our
experiments demonstrate that a certain message length and vocabulary size are
required for structure to emerge, but they also illustrate that more
sophisticated game scenarios are required to obtain syntactic properties more
akin to those observed in human language. We argue that UGI techniques should
be part of the standard toolkit for analysing emergent languages and release a
comprehensive library to facilitate such analysis for future researchers.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 15:06:27 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 17:52:45 GMT'}]",2020-10-12,"[['van der Wal', 'Oskar', ''], ['de Boer', 'Silvan', ''], ['Bruni', 'Elia', ''], ['Hupkes', 'Dieuwke', '']]"
1266897,2004.02127,Zuchao Li,"Zuchao Li, Hai Zhao, Rui Wang, Masao Utiyama, Eiichiro Sumita",Reference Language based Unsupervised Neural Machine Translation,"EMNLP 2020, ACL Findings",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Exploiting a common language as an auxiliary for better translation has a
long tradition in machine translation and lets supervised learning-based
machine translation enjoy the enhancement delivered by the well-used pivot
language in the absence of a source language to target language parallel
corpus. The rise of unsupervised neural machine translation (UNMT) almost
completely relieves the parallel corpus curse, though UNMT is still subject to
unsatisfactory performance due to the vagueness of the clues available for its
core back-translation training. Further enriching the idea of pivot translation
by extending the use of parallel corpora beyond the source-target paradigm, we
propose a new reference language-based framework for UNMT, RUNMT, in which the
reference language only shares a parallel corpus with the source, but this
corpus still indicates a signal clear enough to help the reconstruction
training of UNMT through a proposed reference agreement mechanism. Experimental
results show that our methods improve the quality of UNMT over that of a strong
baseline that uses only one auxiliary language, demonstrating the usefulness of
the proposed reference language-based UNMT and establishing a good start for
the community.
","[{'version': 'v1', 'created': 'Sun, 5 Apr 2020 08:28:08 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 15:48:59 GMT'}]",2020-10-12,"[['Li', 'Zuchao', ''], ['Zhao', 'Hai', ''], ['Wang', 'Rui', ''], ['Utiyama', 'Masao', ''], ['Sumita', 'Eiichiro', '']]"
1079048,1901.09501,Shuai Lin,"Shuai Lin, Wentao Wang, Zichao Yang, Xiaodan Liang, Frank F. Xu, Eric
  Xing and Zhiting Hu",Data-to-Text Generation with Style Imitation,"Accepted by EMNLP 2020 Findings. Significant updates over the
  previous version. Code & data are available at
  https://github.com/ha-lins/DTG-SI",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent neural approaches to data-to-text generation have mostly focused on
improving content fidelity while lacking explicit control over writing styles
(e.g., word choices, sentence structures). More traditional systems use
templates to determine the realization of text. Yet manual or automatic
construction of high-quality templates is difficult, and a template acting as
hard constraints could harm content fidelity when it does not match the record
perfectly. We study a new way of stylistic control by using existing sentences
as soft templates. That is, the model learns to imitate the writing style of
any given exemplar sentence, with automatic adaptions to faithfully describe
the content record. The problem is challenging due to the lack of parallel
data. We develop a neural approach that includes a hybrid attention-copy
mechanism, learns with weak supervisions, and is enhanced with a new content
coverage constraint. We conduct experiments in restaurants and sports domains.
Results show our approach achieves stronger performance than a range of
comparison methods. Our approach balances well between content fidelity and
style control given exemplars that match the records to varying degrees.
","[{'version': 'v1', 'created': 'Mon, 28 Jan 2019 03:38:08 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Feb 2019 17:08:56 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Oct 2020 08:09:14 GMT'}]",2020-10-12,"[['Lin', 'Shuai', ''], ['Wang', 'Wentao', ''], ['Yang', 'Zichao', ''], ['Liang', 'Xiaodan', ''], ['Xu', 'Frank F.', ''], ['Xing', 'Eric', ''], ['Hu', 'Zhiting', '']]"
1173797,1909.03759,Danish Contractor,"Nikhil Verma and Abhishek Sharma and Dhiraj Madan and Danish
  Contractor and Harshit Kumar and Sachindra Joshi",Neural Conversational QA: Learning to Reason v.s. Exploiting Patterns,"Accepted at EMNLP 2020. NOTE: An older version of this paper
  presented a model called 'UrcaNet'. Please view the v1 version of this paper
  on arxiv for details on that model. This version does not contain UrcaNet",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural Conversational QA tasks like ShARC require systems to answer questions
based on the contents of a given passage. On studying recent state-of-the-art
models on the ShARCQA task, we found indications that the models learn spurious
clues/patterns in the dataset. Furthermore, we show that a heuristic-based
program designed to exploit these patterns can have performance comparable to
that of the neural models. In this paper we share our findings about four types
of patterns found in the ShARC corpus and describe how neural models exploit
them. Motivated by the aforementioned findings, we create and share a modified
dataset that has fewer spurious patterns, consequently allowing models to learn
better.
","[{'version': 'v1', 'created': 'Mon, 9 Sep 2019 11:05:15 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 07:43:53 GMT'}]",2020-10-12,"[['Verma', 'Nikhil', ''], ['Sharma', 'Abhishek', ''], ['Madan', 'Dhiraj', ''], ['Contractor', 'Danish', ''], ['Kumar', 'Harshit', ''], ['Joshi', 'Sachindra', '']]"
1359824,2010.03494,Sebastian Goodman,"Sebastian Goodman, Nan Ding, Radu Soricut",TeaForN: Teacher-Forcing with N-grams,to be published in EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sequence generation models trained with teacher-forcing suffer from issues
related to exposure bias and lack of differentiability across timesteps. Our
proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these
problems directly, through the use of a stack of N decoders trained to decode
along a secondary time axis that allows model parameter updates based on N
prediction steps. TeaForN can be used with a wide class of decoder
architectures and requires minimal modifications from a standard
teacher-forcing setup. Empirically, we show that TeaForN boosts generation
quality on one Machine Translation benchmark, WMT 2014 English-French, and two
News Summarization benchmarks, CNN/Dailymail and Gigaword.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 15:58:25 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 16:45:20 GMT'}]",2020-10-12,"[['Goodman', 'Sebastian', ''], ['Ding', 'Nan', ''], ['Soricut', 'Radu', '']]"
1344438,2009.02935,Khiem Tran,"Khiem Vinh Tran, Hao Phu Phan, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen","UIT-HSE at WNUT-2020 Task 2: Exploiting CT-BERT for Identifying COVID-19
  Information on the Twitter Social Network","Accepted by 2020 The 6th Workshop on Noisy User-generated Text
  (W-NUT) - EMNLP 2020",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recently, COVID-19 has affected a variety of real-life aspects of the world
and led to dreadful consequences. More and more tweets about COVID-19 has been
shared publicly on Twitter. However, the plurality of those Tweets are
uninformative, which is challenging to build automatic systems to detect the
informative ones for useful AI applications. In this paper, we present our
results at the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19
English Tweets. In particular, we propose our simple but effective approach
using the transformer-based models based on COVID-Twitter-BERT (CT-BERT) with
different fine-tuning techniques. As a result, we achieve the F1-Score of
90.94\% with the third place on the leaderboard of this task which attracted 56
submitted teams in total.
","[{'version': 'v1', 'created': 'Mon, 7 Sep 2020 08:20:31 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 10:23:58 GMT'}]",2020-10-12,"[['Tran', 'Khiem Vinh', ''], ['Phan', 'Hao Phu', ''], ['Van Nguyen', 'Kiet', ''], ['Nguyen', 'Ngan Luu-Thuy', '']]"
1359878,2010.03548,Rajarshi Das,"Rajarshi Das, Ameya Godbole, Nicholas Monath, Manzil Zaheer, Andrew
  McCallum","Probabilistic Case-based Reasoning for Open-World Knowledge Graph
  Completion",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A case-based reasoning (CBR) system solves a new problem by retrieving
`cases' that are similar to the given problem. If such a system can achieve
high accuracy, it is appealing owing to its simplicity, interpretability, and
scalability. In this paper, we demonstrate that such a system is achievable for
reasoning in knowledge-bases (KBs). Our approach predicts attributes for an
entity by gathering reasoning paths from similar entities in the KB. Our
probabilistic model estimates the likelihood that a path is effective at
answering a query about the given entity. The parameters of our model can be
efficiently computed using simple path statistics and require no iterative
optimization. Our model is non-parametric, growing dynamically as new entities
and relations are added to the KB. On several benchmark datasets our approach
significantly outperforms other rule learning approaches and performs
comparably to state-of-the-art embedding-based approaches. Furthermore, we
demonstrate the effectiveness of our model in an ""open-world"" setting where new
entities arrive in an online fashion, significantly outperforming
state-of-the-art approaches and nearly matching the best offline method. Code
available at https://github.com/ameyagodbole/Prob-CBR
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 17:48:12 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 14:44:18 GMT'}]",2020-10-12,"[['Das', 'Rajarshi', ''], ['Godbole', 'Ameya', ''], ['Monath', 'Nicholas', ''], ['Zaheer', 'Manzil', ''], ['McCallum', 'Andrew', '']]"
1360143,2010.03813,Vincent Micheli,"Vincent Micheli, Martin d'Hoffschmidt, Fran\c{c}ois Fleuret","On the importance of pre-training data volume for compact language
  models",EMNLP 2020; typo corrected,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in language modeling have led to computationally intensive
and resource-demanding state-of-the-art models. In an effort towards
sustainable practices, we study the impact of pre-training data volume on
compact language models. Multiple BERT-based models are trained on gradually
increasing amounts of French text. Through fine-tuning on the French Question
Answering Dataset (FQuAD), we observe that well-performing models are obtained
with as little as 100 MB of text. In addition, we show that past critically low
amounts of pre-training data, an intermediate pre-training step on the
task-specific corpus does not yield substantial improvements.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 07:40:21 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 14:36:43 GMT'}]",2020-10-12,"[['Micheli', 'Vincent', ''], [""d'Hoffschmidt"", 'Martin', ''], ['Fleuret', 'François', '']]"
1279283,2004.14513,Julian Michael,"Julian Michael, Jan A. Botha, Ian Tenney","Asking without Telling: Exploring Latent Ontologies in Contextual
  Representations","21 pages, 8 figures, 11 tables. Published in EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The success of pretrained contextual encoders, such as ELMo and BERT, has
brought a great deal of interest in what these models learn: do they, without
explicit supervision, learn to encode meaningful notions of linguistic
structure? If so, how is this structure encoded? To investigate this, we
introduce latent subclass learning (LSL): a modification to existing
classifier-based probing methods that induces a latent categorization (or
ontology) of the probe's inputs. Without access to fine-grained gold labels,
LSL extracts emergent structure from input representations in an interpretable
and quantifiable form. In experiments, we find strong evidence of familiar
categories, such as a notion of personhood in ELMo, as well as novel
ontological distinctions, such as a preference for fine-grained semantic roles
on core arguments. Our results provide unique new evidence of emergent
structure in pretrained encoders, including departures from existing
annotations which are inaccessible to earlier methods.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 23:20:40 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 00:28:21 GMT'}]",2020-10-12,"[['Michael', 'Julian', ''], ['Botha', 'Jan A.', ''], ['Tenney', 'Ian', '']]"
1154435,1907.09899,Ziyun Wang,"Ziyun Wang, Brenden M. Lake",Modeling question asking using neural program generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  People ask questions that are far richer, more informative, and more creative
than current AI systems. We propose a neuro-symbolic framework for modeling
human question asking, which represents questions as formal programs and
generates programs with an encoder-decoder based deep neural network. From
extensive experiments using an information-search game, we show that our method
can ask optimal questions in synthetic settings, and predict which questions
humans are likely to ask in unconstrained settings. We also propose a novel
grammar-based question generation framework trained with reinforcement
learning, which is able to generate creative questions without supervised human
data.
","[{'version': 'v1', 'created': 'Tue, 23 Jul 2019 14:20:21 GMT'}, {'version': 'v2', 'created': 'Thu, 26 Sep 2019 16:18:55 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Oct 2020 13:27:11 GMT'}]",2020-10-12,"[['Wang', 'Ziyun', ''], ['Lake', 'Brenden M.', '']]"
1167253,1908.09083,Sudipta Kar,Sudipta Kar and Gustavo Aguilar and Mirella Lapata and Thamar Solorio,Multi-view Story Characterization from Movie Plot Synopses and Reviews,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper considers the problem of characterizing stories by inferring
properties such as theme and style using written synopses and reviews of
movies. We experiment with a multi-label dataset of movie synopses and a tagset
representing various attributes of stories (e.g., genre, type of events). Our
proposed multi-view model encodes the synopses and reviews using hierarchical
attention and shows improvement over methods that only use synopses. Finally,
we demonstrate how can we take advantage of such a model to extract a
complementary set of story-attributes from reviews without direct supervision.
We have made our dataset and source code publicly available at
https://ritual.uh.edu/ multiview-tag-2020.
","[{'version': 'v1', 'created': 'Sat, 24 Aug 2019 03:27:43 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 22:16:46 GMT'}]",2020-10-12,"[['Kar', 'Sudipta', ''], ['Aguilar', 'Gustavo', ''], ['Lapata', 'Mirella', ''], ['Solorio', 'Thamar', '']]"
1360575,2010.04245,Prudhvi Raj Dachapally,"Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, Yuxuan Chen",Query-Key Normalization for Transformers,"8 pages, 2 figures, accepted at Findings of EMNLP 2020",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Low-resource language translation is a challenging but socially valuable NLP
task. Building on recent work adapting the Transformer's normalization to this
setting, we propose QKNorm, a normalization technique that modifies the
attention mechanism to make the softmax function less prone to arbitrary
saturation without sacrificing expressivity. Specifically, we apply $\ell_2$
normalization along the head dimension of each query and key matrix prior to
multiplying them and then scale up by a learnable parameter instead of dividing
by the square root of the embedding dimension. We show improvements averaging
0.928 BLEU over state-of-the-art bilingual benchmarks for 5 low-resource
translation pairs from the TED Talks corpus and IWSLT'15.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 20:12:35 GMT'}]",2020-10-12,"[['Henry', 'Alex', ''], ['Dachapally', 'Prudhvi Raj', ''], ['Pawar', 'Shubham', ''], ['Chen', 'Yuxuan', '']]"
1360995,2010.04665,Seraphina Goldfarb-Tarrant,"Seraphina Goldfarb-Tarrant, Alexander Robertson, Jasmina Lazic,
  Theodora Tsouloufi, Louise Donnison, Karen Smyth",Scaling Systematic Literature Reviews with Machine Learning Pipelines,In EMNLP 2020 Scholarly Document Processing Workshop,,,,cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Systematic reviews, which entail the extraction of data from large numbers of
scientific documents, are an ideal avenue for the application of machine
learning. They are vital to many fields of science and philanthropy, but are
very time-consuming and require experts. Yet the three main stages of a
systematic review are easily done automatically: searching for documents can be
done via APIs and scrapers, selection of relevant documents can be done via
binary classification, and extraction of data can be done via
sequence-labelling classification. Despite the promise of automation for this
field, little research exists that examines the various ways to automate each
of these tasks. We construct a pipeline that automates each of these aspects,
and experiment with many human-time vs. system quality trade-offs. We test the
ability of classifiers to work well on small amounts of data and to generalise
to data from countries not represented in the training data. We test different
types of data extraction with varying difficulty in annotation, and five
different neural architectures to do the extraction. We find that we can get
surprising accuracy and generalisability of the whole pipeline system with only
2 weeks of human-expert annotation, which is only 15% of the time it takes to
do the whole review manually and can be repeated and extended to new data with
no additional effort.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 16:19:42 GMT'}]",2020-10-12,"[['Goldfarb-Tarrant', 'Seraphina', ''], ['Robertson', 'Alexander', ''], ['Lazic', 'Jasmina', ''], ['Tsouloufi', 'Theodora', ''], ['Donnison', 'Louise', ''], ['Smyth', 'Karen', '']]"
1360988,2010.04658,Shrimai Prabhumoye,"Shrimai Prabhumoye, Brendon Boldt, Ruslan Salakhutdinov, Alan W Black",Case Study: Deontological Ethics in NLP,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work in natural language processing (NLP) has focused on ethical
challenges such as understanding and mitigating bias in data and algorithms;
identifying objectionable content like hate speech, stereotypes and offensive
language; and building frameworks for better system design and data handling
practices. However, there has been little discussion about the ethical
foundations that underlie these efforts. In this work, we study one ethical
theory, namely deontological ethics, from the perspective of NLP. In
particular, we focus on the generalization principle and the respect for
autonomy through informed consent. We provide four case studies to demonstrate
how these principles can be used with NLP systems. We also recommend directions
to avoid the ethical issues in these systems.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 16:04:51 GMT'}]",2020-10-12,"[['Prabhumoye', 'Shrimai', ''], ['Boldt', 'Brendon', ''], ['Salakhutdinov', 'Ruslan', ''], ['Black', 'Alan W', '']]"
1360980,2010.04650,Naomi Saphra,Naomi Saphra and Adam Lopez,LSTMs Compose (and Learn) Bottom-Up,"Published in EMNLP Findings 2020. arXiv admin note: substantial text
  overlap with arXiv:2004.13195",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Recent work in NLP shows that LSTM language models capture hierarchical
structure in language data. In contrast to existing work, we consider the
\textit{learning} process that leads to their compositional behavior. For a
closer look at how an LSTM's sequential representations are composed
hierarchically, we present a related measure of Decompositional Interdependence
(DI) between word meanings in an LSTM, based on their gate interactions. We
connect this measure to syntax with experiments on English language data, where
DI is higher on pairs of words with lower syntactic distance. To explore the
inductive biases that cause these compositional representations to arise during
training, we conduct simple experiments on synthetic data. These synthetic
experiments support a specific hypothesis about how hierarchical structures are
discovered over the course of training: that LSTM constituent representations
are learned bottom-up, relying on effective representations of their shorter
children, rather than learning the longer-range relations independently from
children.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 13:00:32 GMT'}]",2020-10-12,"[['Saphra', 'Naomi', ''], ['Lopez', 'Adam', '']]"
1279749,2004.14979,Vered Shwartz,"Yehudit Meged, Avi Caciularu, Vered Shwartz, Ido Dagan",Paraphrasing vs Coreferring: Two Sides of the Same Coin,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the potential synergy between two different NLP tasks, both
confronting predicate lexical variability: identifying predicate paraphrases,
and event coreference resolution. First, we used annotations from an event
coreference dataset as distant supervision to re-score heuristically-extracted
predicate paraphrases. The new scoring gained more than 18 points in average
precision upon their ranking by the original scoring method. Then, we used the
same re-ranking features as additional inputs to a state-of-the-art event
coreference resolution model, which yielded modest but consistent improvements
to the model's performance. The results suggest a promising direction to
leverage data and models for each of the tasks to the benefit of the other.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:29:17 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 16:48:36 GMT'}]",2020-10-12,"[['Meged', 'Yehudit', ''], ['Caciularu', 'Avi', ''], ['Shwartz', 'Vered', ''], ['Dagan', 'Ido', '']]"
1360971,2010.04641,Zuchao Li,"Zuchao Li, Hai Zhao, Rui Wang, Kevin Parnow",High-order Semantic Role Labeling,"EMNLP 2020, ACL Findings",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semantic role labeling is primarily used to identify predicates, arguments,
and their semantic relationships. Due to the limitations of modeling methods
and the conditions of pre-identified predicates, previous work has focused on
the relationships between predicates and arguments and the correlations between
arguments at most, while the correlations between predicates have been
neglected for a long time. High-order features and structure learning were very
common in modeling such correlations before the neural network era. In this
paper, we introduce a high-order graph structure for the neural semantic role
labeling model, which enables the model to explicitly consider not only the
isolated predicate-argument pairs but also the interaction between the
predicate-argument pairs. Experimental results on 7 languages of the CoNLL-2009
benchmark show that the high-order structural learning techniques are
beneficial to the strong performing SRL models and further boost our baseline
to achieve new state-of-the-art results.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 15:33:54 GMT'}]",2020-10-12,"[['Li', 'Zuchao', ''], ['Zhao', 'Hai', ''], ['Wang', 'Rui', ''], ['Parnow', 'Kevin', '']]"
1360970,2010.04640,Zhen Wu,"Zhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, Rui Xia",Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction,Accepted by Findings of EMNLP,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect-oriented Fine-grained Opinion Extraction (AFOE) aims at extracting
aspect terms and opinion terms from review in the form of opinion pairs or
additionally extracting sentiment polarity of aspect term to form opinion
triplet. Because of containing several opinion factors, the complete AFOE task
is usually divided into multiple subtasks and achieved in the pipeline.
However, pipeline approaches easily suffer from error propagation and
inconvenience in real-world scenarios. To this end, we propose a novel tagging
scheme, Grid Tagging Scheme (GTS), to address the AFOE task in an end-to-end
fashion only with one unified grid tagging task. Additionally, we design an
effective inference strategy on GTS to exploit mutual indication between
different opinion factors for more accurate extractions. To validate the
feasibility and compatibility of GTS, we implement three different GTS models
respectively based on CNN, BiLSTM, and BERT, and conduct experiments on the
aspect-oriented opinion pair extraction and opinion triplet extraction
datasets. Extensive experimental results indicate that GTS models outperform
strong baselines significantly and achieve state-of-the-art performance.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 15:33:50 GMT'}]",2020-10-12,"[['Wu', 'Zhen', ''], ['Ying', 'Chengcan', ''], ['Zhao', 'Fei', ''], ['Fan', 'Zhifang', ''], ['Dai', 'Xinyu', ''], ['Xia', 'Rui', '']]"
1361004,2010.04674,Brian DuSell,Brian DuSell and David Chiang,Learning Context-Free Languages with Nondeterministic Stack RNNs,"13 pages, 5 figures, accepted for publication at CoNLL 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a differentiable stack data structure that simultaneously and
tractably encodes an exponential number of stack configurations, based on
Lang's algorithm for simulating nondeterministic pushdown automata. We call the
combination of this data structure with a recurrent neural network (RNN)
controller a Nondeterministic Stack RNN. We compare our model against existing
stack RNNs on various formal languages, demonstrating that our model converges
more reliably to algorithmic behavior on deterministic tasks, and achieves
lower cross-entropy on inherently nondeterministic tasks.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 16:48:41 GMT'}]",2020-10-12,"[['DuSell', 'Brian', ''], ['Chiang', 'David', '']]"
1360967,2010.04637,Ludovica Pannitto,Ludovica Pannitto and Aur\'elie Herbelot,"Recurrent babbling: evaluating the acquisition of grammar from limited
  input data",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recurrent Neural Networks (RNNs) have been shown to capture various aspects
of syntax from raw linguistic input. In most previous experiments, however,
learning happens over unrealistic corpora, which do not reflect the type and
amount of data a child would be exposed to. This paper remedies this state of
affairs by training a Long Short-Term Memory network (LSTM) over a
realistically sized subset of child-directed input. The behaviour of the
network is analysed over time using a novel methodology which consists in
quantifying the level of grammatical abstraction in the model's generated
output (its ""babbling""), compared to the language it has been exposed to. We
show that the LSTM indeed abstracts new structuresas learning proceeds.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 15:30:05 GMT'}]",2020-10-12,"[['Pannitto', 'Ludovica', ''], ['Herbelot', 'Aurélie', '']]"
1360939,2010.04609,Guohou Shan,"Guohou Shan, James Foulds, Shimei Pan","Causal Feature Selection with Dimension Reduction for Interpretable Text
  Classification","11 pages, 3 pages",,,,cs.LG cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text features that are correlated with class labels, but do not directly
cause them, are sometimesuseful for prediction, but they may not be insightful.
As an alternative to traditional correlation-basedfeature selection, causal
inference could reveal more principled, meaningful relationships betweentext
features and labels. To help researchers gain insight into text data, e.g. for
social scienceapplications, in this paper we investigate a class of
matching-based causal inference methods fortext feature selection. Features
used in document classification are often high dimensional, howeverexisting
causal feature selection methods use Propensity Score Matching (PSM) which is
known to beless effective in high-dimensional spaces. We propose a new causal
feature selection framework thatcombines dimension reduction with causal
inference to improve text feature selection. Experiments onboth synthetic and
real-world data demonstrate the promise of our methods in improving
classificationand enhancing interpretability.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 14:36:49 GMT'}]",2020-10-12,"[['Shan', 'Guohou', ''], ['Foulds', 'James', ''], ['Pan', 'Shimei', '']]"
1360936,2010.04606,Gon\c{c}alo Mordido,Gon\c{c}alo Mordido and Christoph Meinel,"Mark-Evaluate: Assessing Language Generation using Population Estimation
  Methods",Accepted at COLING 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a family of metrics to assess language generation derived from
population estimation methods widely used in ecology. More specifically, we use
mark-recapture and maximum-likelihood methods that have been applied over the
past several decades to estimate the size of closed populations in the wild. We
propose three novel metrics: ME$_\text{Petersen}$ and ME$_\text{CAPTURE}$,
which retrieve a single-valued assessment, and ME$_\text{Schnabel}$ which
returns a double-valued metric to assess the evaluation set in terms of quality
and diversity, separately. In synthetic experiments, our family of methods is
sensitive to drops in quality and diversity. Moreover, our methods show a
higher correlation to human evaluation than existing metrics on several
challenging tasks, namely unconditional language generation, machine
translation, and text summarization.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 14:31:53 GMT'}]",2020-10-12,"[['Mordido', 'Gonçalo', ''], ['Meinel', 'Christoph', '']]"
1360912,2010.04582,Wendi Ren,"Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell,
  Chao Zhang",Denoising Multi-Source Weak Supervision for Neural Text Classification,"16 pages, 7 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the problem of learning neural text classifiers without using any
labeled data, but only easy-to-provide rules as multiple weak supervision
sources. This problem is challenging because rule-induced weak labels are often
noisy and incomplete. To address these two challenges, we design a label
denoiser, which estimates the source reliability using a conditional soft
attention mechanism and then reduces label noise by aggregating rule-annotated
weak labels. The denoised pseudo labels then supervise a neural classifier to
predicts soft labels for unmatched samples, which address the rule coverage
issue. We evaluate our model on five benchmarks for sentiment, topic, and
relation classifications. The results show that our model outperforms
state-of-the-art weakly-supervised and semi-supervised methods consistently,
and achieves comparable performance with fully-supervised methods even without
any labeled data. Our code can be found at
https://github.com/weakrules/Denoise-multi-weak-sources.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 13:57:52 GMT'}]",2020-10-12,"[['Ren', 'Wendi', ''], ['Li', 'Yinghao', ''], ['Su', 'Hanting', ''], ['Kartchner', 'David', ''], ['Mitchell', 'Cassie', ''], ['Zhang', 'Chao', '']]"
1360906,2010.04576,Cheng-Te Li,"Hsin-Yu Chen, Cheng-Te Li","HENIN: Learning Heterogeneous Neural Interaction Networks for
  Explainable Cyberbullying Detection on Social Media","EMNLP 2020 long paper. Code is available at
  https://github.com/HsinYu7330/HENIN",,,,cs.CL cs.AI cs.SI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In the computational detection of cyberbullying, existing work largely
focused on building generic classifiers that rely exclusively on text analysis
of social media sessions. Despite their empirical success, we argue that a
critical missing piece is the model explainability, i.e., why a particular
piece of media session is detected as cyberbullying. In this paper, therefore,
we propose a novel deep model, HEterogeneous Neural Interaction Networks
(HENIN), for explainable cyberbullying detection. HENIN contains the following
components: a comment encoder, a post-comment co-attention sub-network, and
session-session and post-post interaction extractors. Extensive experiments
conducted on real datasets exhibit not only the promising performance of HENIN,
but also highlight evidential comments so that one can understand why a media
session is identified as cyberbullying.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 13:44:34 GMT'}]",2020-10-12,"[['Chen', 'Hsin-Yu', ''], ['Li', 'Cheng-Te', '']]"
1279782,2004.15012,Charles Lovering J,"Rohan Jha, Charles Lovering, Ellie Pavlick",Does Data Augmentation Improve Generalization in NLP?,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural models often exploit superficial features to achieve good performance,
rather than deriving more general features. Overcoming this tendency is a
central challenge in areas such as representation learning and ML fairness.
Recent work has proposed using data augmentation, i.e., generating training
examples where the superficial features fail, as a means of encouraging models
to prefer the stronger features. We design a series of toy learning problems to
test the hypothesis that data augmentation leads models to unlearn weaker
heuristics, but not to learn stronger features in their place. We find partial
support for this hypothesis: Data augmentation often hurts before it helps, and
it is less effective when the preferred strong feature is much more difficult
to extract than the competing weak feature.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:56:30 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 14:33:27 GMT'}]",2020-10-12,"[['Jha', 'Rohan', ''], ['Lovering', 'Charles', ''], ['Pavlick', 'Ellie', '']]"
1360862,2010.04532,Carolina Scarton,Carolina Scarton and Diego F. Silva and Kalina Bontcheva,Measuring What Counts: The case of Rumour Stance Classification,Accepted to AACL-IJCNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Stance classification can be a powerful tool for understanding whether and
which users believe in online rumours. The task aims to automatically predict
the stance of replies towards a given rumour, namely support, deny, question,
or comment. Numerous methods have been proposed and their performance compared
in the RumourEval shared tasks in 2017 and 2019. Results demonstrated that this
is a challenging problem since naturally occurring rumour stance data is highly
imbalanced. This paper specifically questions the evaluation metrics used in
these shared tasks. We re-evaluate the systems submitted to the two RumourEval
tasks and show that the two widely adopted metrics -- accuracy and macro-F1 --
are not robust for the four-class imbalanced task of rumour stance
classification, as they wrongly favour systems with highly skewed accuracy
towards the majority class. To overcome this problem, we propose new evaluation
metrics for rumour stance detection. These are not only robust to imbalanced
data but also score higher systems that are capable of recognising the two most
informative minority classes (support and deny).
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 12:45:27 GMT'}]",2020-10-12,"[['Scarton', 'Carolina', ''], ['Silva', 'Diego F.', ''], ['Bontcheva', 'Kalina', '']]"
1279759,2004.14989,Rachel Bawden,"Rachel Bawden and Biao Zhang and Lisa Yankovskaya and Andre T\""attar
  and Matt Post","A Study in Improving BLEU Reference Coverage with Diverse Automatic
  Paraphrasing",Accepted in the Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate a long-perceived shortcoming in the typical use of BLEU: its
reliance on a single reference. Using modern neural paraphrasing techniques, we
study whether automatically generating additional diverse references can
provide better coverage of the space of valid translations and thereby improve
its correlation with human judgments. Our experiments on the into-English
language directions of the WMT19 metrics task (at both the system and sentence
level) show that using paraphrased references does generally improve BLEU, and
when it does, the more diverse the better. However, we also show that better
results could be achieved if those paraphrases were to specifically target the
parts of the space most relevant to the MT outputs being evaluated. Moreover,
the gains remain slight even when human paraphrases are used, suggesting
inherent limitations to BLEU's capacity to correctly exploit multiple
references. Surprisingly, we also find that adequacy appears to be less
important, as shown by the high results of a strong sampling approach, which
even beats human paraphrases when used with sentence-level BLEU.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:34:52 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Sep 2020 16:17:54 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Oct 2020 21:43:56 GMT'}]",2020-10-12,"[['Bawden', 'Rachel', ''], ['Zhang', 'Biao', ''], ['Yankovskaya', 'Lisa', ''], ['Tättar', 'Andre', ''], ['Post', 'Matt', '']]"
1360859,2010.04529,Leyang Cui,"Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie,
  Yue Zhang",What Have We Achieved on Text Summarization?,Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning has led to significant improvement in text summarization with
various methods investigated and improved ROUGE scores reported over the years.
However, gaps still exist between summaries produced by automatic summarizers
and human professionals. Aiming to gain more understanding of summarization
systems with respect to their strengths and limits on a fine-grained syntactic
and semantic level, we consult the Multidimensional Quality Metric(MQM) and
quantify 8 major sources of errors on 10 representative summarization models
manually. Primarily, we find that 1) under similar settings, extractive
summarizers are in general better than their abstractive counterparts thanks to
strength in faithfulness and factual-consistency; 2) milestone techniques such
as copy, coverage and hybrid extractive/abstractive methods do bring specific
improvements but also demonstrate limitations; 3) pre-training techniques, and
in particular sequence-to-sequence pre-training, are highly effective for
improving text summarization, with BART giving the best results.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 12:39:33 GMT'}]",2020-10-12,"[['Huang', 'Dandan', ''], ['Cui', 'Leyang', ''], ['Yang', 'Sen', ''], ['Bao', 'Guangsheng', ''], ['Wang', 'Kun', ''], ['Xie', 'Jun', ''], ['Zhang', 'Yue', '']]"
1361034,2010.04704,Shawn Tan,"Shawn Tan and Yikang Shen and Timothy J. O'Donnell and Alessandro
  Sordoni and Aaron Courville",Recursive Top-Down Production for Sentence Generation with Latent Trees,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We model the recursive production property of context-free grammars for
natural and synthetic languages. To this end, we present a dynamic programming
algorithm that marginalises over latent binary tree structures with $N$ leaves,
allowing us to compute the likelihood of a sequence of $N$ tokens under a
latent tree model, which we maximise to train a recursive neural function. We
demonstrate performance on two synthetic tasks: SCAN (Lake and Baroni, 2017),
where it outperforms previous models on the LENGTH split, and English question
formation (McCoy et al., 2020), where it performs comparably to decoders with
the ground-truth tree structure. We also present experimental results on
German-English translation on the Multi30k dataset (Elliott et al., 2016), and
qualitatively analyse the induced tree structures our model learns for the SCAN
tasks and the German-English translation task.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 17:47:16 GMT'}]",2020-10-12,"[['Tan', 'Shawn', ''], ['Shen', 'Yikang', ''], [""O'Donnell"", 'Timothy J.', ''], ['Sordoni', 'Alessandro', ''], ['Courville', 'Aaron', '']]"
1279277,2004.14507,Qingfu Zhu,"Qingfu Zhu, Weinan Zhang, Ting Liu, William Yang Wang",Counterfactual Off-Policy Training for Neural Response Generation,,,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open-domain dialogue generation suffers from the data insufficiency problem
due to the vast size of potential responses. In this paper, we propose to
explore potential responses by counterfactual reasoning. Given an observed
response, the counterfactual reasoning model automatically infers the outcome
of an alternative policy that could have been taken. The resulting
counterfactual response synthesized in hindsight is of higher quality than the
response synthesized from scratch. Training on the counterfactual responses
under the adversarial learning framework helps to explore the high-reward area
of the potential response space. An empirical study on the DailyDialog dataset
shows that our approach significantly outperforms the HRED model as well as the
conventional adversarial learning approaches.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 22:46:28 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 07:47:45 GMT'}]",2020-10-12,"[['Zhu', 'Qingfu', ''], ['Zhang', 'Weinan', ''], ['Liu', 'Ting', ''], ['Wang', 'William Yang', '']]"
1358930,2010.02600,Isabelle G. Lee,"Isabelle G. Lee, Vera Zu, Sai Srujana Buddi, Dennis Liang, Purva
  Kulkarni, Jack G.M. Fitzgerald",Converting the Point of View of Messages Spoken to Virtual Assistants,"10 pages, 11 figures, Findings of EMNLP 2020",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Virtual Assistants can be quite literal at times. If the user says ""tell Bob
I love him,"" most virtual assistants will extract the message ""I love him"" and
send it to the user's contact named Bob, rather than properly converting the
message to ""I love you."" We designed a system to allow virtual assistants to
take a voice message from one user, convert the point of view of the message,
and then deliver the result to its target user. We developed a rule-based
model, which integrates a linear text classification model, part-of-speech
tagging, and constituency parsing with rule-based transformation methods. We
also investigated Neural Machine Translation (NMT) approaches, including LSTMs,
CopyNet, and T5. We explored 5 metrics to gauge both naturalness and
faithfulness automatically, and we chose to use BLEU plus METEOR for
faithfulness and relative perplexity using a separately trained language model
(GPT) for naturalness. Transformer-Copynet and T5 performed similarly on
faithfulness metrics, with T5 achieving slight edge, a BLEU score of 63.8 and a
METEOR score of 83.0. CopyNet was the most natural, with a relative perplexity
of 1.59. CopyNet also has 37 times fewer parameters than T5. We have publicly
released our dataset, which is composed of 46,565 crowd-sourced samples.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 10:19:39 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 01:31:27 GMT'}]",2020-10-12,"[['Lee', 'Isabelle G.', ''], ['Zu', 'Vera', ''], ['Buddi', 'Sai Srujana', ''], ['Liang', 'Dennis', ''], ['Kulkarni', 'Purva', ''], ['Fitzgerald', 'Jack G. M.', '']]"
1349568,2009.08065,Zhenglun Kong,"Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang
  Liu, Caiwen Ding","Efficient Transformer-based Large Scale Language Representations using
  Hardware-friendly Block Structured Pruning",Accepted to Findings of EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained large-scale language models have increasingly demonstrated high
accuracy on many natural language processing (NLP) tasks. However, the limited
weight storage and computational speed on hardware platforms have impeded the
popularity of pre-trained models, especially in the era of edge computing. In
this work, we propose an efficient transformer-based large-scale language
representation using hardware-friendly block structure pruning. We incorporate
the reweighted group Lasso into block-structured pruning for optimization.
Besides the significantly reduced weight storage and computation, the proposed
approach achieves high compression rates. Experimental results on different
models (BERT, RoBERTa, and DistilBERT) on the General Language Understanding
Evaluation (GLUE) benchmark tasks show that we achieve up to 5.0x with zero or
minor accuracy degradation on certain task(s). Our proposed method is also
orthogonal to existing compact pre-trained language models such as DistilBERT
using knowledge distillation, since a further 1.79x average compression rate
can be achieved on top of DistilBERT with zero or minor accuracy degradation.
It is suitable to deploy the final compressed model on resource-constrained
edge devices.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 04:45:47 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Sep 2020 20:09:04 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Oct 2020 19:14:32 GMT'}]",2020-10-12,"[['Li', 'Bingbing', ''], ['Kong', 'Zhenglun', ''], ['Zhang', 'Tianyun', ''], ['Li', 'Ji', ''], ['Li', 'Zhengang', ''], ['Liu', 'Hang', ''], ['Ding', 'Caiwen', '']]"
1358825,2010.02495,Praveen Kumar Bodigutla,"Praveen Kumar Bodigutla, Aditya Tiwari, Josep Valls Vargas, Lazaros
  Polymenakos, Spyros Matsoukas","Joint Turn and Dialogue level User Satisfaction Estimation on
  Multi-Domain Conversations","Findings of EMNLP, 2020",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialogue level quality estimation is vital for optimizing data driven
dialogue management. Current automated methods to estimate turn and dialogue
level user satisfaction employ hand-crafted features and rely on complex
annotation schemes, which reduce the generalizability of the trained models. We
propose a novel user satisfaction estimation approach which minimizes an
adaptive multi-task loss function in order to jointly predict turn-level
Response Quality labels provided by experts and explicit dialogue-level ratings
provided by end users. The proposed BiLSTM based deep neural net model
automatically weighs each turn's contribution towards the estimated
dialogue-level rating, implicitly encodes temporal dependencies, and removes
the need to hand-craft features.
  On dialogues sampled from 28 Alexa domains, two dialogue systems and three
user groups, the joint dialogue-level satisfaction estimation model achieved up
to an absolute 27% (0.43->0.70) and 7% (0.63->0.70) improvement in linear
correlation performance over baseline deep neural net and benchmark Gradient
boosting regression models, respectively.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 05:53:13 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 21:10:47 GMT'}]",2020-10-12,"[['Bodigutla', 'Praveen Kumar', ''], ['Tiwari', 'Aditya', ''], ['Vargas', 'Josep Valls', ''], ['Polymenakos', 'Lazaros', ''], ['Matsoukas', 'Spyros', '']]"
1336866,2008.09144,Diedre Carmo,"Diedre Carmo, Marcos Piau, Israel Campiotti, Rodrigo Nogueira, Roberto
  Lotufo","PTT5: Pretraining and validating the T5 model on Brazilian Portuguese
  data",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In natural language processing (NLP), there is a need for more resources in
Portuguese, since much of the data used in the state-of-the-art research is in
other languages. In this paper, we pretrain a T5 model on the BrWac corpus, an
extensive collection of web pages in Portuguese, and evaluate its performance
against other Portuguese pretrained models and multilingual models on three
different tasks. We show that our Portuguese pretrained models have
significantly better performance over the original T5 models. Moreover, we
demonstrate the positive impact of using a Portuguese vocabulary. Our code and
models are available at https://github.com/unicamp-dl/PTT5.
","[{'version': 'v1', 'created': 'Thu, 20 Aug 2020 18:10:13 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 18:37:54 GMT'}]",2020-10-12,"[['Carmo', 'Diedre', ''], ['Piau', 'Marcos', ''], ['Campiotti', 'Israel', ''], ['Nogueira', 'Rodrigo', ''], ['Lotufo', 'Roberto', '']]"
1299988,2006.05474,Changhan Wang,"Changhan Wang, Juan Pino, Jiatao Gu","Improving Cross-Lingual Transfer Learning for End-to-End Speech
  Recognition with Speech Translation",Accepted to INTERSPEECH 2020,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transfer learning from high-resource languages is known to be an efficient
way to improve end-to-end automatic speech recognition (ASR) for low-resource
languages. Pre-trained or jointly trained encoder-decoder models, however, do
not share the language modeling (decoder) for the same language, which is
likely to be inefficient for distant target languages. We introduce
speech-to-text translation (ST) as an auxiliary task to incorporate additional
knowledge of the target language and enable transferring from that target
language. Specifically, we first translate high-resource ASR transcripts into a
target low-resource language, with which a ST model is trained. Both ST and
target ASR share the same attention-based encoder-decoder architecture and
vocabulary. The former task then provides a fully pre-trained model for the
latter, bringing up to 24.6% word error rate (WER) reduction to the baseline
(direct transfer from high-resource ASR). We show that training ST with human
translations is not necessary. ST trained with machine translation (MT)
pseudo-labels brings consistent gains. It can even outperform those using human
labels when transferred to target ASR by leveraging only 500K MT examples. Even
with pseudo-labels from low-resource MT (200K examples), ST-enhanced transfer
brings up to 8.9% WER reduction to direct transfer.
","[{'version': 'v1', 'created': 'Tue, 9 Jun 2020 19:34:11 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 04:07:38 GMT'}]",2020-10-12,"[['Wang', 'Changhan', ''], ['Pino', 'Juan', ''], ['Gu', 'Jiatao', '']]"
1357955,2010.01625,Sheena Panthaplackel,"Sheena Panthaplackel, Junyi Jessy Li, Milos Gligoric, Raymond J.
  Mooney","Deep Just-In-Time Inconsistency Detection Between Comments and Source
  Code",,,,,cs.SE cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language comments convey key aspects of source code such as
implementation, usage, and pre- and post-conditions. Failure to update comments
accordingly when the corresponding code is modified introduces inconsistencies,
which is known to lead to confusion and software bugs. In this paper, we aim to
detect whether a comment becomes inconsistent as a result of changes to the
corresponding body of code, in order to catch potential inconsistencies
just-in-time, i.e., before they are committed to a version control system. To
achieve this, we develop a deep-learning approach that learns to correlate a
comment with code changes. By evaluating on a large corpus of comment/code
pairs spanning various comment types, we show that our model outperforms
multiple baselines by significant margins. For extrinsic evaluation, we show
the usefulness of our approach by combining it with a comment update model to
build a more comprehensive automatic comment maintenance system which can both
detect and resolve inconsistent comments based on code changes.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 16:49:28 GMT'}]",2020-10-12,"[['Panthaplackel', 'Sheena', ''], ['Li', 'Junyi Jessy', ''], ['Gligoric', 'Milos', ''], ['Mooney', 'Raymond J.', '']]"
1361036,2010.04706,Katherine Keith,"Katherine A. Keith, Christoph Teichmann, Brendan O'Connor, Edgar Meij","Uncertainty over Uncertainty: Investigating the Assumptions,
  Annotations, and Text Measurements of Economic Policy Uncertainty","Accepted to the 2020 Natural Language Processing + Computational
  Social Science Workshop (NLP+CSS) at EMNLP","2020 Natural Language Processing + Computational Social Science
  Workshop (NLP+CSS) at EMNLP",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Methods and applications are inextricably linked in science, and in
particular in the domain of text-as-data. In this paper, we examine one such
text-as-data application, an established economic index that measures economic
policy uncertainty from keyword occurrences in news. This index, which is shown
to correlate with firm investment, employment, and excess market returns, has
had substantive impact in both the private sector and academia. Yet, as we
revisit and extend the original authors' annotations and text measurements we
find interesting text-as-data methodological research questions: (1) Are
annotator disagreements a reflection of ambiguity in language? (2) Do
alternative text measurements correlate with one another and with measures of
external predictive validity? We find for this application (1) some annotator
disagreements of economic policy uncertainty can be attributed to ambiguity in
language, and (2) switching measurements from keyword-matching to supervised
machine learning classifiers results in low correlation, a concerning
implication for the validity of the index.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 17:50:29 GMT'}]",2020-10-12,"[['Keith', 'Katherine A.', ''], ['Teichmann', 'Christoph', ''], [""O'Connor"", 'Brendan', ''], ['Meij', 'Edgar', '']]"
1310344,2006.15830,Jinhyuk Lee,"Jinhyuk Lee, Sean S. Yi, Minbyul Jeong, Mujeen Sung, Wonjin Yoon,
  Yonghwa Choi, Miyoung Ko, Jaewoo Kang",Answering Questions on COVID-19 in Real-Time,"10 pages, EMNLP NLP-COVID Workshop 2020",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recent outbreak of the novel coronavirus is wreaking havoc on the world
and researchers are struggling to effectively combat it. One reason why the
fight is difficult is due to the lack of information and knowledge. In this
work, we outline our effort to contribute to shrinking this knowledge vacuum by
creating covidAsk, a question answering (QA) system that combines biomedical
text mining and QA techniques to provide answers to questions in real-time. Our
system also leverages information retrieval (IR) approaches to provide
entity-level answers that are complementary to QA models. Evaluation of
covidAsk is carried out by using a manually created dataset called COVID-19
Questions which is based on information from various sources, including the CDC
and the WHO. We hope our system will be able to aid researchers in their search
for knowledge and information not only for COVID-19, but for future pandemics
as well.
","[{'version': 'v1', 'created': 'Mon, 29 Jun 2020 06:34:35 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 08:42:30 GMT'}]",2020-10-12,"[['Lee', 'Jinhyuk', ''], ['Yi', 'Sean S.', ''], ['Jeong', 'Minbyul', ''], ['Sung', 'Mujeen', ''], ['Yoon', 'Wonjin', ''], ['Choi', 'Yonghwa', ''], ['Ko', 'Miyoung', ''], ['Kang', 'Jaewoo', '']]"
1313734,2007.02220,Roei Schuster,"Roei Schuster, Congzheng Song, Eran Tromer, Vitaly Shmatikov",You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion,Accepted at USENIX Security '21,,,,cs.CR cs.CL cs.LG cs.PL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Code autocompletion is an integral feature of modern code editors and IDEs.
The latest generation of autocompleters uses neural language models, trained on
public open-source code repositories, to suggest likely (not just statically
feasible) completions given the current context.
  We demonstrate that neural code autocompleters are vulnerable to poisoning
attacks. By adding a few specially-crafted files to the autocompleter's
training corpus (data poisoning), or else by directly fine-tuning the
autocompleter on these files (model poisoning), the attacker can influence its
suggestions for attacker-chosen contexts. For example, the attacker can ""teach""
the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3
for the SSL/TLS protocol version, or a low iteration count for password-based
encryption. Moreover, we show that these attacks can be targeted: an
autocompleter poisoned by a targeted attack is much more likely to suggest the
insecure completion for files from a specific repo or specific developer.
  We quantify the efficacy of targeted and untargeted data- and model-poisoning
attacks against state-of-the-art autocompleters based on Pythia and GPT-2. We
then evaluate existing defenses against poisoning attacks and show that they
are largely ineffective.
","[{'version': 'v1', 'created': 'Sun, 5 Jul 2020 01:13:36 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Jul 2020 21:34:38 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Oct 2020 23:12:25 GMT'}]",2020-10-12,"[['Schuster', 'Roei', ''], ['Song', 'Congzheng', ''], ['Tromer', 'Eran', ''], ['Shmatikov', 'Vitaly', '']]"
1317165,2007.05651,Jinfeng Li,"Jinfeng Li, Yuliang Li, Xiaolan Wang, Wang-Chiew Tan","Deep or Simple Models for Semantic Tagging? It Depends on your Data
  [Experiments]",,,,,cs.CL cs.DB cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semantic tagging, which has extensive applications in text mining, predicts
whether a given piece of text conveys the meaning of a given semantic tag. The
problem of semantic tagging is largely solved with supervised learning and
today, deep learning models are widely perceived to be better for semantic
tagging. However, there is no comprehensive study supporting the popular
belief. Practitioners often have to train different types of models for each
semantic tagging task to identify the best model. This process is both
expensive and inefficient.
  We embark on a systematic study to investigate the following question: Are
deep models the best performing model for all semantic tagging tasks? To answer
this question, we compare deep models against ""simple models"" over datasets
with varying characteristics. Specifically, we select three prevalent deep
models (i.e. CNN, LSTM, and BERT) and two simple models (i.e. LR and SVM), and
compare their performance on the semantic tagging task over 21 datasets.
Results show that the size, the label ratio, and the label cleanliness of a
dataset significantly impact the quality of semantic tagging. Simple models
achieve similar tagging quality to deep models on large datasets, but the
runtime of simple models is much shorter. Moreover, simple models can achieve
better tagging quality than deep models when targeting datasets show worse
label cleanliness and/or more severe imbalance. Based on these findings, our
study can systematically guide practitioners in selecting the right learning
model for their semantic tagging task.
","[{'version': 'v1', 'created': 'Sat, 11 Jul 2020 00:05:50 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 22:45:08 GMT'}]",2020-10-12,"[['Li', 'Jinfeng', ''], ['Li', 'Yuliang', ''], ['Wang', 'Xiaolan', ''], ['Tan', 'Wang-Chiew', '']]"
1356008,2009.14505,Somak Aditya,"Pratik Joshi, Somak Aditya, Aalok Sathe, Monojit Choudhury",TaxiNLI: Taking a Ride up the NLU Hill,"15 pages, 9 figures, 4 tables. Accepted at CoNLL 2020",,,,cs.AI cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Pre-trained Transformer-based neural architectures have consistently achieved
state-of-the-art performance in the Natural Language Inference (NLI) task.
Since NLI examples encompass a variety of linguistic, logical, and reasoning
phenomena, it remains unclear as to which specific concepts are learnt by the
trained systems and where they can achieve strong generalization. To
investigate this question, we propose a taxonomic hierarchy of categories that
are relevant for the NLI task. We introduce TAXINLI, a new dataset, that has
10k examples from the MNLI dataset (Williams et al., 2018) with these taxonomic
labels. Through various experiments on TAXINLI, we observe that whereas for
certain taxonomic categories SOTA neural models have achieved near perfect
accuracies - a large jump over the previous models - some categories still
remain difficult. Our work adds to the growing body of literature that shows
the gaps in the current NLI systems and datasets through a systematic
presentation and analysis of reasoning categories.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 08:45:25 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 04:28:04 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Oct 2020 11:07:49 GMT'}]",2020-10-12,"[['Joshi', 'Pratik', ''], ['Aditya', 'Somak', ''], ['Sathe', 'Aalok', ''], ['Choudhury', 'Monojit', '']]"
1268807,2004.04037,Lu Hou,"Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu",DynaBERT: Dynamic BERT with Adaptive Width and Depth,NeurIPS-2020 (Spotlight),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The pre-trained language models like BERT, though powerful in many natural
language processing tasks, are both computation and memory expensive. To
alleviate this problem, one approach is to compress them for specific tasks
before deployment. However, recent works on BERT compression usually compress
the large BERT model to a fixed smaller size. They can not fully satisfy the
requirements of different edge devices with various hardware performances. In
this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT),
which can flexibly adjust the size and latency by selecting adaptive width and
depth. The training process of DynaBERT includes first training a
width-adaptive BERT and then allowing both adaptive width and depth, by
distilling knowledge from the full-sized model to small sub-networks. Network
rewiring is also used to keep the more important attention heads and neurons
shared by more sub-networks. Comprehensive experiments under various efficiency
constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its
largest size has comparable performance as BERT-base (or RoBERTa-base), while
at smaller widths and depths consistently outperforms existing BERT compression
methods. Code is available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.
","[{'version': 'v1', 'created': 'Wed, 8 Apr 2020 15:06:28 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 08:51:37 GMT'}]",2020-10-12,"[['Hou', 'Lu', ''], ['Huang', 'Zhiqi', ''], ['Shang', 'Lifeng', ''], ['Jiang', 'Xin', ''], ['Chen', 'Xiao', ''], ['Liu', 'Qun', '']]"
1355202,2009.13699,Brian Lester,Brian Lester,Leader: Prefixing a Length for Faster Word Vector Serialization,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Two competing file formats have become the de facto standards for
distributing pre-trained word embeddings. Both are named after the most popular
pre-trained embeddings that are distributed in that format. The GloVe format is
an entirely text based format that suffers from huge file sizes and slow reads,
and the word2vec format is a smaller binary format that mixes a textual
representation of words with a binary representation of the vectors themselves.
Both formats have problems that we solve with a new format we call the Leader
format. We include a word length prefix for faster reads while maintaining the
smaller file size a binary format offers. We also created a minimalist library
to facilitate the reading and writing of various word vector formats, as well
as tools for converting pre-trained embeddings to our new Leader format.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 00:25:24 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 04:49:24 GMT'}]",2020-10-12,"[['Lester', 'Brian', '']]"
1321790,2007.10276,Juan Banda,"Ramya Tekumalla, Juan M. Banda",Characterizing drug mentions in COVID-19 Twitter Chatter,"7 pages, 2 figures and 5 tables",,,,cs.IR cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Since the classification of COVID-19 as a global pandemic, there have been
many attempts to treat and contain the virus. Although there is no specific
antiviral treatment recommended for COVID-19, there are several drugs that can
potentially help with symptoms. In this work, we mined a large twitter dataset
of 424 million tweets of COVID-19 chatter to identify discourse around drug
mentions. While seemingly a straightforward task, due to the informal nature of
language use in Twitter, we demonstrate the need of machine learning alongside
traditional automated methods to aid in this task. By applying these
complementary methods, we are able to recover almost 15% additional data,
making misspelling handling a needed task as a pre-processing step when dealing
with social media data.
","[{'version': 'v1', 'created': 'Mon, 20 Jul 2020 16:56:46 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 15:35:23 GMT'}]",2020-10-12,"[['Tekumalla', 'Ramya', ''], ['Banda', 'Juan M.', '']]"
1351373,2009.09870,Seraphina Goldfarb-Tarrant,"Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel,
  Nanyun Peng",Content Planning for Neural Story Generation with Aristotelian Rescoring,"EMNLP 2020, 9 pages",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Long-form narrative text generated from large language models manages a
fluent impersonation of human writing, but only at the local sentence level,
and lacks structure or global cohesion. We posit that many of the problems of
story generation can be addressed via high-quality content planning, and
present a system that focuses on how to learn good plot structures to guide
story generation. We utilize a plot-generation language model along with an
ensemble of rescoring models that each implement an aspect of good
story-writing as detailed in Aristotle's Poetics. We find that stories written
with our more principled plot-structure are both more relevant to a given
prompt and higher quality than baselines that do not content plan, or that plan
in an unprincipled way.
","[{'version': 'v1', 'created': 'Mon, 21 Sep 2020 13:41:32 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 16:28:23 GMT'}]",2020-10-12,"[['Goldfarb-Tarrant', 'Seraphina', ''], ['Chakrabarty', 'Tuhin', ''], ['Weischedel', 'Ralph', ''], ['Peng', 'Nanyun', '']]"
1360850,2010.04520,Xuefeng Bai,"Xuefeng Bai, Linfeng Song and Yue Zhang",Online Back-Parsing for AMR-to-Text Generation,To appear in EMNLP2020 main conference,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  AMR-to-text generation aims to recover a text containing the same meaning as
an input AMR graph. Current research develops increasingly powerful graph
encoders to better represent AMR graphs, with decoders based on standard
language modeling being used to generate outputs. We propose a decoder that
back predicts projected AMR graphs on the target sentence during text
generation. As the result, our outputs can better preserve the input meaning
than standard decoders. Experiments on two AMR benchmarks show the superiority
of our model over the previous state-of-the-art system based on graph
Transformer.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 12:08:14 GMT'}]",2020-10-12,"[['Bai', 'Xuefeng', ''], ['Song', 'Linfeng', ''], ['Zhang', 'Yue', '']]"
1360873,2010.04543,Carolina Scarton,"Jo\~ao A. Leite and Diego F. Silva and Kalina Bontcheva and Carolina
  Scarton","Toxic Language Detection in Social Media for Brazilian Portuguese: New
  Dataset and Multilingual Analysis",Accepted to AACL-IJCNLP 2020,,,,cs.CL cs.LG cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Hate speech and toxic comments are a common concern of social media platform
users. Although these comments are, fortunately, the minority in these
platforms, they are still capable of causing harm. Therefore, identifying these
comments is an important task for studying and preventing the proliferation of
toxicity in social media. Previous work in automatically detecting toxic
comments focus mainly in English, with very few work in languages like
Brazilian Portuguese. In this paper, we propose a new large-scale dataset for
Brazilian Portuguese with tweets annotated as either toxic or non-toxic or in
different types of toxicity. We present our dataset collection and annotation
process, where we aimed to select candidates covering multiple demographic
groups. State-of-the-art BERT models were able to achieve 76% macro-F1 score
using monolingual data in the binary case. We also show that large-scale
monolingual data is still needed to create more accurate models, despite recent
advances in multilingual approaches. An error analysis and experiments with
multi-label classification show the difficulty of classifying certain types of
toxic comments that appear less frequently in our data and highlights the need
to develop models that are aware of different categories of toxicity.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 13:05:19 GMT'}]",2020-10-12,"[['Leite', 'João A.', ''], ['Silva', 'Diego F.', ''], ['Bontcheva', 'Kalina', ''], ['Scarton', 'Carolina', '']]"
1360816,2010.04486,Pierangelo Lombardo,"Pierangelo Lombardo, Alessio Boiardi, Luca Colombo, Angelo Schiavone,
  Nicol\`o Tamagnone","Top-Rank-Focused Adaptive Vote Collection for the Evaluation of
  Domain-Specific Semantic Models","This is a pre-print of an article published in the proceedings of the
  2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",,,,cs.CL cs.IR cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The growth of domain-specific applications of semantic models, boosted by the
recent achievements of unsupervised embedding learning algorithms, demands
domain-specific evaluation datasets. In many cases, content-based recommenders
being a prime example, these models are required to rank words or texts
according to their semantic relatedness to a given concept, with particular
focus on top ranks. In this work, we give a threefold contribution to address
these requirements: (i) we define a protocol for the construction, based on
adaptive pairwise comparisons, of a relatedness-based evaluation dataset
tailored on the available resources and optimized to be particularly accurate
in top-rank evaluation; (ii) we define appropriate metrics, extensions of
well-known ranking correlation coefficients, to evaluate a semantic model via
the aforementioned dataset by taking into account the greater significance of
top ranks. Finally, (iii) we define a stochastic transitivity model to simulate
semantic-driven pairwise comparisons, which confirms the effectiveness of the
proposed dataset construction protocol.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 10:20:58 GMT'}]",2020-10-12,"[['Lombardo', 'Pierangelo', ''], ['Boiardi', 'Alessio', ''], ['Colombo', 'Luca', ''], ['Schiavone', 'Angelo', ''], ['Tamagnone', 'Nicolò', '']]"
1279781,2004.15011,Isabel Cachola,"Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S. Weld",TLDR: Extreme Summarization of Scientific Documents,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We introduce TLDR generation, a new form of extreme summarization, for
scientific papers. TLDR generation involves high source compression and
requires expert background knowledge and understanding of complex
domain-specific language. To facilitate study on this task, we introduce
SciTLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR
contains both author-written and expert-derived TLDRs, where the latter are
collected using a novel annotation protocol that produces high-quality
summaries while minimizing annotation burden. We propose CATTS, a simple yet
effective learning strategy for generating TLDRs that exploits titles as an
auxiliary training signal. CATTS improves upon strong baselines under both
automated metrics and human evaluations. Data and code are publicly available
at https://github.com/allenai/scitldr.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:56:18 GMT'}, {'version': 'v2', 'created': 'Sat, 2 May 2020 09:09:24 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Oct 2020 22:41:44 GMT'}]",2020-10-12,"[['Cachola', 'Isabel', ''], ['Lo', 'Kyle', ''], ['Cohan', 'Arman', ''], ['Weld', 'Daniel S.', '']]"
1360579,2010.04249,Ansel MacLaughlin,"Ansel MacLaughlin, Jwala Dhamala, Anoop Kumar, Sriram Venkatapathy,
  Ragav Venkatesan, Rahul Gupta","Evaluating the Effectiveness of Efficient Neural Architecture Search for
  Sentence-Pair Tasks",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural Architecture Search (NAS) methods, which automatically learn entire
neural model or individual neural cell architectures, have recently achieved
competitive or state-of-the-art (SOTA) performance on variety of natural
language processing and computer vision tasks, including language modeling,
natural language inference, and image classification. In this work, we explore
the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search
(ENAS) (Pham et al., 2018) to two sentence pair tasks, paraphrase detection and
semantic textual similarity. We use ENAS to perform a micro-level search and
learn a task-optimized RNN cell architecture as a drop-in replacement for an
LSTM. We explore the effectiveness of ENAS through experiments on three
datasets (MRPC, SICK, STS-B), with two different models (ESIM, BiLSTM-Max), and
two sets of embeddings (Glove, BERT). In contrast to prior work applying ENAS
to NLP tasks, our results are mixed -- we find that ENAS architectures
sometimes, but not always, outperform LSTMs and perform similarly to random
architecture search.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 20:26:34 GMT'}]",2020-10-12,"[['MacLaughlin', 'Ansel', ''], ['Dhamala', 'Jwala', ''], ['Kumar', 'Anoop', ''], ['Venkatapathy', 'Sriram', ''], ['Venkatesan', 'Ragav', ''], ['Gupta', 'Rahul', '']]"
1360590,2010.04260,Akbar Siami Namin,"Faranak Abri, Luis Felipe Gutierrez, Akbar Siami Namin, Keith S.
  Jones, David R. W. Sears",Fake Reviews Detection through Analysis of Linguistic Features,"The pre-print of a paper to appear in the proceedings of the IEEE
  International Conference on Machine Learning Applications (ICMLA 2020), 11
  pages, 3 figures, 5 tables",,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Online reviews play an integral part for success or failure of businesses.
Prior to purchasing services or goods, customers first review the online
comments submitted by previous customers. However, it is possible to
superficially boost or hinder some businesses through posting counterfeit and
fake reviews. This paper explores a natural language processing approach to
identify fake reviews. We present a detailed analysis of linguistic features
for distinguishing fake and trustworthy online reviews. We study 15 linguistic
features and measure their significance and importance towards the
classification schemes employed in this study. Our results indicate that fake
reviews tend to include more redundant terms and pauses, and generally contain
longer sentences. The application of several machine learning classification
algorithms revealed that we were able to discriminate fake from real reviews
with high accuracy using these linguistic features.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 21:16:30 GMT'}]",2020-10-12,"[['Abri', 'Faranak', ''], ['Gutierrez', 'Luis Felipe', ''], ['Namin', 'Akbar Siami', ''], ['Jones', 'Keith S.', ''], ['Sears', 'David R. W.', '']]"
1360614,2010.04284,Hong-Kwang Kuo,"Yinghui Huang, Hong-Kwang Kuo, Samuel Thomas, Zvi Kons, Kartik
  Audhkhasi, Brian Kingsbury, Ron Hoory, Michael Picheny","Leveraging Unpaired Text Data for Training End-to-End Speech-to-Intent
  Systems","5 pages, published in ICASSP 2020",,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Training an end-to-end (E2E) neural network speech-to-intent (S2I) system
that directly extracts intents from speech requires large amounts of
intent-labeled speech data, which is time consuming and expensive to collect.
Initializing the S2I model with an ASR model trained on copious speech data can
alleviate data sparsity. In this paper, we attempt to leverage NLU text
resources. We implemented a CTC-based S2I system that matches the performance
of a state-of-the-art, traditional cascaded SLU system. We performed controlled
experiments with varying amounts of speech and text training data. When only a
tenth of the original data is available, intent classification accuracy
degrades by 7.6% absolute. Assuming we have additional text-to-intent data
(without speech) available, we investigated two techniques to improve the S2I
system: (1) transfer learning, in which acoustic embeddings for intent
classification are tied to fine-tuned BERT text embeddings; and (2) data
augmentation, in which the text-to-intent data is converted into
speech-to-intent data using a multi-speaker text-to-speech system. The proposed
approaches recover 80% of performance lost due to using limited intent-labeled
speech.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 22:16:26 GMT'}]",2020-10-12,"[['Huang', 'Yinghui', ''], ['Kuo', 'Hong-Kwang', ''], ['Thomas', 'Samuel', ''], ['Kons', 'Zvi', ''], ['Audhkhasi', 'Kartik', ''], ['Kingsbury', 'Brian', ''], ['Hoory', 'Ron', ''], ['Picheny', 'Michael', '']]"
1360618,2010.04288,Trang Tran,"Trang Tran, Jiahong Yuan, Yang Liu, Mari Ostendorf",On the Role of Style in Parsing Speech with Neural Models,Interspeech 2019,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The differences in written text and conversational speech are substantial;
previous parsers trained on treebanked text have given very poor results on
spontaneous speech. For spoken language, the mismatch in style also extends to
prosodic cues, though it is less well understood. This paper re-examines the
use of written text in parsing speech in the context of recent advances in
neural language processing. We show that neural approaches facilitate using
written text to improve parsing of spontaneous speech, and that prosody further
improves over this state-of-the-art result. Further, we find an asymmetric
degradation from read vs. spontaneous mismatch, with spontaneous speech more
generally useful for training parsers.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 22:44:19 GMT'}]",2020-10-12,"[['Tran', 'Trang', ''], ['Yuan', 'Jiahong', ''], ['Liu', 'Yang', ''], ['Ostendorf', 'Mari', '']]"
1360623,2010.04293,Trang Tran,"Trang Tran, Morgan Tinkler, Gary Yeung, Abeer Alwan, Mari Ostendorf",Analysis of Disfluency in Children's Speech,Interspeech 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Disfluencies are prevalent in spontaneous speech, as shown in many studies of
adult speech. Less is understood about children's speech, especially in
pre-school children who are still developing their language skills. We present
a novel dataset with annotated disfluencies of spontaneous explanations from 26
children (ages 5--8), interviewed twice over a year-long period. Our
preliminary analysis reveals significant differences between children's speech
in our corpus and adult spontaneous speech from two corpora (Switchboard and
CallHome). Children have higher disfluency and filler rates, tend to use nasal
filled pauses more frequently, and on average exhibit longer reparandums than
repairs, in contrast to adult speakers. Despite the differences, an automatic
disfluency detection system trained on adult (Switchboard) speech transcripts
performs reasonably well on children's speech, achieving an F1 score that is
10\% higher than the score on an adult out-of-domain dataset (CallHome).
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 22:51:25 GMT'}]",2020-10-12,"[['Tran', 'Trang', ''], ['Tinkler', 'Morgan', ''], ['Yeung', 'Gary', ''], ['Alwan', 'Abeer', ''], ['Ostendorf', 'Mari', '']]"
1360625,2010.04295,Yang Li,"Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, Zhiwei Guan","Widget Captioning: Generating Natural Language Description for Mobile
  User Interface Elements","16 pages, EMNLP 2020",,,,cs.LG cs.AI cs.CL cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language descriptions of user interface (UI) elements such as
alternative text are crucial for accessibility and language-based interaction
in general. Yet, these descriptions are constantly missing in mobile UIs. We
propose widget captioning, a novel task for automatically generating language
descriptions for UI elements from multimodal input including both the image and
the structural representations of user interfaces. We collected a large-scale
dataset for widget captioning with crowdsourcing. Our dataset contains 162,859
language phrases created by human workers for annotating 61,285 UI elements
across 21,750 unique UI screens. We thoroughly analyze the dataset, and train
and evaluate a set of deep model configurations to investigate how each feature
modality as well as the choice of learning strategies impact the quality of
predicted captions. The task formulation and the dataset as well as our
benchmark models contribute a solid basis for this novel multimodal captioning
task that connects language and user interfaces.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 22:56:03 GMT'}]",2020-10-12,"[['Li', 'Yang', ''], ['Li', 'Gang', ''], ['He', 'Luheng', ''], ['Zheng', 'Jingjie', ''], ['Li', 'Hong', ''], ['Guan', 'Zhiwei', '']]"
1279289,2004.14519,Wuwei Lan,"Wuwei Lan, Yang Chen, Wei Xu and Alan Ritter",GigaBERT: Zero-shot Transfer Learning from English to Arabic,"8 pages, EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019)
and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable the
effective cross-lingual zero-shot transfer. However, their performance on
Arabic information extraction (IE) tasks is not very well studied. In this
paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is
designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer
learning. We study GigaBERT's effectiveness on zero-short transfer across four
IE tasks: named entity recognition, part-of-speech tagging, argument role
labeling, and relation extraction. Our best model significantly outperforms
mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised
and zero-shot transfer settings. We have made our pre-trained models publicly
available at https://github.com/lanwuwei/GigaBERT.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 00:01:08 GMT'}, {'version': 'v2', 'created': 'Tue, 15 Sep 2020 21:52:04 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 21:00:21 GMT'}, {'version': 'v4', 'created': 'Fri, 9 Oct 2020 00:24:43 GMT'}]",2020-10-12,"[['Lan', 'Wuwei', ''], ['Chen', 'Yang', ''], ['Xu', 'Wei', ''], ['Ritter', 'Alan', '']]"
1360631,2010.04301,Jonathan Shen,"Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang, Isaac Elias, Heiga
  Zen, Yonghui Wu","Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis
  Including Unsupervised Duration Modeling",Under review as a conference paper at ICLR 2021,,,,cs.SD cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper presents Non-Attentive Tacotron based on the Tacotron 2
text-to-speech model, replacing the attention mechanism with an explicit
duration predictor. This improves robustness significantly as measured by
unaligned duration ratio and word deletion rate, two metrics introduced in this
paper for large-scale robustness evaluation using a pre-trained speech
recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron
achieves a 5-scale mean opinion score for naturalness of 4.41, slightly
outperforming Tacotron 2. The duration predictor enables both utterance-wide
and per-phoneme control of duration at inference time. When accurate target
durations are scarce or unavailable in the training data, we propose a method
using a fine-grained variational auto-encoder to train the duration predictor
in a semi-supervised or unsupervised manner, with results almost as good as
supervised training.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 23:41:39 GMT'}]",2020-10-12,"[['Shen', 'Jonathan', ''], ['Jia', 'Ye', ''], ['Chrzanowski', 'Mike', ''], ['Zhang', 'Yu', ''], ['Elias', 'Isaac', ''], ['Zen', 'Heiga', ''], ['Wu', 'Yonghui', '']]"
1360632,2010.04302,Gregory Senay,Gregory Senay and Emmanuelle Salin,"Masked ELMo: An evolution of ELMo towards fully contextual RNN language
  models",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents Masked ELMo, a new RNN-based model for language model
pre-training, evolved from the ELMo language model. Contrary to ELMo which only
uses independent left-to-right and right-to-left contexts, Masked ELMo learns
fully bidirectional word representations. To achieve this, we use the same
Masked language model objective as BERT. Additionally, thanks to optimizations
on the LSTM neuron, the integration of mask accumulation and bidirectional
truncated backpropagation through time, we have increased the training speed of
the model substantially. All these improvements make it possible to pre-train a
better language model than ELMo while maintaining a low computational cost. We
evaluate Masked ELMo by comparing it to ELMo within the same protocol on the
GLUE benchmark, where our model outperforms significantly ELMo and is
competitive with transformer approaches.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 23:58:57 GMT'}]",2020-10-12,"[['Senay', 'Gregory', ''], ['Salin', 'Emmanuelle', '']]"
1360644,2010.04314,Xiaomian Kang,"Xiaomian Kang, Yang Zhao, Jiajun Zhang, Chengqing Zong","Dynamic Context Selection for Document-level Neural Machine Translation
  via Reinforcement Learning",Accepted to EMNLP 2020 long paper,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Document-level neural machine translation has yielded attractive
improvements. However, majority of existing methods roughly use all context
sentences in a fixed scope. They neglect the fact that different source
sentences need different sizes of context. To address this problem, we propose
an effective approach to select dynamic context so that the document-level
translation model can utilize the more useful selected context sentences to
produce better translations. Specifically, we introduce a selection module that
is independent of the translation module to score each candidate context
sentence. Then, we propose two strategies to explicitly select a variable
number of context sentences and feed them into the translation module. We train
the two modules end-to-end via reinforcement learning. A novel reward is
proposed to encourage the selection and utilization of dynamic context
sentences. Experiments demonstrate that our approach can select adaptive
context sentences for different source sentences, and significantly improves
the performance of document-level translation methods.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 01:05:32 GMT'}]",2020-10-12,"[['Kang', 'Xiaomian', ''], ['Zhao', 'Yang', ''], ['Zhang', 'Jiajun', ''], ['Zong', 'Chengqing', '']]"
1297987,2006.03473,Zheng Li,"Zheng Li, Miao Zhao, Qingyang Hong, Lin Li, Zhiyuan Tang, Dong Wang,
  Liming Song and Cheng Yang",AP20-OLR Challenge: Three Tasks and Their Baselines,"arXiv admin note: substantial text overlap with arXiv:1907.07626,
  arXiv:1806.00616, arXiv:1706.09742",,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces the fifth oriental language recognition (OLR) challenge
AP20-OLR, which intends to improve the performance of language recognition
systems, along with APSIPA Annual Summit and Conference (APSIPA ASC). The data
profile, three tasks, the corresponding baselines, and the evaluation
principles are introduced in this paper. The AP20-OLR challenge includes more
languages, dialects and real-life data provided by Speechocean and the NSFC
M2ASR project, and all the data is free for participants. The challenge this
year still focuses on practical and challenging problems, with three tasks: (1)
cross-channel LID, (2) dialect identification and (3) noisy LID. Based on Kaldi
and Pytorch, recipes for i-vector and x-vector systems are also conducted as
baselines for the three tasks. These recipes will be online-published, and
available for participants to configure LID systems. The baseline results on
the three tasks demonstrate that those tasks in this challenge are worth paying
more efforts to achieve better performance.
","[{'version': 'v1', 'created': 'Thu, 4 Jun 2020 16:29:21 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Jul 2020 02:53:38 GMT'}, {'version': 'v3', 'created': 'Fri, 24 Jul 2020 13:28:00 GMT'}, {'version': 'v4', 'created': 'Fri, 9 Oct 2020 09:08:08 GMT'}]",2020-10-12,"[['Li', 'Zheng', ''], ['Zhao', 'Miao', ''], ['Hong', 'Qingyang', ''], ['Li', 'Lin', ''], ['Tang', 'Zhiyuan', ''], ['Wang', 'Dong', ''], ['Song', 'Liming', ''], ['Yang', 'Cheng', '']]"
1360665,2010.04335,Priyanshu Kumar,Priyanshu Kumar and Aadarsh Singh,"NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative
  COVID-19 Tweets using Ensembling and Adversarial Training",,,,,cs.CL cs.IR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We experiment with COVID-Twitter-BERT and RoBERTa models to identify
informative COVID-19 tweets. We further experiment with adversarial training to
make our models robust. The ensemble of COVID-Twitter-BERT and RoBERTa obtains
a F1-score of 0.9096 (on the positive class) on the test data of WNUT-2020 Task
2 and ranks 1st on the leaderboard. The ensemble of the models trained using
adversarial training also produces similar result.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 02:46:51 GMT'}]",2020-10-12,"[['Kumar', 'Priyanshu', ''], ['Singh', 'Aadarsh', '']]"
1360674,2010.04344,Andrea Madotto Mr,"Andrea Madotto, Etsuko Ishii, Zhaojiang Lin, Sumanth Dathathri,
  Pascale Fung",Plug-and-Play Conversational Models,"Accepted in EMNLP findings, and code available at
  https://github.com/andreamad8/PPCM",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There has been considerable progress made towards conversational models that
generate coherent and fluent responses; however, this often involves training
large language models on large dialogue datasets, such as Reddit. These large
conversational models provide little control over the generated responses, and
this control is further limited in the absence of annotated conversational
datasets for attribute specific generation that can be used for fine-tuning the
model. In this paper, we first propose and evaluate plug-and-play methods for
controllable response generation, which does not require dialogue specific
datasets and does not rely on fine-tuning a large model. While effective, the
decoding procedure induces considerable computational overhead, rendering the
conversational model unsuitable for interactive usage. To overcome this, we
introduce an approach that does not require further computation at decoding
time, while also does not require any fine-tuning of a large language model. We
demonstrate, through extensive automatic and human evaluation, a high degree of
control over the generated conversational responses with regard to multiple
desired attributes, while being fluent.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 03:17:51 GMT'}]",2020-10-12,"[['Madotto', 'Andrea', ''], ['Ishii', 'Etsuko', ''], ['Lin', 'Zhaojiang', ''], ['Dathathri', 'Sumanth', ''], ['Fung', 'Pascale', '']]"
1360685,2010.04355,Shang-Wen Li,"Jin Cao, Jun Wang, Wael Hamza, Kelly Vanee, Shang-Wen Li","Style Attuned Pre-training and Parameter Efficient Fine-tuning for
  Spoken Language Understanding",Accepted at INTERSPEECH 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural models have yielded state-of-the-art results in deciphering spoken
language understanding (SLU) problems; however, these models require a
significant amount of domain-specific labeled examples for training, which is
prohibitively expensive. While pre-trained language models like BERT have been
shown to capture a massive amount of knowledge by learning from unlabeled
corpora and solve SLU using fewer labeled examples for adaption, the encoding
of knowledge is implicit and agnostic to downstream tasks. Such encoding
results in model inefficiencies in parameter usage: an entirely new model is
required for every domain. To address these challenges, we introduce a novel
SLU framework, comprising a conversational language modeling (CLM) pre-training
task and a light encoder architecture. The CLM pre-training enables networks to
capture the representation of the language in conversation style with the
presence of ASR errors. The light encoder architecture separates the shared
pre-trained networks from the mappings of generally encoded knowledge to
specific domains of SLU, allowing for the domain adaptation to be performed
solely at the light encoder and thus increasing efficiency. With the framework,
we match the performance of state-of-the-art SLU results on Alexa internal
datasets and on two public ones (ATIS, SNIPS), adding only 4.4% parameters per
task.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 03:53:37 GMT'}]",2020-10-12,"[['Cao', 'Jin', ''], ['Wang', 'Jun', ''], ['Hamza', 'Wael', ''], ['Vanee', 'Kelly', ''], ['Li', 'Shang-Wen', '']]"
1360662,2010.04332,Takumi Ito,"Takumi Ito, Tatsuki Kuribayashi, Masatoshi Hidaka, Jun Suzuki, Kentaro
  Inui",Langsmith: An Interactive Academic Text Revision System,Accepted at EMNLP 2020 (system demonstrations),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the current diversity and inclusion initiatives in the academic
community, researchers with a non-native command of English still face
significant obstacles when writing papers in English. This paper presents the
Langsmith editor, which assists inexperienced, non-native researchers to write
English papers, especially in the natural language processing (NLP) field. Our
system can suggest fluent, academic-style sentences to writers based on their
rough, incomplete phrases or sentences. The system also encourages interaction
between human writers and the computerized revision system. The experimental
results demonstrated that Langsmith helps non-native English-speaker students
write papers in English. The system is available at https://emnlp-demo.editor.
langsmith.co.jp/.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 02:35:14 GMT'}]",2020-10-12,"[['Ito', 'Takumi', ''], ['Kuribayashi', 'Tatsuki', ''], ['Hidaka', 'Masatoshi', ''], ['Suzuki', 'Jun', ''], ['Inui', 'Kentaro', '']]"
1360702,2010.04372,Zhengxuan Wu,"Zhengxuan Wu, Desmond C. Ong","Pragmatically Informative Color Generation by Grounding Contextual
  Modifiers",8 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Grounding language in contextual information is crucial for fine-grained
natural language understanding. One important task that involves grounding
contextual modifiers is color generation. Given a reference color ""green"", and
a modifier ""bluey"", how does one generate a color that could represent ""bluey
green""? We propose a computational pragmatics model that formulates this color
generation task as a recursive game between speakers and listeners. In our
model, a pragmatic speaker reasons about the inferences that a listener would
make, and thus generates a modified color that is maximally informative to help
the listener recover the original referents. In this paper, we show that
incorporating pragmatic information provides significant improvements in
performance compared with other state-of-the-art deep learning models where
pragmatic inference and flexibility in representing colors from a large
continuous space are lacking. Our model has an absolute 98% increase in
performance for the test cases where the reference colors are unseen during
training, and an absolute 40% increase in performance for the test cases where
both the reference colors and the modifiers are unseen during training.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 04:54:54 GMT'}]",2020-10-12,"[['Wu', 'Zhengxuan', ''], ['Ong', 'Desmond C.', '']]"
1360812,2010.04482,Sunil Gundapu,"Sunil Gundapu, Radhika Mamidi",Word Level Language Identification in English Telugu Code Mixed Data,"7 pages, 3 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In a multilingual or sociolingual configuration Intra-sentential Code
Switching (ICS) or Code Mixing (CM) is frequently observed nowadays. In the
world, most of the people know more than one language. CM usage is especially
apparent in social media platforms. Moreover, ICS is particularly significant
in the context of technology, health, and law where conveying the upcoming
developments are difficult in one's native language. In applications like
dialog systems, machine translation, semantic parsing, shallow parsing, etc. CM
and Code Switching pose serious challenges. To do any further advancement in
code-mixed data, the necessary step is Language Identification. In this paper,
we present a study of various models - Nave Bayes Classifier, Random Forest
Classifier, Conditional Random Field (CRF), and Hidden Markov Model (HMM) for
Language Identification in English - Telugu Code Mixed Data. Considering the
paucity of resources in code mixed languages, we proposed the CRF model and HMM
model for word level language identification. Our best performing system is
CRF-based with an f1-score of 0.91.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 10:15:06 GMT'}]",2020-10-12,"[['Gundapu', 'Sunil', ''], ['Mamidi', 'Radhika', '']]"
1360810,2010.04480,Marina Fomicheva,"Marina Fomicheva, Shuo Sun, Erick Fonseca, Fr\'ed\'eric Blain, Vishrav
  Chaudhary, Francisco Guzm\'an, Nina Lopatina, Lucia Specia and Andr\'e F. T.
  Martins",MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present MLQE-PE, a new dataset for Machine Translation (MT) Quality
Estimation (QE) and Automatic Post-Editing (APE). The dataset contains seven
language pairs, with human labels for 9,000 translations per language pair in
the following formats: sentence-level direct assessments and post-editing
effort, and word-level good/bad labels. It also contains the post-edited
sentences, as well as titles of the articles where the sentences were extracted
from, and the neural MT models used to translate the text.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 10:12:02 GMT'}]",2020-10-12,"[['Fomicheva', 'Marina', ''], ['Sun', 'Shuo', ''], ['Fonseca', 'Erick', ''], ['Blain', 'Frédéric', ''], ['Chaudhary', 'Vishrav', ''], ['Guzmán', 'Francisco', ''], ['Lopatina', 'Nina', ''], ['Specia', 'Lucia', ''], ['Martins', 'André F. T.', '']]"
1360776,2010.04446,Wen-Chin Huang,"Wen-Chin Huang, Patrick Lumban Tobing, Yi-Chiao Wu, Kazuhiro
  Kobayashi, Tomoki Toda","The NU Voice Conversion System for the Voice Conversion Challenge 2020:
  On the Effectiveness of Sequence-to-sequence Models and Autoregressive Neural
  Vocoders","Accepted to the ISCA Joint Workshop for the Blizzard Challenge and
  Voice Conversion Challenge 2020",,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present the voice conversion (VC) systems developed at
Nagoya University (NU) for the Voice Conversion Challenge 2020 (VCC2020). We
aim to determine the effectiveness of two recent significant technologies in
VC: sequence-to-sequence (seq2seq) models and autoregressive (AR) neural
vocoders. Two respective systems were developed for the two tasks in the
challenge: for task 1, we adopted the Voice Transformer Network, a
Transformer-based seq2seq VC model, and extended it with synthetic parallel
data to tackle nonparallel data; for task 2, we used the frame-based cyclic
variational autoencoder (CycleVAE) to model the spectral features of a speech
waveform and the AR WaveNet vocoder with additional fine-tuning. By comparing
with the baseline systems, we confirmed that the seq2seq modeling can improve
the conversion similarity and that the use of AR vocoders can improve the
naturalness of the converted speech.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 09:19:37 GMT'}]",2020-10-12,"[['Huang', 'Wen-Chin', ''], ['Tobing', 'Patrick Lumban', ''], ['Wu', 'Yi-Chiao', ''], ['Kobayashi', 'Kazuhiro', ''], ['Toda', 'Tomoki', '']]"
1360768,2010.04438,Harris Chan,"Harris Chan, Jamie Kiros, William Chan","Multichannel Generative Language Model: Learning All Possible
  Factorizations Within and Across Channels","10 pages (+3 appendix), 11 figures, 5 tables. Accepted to Findings of
  EMNLP 2020",,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A channel corresponds to a viewpoint or transformation of an underlying
meaning. A pair of parallel sentences in English and French express the same
underlying meaning, but through two separate channels corresponding to their
languages. In this work, we present the Multichannel Generative Language Model
(MGLM). MGLM is a generative joint distribution model over channels. MGLM
marginalizes over all possible factorizations within and across all channels.
MGLM endows flexible inference, including unconditional generation, conditional
generation (where 1 channel is observed and other channels are generated), and
partially observed generation (where incomplete observations are spread across
all the channels). We experiment with the Multi30K dataset containing English,
French, Czech, and German. We demonstrate experiments with unconditional,
conditional, and partially conditional generation. We provide qualitative
samples sampled unconditionally from the generative joint distribution. We also
quantitatively analyze the quality-diversity trade-offs and find MGLM
outperforms traditional bilingual discriminative models.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 08:52:24 GMT'}]",2020-10-12,"[['Chan', 'Harris', ''], ['Kiros', 'Jamie', ''], ['Chan', 'William', '']]"
1360759,2010.04429,Patrick Lumban Tobing,"Patrick Lumban Tobing, Yi-Chiao Wu, Tomoki Toda","Baseline System of Voice Conversion Challenge 2020 with Cyclic
  Variational Autoencoder and Parallel WaveGAN",,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present a description of the baseline system of Voice
Conversion Challenge (VCC) 2020 with a cyclic variational autoencoder
(CycleVAE) and Parallel WaveGAN (PWG), i.e., CycleVAEPWG. CycleVAE is a
nonparallel VAE-based voice conversion that utilizes converted acoustic
features to consider cyclically reconstructed spectra during optimization. On
the other hand, PWG is a non-autoregressive neural vocoder that is based on a
generative adversarial network for a high-quality and fast waveform generator.
In practice, the CycleVAEPWG system can be straightforwardly developed with the
VCC 2020 dataset using a unified model for both Task 1 (intralingual) and Task
2 (cross-lingual), where our open-source implementation is available at
https://github.com/bigpon/vcc20_baseline_cyclevae. The results of VCC 2020 have
demonstrated that the CycleVAEPWG baseline achieves the following: 1) a mean
opinion score (MOS) of 2.87 in naturalness and a speaker similarity percentage
(Sim) of 75.37% for Task 1, and 2) a MOS of 2.56 and a Sim of 56.46% for Task
2, showing an approximately or nearly average score for naturalness and an
above average score for speaker similarity.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 08:25:38 GMT'}]",2020-10-12,"[['Tobing', 'Patrick Lumban', ''], ['Wu', 'Yi-Chiao', ''], ['Toda', 'Tomoki', '']]"
1360741,2010.04411,Xiangpeng Wei,"Xiangpeng Wei and Heng Yu and Yue Hu and Rongxiang Weng and Luxi Xing
  and Weihua Luo",Uncertainty-Aware Semantic Augmentation for Neural Machine Translation,"Accepted to EMNLP 2020, 12 pages, 2 figures, 9 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As a sequence-to-sequence generation task, neural machine translation (NMT)
naturally contains intrinsic uncertainty, where a single sentence in one
language has multiple valid counterparts in the other. However, the dominant
methods for NMT only observe one of them from the parallel corpora for the
model training but have to deal with adequate variations under the same meaning
at inference. This leads to a discrepancy of the data distribution between the
training and the inference phases. To address this problem, we propose
uncertainty-aware semantic augmentation, which explicitly captures the
universal semantic information among multiple semantically-equivalent source
sentences and enhances the hidden representations with this information for
better translations. Extensive experiments on various translation tasks reveal
that our approach significantly outperforms the strong baselines and the
existing methods.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 07:48:09 GMT'}]",2020-10-12,"[['Wei', 'Xiangpeng', ''], ['Yu', 'Heng', ''], ['Hu', 'Yue', ''], ['Weng', 'Rongxiang', ''], ['Xing', 'Luxi', ''], ['Luo', 'Weihua', '']]"
1360692,2010.04362,Brian Lester,"Brian Lester, Daniel Pressel, Amy Hemmeter, Sagnik Ray Choudhury,
  Srinivas Bangalore","Constrained Decoding for Computationally Efficient Named Entity
  Recognition Taggers",Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current state-of-the-art models for named entity recognition (NER) are neural
models with a conditional random field (CRF) as the final layer. Entities are
represented as per-token labels with a special structure in order to decode
them into spans. Current work eschews prior knowledge of how the span encoding
scheme works and relies on the CRF learning which transitions are illegal and
which are not to facilitate global coherence. We find that by constraining the
output to suppress illegal transitions we can train a tagger with a
cross-entropy loss twice as fast as a CRF with differences in F1 that are
statistically insignificant, effectively eliminating the need for a CRF. We
analyze the dynamics of tag co-occurrence to explain when these constraints are
most effective and provide open source implementations of our tagger in both
PyTorch and TensorFlow.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 04:07:52 GMT'}]",2020-10-12,"[['Lester', 'Brian', ''], ['Pressel', 'Daniel', ''], ['Hemmeter', 'Amy', ''], ['Choudhury', 'Sagnik Ray', ''], ['Bangalore', 'Srinivas', '']]"
1360719,2010.04389,Wenhao Yu,"Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng
  Ji, Meng Jiang",A Survey of Knowledge-Enhanced Text Generation,"44 pages; Preprint; A paper and code collection is available at
  https://github.com/wyu97/KENLG-Reading",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The goal of text generation is to make machines express in human language. It
is one of the most important yet challenging tasks in natural language
processing (NLP). Since 2014, various neural encoder-decoder models pioneered
by Seq2Seq have been proposed to achieve the goal by learning to map input text
to output text. However, the input text alone often provides limited knowledge
to generate the desired output, so the performance of text generation is still
far from satisfaction in many real-world scenarios. To address this issue,
researchers have considered incorporating various forms of knowledge beyond the
input text into the generation models. This research direction is known as
knowledge-enhanced text generation. In this survey, we present a comprehensive
review of the research on knowledge enhanced text generation over the past five
years. The main content includes two parts: (i) general methods and
architectures for integrating knowledge into text generation; (ii) specific
techniques and applications according to different forms of knowledge data.
This survey can have broad audiences, researchers and practitioners, in
academia and industry.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 06:46:46 GMT'}]",2020-10-12,"[['Yu', 'Wenhao', ''], ['Zhu', 'Chenguang', ''], ['Li', 'Zaitang', ''], ['Hu', 'Zhiting', ''], ['Wang', 'Qingyun', ''], ['Ji', 'Heng', ''], ['Jiang', 'Meng', '']]"
1360725,2010.04395,Sunil Gundapu,"Sunil Gundapu, Radhika Mamidi","gundapusunil at SemEval-2020 Task 9: Syntactic Semantic LSTM
  Architecture for SENTIment Analysis of Code-MIXed Data","6 pages, 2 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The phenomenon of mixing the vocabulary and syntax of multiple languages
within the same utterance is called Code-Mixing. This is more evident in
multilingual societies. In this paper, we have developed a system for SemEval
2020: Task 9 on Sentiment Analysis for Code-Mixed Social Media Text. Our system
first generates two types of embeddings for the social media text. In those,
the first one is character level embeddings to encode the character level
information and to handle the out-of-vocabulary entries and the second one is
FastText word embeddings for capturing morphology and semantics. These two
embeddings were passed to the LSTM network and the system outperformed the
baseline model.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 07:07:04 GMT'}]",2020-10-12,"[['Gundapu', 'Sunil', ''], ['Mamidi', 'Radhika', '']]"
1360713,2010.04383,Yan Zhang,"Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B. Cohen, Zuozhu
  Liu, Lidong Bing","Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text
  Generation","Accepted to EMNLP 2020, long paper",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  AMR-to-text generation is used to transduce Abstract Meaning Representation
structures (AMR) into text. A key challenge in this task is to efficiently
learn effective graph representations. Previously, Graph Convolution Networks
(GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to
capture non-local information and additionally, they follow a local
(first-order) information aggregation scheme. To account for these issues,
larger and deeper GCN models are required to capture more complex interactions.
In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight
Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local
interactions by synthesizing higher order information from the input graphs. We
further develop two novel parameter saving strategies based on the group graph
convolutions and weight tied convolutions to reduce memory usage and model
complexity. With the help of these strategies, we are able to train a model
with fewer parameters while maintaining the model capacity. Experiments
demonstrate that LDGCNs outperform state-of-the-art models on two benchmark
datasets for AMR-to-text generation with significantly fewer parameters.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 06:03:46 GMT'}]",2020-10-12,"[['Zhang', 'Yan', ''], ['Guo', 'Zhijiang', ''], ['Teng', 'Zhiyang', ''], ['Lu', 'Wei', ''], ['Cohen', 'Shay B.', ''], ['Liu', 'Zuozhu', ''], ['Bing', 'Lidong', '']]"
1360710,2010.04380,Shuhao Gu,"Shuhao Gu, Jinchao Zhang, Fandong Meng, Yang Feng, Wanying Xie, Jie
  Zhou, Dong Yu",Token-level Adaptive Training for Neural Machine Translation,12 pages; Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There exists a token imbalance phenomenon in natural language as different
tokens appear with different frequencies, which leads to different learning
difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT
model usually adopts trivial equal-weighted objectives for target tokens with
different frequencies and tends to generate more high-frequency tokens and less
low-frequency tokens compared with the golden token distribution. However,
low-frequency tokens may carry critical semantic information that will affect
the translation quality once they are neglected. In this paper, we explored
target token-level adaptive objectives based on token frequencies to assign
appropriate weights for each target token during training. We aimed that those
meaningful but relatively low-frequency words could be assigned with larger
weights in objectives to encourage the model to pay more attention to these
tokens. Our method yields consistent improvements in translation quality on
ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain
more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases
compared with baseline, respectively. Further analyses show that our method can
also improve the lexical diversity of translation.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 05:55:05 GMT'}]",2020-10-12,"[['Gu', 'Shuhao', ''], ['Zhang', 'Jinchao', ''], ['Meng', 'Fandong', ''], ['Feng', 'Yang', ''], ['Xie', 'Wanying', ''], ['Zhou', 'Jie', ''], ['Yu', 'Dong', '']]"
1360709,2010.04379,Ryosuke Kohita,"Ryosuke Kohita, Akifumi Wachi, Yang Zhao, Ryuki Tachibana",Q-learning with Language Model for Edit-based Unsupervised Summarization,"14 pages, 4 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unsupervised methods are promising for abstractive text summarization in that
the parallel corpora is not required. However, their performance is still far
from being satisfied, therefore research on promising solutions is on-going. In
this paper, we propose a new approach based on Q-learning with an edit-based
summarization. The method combines two key modules to form an Editorial Agent
and Language Model converter (EALM). The agent predicts edit actions (e.t.,
delete, keep, and replace), and then the LM converter deterministically
generates a summary on the basis of the action signals. Q-learning is leveraged
to train the agent to produce proper edit actions. Experimental results show
that EALM delivered competitive performance compared with the previous
encoder-decoder-based methods, even with truly zero paired data (i.e., no
validation set). Defining the task as Q-learning enables us not only to develop
a competitive method but also to make the latest techniques in reinforcement
learning available for unsupervised summarization. We also conduct qualitative
analysis, providing insights into future study on unsupervised summarizers.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 05:47:00 GMT'}]",2020-10-12,"[['Kohita', 'Ryosuke', ''], ['Wachi', 'Akifumi', ''], ['Zhao', 'Yang', ''], ['Tachibana', 'Ryuki', '']]"
1360707,2010.04377,Tanmoy Chakraborty,"Sarah Masud, Subhabrata Dutta, Sakshi Makkar, Chhavi Jain, Vikram
  Goyal, Amitava Das, Tanmoy Chakraborty","Hate is the New Infodemic: A Topic-aware Modeling of Hate Speech
  Diffusion on Twitter","6 table, 9 figures, Full paper in 37th International Conference on
  Data Engineering (ICDE)",,,,cs.SI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Online hate speech, particularly over microblogging platforms like Twitter,
has emerged as arguably the most severe issue of the past decade. Several
countries have reported a steep rise in hate crimes infuriated by malicious
hate campaigns. While the detection of hate speech is one of the emerging
research areas, the generation and spread of topic-dependent hate in the
information network remain under-explored. In this work, we focus on exploring
user behaviour, which triggers the genesis of hate speech on Twitter and how it
diffuses via retweets. We crawl a large-scale dataset of tweets, retweets, user
activity history, and follower networks, comprising over 161 million tweets
from more than $41$ million unique users. We also collect over 600k
contemporary news articles published online. We characterize different signals
of information that govern these dynamics. Our analyses differentiate the
diffusion dynamics in the presence of hate from usual information diffusion.
This motivates us to formulate the modelling problem in a topic-aware setting
with real-world knowledge. For predicting the initiation of hate speech for any
given hashtag, we propose multiple feature-rich models, with the best
performing one achieving a macro F1 score of 0.65. Meanwhile, to predict the
retweet dynamics on Twitter, we propose RETINA, a novel neural architecture
that incorporates exogenous influence using scaled dot-product attention.
RETINA achieves a macro F1-score of 0.85, outperforming multiple
state-of-the-art models. Our analysis reveals the superlative power of RETINA
to predict the retweet dynamics of hateful content compared to the existing
diffusion models.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 05:43:08 GMT'}]",2020-10-12,"[['Masud', 'Sarah', ''], ['Dutta', 'Subhabrata', ''], ['Makkar', 'Sakshi', ''], ['Jain', 'Chhavi', ''], ['Goyal', 'Vikram', ''], ['Das', 'Amitava', ''], ['Chakraborty', 'Tanmoy', '']]"
1360703,2010.04373,Brian Lester,Brian Lester,iobes: A Library for Span-Level Processing,NLP-OSS 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many tasks in natural language processing, such as named entity recognition
and slot-filling, involve identifying and labeling specific spans of text. In
order to leverage common models, these tasks are often recast as sequence
labeling tasks. Each token is given a label and these labels are prefixed with
special tokens such as B- or I-. After a model assigns labels to each token,
these prefixes are used to group the tokens into spans.
  Properly parsing these annotations is critical for producing fair and
comparable metrics; however, despite its importance, there is not an
easy-to-use, standardized, programmatically integratable library to help work
with span labeling. To remedy this, we introduce our open-source library,
iobes. iobes is used for parsing, converting, and processing spans represented
as token level decisions.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 05:03:48 GMT'}]",2020-10-12,"[['Lester', 'Brian', '']]"
1360718,2010.04388,Jennifer D'Souza,"Jennifer D'Souza, S\""oren Auer","Graphing Contributions in Natural Language Processing Research:
  Intra-Annotator Agreement on a Trial Dataset","For a forthcoming submission to JDIS Special Issue Organized by EEKE
  @ JCDL 2020",,,,cs.CL cs.DL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Purpose: To stabilize the NLPContributionGraph scheme for the surface
structuring of contributions information in Natural Language Processing (NLP)
scholarly articles via a two-stage annotation methodology: first stage - to
define the scheme; and second stage - to stabilize the graphing model.
  Approach: Re-annotate, a second time, the contributions-pertinent information
across 50 prior-annotated NLP scholarly articles in terms of a data pipeline
comprising: contribution-centered sentences, phrases, and triples. To this end
specifically, care was taken in the second annotation stage to reduce
annotation noise while formulating the guidelines for our proposed novel NLP
contributions structuring scheme.
  Findings: The application of NLPContributionGraph on the 50 articles resulted
in finally in a dataset of 900 contribution-focused sentences, 4,702
contribution-information-centered phrases, and 2,980 surface-structured
triples. The intra-annotation agreement between the first and second stages, in
terms of F1, was 67.92% for sentences, 41.82% for phrases, and 22.31% for
triples indicating that with an increased granularity of the information, the
annotation decision variance is greater.
  Practical Implications: Demonstrate NLPContributionGraph data integrated in
the Open Research Knowledge Graph (ORKG), a next-generation KG-based digital
library with compute enabled over structured scholarly knowledge, as a viable
aid to assist researchers in their day-to-day tasks.
  Value: NLPContributionGraph is a novel scheme to obtain research
contribution-centered graphs from NLP articles which to the best of our
knowledge does not exist in the community. And our quantitative evaluations
over the two-stage annotation tasks offer insights into task difficulty.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 06:45:35 GMT'}]",2020-10-12,"[[""D'Souza"", 'Jennifer', ''], ['Auer', 'Sören', '']]"
1360633,2010.04303,Javid Ebrahimi,"Javid Ebrahimi, Dhruv Gelda, Wei Zhang",How Can Self-Attention Networks Recognize Dyck-n Languages?,,Findings of EMNLP 2020,,,cs.CL cs.FL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We focus on the recognition of Dyck-n ($\mathcal{D}_n$) languages with
self-attention (SA) networks, which has been deemed to be a difficult task for
these networks. We compare the performance of two variants of SA, one with a
starting symbol (SA$^+$) and one without (SA$^-$). Our results show that SA$^+$
is able to generalize to longer sequences and deeper dependencies. For
$\mathcal{D}_2$, we find that SA$^-$ completely breaks down on long sequences
whereas the accuracy of SA$^+$ is 58.82$\%$. We find attention maps learned by
$\text{SA}{^+}$ to be amenable to interpretation and compatible with a
stack-based language recognizer. Surprisingly, the performance of SA networks
is at par with LSTMs, which provides evidence on the ability of SA to learn
hierarchies without recursion.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 00:03:17 GMT'}]",2020-10-12,"[['Ebrahimi', 'Javid', ''], ['Gelda', 'Dhruv', ''], ['Zhang', 'Wei', '']]"
1358637,2010.02307,Wenhu Chen,"Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang",KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation,Accepted to Main Conference of EMNLP 2020 as Long Paper,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Data-to-text generation has recently attracted substantial interests due to
its wide applications. Existing methods have shown impressive performance on an
array of tasks. However, they rely on a significant amount of labeled data for
each task, which is costly to acquire and thus limits their application to new
tasks and domains. In this paper, we propose to leverage pre-training and
transfer learning to address this issue. We propose a knowledge-grounded
pre-training (KGPT), which consists of two parts, 1) a general
knowledge-grounded generation model to generate knowledge-enriched text. 2) a
pre-training paradigm on a massive knowledge-grounded text corpus crawled from
the web. The pre-trained model can be fine-tuned on various data-to-text
generation tasks to generate task-specific text. We adopt three settings,
namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness.
Under the fully-supervised setting, our model can achieve remarkable gains over
the known baselines. Under zero-shot setting, our model without seeing any
examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail.
Under the few-shot setting, our model only needs about one-fifteenth as many
labeled examples to achieve the same level of performance as baseline models.
These experiments consistently prove the strong generalization ability of our
proposed framework https://github.com/wenhuchen/KGPT.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 19:59:05 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 18:09:49 GMT'}]",2020-10-13,"[['Chen', 'Wenhu', ''], ['Su', 'Yu', ''], ['Yan', 'Xifeng', ''], ['Wang', 'William Yang', '']]"
1358586,2010.02256,Morteza Pourreza Shahri,"Morteza Pourreza Shahri, Amir Tahmasebi, Bingyang Ye, Henghui Zhu,
  Javed Aslam, Timothy Ferris",An Ensemble Approach for Automatic Structuring of Radiology Reports,Accepted by the 3rd Clinical NLP Workshop at EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic structuring of electronic medical records is of high demand for
clinical workflow solutions to facilitate extraction, storage, and querying of
patient care information. However, developing a scalable solution is extremely
challenging, specifically for radiology reports, as most healthcare institutes
use either no template or department/institute specific templates. Moreover,
radiologists' reporting style varies from one to another as sentences are
telegraphic and do not follow general English grammar rules. We present an
ensemble method that consolidates the predictions of three models, capturing
various attributes of textual information for automatic labeling of sentences
with section labels. These three models are: 1) Focus Sentence model, capturing
context of the target sentence; 2) Surrounding Context model, capturing the
neighboring context of the target sentence; and finally, 3) Formatting/Layout
model, aimed at learning report formatting cues. We utilize Bi-directional
LSTMs, followed by sentence encoders, to acquire the context. Furthermore, we
define several features that incorporate the structure of reports. We compare
our proposed approach against multiple baselines and state-of-the-art
approaches on a proprietary dataset as well as 100 manually annotated radiology
notes from the MIMIC-III dataset, which we are making publicly available. Our
proposed approach significantly outperforms other approaches by achieving 97.1%
accuracy.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 18:11:23 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 00:06:51 GMT'}]",2020-10-13,"[['Shahri', 'Morteza Pourreza', ''], ['Tahmasebi', 'Amir', ''], ['Ye', 'Bingyang', ''], ['Zhu', 'Henghui', ''], ['Aslam', 'Javed', ''], ['Ferris', 'Timothy', '']]"
1359313,2010.02983,Florian Mai,"Florian Mai (1 and 2), Nikolaos Pappas (3), Ivan Montero (3), Noah A.
  Smith (3 and 4), James Henderson (1) ((1) Idiap Research Institute, (2) EPFL,
  (3) University of Washington, (4) Allen Institute for Artificial
  Intelligence)",Plug and Play Autoencoders for Conditional Text Generation,To be published in EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text autoencoders are commonly used for conditional generation tasks such as
style transfer. We propose methods which are plug and play, where any
pretrained autoencoder can be used, and only require learning a mapping within
the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb).
This reduces the need for labeled training data for the task and makes the
training procedure more efficient. Crucial to the success of this method is a
loss term for keeping the mapped embedding on the manifold of the autoencoder
and a mapping which is trained to navigate the manifold by learning offset
vectors. Evaluations on style transfer tasks both with and without
sequence-to-sequence supervision show that our method performs better than or
comparable to strong baselines while being up to four times faster.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 19:18:06 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 08:20:59 GMT'}]",2020-10-13,"[['Mai', 'Florian', '', '1 and 2'], ['Pappas', 'Nikolaos', '', '3 and 4'], ['Montero', 'Ivan', '', '3 and 4'], ['Smith', 'Noah A.', '', '3 and 4'], ['Henderson', 'James', '']]"
1277176,2004.12406,Mengjie Zhao,"Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, Hinrich Sch\""utze","Masking as an Efficient Alternative to Finetuning for Pretrained
  Language Models",EMNLP 2020; MZ and TL contribute equally,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We present an efficient method of utilizing pretrained language models, where
we learn selective binary masks for pretrained weights in lieu of modifying
them through finetuning. Extensive evaluations of masking BERT and RoBERTa on a
series of NLP tasks show that our masking scheme yields performance comparable
to finetuning, yet has a much smaller memory footprint when several tasks need
to be inferred simultaneously. Through intrinsic evaluations, we show that
representations computed by masked language models encode information necessary
for solving downstream tasks. Analyzing the loss landscape, we show that
masking and finetuning produce models that reside in minima that can be
connected by a line segment with nearly constant test accuracy. This confirms
that masking can be utilized as an efficient alternative to finetuning.
","[{'version': 'v1', 'created': 'Sun, 26 Apr 2020 15:03:47 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 11:52:08 GMT'}]",2020-10-13,"[['Zhao', 'Mengjie', ''], ['Lin', 'Tao', ''], ['Mi', 'Fei', ''], ['Jaggi', 'Martin', ''], ['Schütze', 'Hinrich', '']]"
1358758,2010.02428,Tao Li,"Tao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, Vivek Srikumar",UnQovering Stereotyping Biases via Underspecified Questions,Accepted at Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While language embeddings have been shown to have stereotyping biases, how
these biases affect downstream question answering (QA) models remains
unexplored. We present UNQOVER, a general framework to probe and quantify
biases through underspecified questions. We show that a naive use of model
scores can lead to incorrect bias estimates due to two forms of reasoning
errors: positional dependence and question independence. We design a formalism
that isolates the aforementioned errors. As case studies, we use this metric to
analyze four important classes of stereotypes: gender, nationality, ethnicity,
and religion. We probe five transformer-based QA models trained on two QA
datasets, along with their underlying language models. Our broad study reveals
that (1) all these models, with and without fine-tuning, have notable
stereotyping biases in these classes; (2) larger models often have higher bias;
and (3) the effect of fine-tuning on bias varies strongly with the dataset and
the model size.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 01:49:52 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 04:51:22 GMT'}, {'version': 'v3', 'created': 'Sat, 10 Oct 2020 01:48:31 GMT'}]",2020-10-13,"[['Li', 'Tao', ''], ['Khot', 'Tushar', ''], ['Khashabi', 'Daniel', ''], ['Sabharwal', 'Ashish', ''], ['Srikumar', 'Vivek', '']]"
1358811,2010.02481,Hoang Nguyen,"Hoang Nguyen, Chenwei Zhang, Congying Xia, Philip S. Yu","Dynamic Semantic Matching and Aggregation Network for Few-shot Intent
  Detection","10 pages, 3 figures. To appear in Findings of EMNLP 2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Few-shot Intent Detection is challenging due to the scarcity of available
annotated utterances. Although recent works demonstrate that multi-level
matching plays an important role in transferring learned knowledge from seen
training classes to novel testing classes, they rely on a static similarity
measure and overly fine-grained matching components. These limitations inhibit
generalizing capability towards Generalized Few-shot Learning settings where
both seen and novel classes are co-existent. In this paper, we propose a novel
Semantic Matching and Aggregation Network where semantic components are
distilled from utterances via multi-head self-attention with additional dynamic
regularization constraints. These semantic components capture high-level
information, resulting in more effective matching between instances. Our
multi-perspective matching method provides a comprehensive matching measure to
enhance representations of both labeled and unlabeled instances. We also
propose a more challenging evaluation setting that considers classification on
the joint all-class label space. Extensive experimental results demonstrate the
effectiveness of our method. Our code and data are publicly available.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 05:16:38 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 06:07:12 GMT'}]",2020-10-13,"[['Nguyen', 'Hoang', ''], ['Zhang', 'Chenwei', ''], ['Xia', 'Congying', ''], ['Yu', 'Philip S.', '']]"
1291482,2005.11687,Nikola Milo\v{s}evi\'c Dr,"Nikola Milosevic, Gangamma Kalappa, Hesam Dadafarin, Mahmoud Azimaee,
  Goran Nenadic","MASK: A flexible framework to facilitate de-identification of clinical
  texts",,,,,cs.CL cs.IR cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Medical health records and clinical summaries contain a vast amount of
important information in textual form that can help advancing research on
treatments, drugs and public health. However, the majority of these information
is not shared because they contain private information about patients, their
families, or medical staff treating them. Regulations such as HIPPA in the US,
PHIPPA in Canada and GDPR regulate the protection, processing and distribution
of this information. In case this information is de-identified and personal
information are replaced or redacted, they could be distributed to the research
community. In this paper, we present MASK, a software package that is designed
to perform the de-identification task. The software is able to perform named
entity recognition using some of the state-of-the-art techniques and then mask
or redact recognized entities. The user is able to select named entity
recognition algorithm (currently implemented are two versions of CRF-based
techniques and BiLSTM-based neural network with pre-trained GLoVe and ELMo
embedding) and masking algorithm (e.g. shift dates, replace names/locations,
totally redact entity).
","[{'version': 'v1', 'created': 'Sun, 24 May 2020 08:53:00 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 20:09:00 GMT'}]",2020-10-13,"[['Milosevic', 'Nikola', ''], ['Kalappa', 'Gangamma', ''], ['Dadafarin', 'Hesam', ''], ['Azimaee', 'Mahmoud', ''], ['Nenadic', 'Goran', '']]"
1361256,2010.04926,Yian Zhang,Yian Zhang,Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?,"6 pages, 3 figures, 3 tables, to appear in Proceedings of the 2020
  EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for
  NLP",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent latent tree learning models can learn constituency parsing without any
exposure to human-annotated tree structures. One such model is ON-LSTM (Shen et
al., 2019), which is trained on language modelling and has
near-state-of-the-art performance on unsupervised parsing. In order to better
understand the performance and consistency of the model as well as how the
parses it generates are different from gold-standard PTB parses, we replicate
the model with different restarts and examine their parses. We find that (1)
the model has reasonably consistent parsing behaviors across different
restarts, (2) the model struggles with the internal structures of complex noun
phrases, (3) the model has a tendency to overestimate the height of the split
points right before verbs. We speculate that both problems could potentially be
solved by adopting a different training task other than unidirectional language
modelling.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 07:12:48 GMT'}]",2020-10-13,"[['Zhang', 'Yian', '']]"
1201485,1911.02891,Lifu Tu,"Lifu Tu, Richard Yuanzhe Pang, Kevin Gimpel","Improving Joint Training of Inference Networks and Structured Prediction
  Energy Networks",EMNLP 2020 Workshop on Structured Prediction for NLP (SPNLP),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep energy-based models are powerful, but pose challenges for learning and
inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an
efficient framework for energy-based models by training ""inference networks"" to
approximate structured inference instead of using gradient descent. However,
their alternating optimization approach suffers from instabilities during
training, requiring additional loss terms and careful hyperparameter tuning. In
this paper, we contribute several strategies to stabilize and improve this
joint training of energy functions and inference networks for structured
prediction. We design a compound objective to jointly train both cost-augmented
and test-time inference networks along with the energy function. We propose
joint parameterizations for the inference networks that encourage them to
capture complementary functionality during learning. We empirically validate
our strategies on two sequence labeling tasks, showing easier paths to strong
performance than prior work, as well as further improvements with global energy
terms.
","[{'version': 'v1', 'created': 'Thu, 7 Nov 2019 13:26:07 GMT'}, {'version': 'v2', 'created': 'Sat, 10 Oct 2020 16:19:50 GMT'}]",2020-10-13,"[['Tu', 'Lifu', ''], ['Pang', 'Richard Yuanzhe', ''], ['Gimpel', 'Kevin', '']]"
1361911,2010.05581,Sicheng Yu,"Sicheng Yu, Yulei Niu, Shuohang Wang, Jing Jiang, Qianru Sun","Counterfactual Variable Control for Robust and Interpretable Question
  Answering",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural network based question answering (QA) models are neither robust
nor explainable in many cases. For example, a multiple-choice QA model, tested
without any input of question, is surprisingly ""capable"" to predict the most of
correct options. In this paper, we inspect such spurious ""capability"" of QA
models using causal inference. We find the crux is the shortcut correlation,
e.g., unrobust word alignment between passage and options learned by the
models. We propose a novel approach called Counterfactual Variable Control
(CVC) that explicitly mitigates any shortcut correlation and preserves the
comprehensive reasoning for robust QA. Specifically, we leverage multi-branch
architecture that allows us to disentangle robust and shortcut correlations in
the training process of QA. We then conduct two novel CVC inference methods (on
trained models) to capture the effect of comprehensive reasoning as the final
prediction. For evaluation, we conduct extensive experiments using two BERT
backbones on both multi-choice and span-extraction QA benchmarks. The results
show that our CVC achieves high robustness against a variety of adversarial
attacks in QA while maintaining good interpretation ability.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 10:09:05 GMT'}]",2020-10-13,"[['Yu', 'Sicheng', ''], ['Niu', 'Yulei', ''], ['Wang', 'Shuohang', ''], ['Jiang', 'Jing', ''], ['Sun', 'Qianru', '']]"
1361917,2010.05587,Debjit Paul,Debjit Paul and Anette Frank,Social Commonsense Reasoning with Multi-Head Knowledge Attention,Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Social Commonsense Reasoning requires understanding of text, knowledge about
social events and their pragmatic implications, as well as commonsense
reasoning skills. In this work we propose a novel multi-head knowledge
attention model that encodes semi-structured commonsense inference rules and
learns to incorporate them in a transformer-based reasoning cell. We assess the
model's performance on two tasks that require different reasoning skills:
Abductive Natural Language Inference and Counterfactual Invariance Prediction
as a new task. We show that our proposed model improves performance over strong
state-of-the-art models (i.e., RoBERTa) across both reasoning tasks. Notably we
are, to the best of our knowledge, the first to demonstrate that a model that
learns to perform counterfactual reasoning helps predicting the best
explanation in an abductive reasoning task. We validate the robustness of the
model's reasoning capabilities by perturbing the knowledge and provide
qualitative analysis on the model's knowledge incorporation capabilities.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 10:24:40 GMT'}]",2020-10-13,"[['Paul', 'Debjit', ''], ['Frank', 'Anette', '']]"
1361924,2010.05594,Wei Peng,"Ting Han, Ximing Liu, Ryuichi Takanobu, Yixin Lian, Chongxuan Huang,
  Wei Peng, Minlie Huang","MultiWOZ 2.3: A multi-domain task-oriented dataset enhanced with
  annotation corrections and co-reference annotation",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task-oriented dialogue systems have made unprecedented progress with multiple
state-of-the-art (SOTA) models underpinned by a number of publicly available
MultiWOZ datasets. Dialogue state annotations are error-prone, leading to
sub-optimal performance. Various efforts have been put in rectifying the
annotation errors presented in the original MultiWOZ dataset. In this paper, we
introduce MultiWOZ 2.3, in which we differentiate incorrect annotations in
dialogue acts from dialogue states, identifying a lack of co-reference when
publishing the updated dataset. To ensure consistency between dialogue acts and
dialogue states, we implement co-reference features and unify annotations of
dialogue acts and dialogue states. We update the state of the art performance
of natural language understanding and dialog state tracking on MultiWOZ 2.3,
where the results show significant improvements than on previous versions of
MultiWOZ datasets (2.0-2.2).
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 10:53:19 GMT'}]",2020-10-13,"[['Han', 'Ting', ''], ['Liu', 'Ximing', ''], ['Takanobu', 'Ryuichi', ''], ['Lian', 'Yixin', ''], ['Huang', 'Chongxuan', ''], ['Peng', 'Wei', ''], ['Huang', 'Minlie', '']]"
1361937,2010.05607,Jasmijn Bastings,Jasmijn Bastings and Katja Filippova,"The elephant in the interpretability room: Why use attention as
  explanation when we have saliency methods?",Accepted at BlackboxNLP 2020,"Proceedings of the 2020 EMNLP Workshop BlackboxNLP: Analyzing and
  Interpreting Neural Networks for NLP",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is a recent surge of interest in using attention as explanation of
model predictions, with mixed evidence on whether attention can be used as
such. While attention conveniently gives us one weight per input token and is
easily extracted, it is often unclear toward what goal it is used as
explanation. We find that often that goal, whether explicitly stated or not, is
to find out what input tokens are the most relevant to a prediction, and that
the implied user for the explanation is a model developer. For this goal and
user, we argue that input saliency methods are better suited, and that there
are no compelling reasons to use attention, despite the coincidence that it
provides a weight for each input. With this position paper, we hope to shift
some of the recent focus on attention to saliency methods, and for authors to
clearly state the goal and user for their explanations.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 11:27:47 GMT'}]",2020-10-13,"[['Bastings', 'Jasmijn', ''], ['Filippova', 'Katja', '']]"
1361939,2010.05609,Amine Abdaoui,"Amine Abdaoui, Camille Pradel and Gr\'egoire Sigel",Load What You Need: Smaller Versions of Multilingual BERT,,SustaiNLP / EMNLP 2020,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained Transformer-based models are achieving state-of-the-art results
on a variety of Natural Language Processing data sets. However, the size of
these models is often a drawback for their deployment in real production
applications. In the case of multilingual models, most of the parameters are
located in the embeddings layer. Therefore, reducing the vocabulary size should
have an important impact on the total number of parameters. In this paper, we
propose to generate smaller models that handle fewer number of languages
according to the targeted corpora. We present an evaluation of smaller versions
of multilingual BERT on the XNLI data set, but we believe that this method may
be applied to other multilingual transformers. The obtained results confirm
that we can generate smaller models that keep comparable results, while
reducing up to 45% of the total number of parameters. We compared our models
with DistilmBERT (a distilled version of multilingual BERT) and showed that
unlike language reduction, distillation induced a 1.7% to 6% drop in the
overall accuracy on the XNLI data set. The presented models and code are
publicly available.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 11:29:06 GMT'}]",2020-10-13,"[['Abdaoui', 'Amine', ''], ['Pradel', 'Camille', ''], ['Sigel', 'Grégoire', '']]"
1361963,2010.05633,Omnia Zayed,"Omnia Zayed, John P. McCrae, Paul Buitelaar",Contextual Modulation for Relation-Level Metaphor Identification,accepted at Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Identifying metaphors in text is very challenging and requires comprehending
the underlying comparison. The automation of this cognitive process has gained
wide attention lately. However, the majority of existing approaches concentrate
on word-level identification by treating the task as either single-word
classification or sequential labelling without explicitly modelling the
interaction between the metaphor components. On the other hand, while existing
relation-level approaches implicitly model this interaction, they ignore the
context where the metaphor occurs. In this work, we address these limitations
by introducing a novel architecture for identifying relation-level metaphoric
expressions of certain grammatical relations based on contextual modulation. In
a methodology inspired by works in visual reasoning, our approach is based on
conditioning the neural network computation on the deep contextualised features
of the candidate expressions using feature-wise linear modulation. We
demonstrate that the proposed architecture achieves state-of-the-art results on
benchmark datasets. The proposed methodology is generic and could be applied to
other textual classification problems that benefit from contextual interaction.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 12:07:02 GMT'}]",2020-10-13,"[['Zayed', 'Omnia', ''], ['McCrae', 'John P.', ''], ['Buitelaar', 'Paul', '']]"
1358124,2010.01794,Steven Y. Feng,"Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, Eduard
  Hovy",GenAug: Data Augmentation for Finetuning Text Generators,"EMNLP 2020 Deep Learning Inside Out (DeeLIO) Workshop; Code available
  at https://github.com/styfeng/GenAug",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we investigate data augmentation for text generation, which we
call GenAug. Text generation and language modeling are important tasks within
natural language processing, and are especially challenging for low-data
regimes. We propose and evaluate various augmentation methods, including some
that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp
Reviews. We also examine the relationship between the amount of augmentation
and the quality of the generated text. We utilize several metrics that evaluate
important aspects of the generated text including its diversity and fluency.
Our experiments demonstrate that insertion of character-level synthetic noise
and keyword replacement with hypernyms are effective augmentation methods, and
that the quality of generations improves to a peak at approximately three times
the amount of original data.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 05:46:39 GMT'}, {'version': 'v2', 'created': 'Sat, 10 Oct 2020 06:00:03 GMT'}]",2020-10-13,"[['Feng', 'Steven Y.', ''], ['Gangal', 'Varun', ''], ['Kang', 'Dongyeop', ''], ['Mitamura', 'Teruko', ''], ['Hovy', 'Eduard', '']]"
1361977,2010.05647,Inbar Oren,"Inbar Oren, Jonathan Herzig, Nitish Gupta, Matt Gardner, Jonathan
  Berant",Improving Compositional Generalization in Semantic Parsing,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generalization of models to out-of-distribution (OOD) data has captured
tremendous attention recently. Specifically, compositional generalization,
i.e., whether a model generalizes to new structures built of components
observed during training, has sparked substantial interest. In this work, we
investigate compositional generalization in semantic parsing, a natural
test-bed for compositional generalization, as output programs are constructed
from sub-components. We analyze a wide variety of models and propose multiple
extensions to the attention module of the semantic parser, aiming to improve
compositional generalization. We find that the following factors improve
compositional generalization: (a) using contextual representations, such as
ELMo and BERT, (b) informing the decoder what input tokens have previously been
attended to, (c) training the decoder attention to agree with pre-computed
token alignments, and (d) downsampling examples corresponding to frequent
program templates. While we substantially reduce the gap between
in-distribution and OOD generalization, performance on OOD compositions is
still substantially lower.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 12:34:58 GMT'}]",2020-10-13,"[['Oren', 'Inbar', ''], ['Herzig', 'Jonathan', ''], ['Gupta', 'Nitish', ''], ['Gardner', 'Matt', ''], ['Berant', 'Jonathan', '']]"
1362000,2010.05670,Francois Meyer,Francois Meyer and Martha Lewis,Modelling Lexical Ambiguity with Density Matrices,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Words can have multiple senses. Compositional distributional models of
meaning have been argued to deal well with finer shades of meaning variation
known as polysemy, but are not so well equipped to handle word senses that are
etymologically unrelated, or homonymy. Moving from vectors to density matrices
allows us to encode a probability distribution over different senses of a word,
and can also be accommodated within a compositional distributional model of
meaning. In this paper we present three new neural models for learning density
matrices from a corpus, and test their ability to discriminate between word
senses on a range of compositional datasets. When paired with a particular
composition method, our best model outperforms existing vector-based
compositional models as well as strong sentence encoders.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 13:08:45 GMT'}]",2020-10-13,"[['Meyer', 'Francois', ''], ['Lewis', 'Martha', '']]"
1350055,2009.08552,Liang Qiu,"Liang Qiu, Yizhou Zhao, Weiyan Shi, Yuan Liang, Feng Shi, Tao Yuan,
  Zhou Yu, Song-Chun Zhu",Structured Attention for Unsupervised Dialogue Structure Induction,Long paper accepted by EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inducing a meaningful structural representation from one or a set of
dialogues is a crucial but challenging task in computational linguistics.
Advancement made in this area is critical for dialogue system design and
discourse analysis. It can also be extended to solve grammatical inference. In
this work, we propose to incorporate structured attention layers into a
Variational Recurrent Neural Network (VRNN) model with discrete latent states
to learn dialogue structure in an unsupervised fashion. Compared to a vanilla
VRNN, structured attention enables a model to focus on different parts of the
source sentence embeddings while enforcing a structural inductive bias.
Experiments show that on two-party dialogue datasets, VRNN with structured
attention learns semantic structures that are similar to templates used to
generate this dialogue corpus. While on multi-party dialogue datasets, our
model learns an interactive structure demonstrating its capability of
distinguishing speakers or addresses, automatically disentangling dialogues
without explicit human annotation.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 23:07:03 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 18:33:18 GMT'}]",2020-10-13,"[['Qiu', 'Liang', ''], ['Zhao', 'Yizhou', ''], ['Shi', 'Weiyan', ''], ['Liang', 'Yuan', ''], ['Shi', 'Feng', ''], ['Yuan', 'Tao', ''], ['Yu', 'Zhou', ''], ['Zhu', 'Song-Chun', '']]"
1362030,2010.05700,Kalpesh Krishna,"Kalpesh Krishna, John Wieting, Mohit Iyyer",Reformulating Unsupervised Style Transfer as Paraphrase Generation,EMNLP 2020 camera-ready (26 pages),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern NLP defines the task of style transfer as modifying the style of a
given sentence without appreciably changing its semantics, which implies that
the outputs of style transfer systems should be paraphrases of their inputs.
However, many existing systems purportedly designed for style transfer
inherently warp the input's meaning through attribute transfer, which changes
semantic properties such as sentiment. In this paper, we reformulate
unsupervised style transfer as a paraphrase generation problem, and present a
simple methodology based on fine-tuning pretrained language models on
automatically generated paraphrase data. Despite its simplicity, our method
significantly outperforms state-of-the-art style transfer systems on both human
and automatic evaluations. We also survey 23 style transfer papers and discover
that existing automatic metrics can be easily gamed and propose fixed variants.
Finally, we pivot to a more real-world style transfer setting by collecting a
large dataset of 15M sentences in 11 diverse styles, which we use for an
in-depth analysis of our system.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 13:31:01 GMT'}]",2020-10-13,"[['Krishna', 'Kalpesh', ''], ['Wieting', 'John', ''], ['Iyyer', 'Mohit', '']]"
1362040,2010.05710,Ofir Arviv,"Ofir Arviv, Ruixiang Cui, Daniel Hershcovich",HUJI-KU at MRP~2020: Two Transition-based Neural Parsers,,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  This paper describes the HUJI-KU system submission to the shared task on
Cross-Framework Meaning Representation Parsing (MRP) at the 2020 Conference for
Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR
parser, which were, respectively, the baseline system and winning system in the
2019 MRP shared task. Both are transition-based parsers using BERT
contextualized embeddings. We generalized TUPA to support the newly-added MRP
frameworks and languages, and experimented with multitask learning with the
HIT-SCIR parser. We reached 4th place in both the cross-framework and
cross-lingual tracks.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 13:41:32 GMT'}]",2020-10-13,"[['Arviv', 'Ofir', ''], ['Cui', 'Ruixiang', ''], ['Hershcovich', 'Daniel', '']]"
1362055,2010.05725,Ethan Wilcox,"Ethan Wilcox, Peng Qian, Richard Futrell, Ryosuke Kohita, Roger Levy
  and Miguel Ballesteros","Structural Supervision Improves Few-Shot Learning and Syntactic
  Generalization in Neural Language Models",To appear at EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Humans can learn structural properties about a word from minimal experience,
and deploy their learned syntactic representations uniformly in different
grammatical contexts. We assess the ability of modern neural language models to
reproduce this behavior in English and evaluate the effect of structural
supervision on learning outcomes. First, we assess few-shot learning
capabilities by developing controlled experiments that probe models' syntactic
nominal number and verbal argument structure generalizations for tokens seen as
few as two times during training. Second, we assess invariance properties of
learned representation: the ability of a model to transfer syntactic
generalizations from a base context (e.g., a simple declarative active-voice
sentence) to a transformed context (e.g., an interrogative sentence). We test
four models trained on the same dataset: an n-gram baseline, an LSTM, and two
LSTM-variants trained with explicit structural supervision (Dyer et al.,2016;
Charniak et al., 2016). We find that in most cases, the neural models are able
to induce the proper syntactic generalizations after minimal exposure, often
from just two examples during training, and that the two structurally
supervised models generalize more accurately than the LSTM model. All neural
models are able to leverage information learned in base contexts to drive
expectations in transformed contexts, indicating that they have learned some
invariance properties of syntax.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 14:12:37 GMT'}]",2020-10-13,"[['Wilcox', 'Ethan', ''], ['Qian', 'Peng', ''], ['Futrell', 'Richard', ''], ['Kohita', 'Ryosuke', ''], ['Levy', 'Roger', ''], ['Ballesteros', 'Miguel', '']]"
1336532,2008.08810,Son T. Luu,"Son T. Luu, Kiet Van Nguyen, Anh Gia-Tuan Nguyen and Ngan Luu-Thuy
  Nguyen","An Experimental Study of Deep Neural Network Models for Vietnamese
  Multiple-Choice Reading Comprehension","Accepted by the 2020 IEEE Eighth International Conference on
  Communications and Electronics",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine reading comprehension (MRC) is a challenging task in natural language
processing that makes computers understanding natural language texts and answer
questions based on those texts. There are many techniques for solving this
problems, and word representation is a very important technique that impact
most to the accuracy of machine reading comprehension problem in the popular
languages like English and Chinese. However, few studies on MRC have been
conducted in low-resource languages such as Vietnamese. In this paper, we
conduct several experiments on neural network-based model to understand the
impact of word representation to the Vietnamese multiple-choice machine reading
comprehension. Our experiments include using the Co-match model on six
different Vietnamese word embeddings and the BERT model for multiple-choice
reading comprehension. On the ViMMRC corpus, the accuracy of BERT model is
61.28% on test set.
","[{'version': 'v1', 'created': 'Thu, 20 Aug 2020 07:29:14 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 15:10:12 GMT'}]",2020-10-13,"[['Luu', 'Son T.', ''], ['Van Nguyen', 'Kiet', ''], ['Nguyen', 'Anh Gia-Tuan', ''], ['Nguyen', 'Ngan Luu-Thuy', '']]"
1362061,2010.05731,Ivan Vuli\'c,"Ivan Vuli\'c, Edoardo Maria Ponti, Robert Litschko, Goran Glava\v{s},
  Anna Korhonen",Probing Pretrained Language Models for Lexical Semantics,EMNLP 2020: Long paper,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The success of large pretrained language models (LMs) such as BERT and
RoBERTa has sparked interest in probing their representations, in order to
unveil what types of knowledge they implicitly capture. While prior research
focused on morphosyntactic, semantic, and world knowledge, it remains unclear
to which extent LMs also derive lexical type-level knowledge from words in
context. In this work, we present a systematic empirical analysis across six
typologically diverse languages and five different lexical tasks, addressing
the following questions: 1) How do different lexical knowledge extraction
strategies (monolingual versus multilingual source LM, out-of-context versus
in-context encoding, inclusion of special tokens, and layer-wise averaging)
impact performance? How consistent are the observed effects across tasks and
languages? 2) Is lexical knowledge stored in few parameters, or is it scattered
throughout the network? 3) How do these representations fare against
traditional static word vectors in lexical tasks? 4) Does the lexical
information emerging from independently trained monolingual LMs display latent
similarities? Our main results indicate patterns and best practices that hold
universally, but also point to prominent variations across languages and tasks.
Moreover, we validate the claim that lower Transformer layers carry more
type-level lexical knowledge, but also show that this knowledge is distributed
across multiple layers.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 14:24:01 GMT'}]",2020-10-13,"[['Vulić', 'Ivan', ''], ['Ponti', 'Edoardo Maria', ''], ['Litschko', 'Robert', ''], ['Glavaš', 'Goran', ''], ['Korhonen', 'Anna', '']]"
1362062,2010.05732,Francis Ferraro,Rajat Patel and Francis Ferraro,"On the Complementary Nature of Knowledge Graph Embedding, Fine Grain
  Entity Types, and Language Modeling",To appear at the EMNLP 2020 Workshop on Deep Learning Inside Out,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We demonstrate the complementary natures of neural knowledge graph embedding,
fine-grain entity type prediction, and neural language modeling. We show that a
language model-inspired knowledge graph embedding approach yields both improved
knowledge graph embeddings and fine-grain entity type representations. Our work
also shows that jointly modeling both structured knowledge tuples and language
improves both.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 14:26:48 GMT'}]",2020-10-13,"[['Patel', 'Rajat', ''], ['Ferraro', 'Francis', '']]"
1362066,2010.05736,Marco Di Giovanni,Marco Di Giovanni and Marco Brambilla,EFSG: Evolutionary Fooling Sentences Generator,"13 pages, 19 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large pre-trained language representation models (LMs) have recently
collected a huge number of successes in many NLP tasks.
  In 2018 BERT, and later its successors (e.g. RoBERTa), obtained
state-of-the-art results in classical benchmark tasks, such as GLUE benchmark.
  After that, works about adversarial attacks have been published to test their
generalization proprieties and robustness.
  In this work, we design Evolutionary Fooling Sentences Generator (EFSG), a
model- and task-agnostic adversarial attack algorithm built using an
evolutionary approach to generate false-positive sentences for binary
classification tasks.
  We successfully apply EFSG to CoLA and MRPC tasks, on BERT and RoBERTa,
comparing performances. Results prove the presence of weak spots in
state-of-the-art LMs.
  We finally test adversarial training as a data augmentation defence approach
against EFSG, obtaining stronger improved models with no loss of accuracy when
tested on the original datasets.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 14:28:48 GMT'}]",2020-10-13,"[['Di Giovanni', 'Marco', ''], ['Brambilla', 'Marco', '']]"
1204748,1911.06154,Ahmed El-Kishky,"Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzman, Philipp Koehn",CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs,EMNLP 2020,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cross-lingual document alignment aims to identify pairs of documents in two
distinct languages that are of comparable content or translations of each
other. In this paper, we exploit the signals embedded in URLs to label web
documents at scale with an average precision of 94.5% across different language
pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify
web document pairs that are translations of each other. We release a new web
dataset consisting of over 392 million URL pairs from Common Crawl covering
documents in 8144 language pairs of which 137 pairs include English. In
addition to curating this massive dataset, we introduce baseline methods that
leverage cross-lingual representations to identify aligned documents based on
their textual content. Finally, we demonstrate the value of this parallel
documents dataset through a downstream task of mining parallel sentences and
measuring the quality of machine translations from models trained on this mined
data. Our objective in releasing this dataset is to foster new research in
cross-lingual NLP across a variety of low, medium, and high-resource languages.
","[{'version': 'v1', 'created': 'Sun, 10 Nov 2019 02:09:11 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 06:00:35 GMT'}]",2020-10-13,"[['El-Kishky', 'Ahmed', ''], ['Chaudhary', 'Vishrav', ''], ['Guzman', 'Francisco', ''], ['Koehn', 'Philipp', '']]"
1362068,2010.05738,Sopan Khosla,"Sopan Khosla, Carolyn Rose",Using Type Information to Improve Entity Coreference Resolution,Accepted as Long Paper at CODI workshop EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Coreference resolution (CR) is an essential part of discourse analysis. Most
recently, neural approaches have been proposed to improve over SOTA models from
earlier paradigms. So far none of the published neural models leverage external
semantic knowledge such as type information. This paper offers the first such
model and evaluation, demonstrating modest gains in accuracy by introducing
either gold standard or predicted types. In the proposed approach, type
information serves both to (1) improve mention representation and (2) create a
soft type consistency check between coreference candidate mentions. Our
evaluation covers two different grain sizes of types over four different
benchmark corpora.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 14:32:39 GMT'}]",2020-10-13,"[['Khosla', 'Sopan', ''], ['Rose', 'Carolyn', '']]"
1349833,2009.08330,Xinyu Wang,"Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei
  Huang, Kewei Tu","More Embeddings, Better Sequence Labelers?","Accepted to Findings of EMNLP 2020. Camera-ready, 16 pages",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Recent work proposes a family of contextual embeddings that significantly
improves the accuracy of sequence labelers over non-contextual embeddings.
However, there is no definite conclusion on whether we can build better
sequence labelers by combining different kinds of embeddings in various
settings. In this paper, we conduct extensive experiments on 3 tasks over 18
datasets and 8 languages to study the accuracy of sequence labeling with
various embedding concatenations and make three observations: (1) concatenating
more embedding variants leads to better accuracy in rich-resource and
cross-domain settings and some conditions of low-resource settings; (2)
concatenating additional contextual sub-word embeddings with contextual
character embeddings hurts the accuracy in extremely low-resource settings; (3)
based on the conclusion of (1), concatenating additional similar contextual
embeddings cannot lead to further improvements. We hope these conclusions can
help people build stronger sequence labelers in various settings.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 14:28:27 GMT'}, {'version': 'v2', 'created': 'Sat, 10 Oct 2020 13:55:04 GMT'}]",2020-10-13,"[['Wang', 'Xinyu', ''], ['Jiang', 'Yong', ''], ['Bach', 'Nguyen', ''], ['Wang', 'Tao', ''], ['Huang', 'Zhongqiang', ''], ['Huang', 'Fei', ''], ['Tu', 'Kewei', '']]"
1361899,2010.05569,Suranjana Samanta,"Suranjana Samanta, Ajay Gupta, Prateeti Mohapatra, Amar Prakash Azad","Carbon to Diamond: An Incident Remediation Assistant System From Site
  Reliability Engineers' Conversations in Hybrid Cloud Operations","6 Pages, 5 figures, 2 tables",,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conversational channels are changing the landscape of hybrid cloud service
management. These channels are becoming important avenues for Site Reliability
Engineers (SREs) %Subject Matter Experts (SME) to collaboratively work together
to resolve an incident or issue. Identifying segmented conversations and
extracting key insights or artefacts from them can help engineers to improve
the efficiency of the incident remediation process by using information
retrieval mechanisms for similar incidents. However, it has been empirically
observed that due to the semi-formal behavior of such conversations (human
language) they are very unique in nature and also contain lot of
domain-specific terms. This makes it difficult to use the standard natural
language processing frameworks directly, which are popularly used in standard
NLP tasks. %It is important to identify the correct keywords and artefacts like
symptoms, issue etc., present in the conversation chats. In this paper, we
build a framework that taps into the conversational channels and uses various
learning methods to (a) understand and extract key artefacts from conversations
like diagnostic steps and resolution actions taken, and (b) present an approach
to identify past conversations about similar issues. Experimental results on
our dataset show the efficacy of our proposed method.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 09:43:35 GMT'}]",2020-10-13,"[['Samanta', 'Suranjana', ''], ['Gupta', 'Ajay', ''], ['Mohapatra', 'Prateeti', ''], ['Azad', 'Amar Prakash', '']]"
1361709,2010.05379,Qinxin Wang,"Qinxin Wang, Hao Tan, Sheng Shen, Michael W. Mahoney, Zhewei Yao","MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase
  Grounding",,,,,cs.CL cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Phrase localization is a task that studies the mapping from textual phrases
to regions of an image. Given difficulties in annotating phrase-to-object
datasets at scale, we develop a Multimodal Alignment Framework (MAF) to
leverage more widely-available caption-image datasets, which can then be used
as a form of weak supervision. We first present algorithms to model
phrase-object relevance by leveraging fine-grained visual representations and
visually-aware language representations. By adopting a contrastive objective,
our method uses information in caption-image pairs to boost the performance in
weakly-supervised scenarios. Experiments conducted on the widely-adopted
Flickr30k dataset show a significant improvement over existing
weakly-supervised methods. With the help of the visually-aware language
representations, we can also improve the previous best unsupervised result by
5.56%. We conduct ablation studies to show that both our novel model and our
weakly-supervised strategies significantly contribute to our strong results.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 00:43:52 GMT'}]",2020-10-13,"[['Wang', 'Qinxin', ''], ['Tan', 'Hao', ''], ['Shen', 'Sheng', ''], ['Mahoney', 'Michael W.', ''], ['Yao', 'Zhewei', '']]"
1361714,2010.05384,Yao-Chung Fan,"Ho-Lam Chung, Ying-Hong Chan, Yao-Chung Fan","A BERT-based Distractor Generation Scheme with Multi-tasking and
  Negative Answer Training Strategies",Accepted by EMNLP2020 Findings,,,,cs.CL cs.AI,http://creativecommons.org/publicdomain/zero/1.0/,"  In this paper, we investigate the following two limitations for the existing
distractor generation (DG) methods. First, the quality of the existing DG
methods are still far from practical use. There is still room for DG quality
improvement. Second, the existing DG designs are mainly for single distractor
generation. However, for practical MCQ preparation, multiple distractors are
desired. Aiming at these goals, in this paper, we present a new distractor
generation scheme with multi-tasking and negative answer training strategies
for effectively generating \textit{multiple} distractors. The experimental
results show that (1) our model advances the state-of-the-art result from 28.65
to 39.81 (BLEU 1 score) and (2) the generated multiple distractors are diverse
and show strong distracting power for multiple choice question.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 01:22:29 GMT'}]",2020-10-13,"[['Chung', 'Ho-Lam', ''], ['Chan', 'Ying-Hong', ''], ['Fan', 'Yao-Chung', '']]"
1361736,2010.05406,Mingzhe Li,"Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao and
  Rui Yan","VMSMO: Learning to Generate Multimodal Summary for Video-based News
  Articles","Accepted by The 2020 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2020)",,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A popular multimedia news format nowadays is providing users with a lively
video and a corresponding news article, which is employed by influential news
media including CNN, BBC, and social media including Twitter and Weibo. In such
a case, automatically choosing a proper cover frame of the video and generating
an appropriate textual summary of the article can help editors save time, and
readers make the decision more effectively. Hence, in this paper, we propose
the task of Video-based Multimodal Summarization with Multimodal Output (VMSMO)
to tackle such a problem. The main challenge in this task is to jointly model
the temporal dependency of video with semantic meaning of article. To this end,
we propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of
a dual interaction module and multimodal generator. In the dual interaction
module, we propose a conditional self-attention mechanism that captures local
semantic information within video and a global-attention mechanism that handles
the semantic relationship between news text and video from a high level.
Extensive experiments conducted on a large-scale real-world VMSMO dataset show
that DIMS achieves the state-of-the-art performance in terms of both automatic
metrics and human evaluations.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 02:19:16 GMT'}]",2020-10-13,"[['Li', 'Mingzhe', ''], ['Chen', 'Xiuying', ''], ['Gao', 'Shen', ''], ['Chan', 'Zhangming', ''], ['Zhao', 'Dongyan', ''], ['Yan', 'Rui', '']]"
1361749,2010.05419,Jens Tuyls,"Junlin Wang, Jens Tuyls, Eric Wallace, Sameer Singh",Gradient-based Analysis of NLP Models is Manipulable,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Gradient-based analysis methods, such as saliency map visualizations and
adversarial input perturbations, have found widespread use in interpreting
neural NLP models due to their simplicity, flexibility, and most importantly,
their faithfulness. In this paper, however, we demonstrate that the gradients
of a model are easily manipulable, and thus bring into question the reliability
of gradient-based analyses. In particular, we merge the layers of a target
model with a Facade that overwhelms the gradients without affecting the
predictions. This Facade can be trained to have gradients that are misleading
and irrelevant to the task, such as focusing only on the stop words in the
input. On a variety of NLP tasks (text classification, NLI, and QA), we show
that our method can manipulate numerous gradient-based analysis techniques:
saliency maps, input reduction, and adversarial perturbations all identify
unimportant or targeted tokens as being highly important. The code and a
tutorial of this paper is available at http://ucinlp.github.io/facade.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 02:54:22 GMT'}]",2020-10-13,"[['Wang', 'Junlin', ''], ['Tuyls', 'Jens', ''], ['Wallace', 'Eric', ''], ['Singh', 'Sameer', '']]"
1361762,2010.05432,Md Mosharaf Hossain,"Md Mosharaf Hossain, Antonios Anastasopoulos, Eduardo Blanco, and
  Alexis Palmer","It's not a Non-Issue: Negation as a Source of Error in Machine
  Translation",Accepted at the Findings of EMNLP2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As machine translation (MT) systems progress at a rapid pace, questions of
their adequacy linger. In this study we focus on negation, a universal, core
property of human language that significantly affects the semantics of an
utterance. We investigate whether translating negation is an issue for modern
MT systems using 17 translation directions as test bed. Through thorough
analysis, we find that indeed the presence of negation can significantly impact
downstream quality, in some cases resulting in quality reductions of more than
60%. We also provide a linguistically motivated analysis that directly explains
the majority of our findings. We release our annotations and code to replicate
our analysis here: https://github.com/mosharafhossain/negation-mt.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 03:34:44 GMT'}]",2020-10-13,"[['Hossain', 'Md Mosharaf', ''], ['Anastasopoulos', 'Antonios', ''], ['Blanco', 'Eduardo', ''], ['Palmer', 'Alexis', '']]"
1361774,2010.05444,Hai Hu,"Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kuebler, Lawrence S.
  Moss",OCNLI: Original Chinese Natural Language Inference,Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the tremendous recent progress on natural language inference (NLI),
driven largely by large-scale investment in new datasets (e.g., SNLI, MNLI) and
advances in modeling, most progress has been limited to English due to a lack
of reliable datasets for most of the world's languages. In this paper, we
present the first large-scale NLI dataset (consisting of ~56,000 annotated
sentence pairs) for Chinese called the Original Chinese Natural Language
Inference dataset (OCNLI). Unlike recent attempts at extending NLI to other
languages, our dataset does not rely on any automatic translation or non-expert
annotation. Instead, we elicit annotations from native speakers specializing in
linguistics. We follow closely the annotation protocol used for MNLI, but
create new strategies for eliciting diverse hypotheses. We establish several
baseline results on our dataset using state-of-the-art pre-trained models for
Chinese, and find even the best performing models to be far outpaced by human
performance (~12% absolute performance gap), making it a challenging new
resource that we hope will help to accelerate progress in Chinese NLU. To the
best of our knowledge, this is the first human-elicited MNLI-style corpus for a
non-English language.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 04:25:48 GMT'}]",2020-10-13,"[['Hu', 'Hai', ''], ['Richardson', 'Kyle', ''], ['Xu', 'Liang', ''], ['Li', 'Lu', ''], ['Kuebler', 'Sandra', ''], ['Moss', 'Lawrence S.', '']]"
1361775,2010.05445,Fahimeh Saleh Miss,"Fahimeh Saleh, Wray Buntine, Gholamreza Haffari","Collective Wisdom: Improving Low-resource Neural Machine Translation
  using Adaptive Knowledge Distillation",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Scarcity of parallel sentence-pairs poses a significant hurdle for training
high-quality Neural Machine Translation (NMT) models in bilingually
low-resource scenarios. A standard approach is transfer learning, which
involves taking a model trained on a high-resource language-pair and
fine-tuning it on the data of the low-resource MT condition of interest.
However, it is not clear generally which high-resource language-pair offers the
best transfer learning for the target MT setting. Furthermore, different
transferred models may have complementary semantic and/or syntactic strengths,
hence using only one model may be sub-optimal. In this paper, we tackle this
problem using knowledge distillation, where we propose to distill the knowledge
of ensemble of teacher models to a single student model. As the quality of
these teacher models varies, we propose an effective adaptive knowledge
distillation approach to dynamically adjust the contribution of the teacher
models during the distillation process. Experiments on transferring from a
collection of six language pairs from IWSLT to five low-resource language-pairs
from TED Talks demonstrate the effectiveness of our approach, achieving up to
+0.9 BLEU score improvement compared to strong baselines.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 04:26:46 GMT'}]",2020-10-13,"[['Saleh', 'Fahimeh', ''], ['Buntine', 'Wray', ''], ['Haffari', 'Gholamreza', '']]"
1361795,2010.05465,Najoung Kim,Najoung Kim and Tal Linzen,"COGS: A Compositional Generalization Challenge Based on Semantic
  Interpretation",Accepted to EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language is characterized by compositionality: the meaning of a
complex expression is constructed from the meanings of its constituent parts.
To facilitate the evaluation of the compositional abilities of language
processing architectures, we introduce COGS, a semantic parsing dataset based
on a fragment of English. The evaluation portion of COGS contains multiple
systematic gaps that can only be addressed by compositional generalization;
these include new combinations of familiar syntactic structures, or new
combinations of familiar words and familiar structures. In experiments with
Transformers and LSTMs, we found that in-distribution accuracy on the COGS test
set was near-perfect (96--99%), but generalization accuracy was substantially
lower (16--35%) and showed high sensitivity to random seed ($\pm$6--8%). These
findings indicate that contemporary standard NLP models are limited in their
compositional generalization capacity, and position COGS as a good way to
measure progress.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 05:45:44 GMT'}]",2020-10-13,"[['Kim', 'Najoung', ''], ['Linzen', 'Tal', '']]"
1361801,2010.05471,Qiansheng Wang,"Zhen Wang, Qiansheng Wang, Chengguo Lv, Xue Cao and Guohong Fu",Unseen Target Stance Detection with Adversarial Domain Generalization,,,10.1109/IJCNN48605.2020.9206635,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although stance detection has made great progress in the past few years, it
is still facing the problem of unseen targets. In this study, we investigate
the domain difference between targets and thus incorporate attention-based
conditional encoding with adversarial domain generalization to perform unseen
target stance detection. Experimental results show that our approach achieves
new state-of-the-art performance on the SemEval-2016 dataset, demonstrating the
importance of domain difference between targets in unseen target stance
detection.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 06:12:18 GMT'}]",2020-10-13,"[['Wang', 'Zhen', ''], ['Wang', 'Qiansheng', ''], ['Lv', 'Chengguo', ''], ['Cao', 'Xue', ''], ['Fu', 'Guohong', '']]"
1361826,2010.05496,HyeonJun Kim,HyeonJun Kim,"Feature Extraction of Text for Deep Learning Algorithms: Application on
  Fake News Dectection",8 pages,,,,cs.CL stat.ML,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Feature extraction is important process of machine learning and even deep
learning, as the process make algorithms function more efficiently, and also
accurate. In natural language processing used in deception detection such as
fake news detection, several ways of feature extraction in statistical aspect
had been introduced (e.g. N-gram). In this research, it will be shown that by
using deep learning algorithms and alphabet frequencies of the original text of
a news without any information about the sequence of the alphabet can actually
be used to classify fake news and trustworthy ones in high accuracy (85%). As
this pre-processing method make the data notably compact but also include the
feature that is needed for the classifier, it seems that alphabet frequencies
contains some useful features for understanding complex context or meaning of
the original text.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 07:43:01 GMT'}]",2020-10-13,"[['Kim', 'HyeonJun', '']]"
1361841,2010.05511,Jianhao Yan,"Lin Qiao, Jianhao Yan, Fandong Meng, Zhendong Yang, Jie Zhou","A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge
  Graph",Accepted as a regular paper in Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating a vivid, novel, and diverse essay with only several given topic
words is a challenging task of natural language generation. In previous work,
there are two problems left unsolved: neglect of sentiment beneath the text and
insufficient utilization of topic-related knowledge. Therefore, we propose a
novel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge
Graph enhanced decoder, named SCTKG, which is based on the conditional
variational autoencoder (CVAE) framework. We firstly inject the sentiment
information into the generator for controlling sentiment for each sentence,
which leads to various generated essays. Then we design a Topic Knowledge Graph
enhanced decoder. Unlike existing models that use knowledge entities
separately, our model treats the knowledge graph as a whole and encodes more
structured, connected semantic information in the graph to generate a more
relevant essay. Experimental results show that our SCTKG can generate sentiment
controllable essays and outperform the state-of-the-art approach in terms of
topic relevance, fluency, and diversity on both automatic and human evaluation.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 08:06:12 GMT'}]",2020-10-13,"[['Qiao', 'Lin', ''], ['Yan', 'Jianhao', ''], ['Meng', 'Fandong', ''], ['Yang', 'Zhendong', ''], ['Zhou', 'Jie', '']]"
1361852,2010.05522,Guirong Bai,"Guirong Bai, Shizhu He, Kang Liu, Jun Zhao, Zaiqing Nie",Pre-trained Language Model Based Active Learning for Sentence Matching,Accepted by the conference of coling 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Active learning is able to significantly reduce the annotation cost for
data-driven techniques. However, previous active learning approaches for
natural language processing mainly depend on the entropy-based uncertainty
criterion, and ignore the characteristics of natural language. In this paper,
we propose a pre-trained language model based active learning approach for
sentence matching. Differing from previous active learning, it can provide
linguistic criteria to measure instances and help select more efficient
instances for annotation. Experiments demonstrate our approach can achieve
greater accuracy with fewer labeled training instances.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 08:24:36 GMT'}]",2020-10-13,"[['Bai', 'Guirong', ''], ['He', 'Shizhu', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', ''], ['Nie', 'Zaiqing', '']]"
1361863,2010.05533,Cunliang Kong,"Cunliang Kong, Liner Yang, Tianzuo Zhang, Qinan Fan, Zhenghao Liu, Yun
  Chen, Erhong Yang",Toward Cross-Lingual Definition Generation for Language Learners,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating dictionary definitions automatically can prove useful for language
learners. However, it's still a challenging task of cross-lingual definition
generation. In this work, we propose to generate definitions in English for
words in various languages. To achieve this, we present a simple yet effective
approach based on publicly available pretrained language models. In this
approach, models can be directly applied to other languages after trained on
the English dataset. We demonstrate the effectiveness of this approach on
zero-shot definition generation. Experiments and manual analyses on newly
constructed datasets show that our models have a strong cross-lingual transfer
ability and can generate fluent English definitions for Chinese words. We
further measure the lexical complexity of generated and reference definitions.
The results show that the generated definitions are much simpler, which is more
suitable for language learners.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 08:45:28 GMT'}]",2020-10-13,"[['Kong', 'Cunliang', ''], ['Yang', 'Liner', ''], ['Zhang', 'Tianzuo', ''], ['Fan', 'Qinan', ''], ['Liu', 'Zhenghao', ''], ['Chen', 'Yun', ''], ['Yang', 'Erhong', '']]"
1361872,2010.05542,Dawn Knight,"Dawn Knight, Steve Morris, Tess Fitzpatrick, Paul Rayson, Irena
  Spasi\'c, Enlli M\^on Thomas","The National Corpus of Contemporary Welsh: Project Report | Y Corpws
  Cenedlaethol Cymraeg Cyfoes: Adroddiad y Prosiect",English-Welsh bilingual project report,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This report provides an overview of the CorCenCC project and the online
corpus resource that was developed as a result of work on the project. The
report lays out the theoretical underpinnings of the research, demonstrating
how the project has built on and extended this theory. We also raise and
discuss some of the key operational questions that arose during the course of
the project, outlining the ways in which they were answered, the impact of
these decisions on the resource that has been produced and the longer-term
contribution they will make to practices in corpus-building. Finally, we
discuss some of the applications and the utility of the work, outlining the
impact that CorCenCC is set to have on a range of different individuals and
user groups.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 08:57:58 GMT'}]",2020-10-13,"[['Knight', 'Dawn', ''], ['Morris', 'Steve', ''], ['Fitzpatrick', 'Tess', ''], ['Rayson', 'Paul', ''], ['Spasić', 'Irena', ''], ['Thomas', 'Enlli Môn', '']]"
1361879,2010.05549,Yash Sharma,"Yash Sharma, Basil Abraham, Karan Taneja, Preethi Jyothi","Improving Low Resource Code-switched ASR using Augmented Code-switched
  TTS","Interspeech 2020, 5 pages",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building Automatic Speech Recognition (ASR) systems for code-switched speech
has recently gained renewed attention due to the widespread use of speech
technologies in multilingual communities worldwide. End-to-end ASR systems are
a natural modeling choice due to their ease of use and superior performance in
monolingual settings. However, it is well known that end-to-end systems require
large amounts of labeled speech. In this work, we investigate improving
code-switched ASR in low resource settings via data augmentation using
code-switched text-to-speech (TTS) synthesis. We propose two targeted
techniques to effectively leverage TTS speech samples: 1) Mixup, an existing
technique to create new training samples via linear interpolation of existing
samples, applied to TTS and real speech samples, and 2) a new loss function,
used in conjunction with TTS samples, to encourage code-switched predictions.
We report significant improvements in ASR performance achieving absolute word
error rate (WER) reductions of up to 5%, and measurable improvement in code
switching using our proposed techniques on a Hindi-English code-switched ASR
task.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 09:15:12 GMT'}]",2020-10-13,"[['Sharma', 'Yash', ''], ['Abraham', 'Basil', ''], ['Taneja', 'Karan', ''], ['Jyothi', 'Preethi', '']]"
1361897,2010.05567,Rahul Aralikatte,"Rahul Aralikatte, Mostafa Abdou, Heather Lent, Daniel Hershcovich,
  Anders S{\o}gaard",Joint Semantic Analysis with Document-Level Cross-Task Coherence Rewards,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Coreference resolution and semantic role labeling are NLP tasks that capture
different aspects of semantics, indicating respectively, which expressions
refer to the same entity, and what semantic roles expressions serve in the
sentence. However, they are often closely interdependent, and both generally
necessitate natural language understanding. Do they form a coherent abstract
representation of documents? We present a neural network architecture for joint
coreference resolution and semantic role labeling for English, and train graph
neural networks to model the 'coherence' of the combined shallow semantic
graph. Using the resulting coherence score as a reward for our joint semantic
analyzer, we use reinforcement learning to encourage global coherence over the
document and between semantic annotations. This leads to improvements on both
tasks in multiple datasets from different domains, and across a range of
encoders of different expressivity, calling, we believe, for a more holistic
approach to semantics in NLP.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 09:36:24 GMT'}]",2020-10-13,"[['Aralikatte', 'Rahul', ''], ['Abdou', 'Mostafa', ''], ['Lent', 'Heather', ''], ['Hershcovich', 'Daniel', ''], ['Søgaard', 'Anders', '']]"
1349732,2009.08229,Xinyu Wang,"Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei
  Huang, Kewei Tu","AIN: Fast and Accurate Sequence Labeling with Approximate Inference
  Network","Accept to Main Conference of EMNLP 2020 (Short). Camera-ready, 8
  Pages",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The linear-chain Conditional Random Field (CRF) model is one of the most
widely-used neural sequence labeling approaches. Exact probabilistic inference
algorithms such as the forward-backward and Viterbi algorithms are typically
applied in training and prediction stages of the CRF model. However, these
algorithms require sequential computation that makes parallelization
impossible. In this paper, we propose to employ a parallelizable approximate
variational inference algorithm for the CRF model. Based on this algorithm, we
design an approximate inference network that can be connected with the encoder
of the neural CRF model to form an end-to-end network, which is amenable to
parallelization for faster training and prediction. The empirical results show
that our proposed approaches achieve a 12.7-fold improvement in decoding speed
with long sentences and a competitive accuracy compared with the traditional
CRF approach.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 12:18:43 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 11:59:20 GMT'}]",2020-10-13,"[['Wang', 'Xinyu', ''], ['Jiang', 'Yong', ''], ['Bach', 'Nguyen', ''], ['Wang', 'Tao', ''], ['Huang', 'Zhongqiang', ''], ['Huang', 'Fei', ''], ['Tu', 'Kewei', '']]"
1361902,2010.05572,Suranjana Samanta,"Debanjana Kar, Suranjana Samanta, Amar Prakash Azad",Meta-Context Transformers for Domain-Specific Response Generation,"7+2 pages, 6 figures, 4 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the tremendous success of neural dialogue models in recent years, it
suffers a lack of relevance, diversity, and some times coherence in generated
responses. Lately, transformer-based models, such as GPT-2, have revolutionized
the landscape of dialogue generation by capturing the long-range structures
through language modeling. Though these models have exhibited excellent
language coherence, they often lack relevance and terms when used for
domain-specific response generation. In this paper, we present DSRNet (Domain
Specific Response Network), a transformer-based model for dialogue response
generation by reinforcing domain-specific attributes. In particular, we extract
meta attributes from context and infuse them with the context utterances for
better attention over domain-specific key terms and relevance. We study the use
of DSRNet in a multi-turn multi-interlocutor environment for domain-specific
response generation. In our experiments, we evaluate DSRNet on Ubuntu dialogue
datasets, which are mainly composed of various technical domain related
dialogues for IT domain issue resolutions and also on CamRest676 dataset, which
contains restaurant domain conversations. Trained with maximum likelihood
objective, our model shows significant improvement over the state-of-the-art
for multi-turn dialogue systems supported by better BLEU and semantic
similarity (BertScore) scores. Besides, we also observe that the responses
produced by our model carry higher relevance due to the presence of
domain-specific key attributes that exhibit better overlap with the attributes
of the context. Our analysis shows that the performance improvement is mostly
due to the infusion of key terms along with dialogues which result in better
attention over domain-relevant terms. Other contributing factors include joint
modeling of dialogue context with the domain-specific meta attributes and
topics.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 09:49:27 GMT'}]",2020-10-13,"[['Kar', 'Debanjana', ''], ['Samanta', 'Suranjana', ''], ['Azad', 'Amar Prakash', '']]"
1329245,2008.01523,Haiyue Song,"Akiko Aizawa, Frederic Bergeron, Junjie Chen, Fei Cheng, Katsuhiko
  Hayashi, Kentaro Inui, Hiroyoshi Ito, Daisuke Kawahara, Masaru Kitsuregawa,
  Hirokazu Kiyomaru, Masaki Kobayashi, Takashi Kodama, Sadao Kurohashi,
  Qianying Liu, Masaki Matsubara, Yusuke Miyao, Atsuyuki Morishima, Yugo
  Murawaki, Kazumasa Omura, Haiyue Song, Eiichiro Sumita, Shinji Suzuki, Ribeka
  Tanaka, Yu Tanaka, Masashi Toyoda, Nobuhiro Ueda, Honai Ueoka, Masao Utiyama,
  Ying Zhong",A System for Worldwide COVID-19 Information Aggregation,Accepted to EMNLP 2020 Workshop NLP-COVID,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The global pandemic of COVID-19 has made the public pay close attention to
related news, covering various domains, such as sanitation, treatment, and
effects on education. Meanwhile, the COVID-19 condition is very different among
the countries (e.g., policies and development of the epidemic), and thus
citizens would be interested in news in foreign countries. We build a system
for worldwide COVID-19 information aggregation containing reliable articles
from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related
website dataset collected through crowdsourcing ensures the quality of the
articles. A neural machine translation module translates articles in other
languages into Japanese and English. A BERT-based topic-classifier trained on
our article-topic pair dataset helps users find their interested information
efficiently by putting articles into different categories.
","[{'version': 'v1', 'created': 'Tue, 28 Jul 2020 01:33:54 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 05:36:36 GMT'}]",2020-10-13,"[['Aizawa', 'Akiko', ''], ['Bergeron', 'Frederic', ''], ['Chen', 'Junjie', ''], ['Cheng', 'Fei', ''], ['Hayashi', 'Katsuhiko', ''], ['Inui', 'Kentaro', ''], ['Ito', 'Hiroyoshi', ''], ['Kawahara', 'Daisuke', ''], ['Kitsuregawa', 'Masaru', ''], ['Kiyomaru', 'Hirokazu', ''], ['Kobayashi', 'Masaki', ''], ['Kodama', 'Takashi', ''], ['Kurohashi', 'Sadao', ''], ['Liu', 'Qianying', ''], ['Matsubara', 'Masaki', ''], ['Miyao', 'Yusuke', ''], ['Morishima', 'Atsuyuki', ''], ['Murawaki', 'Yugo', ''], ['Omura', 'Kazumasa', ''], ['Song', 'Haiyue', ''], ['Sumita', 'Eiichiro', ''], ['Suzuki', 'Shinji', ''], ['Tanaka', 'Ribeka', ''], ['Tanaka', 'Yu', ''], ['Toyoda', 'Masashi', ''], ['Ueda', 'Nobuhiro', ''], ['Ueoka', 'Honai', ''], ['Utiyama', 'Masao', ''], ['Zhong', 'Ying', '']]"
1362087,2010.05757,Carsten Eickhoff,"Aaron S. Eisman, Nishant R. Shah, Carsten Eickhoff, George Zerveas,
  Elizabeth S. Chen, Wen-Chih Wu, Indra Neil Sarkar","Extracting Angina Symptoms from Clinical Notes Using Pre-Trained
  Transformer Architectures",,AMIA Annual Symposium 2020,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Anginal symptoms can connote increased cardiac risk and a need for change in
cardiovascular management. This study evaluated the potential to extract these
symptoms from physician notes using the Bidirectional Encoder from Transformers
language model fine-tuned on a domain-specific corpus. The history of present
illness section of 459 expert annotated primary care physician notes from
consecutive patients referred for cardiac testing without known atherosclerotic
cardiovascular disease were included. Notes were annotated for positive and
negative mentions of chest pain and shortness of breath characterization. The
results demonstrate high sensitivity and specificity for the detection of chest
pain or discomfort, substernal chest pain, shortness of breath, and dyspnea on
exertion. Small sample size limited extracting factors related to provocation
and palliation of chest pain. This study provides a promising starting point
for the natural language processing of physician notes to characterize
clinically actionable anginal symptoms.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 14:53:32 GMT'}]",2020-10-13,"[['Eisman', 'Aaron S.', ''], ['Shah', 'Nishant R.', ''], ['Eickhoff', 'Carsten', ''], ['Zerveas', 'George', ''], ['Chen', 'Elizabeth S.', ''], ['Wu', 'Wen-Chih', ''], ['Sarkar', 'Indra Neil', '']]"
1362104,2010.05774,Sagar Samtani,"Sagar Samtani, Hongyi Zhu, Balaji Padmanabhan, Yidong Chai, Hsinchun
  Chen",Deep Learning for Information Systems Research,"56 pages total, 1 page title and authors, 42 pages main text, 13
  pages appendix",,,,cs.LG cs.CL stat.ME stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Artificial Intelligence (AI) has rapidly emerged as a key disruptive
technology in the 21st century. At the heart of modern AI lies Deep Learning
(DL), an emerging class of algorithms that has enabled today's platforms and
organizations to operate at unprecedented efficiency, effectiveness, and scale.
Despite significant interest, IS contributions in DL have been limited, which
we argue is in part due to issues with defining, positioning, and conducting DL
research. Recognizing the tremendous opportunity here for the IS community,
this work clarifies, streamlines, and presents approaches for IS scholars to
make timely and high-impact contributions. Related to this broader goal, this
paper makes five timely contributions. First, we systematically summarize the
major components of DL in a novel Deep Learning for Information Systems
Research (DL-ISR) schematic that illustrates how technical DL processes are
driven by key factors from an application environment. Second, we present a
novel Knowledge Contribution Framework (KCF) to help IS scholars position their
DL contributions for maximum impact. Third, we provide ten guidelines to help
IS scholars generate rigorous and relevant DL-ISR in a systematic, high-quality
fashion. Fourth, we present a review of prevailing journal and conference
venues to examine how IS scholars have leveraged DL for various research
inquiries. Finally, we provide a unique perspective on how IS scholars can
formulate DL-ISR inquiries by carefully considering the interplay of business
function(s), application areas(s), and the KCF. This perspective intentionally
emphasizes inter-disciplinary, intra-disciplinary, and cross-IS tradition
perspectives. Taken together, these contributions provide IS scholars a timely
framework to advance the scale, scope, and impact of deep learning research.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 15:23:05 GMT'}]",2020-10-13,"[['Samtani', 'Sagar', ''], ['Zhu', 'Hongyi', ''], ['Padmanabhan', 'Balaji', ''], ['Chai', 'Yidong', ''], ['Chen', 'Hsinchun', '']]"
1268862,2004.04092,Chunyuan Li,"Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang,
  Jianfeng Gao",Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space,"Accepted in EMNLP 2020; Code: https://github.com/ChunyuanLI/Optimus
  Demo: http://aka.ms/optimus",,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When trained effectively, the Variational Autoencoder (VAE) can be both a
powerful generative model and an effective representation learning framework
for natural language. In this paper, we propose the first large-scale language
VAE model, Optimus. A universal latent embedding space for sentences is first
pre-trained on large text corpus, and then fine-tuned for various language
generation and understanding tasks. Compared with GPT-2, Optimus enables guided
language generation from an abstract level using the latent vectors. Compared
with BERT, Optimus can generalize better on low-resource language understanding
tasks due to the smooth latent space structure. Extensive experimental results
on a wide range of language tasks demonstrate the effectiveness of Optimus. It
achieves new state-of-the-art on VAE language modeling benchmarks. We hope that
our first pre-trained big VAE language model itself and results can help the
NLP community renew the interests of deep generative models in the era of
large-scale pre-training, and make these principled methods more practical.
","[{'version': 'v1', 'created': 'Sun, 5 Apr 2020 06:20:18 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 19:11:42 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Oct 2020 00:41:43 GMT'}, {'version': 'v4', 'created': 'Sun, 11 Oct 2020 23:33:10 GMT'}]",2020-10-13,"[['Li', 'Chunyuan', ''], ['Gao', 'Xiang', ''], ['Li', 'Yuan', ''], ['Peng', 'Baolin', ''], ['Li', 'Xiujun', ''], ['Zhang', 'Yizhe', ''], ['Gao', 'Jianfeng', '']]"
1196285,1910.12366,Kalpesh Krishna,"Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas
  Papernot, Mohit Iyyer",Thieves on Sesame Street! Model Extraction of BERT-based APIs,ICLR 2020 Camera Ready (19 pages),,,,cs.CL cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the problem of model extraction in natural language processing, in
which an adversary with only query access to a victim model attempts to
reconstruct a local copy of that model. Assuming that both the adversary and
victim model fine-tune a large pretrained language model such as BERT (Devlin
et al. 2019), we show that the adversary does not need any real training data
to successfully mount the attack. In fact, the attacker need not even use
grammatical or semantically meaningful queries: we show that random sequences
of words coupled with task-specific heuristics form effective queries for model
extraction on a diverse set of NLP tasks, including natural language inference
and question answering. Our work thus highlights an exploit only made feasible
by the shift towards transfer learning methods within the NLP community: for a
query budget of a few hundred dollars, an attacker can extract a model that
performs only slightly worse than the victim model. Finally, we study two
defense strategies against model extraction---membership classification and API
watermarking---which while successful against naive adversaries, are
ineffective against more sophisticated ones.
","[{'version': 'v1', 'created': 'Sun, 27 Oct 2019 22:09:13 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Jan 2020 03:20:52 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Oct 2020 12:14:05 GMT'}]",2020-10-13,"[['Krishna', 'Kalpesh', ''], ['Tomar', 'Gaurav Singh', ''], ['Parikh', 'Ankur P.', ''], ['Papernot', 'Nicolas', ''], ['Iyyer', 'Mohit', '']]"
1355336,2009.13833,Gaurav Arora,"Gaurav Arora, Chirag Jain, Manas Chaturvedi, Krupal Modi",HINT3: Raising the bar for Intent Detection in the Wild,Accepted at EMNLP-2020's Insights workshop,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Intent Detection systems in the real world are exposed to complexities of
imbalanced datasets containing varying perception of intent, unintended
correlations and domain-specific aberrations. To facilitate benchmarking which
can reflect near real-world scenarios, we introduce 3 new datasets created from
live chatbots in diverse domains. Unlike most existing datasets that are
crowdsourced, our datasets contain real user queries received by the chatbots
and facilitates penalising unwanted correlations grasped during the training
process. We evaluate 4 NLU platforms and a BERT based classifier and find that
performance saturates at inadequate levels on test sets because all systems
latch on to unintended patterns in training data.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 07:44:37 GMT'}, {'version': 'v2', 'created': 'Sat, 10 Oct 2020 07:52:18 GMT'}]",2020-10-13,"[['Arora', 'Gaurav', ''], ['Jain', 'Chirag', ''], ['Chaturvedi', 'Manas', ''], ['Modi', 'Krupal', '']]"
1279616,2004.14846,Elizabeth Nielsen,"Elizabeth Nielsen, Mark Steedman, Sharon Goldwater",The role of context in neural pitch accent detection in English,,"Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing",,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prosody is a rich information source in natural language, serving as a marker
for phenomena such as contrast. In order to make this information available to
downstream tasks, we need a way to detect prosodic events in speech. We propose
a new model for pitch accent detection, inspired by the work of Stehwien et al.
(2018), who presented a CNN-based model for this task. Our model makes greater
use of context by using full utterances as input and adding an LSTM layer. We
find that these innovations lead to an improvement from 87.5% to 88.7% accuracy
on pitch accent detection on American English speech in the Boston University
Radio News Corpus, a state-of-the-art result. We also find that a simple
baseline that just predicts a pitch accent on every content word yields 82.2%
accuracy, and we suggest that this is the appropriate baseline for this task.
Finally, we conduct ablation tests that show pitch is the most important
acoustic feature for this task and this corpus.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 14:59:05 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 08:25:23 GMT'}]",2020-10-13,"[['Nielsen', 'Elizabeth', ''], ['Steedman', 'Mark', ''], ['Goldwater', 'Sharon', '']]"
1279654,2004.14884,Arthur Bra\v{z}inskas,"Arthur Bra\v{z}inskas, Mirella Lapata, Ivan Titov",Few-Shot Learning for Opinion Summarization,EMNLP 2020,,,,cs.LG cs.CL cs.NE stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Opinion summarization is the automatic creation of text reflecting subjective
information expressed in multiple documents, such as user reviews of a product.
The task is practically important and has attracted a lot of attention.
However, due to the high cost of summary production, datasets large enough for
training supervised models are lacking. Instead, the task has been
traditionally approached with extractive methods that learn to select text
fragments in an unsupervised or weakly-supervised way. Recently, it has been
shown that abstractive summaries, potentially more fluent and better at
reflecting conflicting information, can also be produced in an unsupervised
fashion. However, these models, not being exposed to actual summaries, fail to
capture their essential properties. In this work, we show that even a handful
of summaries is sufficient to bootstrap generation of the summary text with all
expected properties, such as writing style, informativeness, fluency, and
sentiment preservation. We start by training a conditional Transformer language
model to generate a new product review given other available reviews of the
product. The model is also conditioned on review properties that are directly
related to summaries; the properties are derived from reviews with no manual
effort. In the second stage, we fine-tune a plug-in module that learns to
predict property values on a handful of summaries. This lets us switch the
generator to the summarization mode. We show on Amazon and Yelp datasets that
our approach substantially outperforms previous extractive and abstractive
methods in automatic and human evaluation.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 15:37:38 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 19:45:25 GMT'}, {'version': 'v3', 'created': 'Sat, 10 Oct 2020 06:30:38 GMT'}]",2020-10-13,"[['Bražinskas', 'Arthur', ''], ['Lapata', 'Mirella', ''], ['Titov', 'Ivan', '']]"
1279737,2004.14967,Hannah Rashkin,"Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and Jianfeng Gao","PlotMachines: Outline-Conditioned Generation with Dynamic Plot State
  Tracking",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose the task of outline-conditioned story generation: given an outline
as a set of phrases that describe key characters and events to appear in a
story, the task is to generate a coherent narrative that is consistent with the
provided outline. This task is challenging as the input only provides a rough
sketch of the plot, and thus, models need to generate a story by interweaving
the key points provided in the outline. This requires the model to keep track
of the dynamic states of the latent plot, conditioning on the input outline
while generating the full story. We present PlotMachines, a neural narrative
model that learns to transform an outline into a coherent story by tracking the
dynamic plot states. In addition, we enrich PlotMachines with high-level
discourse structure so that the model can learn different writing styles
corresponding to different parts of the narrative. Comprehensive experiments
over three fiction and non-fiction datasets demonstrate that large-scale
language models, such as GPT-2 and Grover, despite their impressive generation
performance, are not sufficient in generating coherent narratives for the given
outline, and dynamic plot state tracking is important for composing narratives
with tighter, more consistent plots.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:16:31 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 23:40:11 GMT'}]",2020-10-13,"[['Rashkin', 'Hannah', ''], ['Celikyilmaz', 'Asli', ''], ['Choi', 'Yejin', ''], ['Gao', 'Jianfeng', '']]"
1279773,2004.15003,Sho Yokoi,"Sho Yokoi, Ryo Takahashi, Reina Akama, Jun Suzuki, Kentaro Inui",Word Rotator's Distance,"17 pages, accepted at EMNLP",EMNLP 2020,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One key principle for assessing textual similarity is measuring the degree of
semantic overlap between two texts by considering the word alignment. Such
alignment-based approaches are both intuitive and interpretable; however, they
are empirically inferior to the simple cosine similarity between
general-purpose sentence vectors. To remedy this, we focus on the fact that the
norm of word vectors is a good proxy for word importance, and the angle of them
is a good proxy for word similarity. Alignment-based approaches do not
distinguish the norm and direction, whereas sentence-vector approaches
automatically use the norm as the word importance. Accordingly, we propose to
decouple word vectors into their norm and direction then computing the
alignment-based similarity using earth mover's distance (optimal transport
cost), which we refer to as word rotator's distance. Furthermore, we
demonstrate how to grow the norm and direction of word vectors (vector
converter); this is a new systematic approach derived from the sentence-vector
estimation methods, which can significantly improve the performance of the
proposed method. On several STS benchmarks, our simple proposed methods
outperformed not only alignment-based approaches but also strong baselines.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:48:42 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 17:56:57 GMT'}]",2020-10-13,"[['Yokoi', 'Sho', ''], ['Takahashi', 'Ryo', ''], ['Akama', 'Reina', ''], ['Suzuki', 'Jun', ''], ['Inui', 'Kentaro', '']]"
1321535,2007.10021,Abhishek Singh,Abhishek Singh and Surya Pratap Singh Parmar,"Voice@SRIB at SemEval-2020 Task 9 and 12: Stacked Ensembling method for
  Sentiment and Offensiveness detection in Social Media","Changed title and few more changes. This version will be published in
  SemEval2020. Added code Link",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  In social-media platforms such as Twitter, Facebook, and Reddit, people
prefer to use code-mixed language such as Spanish-English, Hindi-English to
express their opinions. In this paper, we describe different models we used,
using the external dataset to train embeddings, ensembling methods for
Sentimix, and OffensEval tasks. The use of pre-trained embeddings usually helps
in multiple tasks such as sentence classification, and machine translation. In
this experiment, we haveused our trained code-mixed embeddings and twitter
pre-trained embeddings to SemEval tasks. We evaluate our models on macro
F1-score, precision, accuracy, and recall on the datasets. We intend to show
that hyper-parameter tuning and data pre-processing steps help a lot in
improving the scores. In our experiments, we are able to achieve 0.886 F1-Macro
on OffenEval Greek language subtask post-evaluation, whereas the highest is
0.852 during the Evaluation Period. We stood third in Spanglish competition
with our best F1-score of 0.756. Codalab username is asking28.
","[{'version': 'v1', 'created': 'Mon, 20 Jul 2020 11:54:43 GMT'}, {'version': 'v2', 'created': 'Sat, 5 Sep 2020 17:06:19 GMT'}, {'version': 'v3', 'created': 'Sun, 11 Oct 2020 10:02:35 GMT'}]",2020-10-13,"[['Singh', 'Abhishek', ''], ['Parmar', 'Surya Pratap Singh', '']]"
1266623,2004.01853,Yanyan Zou,"Yanyan Zou, Xingxing Zhang, Wei Lu, Furu Wei and Ming Zhou","Pre-training for Abstractive Document Summarization by Reinstating
  Source Text","EMNLP2020 Camera-Ready, 15 pages",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Abstractive document summarization is usually modeled as a
sequence-to-sequence (Seq2Seq) learning problem. Unfortunately, training large
Seq2Seq based summarization models on limited supervised summarization data is
challenging. This paper presents three pre-training objectives which allow us
to pre-train a Seq2Seq based abstractive summarization model on unlabeled text.
The main idea is that, given an input text artificially constructed from a
document, a model is pre-trained to reinstate the original document. These
objectives include sentence reordering, next sentence generation, and masked
document generation, which have close relations with the abstractive document
summarization task. Experiments on two benchmark summarization datasets (i.e.,
CNN/DailyMail and New York Times) show that all three objectives can improve
performance upon baselines. Compared to models pre-trained on large-scale data
(more than 160GB), our method, with only 19GB text for pre-training, achieves
comparable results, which demonstrates its effectiveness.
","[{'version': 'v1', 'created': 'Sat, 4 Apr 2020 05:06:26 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 07:25:33 GMT'}, {'version': 'v3', 'created': 'Fri, 2 Oct 2020 17:27:19 GMT'}, {'version': 'v4', 'created': 'Sun, 11 Oct 2020 14:53:42 GMT'}]",2020-10-13,"[['Zou', 'Yanyan', ''], ['Zhang', 'Xingxing', ''], ['Lu', 'Wei', ''], ['Wei', 'Furu', ''], ['Zhou', 'Ming', '']]"
1152486,1907.07950,Miryam de Lhoneux,"Miryam de Lhoneux, Sara Stymne, Joakim Nivre","What Should/Do/Can LSTMs Learn When Parsing Auxiliary Verb
  Constructions?",Accepted by the Computational Linguistics journal,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is a growing interest in investigating what neural NLP models learn
about language. A prominent open question is the question of whether or not it
is necessary to model hierarchical structure. We present a linguistic
investigation of a neural parser adding insights to this question. We look at
transitivity and agreement information of auxiliary verb constructions (AVCs)
in comparison to finite main verbs (FMVs). This comparison is motivated by
theoretical work in dependency grammar and in particular the work of Tesni\`ere
(1959) where AVCs and FMVs are both instances of a nucleus, the basic unit of
syntax. An AVC is a dissociated nucleus, it consists of at least two words, and
an FMV is its non-dissociated counterpart, consisting of exactly one word. We
suggest that the representation of AVCs and FMVs should capture similar
information. We use diagnostic classifiers to probe agreement and transitivity
information in vectors learned by a transition-based neural parser in four
typologically different languages. We find that the parser learns different
information about AVCs and FMVs if only sequential models (BiLSTMs) are used in
the architecture but similar information when a recursive layer is used. We
find explanations for why this is the case by looking closely at how
information is learned in the network and looking at what happens with
different dependency representations of AVCs. We conclude that there may be
benefits to using a recursive layer in dependency parsing and that we have not
yet found the best way to integrate it in our parsers.
","[{'version': 'v1', 'created': 'Thu, 18 Jul 2019 09:37:38 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 07:31:52 GMT'}]",2020-10-13,"[['de Lhoneux', 'Miryam', ''], ['Stymne', 'Sara', ''], ['Nivre', 'Joakim', '']]"
1264510,2003.14155,Roman Klinger,"Jan Hofmann, Enrica Troiano, Kai Sassenberg, and Roman Klinger",Appraisal Theories for Emotion Classification in Text,Accepted at COLING 2020,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Automatic emotion categorization has been predominantly formulated as text
classification in which textual units are assigned to an emotion from a
predefined inventory, for instance following the fundamental emotion classes
proposed by Paul Ekman (fear, joy, anger, disgust, sadness, surprise) or Robert
Plutchik (adding trust, anticipation). This approach ignores existing
psychological theories to some degree, which provide explanations regarding the
perception of events. For instance, the description that somebody discovers a
snake is associated with fear, based on the appraisal as being an unpleasant
and non-controllable situation. This emotion reconstruction is even possible
without having access to explicit reports of a subjective feeling (for instance
expressing this with the words ""I am afraid.""). Automatic classification
approaches therefore need to learn properties of events as latent variables
(for instance that the uncertainty and the mental or physical effort associated
with the encounter of a snake leads to fear). With this paper, we propose to
make such interpretations of events explicit, following theories of cognitive
appraisal of events, and show their potential for emotion classification when
being encoded in classification models. Our results show that high quality
appraisal dimension assignments in event descriptions lead to an improvement in
the classification of discrete emotion categories. We make our corpus of
appraisal-annotated emotion-associated event descriptions publicly available.
","[{'version': 'v1', 'created': 'Tue, 31 Mar 2020 12:43:54 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Apr 2020 09:43:12 GMT'}, {'version': 'v3', 'created': 'Tue, 7 Apr 2020 08:14:17 GMT'}, {'version': 'v4', 'created': 'Thu, 28 May 2020 15:48:56 GMT'}, {'version': 'v5', 'created': 'Mon, 12 Oct 2020 11:45:01 GMT'}]",2020-10-13,"[['Hofmann', 'Jan', ''], ['Troiano', 'Enrica', ''], ['Sassenberg', 'Kai', ''], ['Klinger', 'Roman', '']]"
1283549,2005.03754,Esin Durmus,Esin Durmus and He He and Mona Diab,"FEQA: A Question Answering Evaluation Framework for Faithfulness
  Assessment in Abstractive Summarization",Accepted to ACL 2020,,10.18653/v1/2020.acl-main.454,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural abstractive summarization models are prone to generate content
inconsistent with the source document, i.e. unfaithful. Existing automatic
metrics do not capture such mistakes effectively. We tackle the problem of
evaluating faithfulness of a generated summary given its source document. We
first collected human annotations of faithfulness for outputs from numerous
models on two datasets. We find that current models exhibit a trade-off between
abstractiveness and faithfulness: outputs with less word overlap with the
source document are more likely to be unfaithful. Next, we propose an automatic
question answering (QA) based metric for faithfulness, FEQA, which leverages
recent advances in reading comprehension. Given question-answer pairs generated
from the summary, a QA model extracts answers from the document; non-matched
answers indicate unfaithful information in the summary. Among metrics based on
word overlap, embedding similarity, and learned language understanding models,
our QA-based metric has significantly higher correlation with human
faithfulness scores, especially on highly abstractive summaries.
","[{'version': 'v1', 'created': 'Thu, 7 May 2020 21:00:08 GMT'}]",2020-10-13,"[['Durmus', 'Esin', ''], ['He', 'He', ''], ['Diab', 'Mona', '']]"
1354315,2009.12812,Wei Zhang,"Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun
  Liu",TernaryBERT: Distillation-aware Ultra-low Bit BERT,Accepted by EMNLP 2020,,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer-based pre-training models like BERT have achieved remarkable
performance in many natural language processing tasks.However, these models are
both computation and memory expensive, hindering their deployment to
resource-constrained devices. In this work, we propose TernaryBERT, which
ternarizes the weights in a fine-tuned BERT model. Specifically, we use both
approximation-based and loss-aware ternarization methods and empirically
investigate the ternarization granularity of different parts of BERT. Moreover,
to reduce the accuracy degradation caused by the lower capacity of low bits, we
leverage the knowledge distillation technique in the training process.
Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT
outperforms the other BERT quantization methods, and even achieves comparable
performance as the full-precision model while being 14.9x smaller.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 10:17:28 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Sep 2020 01:56:35 GMT'}, {'version': 'v3', 'created': 'Sat, 10 Oct 2020 07:24:54 GMT'}]",2020-10-13,"[['Zhang', 'Wei', ''], ['Hou', 'Lu', ''], ['Yin', 'Yichun', ''], ['Shang', 'Lifeng', ''], ['Chen', 'Xiao', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', '']]"
1354037,2009.12534,Gaurav Arora,Gaurav Arora,iNLTK: Natural Language Toolkit for Indic Languages,Accepted at EMNLP2020's NLP-OSS workshop,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present iNLTK, an open-source NLP library consisting of pre-trained
language models and out-of-the-box support for Data Augmentation, Textual
Similarity, Sentence Embeddings, Word Embeddings, Tokenization and Text
Generation in 13 Indic Languages. By using pre-trained models from iNLTK for
text classification on publicly available datasets, we significantly outperform
previously reported results. On these datasets, we also show that by using
pre-trained models and data augmentation from iNLTK, we can achieve more than
95% of the previous best performance by using less than 10% of the training
data. iNLTK is already being widely used by the community and has 40,000+
downloads, 600+ stars and 100+ forks on GitHub. The library is available at
https://github.com/goru001/inltk.
","[{'version': 'v1', 'created': 'Sat, 26 Sep 2020 08:21:32 GMT'}, {'version': 'v2', 'created': 'Sat, 10 Oct 2020 07:46:21 GMT'}]",2020-10-13,"[['Arora', 'Gaurav', '']]"
1208948,1911.10354,Kohei Uehara,"Kohei Uehara, Tatsuya Harada",Unsupervised Keyword Extraction for Full-sentence VQA,EMNLP 2020 workshop: NLP Beyond Text (NLPBT),,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the majority of the existing Visual Question Answering (VQA) research, the
answers consist of short, often single words, as per instructions given to the
annotators during dataset construction. This study envisions a VQA task for
natural situations, where the answers are more likely to be sentences rather
than single words. To bridge the gap between this natural VQA and existing VQA
approaches, a novel unsupervised keyword extraction method is proposed. The
method is based on the principle that the full-sentence answers can be
decomposed into two parts: one that contains new information answering the
question (i.e., keywords), and one that contains information already included
in the question. Discriminative decoders were designed to achieve such
decomposition, and the method was experimentally implemented on VQA datasets
containing full-sentence answers. The results show that the proposed model can
accurately extract the keywords without being given explicit annotations
describing them.
","[{'version': 'v1', 'created': 'Sat, 23 Nov 2019 12:18:03 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Mar 2020 07:37:34 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Oct 2020 09:20:30 GMT'}]",2020-10-13,"[['Uehara', 'Kohei', ''], ['Harada', 'Tatsuya', '']]"
1236549,2001.11136,Ivan Vuli\'c,"Haim Dubossarsky, Ivan Vuli\'c, Roi Reichart, Anna Korhonen","The Secret is in the Spectra: Predicting Cross-lingual Task Performance
  with Spectral Similarity Measures",EMNLP 2020: Long paper,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of
languages at hand: e.g., previous work has suggested there is a connection
between the expected success of bilingual lexicon induction (BLI) and the
assumption of (approximate) isomorphism between monolingual embedding spaces.
In this work we present a large-scale study focused on the correlations between
monolingual embedding space similarity and task performance, covering thousands
of language pairs and four different tasks: BLI, parsing, POS tagging and MT.
We hypothesize that statistics of the spectrum of each monolingual embedding
space indicate how well they can be aligned. We then introduce several
isomorphism measures between two embedding spaces, based on the relevant
statistics of their individual spectra. We empirically show that 1) language
similarity scores derived from such spectral isomorphism measures are strongly
associated with performance observed in different cross-lingual tasks, and 2)
our spectral-based measures consistently outperform previous standard
isomorphism measures, while being computationally more tractable and easier to
interpret. Finally, our measures capture complementary information to
typologically driven language distance measures, and the combination of
measures from the two families yields even higher task performance
correlations.
","[{'version': 'v1', 'created': 'Thu, 30 Jan 2020 00:09:53 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 15:13:13 GMT'}]",2020-10-13,"[['Dubossarsky', 'Haim', ''], ['Vulić', 'Ivan', ''], ['Reichart', 'Roi', ''], ['Korhonen', 'Anna', '']]"
1238184,2002.00761,Ahmed El-Kishky,"Ahmed El-Kishky, Francisco Guzm\'an","Massively Multilingual Document Alignment with Cross-lingual
  Sentence-Mover's Distance","In Proceedings of AACL-IJCNLP, 2020",,,,cs.CL cs.IR cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Document alignment aims to identify pairs of documents in two distinct
languages that are of comparable content or translations of each other. Such
aligned data can be used for a variety of NLP tasks from training cross-lingual
representations to mining parallel data for machine translation. In this paper
we develop an unsupervised scoring function that leverages cross-lingual
sentence embeddings to compute the semantic distance between documents in
different languages. These semantic distances are then used to guide a document
alignment algorithm to properly pair cross-lingual web documents across a
variety of low, mid, and high-resource language pairs. Recognizing that our
proposed scoring function and other state of the art methods are
computationally intractable for long web documents, we utilize a more tractable
greedy algorithm that performs comparably. We experimentally demonstrate that
our distance metric performs better alignment than current baselines
outperforming them by 7% on high-resource language pairs, 15% on mid-resource
language pairs, and 22% on low-resource language pairs.
","[{'version': 'v1', 'created': 'Fri, 31 Jan 2020 05:14:16 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 05:26:32 GMT'}]",2020-10-13,"[['El-Kishky', 'Ahmed', ''], ['Guzmán', 'Francisco', '']]"
1355897,2009.14394,Brian Lester,"Brian Lester, Daniel Pressel, Amy Hemmeter, Sagnik Ray Choudhury and
  Srinivas Bangalore",Multiple Word Embeddings for Increased Diversity of Representation,arXiv admin note: text overlap with arXiv:2001.01167,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most state-of-the-art models in natural language processing (NLP) are neural
models built on top of large, pre-trained, contextual language models that
generate representations of words in context and are fine-tuned for the task at
hand. The improvements afforded by these ""contextual embeddings"" come with a
high computational cost. In this work, we explore a simple technique that
substantially and consistently improves performance over a strong baseline with
negligible increase in run time. We concatenate multiple pre-trained embeddings
to strengthen our representation of words. We show that this concatenation
technique works across many tasks, datasets, and model types. We analyze
aspects of pre-trained embedding similarity and vocabulary coverage and find
that the representational diversity between different pre-trained embeddings is
the driving force of why this technique works. We provide open source
implementations of our models in both TensorFlow and PyTorch.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 02:33:09 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 04:40:15 GMT'}]",2020-10-13,"[['Lester', 'Brian', ''], ['Pressel', 'Daniel', ''], ['Hemmeter', 'Amy', ''], ['Choudhury', 'Sagnik Ray', ''], ['Bangalore', 'Srinivas', '']]"
1362093,2010.05763,Nikolaos Manginas,"Nikolaos Manginas, Ilias Chalkidis and Prodromos Malakasiotis","Layer-wise Guided Training for BERT: Learning Incrementally Refined
  Document Representations","5 pages, short paper at SPNLP 2020 (EMNLP 2020 Workshop)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although BERT is widely used by the NLP community, little is known about its
inner workings. Several attempts have been made to shed light on certain
aspects of BERT, often with contradicting conclusions. A much raised concern
focuses on BERT's over-parameterization and under-utilization issues. To this
end, we propose o novel approach to fine-tune BERT in a structured manner.
Specifically, we focus on Large Scale Multilabel Text Classification (LMTC)
where documents are assigned with one or more labels from a large predefined
set of hierarchically organized labels. Our approach guides specific BERT
layers to predict labels from specific hierarchy levels. Experimenting with two
LMTC datasets we show that this structured fine-tuning approach not only yields
better classification results but also leads to better parameter utilization.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 14:56:22 GMT'}]",2020-10-13,"[['Manginas', 'Nikolaos', ''], ['Chalkidis', 'Ilias', ''], ['Malakasiotis', 'Prodromos', '']]"
1020816,1809.00800,Sho Yokoi,"Sho Yokoi, Sosuke Kobayashi, Kenji Fukumizu, Jun Suzuki, Kentaro Inui","Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse
  Linguistic Expressions",Accepted by EMNLP 2018,EMNLP 2018,10.18653/v1/D18-1203,,cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose a new kernel-based co-occurrence measure that can
be applied to sparse linguistic expressions (e.g., sentences) with a very short
learning time, as an alternative to pointwise mutual information (PMI). As well
as deriving PMI from mutual information, we derive this new measure from the
Hilbert--Schmidt independence criterion (HSIC); thus, we call the new measure
the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of
PMI that allows various similarity metrics (e.g., sentence embeddings) to be
plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast
(linear in the size of the data) matrix calculations regardless of whether we
use linear or nonlinear kernels. Empirically, in a dialogue response selection
task, PHSIC is learned thousands of times faster than an RNN-based PMI while
outperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is
beneficial as a criterion of a data selection task for machine translation
owing to its ability to give high (low) scores to a consistent (inconsistent)
pair with other pairs.
","[{'version': 'v1', 'created': 'Tue, 4 Sep 2018 05:33:00 GMT'}]",2020-10-13,"[['Yokoi', 'Sho', ''], ['Kobayashi', 'Sosuke', ''], ['Fukumizu', 'Kenji', ''], ['Suzuki', 'Jun', ''], ['Inui', 'Kentaro', '']]"
1279125,2004.14355,Nithin Holla,"Nithin Holla, Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova","Learning to Learn to Disambiguate: Meta-Learning for Few-Shot Word Sense
  Disambiguation",Camera-ready: Findings of EMNLP,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The success of deep learning methods hinges on the availability of large
training datasets annotated for the task of interest. In contrast to human
intelligence, these methods lack versatility and struggle to learn and adapt
quickly to new tasks, where labeled data is scarce. Meta-learning aims to solve
this problem by training a model on a large number of few-shot tasks, with an
objective to learn new tasks quickly from a small number of examples. In this
paper, we propose a meta-learning framework for few-shot word sense
disambiguation (WSD), where the goal is to learn to disambiguate unseen words
from only a few labeled instances. Meta-learning approaches have so far been
typically tested in an $N$-way, $K$-shot classification setting where each task
has $N$ classes with $K$ examples per class. Owing to its nature, WSD deviates
from this controlled setup and requires the models to handle a large number of
highly unbalanced classes. We extend several popular meta-learning approaches
to this scenario, and analyze their strengths and weaknesses in this new
challenging setting.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 17:33:31 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Sep 2020 14:40:51 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Oct 2020 10:09:05 GMT'}]",2020-10-13,"[['Holla', 'Nithin', ''], ['Mishra', 'Pushkar', ''], ['Yannakoudakis', 'Helen', ''], ['Shutova', 'Ekaterina', '']]"
1362178,2010.05848,Judy Hanwen Shen,"Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson,
  Agata Lapedriza, Noah Jones, Shixiang Shane Gu, and Rosalind Picard",Human-centric Dialog Training via Offline Reinforcement Learning,To appear in EMNLP 2020 (long paper),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  How can we train a dialog model to produce better conversations by learning
from human feedback, without the risk of humans teaching it harmful chat
behaviors? We start by hosting models online, and gather human feedback from
real-time, open-ended conversations, which we then use to train and improve the
models using offline reinforcement learning (RL). We identify implicit
conversational cues including language similarity, elicitation of laughter,
sentiment, and more, which indicate positive human feedback, and embed these in
multiple reward functions. A well-known challenge is that learning an RL policy
in an offline setting usually fails due to the lack of ability to explore and
the tendency to make over-optimistic estimates of future reward. These problems
become even harder when using RL for language models, which can easily have a
20,000 action vocabulary and many possible reward functions. We solve the
challenge by developing a novel class of offline RL algorithms. These
algorithms use KL-control to penalize divergence from a pre-trained prior
language model, and use a new strategy to make the algorithm pessimistic,
instead of optimistic, in the face of uncertainty. We test the resulting dialog
model with ratings from 80 users in an open-domain setting and find it achieves
significant improvements over existing deep offline RL approaches. The novel
offline RL method is viable for improving any existing generative dialog model
using a static dataset of human feedback.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 16:53:00 GMT'}]",2020-10-13,"[['Jaques', 'Natasha', ''], ['Shen', 'Judy Hanwen', ''], ['Ghandeharioun', 'Asma', ''], ['Ferguson', 'Craig', ''], ['Lapedriza', 'Agata', ''], ['Jones', 'Noah', ''], ['Gu', 'Shixiang Shane', ''], ['Picard', 'Rosalind', '']]"
1362186,2010.05856,Mingda Chen,"Mingda Chen, Sam Wiseman, Kevin Gimpel",Controllable Paraphrasing and Translation with a Syntactic Exemplar,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most prior work on exemplar-based syntactically controlled paraphrase
generation relies on automatically-constructed large-scale paraphrase datasets.
We sidestep this prerequisite by adapting models from prior work to be able to
learn solely from bilingual text (bitext). Despite only using bitext for
training, and in near zero-shot conditions, our single proposed model can
perform four tasks: controlled paraphrase generation in both languages and
controlled machine translation in both language directions. To evaluate these
tasks quantitatively, we create three novel evaluation datasets. Our
experimental results show that our models achieve competitive results on
controlled paraphrase generation and strong performance on controlled machine
translation. Analysis shows that our models learn to disentangle semantics and
syntax in their latent representations.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 17:02:50 GMT'}]",2020-10-13,"[['Chen', 'Mingda', ''], ['Wiseman', 'Sam', ''], ['Gimpel', 'Kevin', '']]"
1362203,2010.05873,Katja Filippova,Katja Filippova,"Controlled Hallucinations: Learning to Generate Faithfully from Noisy
  Data",,Findings of EMNLP 2020,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural text generation (data- or text-to-text) demonstrates remarkable
performance when training data is abundant which for many applications is not
the case. To collect a large corpus of parallel data, heuristic rules are often
used but they inevitably let noise into the data, such as phrases in the output
which cannot be explained by the input. Consequently, models pick up on the
noise and may hallucinate--generate fluent but unsupported text. Our
contribution is a simple but powerful technique to treat such hallucinations as
a controllable aspect of the generated text, without dismissing any input and
without modifying the model architecture. On the WikiBio corpus (Lebret et al.,
2016), a particularly noisy dataset, we demonstrate the efficacy of the
technique both in an automatic and in a human evaluation.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 17:25:02 GMT'}]",2020-10-13,"[['Filippova', 'Katja', '']]"
1362204,2010.05874,Zirui Wang,"Zirui Wang, Yulia Tsvetkov, Orhan Firat, Yuan Cao","Gradient Vaccine: Investigating and Improving Multi-task Optimization in
  Massively Multilingual Models",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Massively multilingual models subsuming tens or even hundreds of languages
pose great challenges to multi-task optimization. While it is a common practice
to apply a language-agnostic procedure optimizing a joint multilingual task
objective, how to properly characterize and take advantage of its underlying
problem structure for improving optimization efficiency remains under-explored.
In this paper, we attempt to peek into the black-box of multilingual
optimization through the lens of loss function geometry. We find that gradient
similarity measured along the optimization trajectory is an important signal,
which correlates well with not only language proximity but also the overall
model performance. Such observation helps us to identify a critical limitation
of existing gradient-based multi-task learning methods, and thus we derive a
simple and scalable optimization procedure, named Gradient Vaccine, which
encourages more geometrically aligned parameter updates for close tasks.
Empirically, our method obtains significant model performance gains on
multilingual machine translation and XTREME benchmark tasks for multilingual
language models. Our work reveals the importance of properly measuring and
utilizing language proximity in multilingual optimization, and has broader
implications for multi-task learning beyond multilingual modeling.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 17:26:34 GMT'}]",2020-10-13,"[['Wang', 'Zirui', ''], ['Tsvetkov', 'Yulia', ''], ['Firat', 'Orhan', ''], ['Cao', 'Yuan', '']]"
1362234,2010.05904,Revanth Reddy,"Rong Zhang, Revanth Gangi Reddy, Md Arafat Sultan, Vittorio Castelli,
  Anthony Ferritto, Radu Florian, Efsun Sarioglu Kayi, Salim Roukos, Avirup
  Sil, Todd Ward",Multi-Stage Pre-training for Low-Resource Domain Adaptation,Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transfer learning techniques are particularly useful in NLP tasks where a
sizable amount of high-quality annotated data is difficult to obtain. Current
approaches directly adapt a pre-trained language model (LM) on in-domain text
before fine-tuning to downstream tasks. We show that extending the vocabulary
of the LM with domain-specific terms leads to further gains. To a bigger
effect, we utilize structure in the unlabeled data to create auxiliary
synthetic tasks, which helps the LM transfer to downstream tasks. We apply
these approaches incrementally on a pre-trained Roberta-large LM and show
considerable performance gain on three tasks in the IT domain: Extractive
Reading Comprehension, Document Ranking and Duplicate Question Detection.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 17:57:00 GMT'}]",2020-10-13,"[['Zhang', 'Rong', ''], ['Reddy', 'Revanth Gangi', ''], ['Sultan', 'Md Arafat', ''], ['Castelli', 'Vittorio', ''], ['Ferritto', 'Anthony', ''], ['Florian', 'Radu', ''], ['Kayi', 'Efsun Sarioglu', ''], ['Roukos', 'Salim', ''], ['Sil', 'Avirup', ''], ['Ward', 'Todd', '']]"
1298233,2006.03719,Zhijing Jin,"Zhijing Jin, Yongyi Yang, Xipeng Qiu, Zheng Zhang","Relation of the Relations: A New Paradigm of the Relation Extraction
  Problem",Passed the reviews of EMNLP; withdrawn for non-technical reasons,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  In natural language, often multiple entities appear in the same text.
However, most previous works in Relation Extraction (RE) limit the scope to
identifying the relation between two entities at a time. Such an approach
induces a quadratic computation time, and also overlooks the interdependency
between multiple relations, namely the relation of relations (RoR). Due to the
significance of RoR in existing datasets, we propose a new paradigm of RE that
considers as a whole the predictions of all relations in the same context.
Accordingly, we develop a data-driven approach that does not require
hand-crafted rules but learns by itself the RoR, using Graph Neural Networks
and a relation matrix transformer. Experiments show that our model outperforms
the state-of-the-art approaches by +1.12\% on the ACE05 dataset and +2.55\% on
SemEval 2018 Task 7.2, which is a substantial improvement on the two
competitive benchmarks.
","[{'version': 'v1', 'created': 'Fri, 5 Jun 2020 22:25:27 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 11:45:16 GMT'}]",2020-10-13,"[['Jin', 'Zhijing', ''], ['Yang', 'Yongyi', ''], ['Qiu', 'Xipeng', ''], ['Zhang', 'Zheng', '']]"
1216857,1912.04961,Sai Prabhakar Pandi Selvaraj,"Sai P. Selvaraj, Sandeep Konam",Medication Regimen Extraction From Medical Conversations,"Proceedings of International Workshop on Health Intelligence
  (W3PHIAI) of the 34th AAAI Conference on Artificial Intelligence, 2020",,,,cs.CL cs.IR stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Extracting relevant information from medical conversations and providing it
to doctors and patients might help in addressing doctor burnout and patient
forgetfulness. In this paper, we focus on extracting the Medication Regimen
(dosage and frequency for medications) discussed in a medical conversation. We
frame the problem as a Question Answering (QA) task and perform comparative
analysis over: a QA approach, a new combined QA and Information Extraction
approach, and other baselines. We use a small corpus of 6,692 annotated
doctor-patient conversations for the task. Clinical conversation corpora are
costly to create, difficult to handle (because of data privacy concerns), and
thus scarce. We address this data scarcity challenge through data augmentation
methods, using publicly available embeddings and pretrain part of the network
on a related task (summarization) to improve the model's performance. Compared
to the baseline, our best-performing models improve the dosage and frequency
extractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94,
respectively. Using our best-performing model, we present the first fully
automated system that can extract Medication Regimen tags from spontaneous
doctor-patient conversations with about $\approx$71% accuracy.
","[{'version': 'v1', 'created': 'Tue, 10 Dec 2019 20:18:39 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2020 04:03:31 GMT'}, {'version': 'v3', 'created': 'Sun, 11 Oct 2020 18:04:27 GMT'}]",2020-10-13,"[['Selvaraj', 'Sai P.', ''], ['Konam', 'Sandeep', '']]"
1303800,2006.09286,Satwik Bhattamishra,"Satwik Bhattamishra, Arkil Patel, Navin Goyal","On the Computational Power of Transformers and its Implications in
  Sequence Modeling",CoNLL 2020,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformers are being used extensively across several sequence modeling
tasks. Significant research effort has been devoted to experimentally probe the
inner workings of Transformers. However, our conceptual and theoretical
understanding of their power and inherent limitations is still nascent. In
particular, the roles of various components in Transformers such as positional
encodings, attention heads, residual connections, and feedforward networks, are
not clear. In this paper, we take a step towards answering these questions. We
analyze the computational power as captured by Turing-completeness. We first
provide an alternate and simpler proof to show that vanilla Transformers are
Turing-complete and then we prove that Transformers with only positional
masking and without any positional encoding are also Turing-complete. We
further analyze the necessity of each component for the Turing-completeness of
the network; interestingly, we find that a particular type of residual
connection is necessary. We demonstrate the practical implications of our
results via experiments on machine translation and synthetic tasks.
","[{'version': 'v1', 'created': 'Tue, 16 Jun 2020 16:27:56 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Sep 2020 19:55:21 GMT'}, {'version': 'v3', 'created': 'Sat, 10 Oct 2020 13:34:20 GMT'}]",2020-10-13,"[['Bhattamishra', 'Satwik', ''], ['Patel', 'Arkil', ''], ['Goyal', 'Navin', '']]"
1350838,2009.09335,Kung-Hsiang Huang,"Kung-Hsiang Huang, Mu Yang, Nanyun Peng",Biomedical Event Extraction with Hierarchical Knowledge Graphs,"8 pages, 3 figures, Findings of EMNLP 2020 (short)",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Biomedical event extraction is critical in understanding biomolecular
interactions described in scientific corpus. One of the main challenges is to
identify nested structured events that are associated with non-indicative
trigger words. We propose to incorporate domain knowledge from Unified Medical
Language System (UMLS) to a pre-trained language model via Graph
Edge-conditioned Attention Networks (GEANet) and hierarchical graph
representation. To better recognize the trigger words, each sentence is first
grounded to a sentence graph based on a jointly modeled hierarchical knowledge
graph from UMLS. The grounded graphs are then propagated by GEANet, a novel
graph neural networks for enhanced capabilities in inferring complex events. On
BioNLP 2011 GENIA Event Extraction task, our approach achieved 1.41% F1 and
3.19% F1 improvements on all events and complex events, respectively. Ablation
studies confirm the importance of GEANet and hierarchical KG.
","[{'version': 'v1', 'created': 'Sun, 20 Sep 2020 02:25:05 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Sep 2020 18:09:50 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Oct 2020 16:38:31 GMT'}]",2020-10-13,"[['Huang', 'Kung-Hsiang', ''], ['Yang', 'Mu', ''], ['Peng', 'Nanyun', '']]"
1304927,2006.10413,Nora Kassner,"Nora Kassner, Benno Krojer, Hinrich Sch\""utze",Are Pretrained Language Models Symbolic Reasoners Over Knowledge?,Accepted to CoNLL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  How can pretrained language models (PLMs) learn factual knowledge from the
training set? We investigate the two most important mechanisms: reasoning and
memorization. Prior work has attempted to quantify the number of facts PLMs
learn, but we present, using synthetic data, the first study that investigates
the causal relation between facts present in training and facts learned by the
PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic
reasoning rules correctly but struggle with others, including two-hop
reasoning. Further analysis suggests that even the application of learned
reasoning rules is flawed. For memorization, we identify schema conformity
(facts systematically supported by other facts) and frequency as key factors
for its success.
","[{'version': 'v1', 'created': 'Thu, 18 Jun 2020 10:40:37 GMT'}, {'version': 'v2', 'created': 'Sat, 10 Oct 2020 10:09:46 GMT'}]",2020-10-13,"[['Kassner', 'Nora', ''], ['Krojer', 'Benno', ''], ['Schütze', 'Hinrich', '']]"
1276840,2004.12070,Zhou Yu,"Zhou Yu, Yuhao Cui, Jun Yu, Meng Wang, Dacheng Tao, Qi Tian",Deep Multimodal Neural Architecture Search,"Accept to ACM MM2020, code available at
  https://github.com/MILVLG/mmnas/",,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Designing effective neural networks is fundamentally important in deep
multimodal learning. Most existing works focus on a single task and design
neural architectures manually, which are highly task-specific and hard to
generalize to different tasks. In this paper, we devise a generalized deep
multimodal neural architecture search (MMnas) framework for various multimodal
learning tasks. Given multimodal input, we first define a set of primitive
operations, and then construct a deep encoder-decoder based unified backbone,
where each encoder or decoder block corresponds to an operation searched from a
predefined operation pool. On top of the unified backbone, we attach
task-specific heads to tackle different multimodal learning tasks. By using a
gradient-based NAS algorithm, the optimal architectures for different tasks are
learned efficiently. Extensive ablation studies, comprehensive analysis, and
comparative experimental results show that the obtained MMnasNet significantly
outperforms existing state-of-the-art approaches across three multimodal
learning tasks (over five datasets), including visual question answering,
image-text matching, and visual grounding.
","[{'version': 'v1', 'created': 'Sat, 25 Apr 2020 07:00:32 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 03:28:08 GMT'}]",2020-10-13,"[['Yu', 'Zhou', ''], ['Cui', 'Yuhao', ''], ['Yu', 'Jun', ''], ['Wang', 'Meng', ''], ['Tao', 'Dacheng', ''], ['Tian', 'Qi', '']]"
1274509,2004.09739,Tanmoy Chakraborty,"Tanya Chowdhury, Sachin Kumar, Tanmoy Chakraborty",Neural Abstractive Summarization with Structural Attention,"7 pages, 4 tables, 2 figures, IJCAI 2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Attentional, RNN-based encoder-decoder architectures have achieved impressive
performance on abstractive summarization of news articles. However, these
methods fail to account for long term dependencies within the sentences of a
document. This problem is exacerbated in multi-document summarization tasks
such as summarizing the popular opinion in threads present in community
question answering (CQA) websites such as Yahoo! Answers and Quora. These
threads contain answers which often overlap or contradict each other. In this
work, we present a hierarchical encoder based on structural attention to model
such inter-sentence and inter-document dependencies. We set the popular
pointer-generator architecture and some of the architectures derived from it as
our baselines and show that they fail to generate good summaries in a
multi-document setting. We further illustrate that our proposed model achieves
significant improvement over the baselines in both single and multi-document
summarization settings -- in the former setting, it beats the best baseline by
1.31 and 7.8 ROUGE-1 points on CNN and CQA datasets, respectively; in the
latter setting, the performance is further improved by 1.6 ROUGE-1 points on
the CQA dataset.
","[{'version': 'v1', 'created': 'Tue, 21 Apr 2020 03:39:15 GMT'}, {'version': 'v2', 'created': 'Sat, 10 Oct 2020 05:32:59 GMT'}]",2020-10-13,"[['Chowdhury', 'Tanya', ''], ['Kumar', 'Sachin', ''], ['Chakraborty', 'Tanmoy', '']]"
1274314,2004.09544,Hyundong Cho,"Hyundong Cho, Jonathan May",Grounding Conversations with Improvised Dialogues,ACL2020 Camera Ready; 9 pages + 5 page appendix,,10.18653/v1/2020.acl-main.218,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Effective dialogue involves grounding, the process of establishing mutual
knowledge that is essential for communication between people. Modern dialogue
systems are not explicitly trained to build common ground, and therefore
overlook this important aspect of communication. Improvisational theater
(improv) intrinsically contains a high proportion of dialogue focused on
building common ground, and makes use of the yes-and principle, a strong
grounding speech act, to establish coherence and an actionable objective
reality. We collect a corpus of more than 26,000 yes-and turns, transcribing
them from improv dialogues and extracting them from larger, but more sparsely
populated movie script dialogue corpora, via a bootstrapped classifier. We
fine-tune chit-chat dialogue systems with our corpus to encourage more
grounded, relevant conversation and confirm these findings with human
evaluations.
","[{'version': 'v1', 'created': 'Mon, 20 Apr 2020 18:05:53 GMT'}, {'version': 'v2', 'created': 'Tue, 19 May 2020 05:34:13 GMT'}]",2020-10-13,"[['Cho', 'Hyundong', ''], ['May', 'Jonathan', '']]"
1312006,2007.00492,Shaoqing Yuan,"Shaoqing Yuan, Parminder Bhatia, Busra Celikkaya, Haiyang Liu,
  Kyunghwan Choi","Towards User Friendly Medication Mapping Using Entity-Boosted Two-Tower
  Neural Network",,,,,cs.CL cs.CY cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in medical entity linking have been applied in the area
of scientific literature and social media data. However, with the adoption of
telemedicine and conversational agents such as Alexa in healthcare settings,
medical name inference has become an important task. Medication name inference
is the task of mapping user friendly medication names from a free-form text to
a concept in a normalized medication list. This is challenging due to the
differences in the use of medical terminology from health care professionals
and user conversations coming from the lay public. We begin with mapping
descriptive medication phrases (DMP) to standard medication names (SMN). Given
the prescriptions of each patient, we want to provide them with the flexibility
of referring to the medication in their preferred ways. We approach this as a
ranking problem which maps SMN to DMP by ordering the list of medications in
the patient's prescription list obtained from pharmacies. Furthermore, we
leveraged the output of intermediate layers and performed medication
clustering. We present the Medication Inference Model (MIM) achieving
state-of-the-art results. By incorporating medical entities based attention, we
have obtained further improvement for ranking models.
","[{'version': 'v1', 'created': 'Wed, 17 Jun 2020 18:56:44 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 18:44:40 GMT'}]",2020-10-13,"[['Yuan', 'Shaoqing', ''], ['Bhatia', 'Parminder', ''], ['Celikkaya', 'Busra', ''], ['Liu', 'Haiyang', ''], ['Choi', 'Kyunghwan', '']]"
1272560,2004.07790,Joe Stacey,"Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Sebastian Riedel,
  Tim Rockt\""aschel","Avoiding the Hypothesis-Only Bias in Natural Language Inference via
  Ensemble Adversarial Training",11 pages,,,,cs.LG cs.AI cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural Language Inference (NLI) datasets contain annotation artefacts
resulting in spurious correlations between the natural language utterances and
their respective entailment classes. These artefacts are exploited by neural
networks even when only considering the hypothesis and ignoring the premise,
leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this
problem via adversarial training, but this can lead to learned sentence
representations that still suffer from the same biases. We show that the bias
can be reduced in the sentence representations by using an ensemble of
adversaries, encouraging the model to jointly decrease the accuracy of these
different adversaries while fitting the data. This approach produces more
robust NLI models, outperforming previous de-biasing efforts when generalised
to 12 other datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In
addition, we find that the optimal number of adversarial classifiers depends on
the dimensionality of the sentence representations, with larger sentence
representations being more difficult to de-bias while benefiting from using a
greater number of adversaries.
","[{'version': 'v1', 'created': 'Thu, 16 Apr 2020 17:37:15 GMT'}, {'version': 'v2', 'created': 'Fri, 17 Apr 2020 17:19:14 GMT'}, {'version': 'v3', 'created': 'Mon, 27 Apr 2020 18:47:32 GMT'}, {'version': 'v4', 'created': 'Sat, 10 Oct 2020 17:12:15 GMT'}]",2020-10-13,"[['Stacey', 'Joe', ''], ['Minervini', 'Pasquale', ''], ['Dubossarsky', 'Haim', ''], ['Riedel', 'Sebastian', ''], ['Rocktäschel', 'Tim', '']]"
1271269,2004.06499,Wietse de Vries,"Wietse de Vries, Andreas van Cranenburgh and Malvina Nissim","What's so special about BERT's layers? A closer look at the NLP pipeline
  in monolingual and multilingual models",Accepted at Findings of EMNLP 2020 (camera-ready),,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Peeking into the inner workings of BERT has shown that its layers resemble
the classical NLP pipeline, with progressively more complex tasks being
concentrated in later layers. To investigate to what extent these results also
hold for a language other than English, we probe a Dutch BERT-based model and
the multilingual BERT model for Dutch NLP tasks. In addition, through a deeper
analysis of part-of-speech tagging, we show that also within a given task,
information is spread over different parts of the network and the pipeline
might not be as neat as it seems. Each layer has different specialisations, so
that it may be more useful to combine information from different layers,
instead of selecting a single one based on the best overall performance.
","[{'version': 'v1', 'created': 'Tue, 14 Apr 2020 13:41:48 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 11:51:34 GMT'}]",2020-10-13,"[['de Vries', 'Wietse', ''], ['van Cranenburgh', 'Andreas', ''], ['Nissim', 'Malvina', '']]"
1351550,2009.10047,Congcong Wang,Congcong Wang and David Lillis,"UCD-CS at W-NUT 2020 Shared Task-3: A Text to Text Approach for COVID-19
  Event Extraction on Social Media","8 pages, 2 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we describe our approach in the shared task: COVID-19 event
extraction from Twitter. The objective of this task is to extract answers from
COVID-related tweets to a set of predefined slot-filling questions. Our
approach treats the event extraction task as a question answering task by
leveraging the transformer-based T5 text-to-text model.
  According to the official evaluation scores returned, namely F1, our
submitted run achieves competitive performance compared to other participating
runs (Top 3). However, we argue that this evaluation may underestimate the
actual performance of runs based on text-generation. Although some such runs
may answer the slot questions well, they may not be an exact string match for
the gold standard answers. To measure the extent of this underestimation, we
adopt a simple exact-answer transformation method aiming at converting the
well-answered predictions to exactly-matched predictions. The results show that
after this transformation our run overall reaches the same level of performance
as the best participating run and state-of-the-art F1 scores in three of five
COVID-related events. Our code is publicly available to aid reproducibility
","[{'version': 'v1', 'created': 'Mon, 21 Sep 2020 17:39:00 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 16:18:53 GMT'}]",2020-10-13,"[['Wang', 'Congcong', ''], ['Lillis', 'David', '']]"
1220390,1912.08494,Sameen Maruf,"Sameen Maruf, Fahimeh Saleh and Gholamreza Haffari","A Survey on Document-level Neural Machine Translation: Methods and
  Evaluation","This article is under-review at an international journal. This arXiv
  version has been made available to solicit feedback",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine translation (MT) is an important task in natural language processing
(NLP) as it automates the translation process and reduces the reliance on human
translators. With the resurgence of neural networks, the translation quality
surpasses that of the translations obtained using statistical techniques for
most language-pairs. Up until a few years ago, almost all of the neural
translation models translated sentences independently, without incorporating
the wider document-context and inter-dependencies among the sentences. The aim
of this survey paper is to highlight the major works that have been undertaken
in the space of document-level machine translation after the neural revolution,
so that researchers can recognise the current state and future directions of
this field. We provide an organisation of the literature based on novelties in
modelling and architectures as well as training and decoding strategies. In
addition, we cover evaluation strategies that have been introduced to account
for the improvements in document MT, including automatic metrics and
discourse-targeted test sets. We conclude by presenting possible avenues for
future exploration in this research field.
","[{'version': 'v1', 'created': 'Wed, 18 Dec 2019 10:07:20 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 23:10:22 GMT'}]",2020-10-13,"[['Maruf', 'Sameen', ''], ['Saleh', 'Fahimeh', ''], ['Haffari', 'Gholamreza', '']]"
1361688,2010.05358,Alex Warstadt,"Alex Warstadt, Yian Zhang, Haau-Sing Li, Haokun Liu, Samuel R. Bowman","Learning Which Features Matter: RoBERTa Acquires a Preference for
  Linguistic Generalizations (Eventually)",accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One reason pretraining on self-supervised linguistic tasks is effective is
that it teaches models features that are helpful for language understanding.
However, we want pretrained models to learn not only to represent linguistic
features, but also to use those features preferentially during fine-turning.
With this goal in mind, we introduce a new English-language diagnostic set
called MSGS (the Mixed Signals Generalization Set), which consists of 20
ambiguous binary classification tasks that we use to test whether a pretrained
model prefers linguistic or surface generalizations during fine-tuning. We
pretrain RoBERTa models from scratch on quantities of data ranging from 1M to
1B words and compare their performance on MSGS to the publicly available
RoBERTa-base. We find that models can learn to represent linguistic features
with little pretraining data, but require far more data to learn to prefer
linguistic generalizations over surface ones. Eventually, with about 30B words
of pretraining data, RoBERTa-base does demonstrate a linguistic bias with some
regularity. We conclude that while self-supervised pretraining is an effective
way to learn helpful inductive biases, there is likely room to improve the rate
at which models learn which features matter.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 22:09:27 GMT'}]",2020-10-13,"[['Warstadt', 'Alex', ''], ['Zhang', 'Yian', ''], ['Li', 'Haau-Sing', ''], ['Liu', 'Haokun', ''], ['Bowman', 'Samuel R.', '']]"
1361699,2010.05369,Roy Bar-Haim,"Roy Bar-Haim, Yoav Kantor, Lilach Eden, Roni Friedman, Dan Lahav and
  Noam Slonim","Quantitative Argument Summarization and Beyond: Cross-Domain Key Point
  Analysis",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When summarizing a collection of views, arguments or opinions on some topic,
it is often desirable not only to extract the most salient points, but also to
quantify their prevalence. Work on multi-document summarization has
traditionally focused on creating textual summaries, which lack this
quantitative aspect. Recent work has proposed to summarize arguments by mapping
them to a small set of expert-generated key points, where the salience of each
key point corresponds to the number of its matching arguments. The current work
advances key point analysis in two important respects: first, we develop a
method for automatic extraction of key points, which enables fully automatic
analysis, and is shown to achieve performance comparable to a human expert.
Second, we demonstrate that the applicability of key point analysis goes well
beyond argumentation data. Using models trained on publicly available
argumentation datasets, we achieve promising results in two additional domains:
municipal surveys and user reviews. An additional contribution is an in-depth
evaluation of argument-to-key point matching models, where we substantially
outperform previous results.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 23:01:51 GMT'}]",2020-10-13,"[['Bar-Haim', 'Roy', ''], ['Kantor', 'Yoav', ''], ['Eden', 'Lilach', ''], ['Friedman', 'Roni', ''], ['Lahav', 'Dan', ''], ['Slonim', 'Noam', '']]"
1361679,2010.05349,Rahim Dehkharghani,"Ali Najafi, Araz Gholipour-Shilabin, Rahim Dehkharghani, Ali
  Mohammadpur-Fard, Meysam Asgari-Chenaghlu","ComStreamClust: A communicative text clustering approach to topic
  detection in streaming data","11 pages, 6 Figures, 4 Tables",,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Topic detection is the task of determining and tracking hot topics in social
media. Twitter is arguably the most popular platform for people to share their
ideas with others about different issues. One such prevalent issue is the
COVID-19 pandemic. Detecting and tracking topics on these kinds of issues would
help governments and healthcare companies deal with this phenomenon. In this
paper, we propose a novel communicative clustering approach, so-called
ComStreamClust for clustering sub-topics inside a broader topic, e.g. COVID-19.
The proposed approach was evaluated on two datasets: the COVID-19 and the FA
CUP. The results obtained from ComStreamClust approve the effectiveness of the
proposed approach when compared to existing methods such as LDA.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 21:19:19 GMT'}]",2020-10-13,"[['Najafi', 'Ali', ''], ['Gholipour-Shilabin', 'Araz', ''], ['Dehkharghani', 'Rahim', ''], ['Mohammadpur-Fard', 'Ali', ''], ['Asgari-Chenaghlu', 'Meysam', '']]"
1361202,2010.04872,Charles Lovering J,Charles Lovering and Ellie Pavlick,Self-play for Data Efficient Language Acquisition,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When communicating, people behave consistently across conversational roles:
People understand the words they say and are able to produce the words they
hear. To date, artificial agents developed for language tasks have lacked such
symmetry, meaning agents trained to produce language are unable to understand
it and vice-versa. In this work, we exploit the symmetric nature of
communication in order to improve both the efficiency and quality of language
acquisition in learning agents. Specifically, we consider the setting in which
an agent must learn to both understand and generate words in an existing
language, but with the assumption that access to interaction with ""oracle""
speakers of the language is very limited. We show that using self-play as a
substitute for direct supervision enables the agent to transfer its knowledge
across roles (e.g. training as a listener but testing as a speaker) and make
better inferences about the ground truth lexicon using only a handful of
interactions with the oracle.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 02:09:19 GMT'}]",2020-10-13,"[['Lovering', 'Charles', ''], ['Pavlick', 'Ellie', '']]"
1361213,2010.04883,Xinyin Ma,"Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao Jia,
  Weiming Lu","Adversarial Self-Supervised Data-Free Distillation for Text
  Classification","11 pages, 5 figures, Accepted to EMNLP2020",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large pre-trained transformer-based language models have achieved impressive
results on a wide range of NLP tasks. In the past few years, Knowledge
Distillation(KD) has become a popular paradigm to compress a computationally
expensive model to a resource-efficient lightweight model. However, most KD
algorithms, especially in NLP, rely on the accessibility of the original
training dataset, which may be unavailable due to privacy issues. To tackle
this problem, we propose a novel two-stage data-free distillation method, named
Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed
for compressing large-scale transformer-based models (e.g., BERT). To avoid
text generation in discrete space, we introduce a Plug & Play Embedding
Guessing method to craft pseudo embeddings from the teacher's hidden knowledge.
Meanwhile, with a self-supervised module to quantify the student's ability, we
adapt the difficulty of pseudo embeddings in an adversarial training manner. To
the best of our knowledge, our framework is the first data-free distillation
framework designed for NLP tasks. We verify the effectiveness of our method on
several text classification datasets.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 02:46:06 GMT'}]",2020-10-13,"[['Ma', 'Xinyin', ''], ['Shen', 'Yongliang', ''], ['Fang', 'Gongfan', ''], ['Chen', 'Chen', ''], ['Jia', 'Chenghao', ''], ['Lu', 'Weiming', '']]"
1361217,2010.04887,Forrest Davis,Forrest Davis and Marten van Schijndel,"Discourse structure interacts with reference but not syntax in neural
  language models","Proceedings of the 2020 Conference on Computational Natural Language
  Learning (CoNLL 2020)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language models (LMs) trained on large quantities of text have been claimed
to acquire abstract linguistic representations. Our work tests the robustness
of these abstractions by focusing on the ability of LMs to learn interactions
between different linguistic representations. In particular, we utilized
stimuli from psycholinguistic studies showing that humans can condition
reference (i.e. coreference resolution) and syntactic processing on the same
discourse structure (implicit causality). We compared both transformer and long
short-term memory LMs to find that, contrary to humans, implicit causality only
influences LM behavior for reference, not syntax, despite model representations
that encode the necessary discourse information. Our results further suggest
that LM behavior can contradict not only learned representations of discourse
but also syntactic agreement, pointing to shortcomings of standard language
modeling.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 03:14:00 GMT'}]",2020-10-13,"[['Davis', 'Forrest', ''], ['van Schijndel', 'Marten', '']]"
1361227,2010.04897,Bo Wang,"John Pougue Biyong, Bo Wang, Terry Lyons and Alejo J Nevado-Holgado","Information Extraction from Swedish Medical Prescriptions with
  Sig-Transformer Encoder",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Relying on large pretrained language models such as Bidirectional Encoder
Representations from Transformers (BERT) for encoding and adding a simple
prediction layer has led to impressive performance in many clinical natural
language processing (NLP) tasks. In this work, we present a novel extension to
the Transformer architecture, by incorporating signature transform with the
self-attention model. This architecture is added between embedding and
prediction layers. Experiments on a new Swedish prescription data show the
proposed architecture to be superior in two of the three information extraction
tasks, comparing to baseline models. Finally, we evaluate two different
embedding approaches between applying Multilingual BERT and translating the
Swedish text to English then encode with a BERT model pretrained on clinical
notes.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 04:22:07 GMT'}]",2020-10-13,"[['Biyong', 'John Pougue', ''], ['Wang', 'Bo', ''], ['Lyons', 'Terry', ''], ['Nevado-Holgado', 'Alejo J', '']]"
1361228,2010.04898,Raviteja Anantha,"Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre,
  Stephen Pulman, Srinivas Chappidi","Open-Domain Question Answering Goes Conversational via Question
  Rewriting","15 pages, 10 tables, 3 figures",,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a new dataset for Question Rewriting in Conversational Context
(QReCC), which contains 14K conversations with 81K question-answer pairs. The
task in QReCC is to find answers to conversational questions within a
collection of 10M web pages (split into 54M passages). Answers to questions in
the same conversation may be distributed across several web pages. QReCC
provides annotations that allow us to train and evaluate individual subtasks of
question rewriting, passage retrieval and reading comprehension required for
the end-to-end conversational question answering (QA) task. We report the
effectiveness of a strong baseline approach that combines the state-of-the-art
model for question rewriting, and competitive models for open-domain QA. Our
results set the first baseline for the QReCC dataset with F1 of 19.07, compared
to the human upper bound of 74.47, indicating the difficulty of the setup and a
large room for improvement.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 04:28:42 GMT'}]",2020-10-13,"[['Anantha', 'Raviteja', ''], ['Vakulenko', 'Svitlana', ''], ['Tu', 'Zhucheng', ''], ['Longpre', 'Shayne', ''], ['Pulman', 'Stephen', ''], ['Chappidi', 'Srinivas', '']]"
1361230,2010.04900,Chiyu Zhang,"Muhammad Abdul-Mageed and Chiyu Zhang and AbdelRahim Elmadany and Lyle
  Ungar","Toward Micro-Dialect Identification in Diaglossic and Code-Switched
  Environments",Accepted in EMNLP 2020,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Although the prediction of dialects is an important language processing task,
with a wide range of applications, existing work is largely limited to
coarse-grained varieties. Inspired by geolocation research, we propose the
novel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new
language model with striking abilities to predict a fine-grained variety (as
small as that of a city) given a single, short message. For modeling, we offer
a range of novel spatially and linguistically-motivated multi-task learning
models. To showcase the utility of our models, we introduce a new, large-scale
dataset of Arabic micro-varieties (low-resource) suited to our tasks. MARBERT
predicts micro-dialects with 9.9% F1, ~76X better than a majority class
baseline. Our new language model also establishes new state-of-the-art on
several external tasks.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 04:35:16 GMT'}]",2020-10-13,"[['Abdul-Mageed', 'Muhammad', ''], ['Zhang', 'Chiyu', ''], ['Elmadany', 'AbdelRahim', ''], ['Ungar', 'Lyle', '']]"
1361233,2010.04903,Yu-An Wang,"Yu-An Wang, Yun-Nung Chen","What Do Position Embeddings Learn? An Empirical Study of Pre-Trained
  Language Model Positional Encoding",Accepted by EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, pre-trained Transformers have dominated the majority of NLP
benchmark tasks. Many variants of pre-trained Transformers have kept breaking
out, and most focus on designing different pre-training objectives or variants
of self-attention. Embedding the position information in the self-attention
mechanism is also an indispensable factor in Transformers however is often
discussed at will. Therefore, this paper carries out an empirical study on
position embeddings of mainstream pre-trained Transformers, which mainly
focuses on two questions: 1) Do position embeddings really learn the meaning of
positions? 2) How do these different learned position embeddings affect
Transformers for NLP tasks? This paper focuses on providing a new insight of
pre-trained position embeddings through feature-level analysis and empirical
experiments on most of iconic NLP tasks. It is believed that our experimental
results can guide the future work to choose the suitable positional encoding
function for specific tasks given the application property.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 05:03:14 GMT'}]",2020-10-13,"[['Wang', 'Yu-An', ''], ['Chen', 'Yun-Nung', '']]"
1044920,1811.00287,Mingxuan Wang,"Mingxuan Wang, Jun Xie, Zhixing Tan, Jinsong Su, Deyi Xiong and Lei Li",Towards Linear Time Neural Machine Translation with Capsule Networks,Accepted as EMNLP2019,,10.18653/v1/D19-1074,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this study, we first investigate a novel capsule network with dynamic
routing for linear time Neural Machine Translation (NMT), referred as
\textsc{CapsNMT}. \textsc{CapsNMT} uses an aggregation mechanism to map the
source sentence into a matrix with pre-determined size, and then applys a deep
LSTM network to decode the target sequence from the source representation.
Unlike the previous work \cite{sutskever2014sequence} to store the source
sentence with a passive and bottom-up way, the dynamic routing policy encodes
the source sentence with an iterative process to decide the credit attribution
between nodes from lower and higher layers. \textsc{CapsNMT} has two core
properties: it runs in time that is linear in the length of the sequences and
provides a more flexible way to select, represent and aggregates the part-whole
information of the source sentence. On WMT14 English-German task and a larger
WMT14 English-French task, \textsc{CapsNMT} achieves comparable results with
the state-of-the-art NMT systems. To the best of our knowledge, this is the
first work that capsule networks have been empirically investigated for
sequence to sequence problems.
","[{'version': 'v1', 'created': 'Thu, 1 Nov 2018 09:37:48 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Aug 2019 13:32:33 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Jul 2020 06:11:14 GMT'}, {'version': 'v4', 'created': 'Mon, 12 Oct 2020 03:41:02 GMT'}]",2020-10-13,"[['Wang', 'Mingxuan', ''], ['Xie', 'Jun', ''], ['Tan', 'Zhixing', ''], ['Su', 'Jinsong', ''], ['Xiong', 'Deyi', ''], ['Li', 'Lei', '']]"
1361254,2010.04924,Vikas Raunak,"Vikas Raunak, Siddharth Dalmia, Vivek Gupta and Florian Metze",On Long-Tailed Phenomena in Neural Machine Translation,Accepted to Findings of EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  State-of-the-art Neural Machine Translation (NMT) models struggle with
generating low-frequency tokens, tackling which remains a major challenge. The
analysis of long-tailed phenomena in the context of structured prediction tasks
is further hindered by the added complexities of search during inference. In
this work, we quantitatively characterize such long-tailed phenomena at two
levels of abstraction, namely, token classification and sequence generation. We
propose a new loss function, the Anti-Focal loss, to better adapt model
training to the structural dependencies of conditional text generation by
incorporating the inductive biases of beam search in the training process. We
show the efficacy of the proposed technique on a number of Machine Translation
(MT) datasets, demonstrating that it leads to significant gains over
cross-entropy across different language pairs, especially on the generation of
low-frequency words. We have released the code to reproduce our results.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 07:00:57 GMT'}]",2020-10-13,"[['Raunak', 'Vikas', ''], ['Dalmia', 'Siddharth', ''], ['Gupta', 'Vivek', ''], ['Metze', 'Florian', '']]"
1361265,2010.04935,Jun Kong,"Jun Kong, Jin Wang and Xuejie Zhang","HPCC-YNU at SemEval-2020 Task 9: A Bilingual Vector Gating Mechanism for
  Sentiment Analysis of Code-Mixed Text","6 pages, 3 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is fairly common to use code-mixing on a social media platform to express
opinions and emotions in multilingual societies. The purpose of this task is to
detect the sentiment of code-mixed social media text. Code-mixed text poses a
great challenge for the traditional NLP system, which currently uses
monolingual resources to deal with the problem of multilingual mixing. This
task has been solved in the past using lexicon lookup in respective sentiment
dictionaries and using a long short-term memory (LSTM) neural network for
monolingual resources. In this paper, we (my codalab username is kongjun)
present a system that uses a bilingual vector gating mechanism for bilingual
resources to complete the task. The model consists of two main parts: the
vector gating mechanism, which combines the character and word levels, and the
attention mechanism, which extracts the important emotional parts of the text.
The results show that the proposed system outperforms the baseline algorithm.
We achieved fifth place in Spanglish and 19th place in Hinglish.The code of
this paper is availabled at : https://github.com/JunKong5/Semveal2020-task9
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 08:02:15 GMT'}]",2020-10-13,"[['Kong', 'Jun', ''], ['Wang', 'Jin', ''], ['Zhang', 'Xuejie', '']]"
1361271,2010.04941,Changlong Yu,"Changlong Yu, Jialong Han, Peifeng Wang, Yangqiu Song, Hongming Zhang,
  Wilfred Ng, Shuming Shi","When Hearst Is not Enough: Improving Hypernymy Detection from Corpus
  with Distributional Models",Accepted by EMNLP2020 Main Conference,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We address hypernymy detection, i.e., whether an is-a relationship exists
between words (x, y), with the help of large textual corpora. Most conventional
approaches to this task have been categorized to be either pattern-based or
distributional. Recent studies suggest that pattern-based ones are superior, if
large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x,
y) pairs relieved. However, they become invalid in some specific sparsity
cases, where x or y is not involved in any pattern. For the first time, this
paper quantifies the non-negligible existence of those specific cases. We also
demonstrate that distributional methods are ideal to make up for pattern-based
ones in such cases. We devise a complementary framework, under which a
pattern-based and a distributional model collaborate seamlessly in cases which
they each prefer. On several benchmark datasets, our framework achieves
competitive improvements and the case study shows its better interpretability.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 08:34:19 GMT'}]",2020-10-13,"[['Yu', 'Changlong', ''], ['Han', 'Jialong', ''], ['Wang', 'Peifeng', ''], ['Song', 'Yangqiu', ''], ['Zhang', 'Hongming', ''], ['Ng', 'Wilfred', ''], ['Shi', 'Shuming', '']]"
1361300,2010.04970,Yingxue Zhang,"Yingxue Zhang, Fandong Meng, Peng Li, Ping Jian, Jie Zhou","MS-Ranker: Accumulating Evidence from Potentially Correct Candidates for
  Answer Selection",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As conventional answer selection (AS) methods generally match the question
with each candidate answer independently, they suffer from the lack of matching
information between the question and the candidate. To address this problem, we
propose a novel reinforcement learning (RL) based multi-step ranking model,
named MS-Ranker, which accumulates information from potentially correct
candidate answers as extra evidence for matching the question with a candidate.
In specific, we explicitly consider the potential correctness of candidates and
update the evidence with a gating mechanism. Moreover, as we use a listwise
ranking reward, our model learns to pay more attention to the overall
performance. Experiments on two benchmarks, namely WikiQA and SemEval-2016 CQA,
show that our model significantly outperforms existing methods that do not rely
on external resources.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 10:36:58 GMT'}]",2020-10-13,"[['Zhang', 'Yingxue', ''], ['Meng', 'Fandong', ''], ['Li', 'Peng', ''], ['Jian', 'Ping', ''], ['Zhou', 'Jie', '']]"
1361301,2010.04971,Issa Annamoradnejad,"Navid Khezrian, Jafar Habibi, Issa Annamoradnejad","Tag Recommendation for Online Q&A Communities based on BERT Pre-Training
  Technique","5 pages, initial results",,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Online Q&A and open source communities use tags and keywords to index,
categorize, and search for specific content. The most obvious advantage of tag
recommendation is the correct classification of information. In this study, we
used the BERT pre-training technique in tag recommendation task for online Q&A
and open-source communities for the first time. Our evaluation on freecode
datasets show that the proposed method, called TagBERT, is more accurate
compared to deep learning and other baseline methods. Moreover, our model
achieved a high stability by solving the problem of previous researches, where
increasing the number of tag recommendations significantly reduced model
performance.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 10:52:22 GMT'}]",2020-10-13,"[['Khezrian', 'Navid', ''], ['Habibi', 'Jafar', ''], ['Annamoradnejad', 'Issa', '']]"
1361306,2010.04976,Hritik Bansal,"Hritik Bansal, Gantavya Bhatt, Sumeet Agarwal","Can RNNs trained on harder subject-verb agreement instances still
  perform well on easier ones?","15 pages, 3 figures, 13 Tables (including Appendix); Submitted for
  review",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The main subject and the associated verb in English must agree in grammatical
number as per the Subject-Verb Agreement (SVA) phenomenon. It has been found
that the presence of a noun between the verb and the main subject, whose
grammatical number is opposite to that of the main subject, can cause speakers
to produce a verb that agrees with the intervening noun rather than the main
noun; the former thus acts as an agreement attractor. Such attractors have also
been shown to pose a challenge for RNN models without explicit hierarchical
bias to perform well on SVA tasks. Previous work suggests that syntactic cues
in the input can aid such models to choose hierarchical rules over linear rules
for number agreement. In this work, we investigate the effects of the choice of
training data, training algorithm, and architecture on hierarchical
generalization. We observe that the models under consideration fail to perform
well on sentences with no agreement attractor when trained solely on natural
sentences with at least one attractor. Even in the presence of this biased
training set, implicit hierarchical bias in the architecture (as in the Ordered
Neurons LSTM) is not enough to capture syntax-sensitive dependencies. These
results suggest that current RNNs do not capture the underlying hierarchical
rules of natural language, but rather use shallower heuristics for their
predictions.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 11:30:58 GMT'}]",2020-10-13,"[['Bansal', 'Hritik', ''], ['Bhatt', 'Gantavya', ''], ['Agarwal', 'Sumeet', '']]"
1361310,2010.04980,Renato Negrinho,"Renato Negrinho, Matthew R. Gormley, Geoffrey J. Gordon",An Empirical Investigation of Beam-Aware Training in Supertagging,"EMNLP Findings 2020 camera-ready. Code can be found at
  https://github.com/negrinho/beam_learn_supertagging",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Structured prediction is often approached by training a locally normalized
model with maximum likelihood and decoding approximately with beam search. This
approach leads to mismatches as, during training, the model is not exposed to
its mistakes and does not use beam search. Beam-aware training aims to address
these problems, but unfortunately, it is not yet widely used due to a lack of
understanding about how it impacts performance, when it is most useful, and
whether it is stable. Recently, Negrinho et al. (2018) proposed a
meta-algorithm that captures beam-aware training algorithms and suggests new
ones, but unfortunately did not provide empirical results. In this paper, we
begin an empirical investigation: we train the supertagging model of Vaswani et
al. (2016) and a simpler model with instantiations of the meta-algorithm. We
explore the influence of various design choices and make recommendations for
choosing them. We observe that beam-aware training improves performance for
both models, with large improvements for the simpler model which must
effectively manage uncertainty during decoding. Our results suggest that a
model must be learned with search to maximize its effectiveness.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 12:25:18 GMT'}]",2020-10-13,"[['Negrinho', 'Renato', ''], ['Gormley', 'Matthew R.', ''], ['Gordon', 'Geoffrey J.', '']]"
1361317,2010.04987,Piyawat Lertvittayakumjorn,"Piyawat Lertvittayakumjorn, Lucia Specia, Francesca Toni",FIND: Human-in-the-Loop Debugging Deep Text Classifiers,17 pages including appendices; To appear at EMNLP 2020,,,,cs.CL cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Since obtaining a perfect training dataset (i.e., a dataset which is
considerably large, unbiased, and well-representative of unseen cases) is
hardly possible, many real-world text classifiers are trained on the available,
yet imperfect, datasets. These classifiers are thus likely to have undesirable
properties. For instance, they may have biases against some sub-populations or
may not work effectively in the wild due to overfitting. In this paper, we
propose FIND -- a framework which enables humans to debug deep learning text
classifiers by disabling irrelevant hidden features. Experiments show that by
using FIND, humans can improve CNN text classifiers which were trained under
different types of imperfect datasets (including datasets with biases and
datasets with dissimilar train-test distributions).
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 12:52:53 GMT'}]",2020-10-13,"[['Lertvittayakumjorn', 'Piyawat', ''], ['Specia', 'Lucia', ''], ['Toni', 'Francesca', '']]"
1361257,2010.04927,Qiansheng Wang,"Qiansheng Wang, Yuxin Liu, Chengguo Lv, Zhen Wang and Guohong Fu",Cue-word Driven Neural Response Generation with a Shrinking Vocabulary,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open-domain response generation is the task of generating sensible and
informative re-sponses to the source sentence. However, neural models tend to
generate safe and mean-ingless responses. While cue-word introducing approaches
encourage responses with concrete semantics and have shown tremendous
potential, they still fail to explore di-verse responses during decoding. In
this paper, we propose a novel but natural approach that can produce multiple
cue-words during decoding, and then uses the produced cue-words to drive
decoding and shrinks the decoding vocabulary. Thus the neural genera-tion model
can explore the full space of responses and discover informative ones with
efficiency. Experimental results show that our approach significantly
outperforms several strong baseline models with much lower decoding complexity.
Especially, our approach can converge to concrete semantics more efficiently
during decoding.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 07:13:32 GMT'}]",2020-10-13,"[['Wang', 'Qiansheng', ''], ['Liu', 'Yuxin', ''], ['Lv', 'Chengguo', ''], ['Wang', 'Zhen', ''], ['Fu', 'Guohong', '']]"
1361193,2010.04863,Hao Huang,"Hao Huang, Guodong Long, Tao Shen, Jing Jiang, Chengqi Zhang","RatE: Relation-Adaptive Translating Embedding for Knowledge Graph
  Completion",Accepted to appear at COLING 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many graph embedding approaches have been proposed for knowledge graph
completion via link prediction. Among those, translating embedding approaches
enjoy the advantages of light-weight structure, high efficiency and great
interpretability. Especially when extended to complex vector space, they show
the capability in handling various relation patterns including symmetry,
antisymmetry, inversion and composition. However, previous translating
embedding approaches defined in complex vector space suffer from two main
issues: 1) representing and modeling capacities of the model are limited by the
translation function with rigorous multiplication of two complex numbers; and
2) embedding ambiguity caused by one-to-many relations is not explicitly
alleviated. In this paper, we propose a relation-adaptive translation function
built upon a novel weighted product in complex space, where the weights are
learnable, relation-specific and independent to embedding size. The translation
function only requires eight more scalar parameters each relation, but improves
expressive power and alleviates embedding ambiguity problem. Based on the
function, we then present our Relation-adaptive translating Embedding (RatE)
approach to score each graph triple. Moreover, a novel negative sampling method
is proposed to utilize both prior knowledge and self-adversarial learning for
effective optimization. Experiments verify RatE achieves state-of-the-art
performance on four link prediction benchmarks.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 01:30:30 GMT'}]",2020-10-13,"[['Huang', 'Hao', ''], ['Long', 'Guodong', ''], ['Shen', 'Tao', ''], ['Jiang', 'Jing', ''], ['Zhang', 'Chengqi', '']]"
1361174,2010.04844,James Michaelov,James A. Michaelov and Benjamin K. Bergen,"How well does surprisal explain N400 amplitude under different
  experimental conditions?",To be presented at CoNLL 2020,,,,cs.CL cs.AI cs.IT cs.LG math.IT q-bio.NC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate the extent to which word surprisal can be used to predict a
neural measure of human language processing difficulty - the N400. To do this,
we use recurrent neural networks to calculate the surprisal of stimuli from
previously published neurolinguistic studies of the N400. We find that
surprisal can predict N400 amplitude in a wide range of cases, and the cases
where it cannot do so provide valuable insight into the neurocognitive
processes underlying the response.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 23:18:23 GMT'}]",2020-10-13,"[['Michaelov', 'James A.', ''], ['Bergen', 'Benjamin K.', '']]"
1361172,2010.04842,Arun Tejasvi Chaganty,Justin Dieter and Arun Tejasvi Chaganty,"Conformal retrofitting via Riemannian manifolds: distilling
  task-specific graphs into pretrained embeddings","14 pages, 5 figures",,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained (language) embeddings are versatile, task-agnostic feature
representations of entities, like words, that are central to many machine
learning applications. These representations can be enriched through
retrofitting, a class of methods that incorporate task-specific domain
knowledge encoded as a graph over a subset of these entities. However, existing
retrofitting algorithms face two limitations: they overfit the observed graph
by failing to represent relationships with missing entities; and they underfit
the observed graph by only learning embeddings in Euclidean manifolds, which
cannot faithfully represent even simple tree-structured or cyclic graphs. We
address these problems with two key contributions: (i) we propose a novel
regularizer, a conformality regularizer, that preserves local geometry from the
pretrained embeddings---enabling generalization to missing entities and (ii) a
new Riemannian feedforward layer that learns to map pre-trained embeddings onto
a non-Euclidean manifold that can better represent the entire graph. Through
experiments on WordNet, we demonstrate that the conformality regularizer
prevents even existing (Euclidean-only) methods from overfitting on link
prediction for missing entities, and---together with the Riemannian feedforward
layer---learns non-Euclidean embeddings that outperform them.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 23:06:57 GMT'}]",2020-10-13,"[['Dieter', 'Justin', ''], ['Chaganty', 'Arun Tejasvi', '']]"
1276968,2004.12198,Mengjie Zhao,"Mengjie Zhao, Philipp Dufter, Yadollah Yaghoobzadeh, Hinrich Sch\""utze","Quantifying the Contextualization of Word Representations with Semantic
  Class Probing",EMNLP Findings 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Pretrained language models have achieved a new state of the art on many NLP
tasks, but there are still many open questions about how and why they work so
well. We investigate the contextualization of words in BERT. We quantify the
amount of contextualization, i.e., how well words are interpreted in context,
by studying the extent to which semantic classes of a word can be inferred from
its contextualized embeddings. Quantifying contextualization helps in
understanding and utilizing pretrained language models. We show that top layer
representations achieve high accuracy inferring semantic classes; that the
strongest contextualization effects occur in the lower layers; that local
context is mostly sufficient for semantic class inference; and that top layer
representations are more task-specific after finetuning while lower layer
representations are more transferable. Finetuning uncovers task related
features, but pretrained knowledge is still largely preserved.
","[{'version': 'v1', 'created': 'Sat, 25 Apr 2020 17:49:37 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 12:26:20 GMT'}]",2020-10-13,"[['Zhao', 'Mengjie', ''], ['Dufter', 'Philipp', ''], ['Yaghoobzadeh', 'Yadollah', ''], ['Schütze', 'Hinrich', '']]"
1291582,2005.11787,Nikolai Rozanov,"Anne Lauscher and Olga Majewska and Leonardo F. R. Ribeiro and Iryna
  Gurevych and Nikolai Rozanov and Goran Glava\v{s}","Common Sense or World Knowledge? Investigating Adapter-Based Knowledge
  Injection into Pretrained Transformers","EMNLP 2020 - DeeLIO, ECML 2020 - DECODEML, 5 pages, 4 tables, 3
  references",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Following the major success of neural language models (LMs) such as BERT or
GPT-2 on a variety of language understanding tasks, recent work focused on
injecting (structured) knowledge from external resources into these models.
While on the one hand, joint pretraining (i.e., training from scratch, adding
objectives based on external knowledge to the primary LM objective) may be
prohibitively computationally expensive, post-hoc fine-tuning on external
knowledge, on the other hand, may lead to the catastrophic forgetting of
distributional knowledge. In this work, we investigate models for complementing
the distributional knowledge of BERT with conceptual knowledge from ConceptNet
and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using
adapter training. While overall results on the GLUE benchmark paint an
inconclusive picture, a deeper analysis reveals that our adapter-based models
substantially outperform BERT (up to 15-20 performance points) on inference
tasks that require the type of conceptual knowledge explicitly present in
ConceptNet and OMCS. All code and experiments are open sourced under:
https://github.com/wluper/retrograph .
","[{'version': 'v1', 'created': 'Sun, 24 May 2020 15:49:57 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 11:31:03 GMT'}]",2020-10-13,"[['Lauscher', 'Anne', ''], ['Majewska', 'Olga', ''], ['Ribeiro', 'Leonardo F. R.', ''], ['Gurevych', 'Iryna', ''], ['Rozanov', 'Nikolai', ''], ['Glavaš', 'Goran', '']]"
1360210,2010.03880,Libo Qin,"Libo Qin, Tailu Liu, Wanxiang Che, Bingbing Kang, Sendong Zhao, Ting
  Liu",A Co-Interactive Transformer for Joint Slot Filling and Intent Detection,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Intent detection and slot filling are two main tasks for building a spoken
language understanding (SLU) system. The two tasks are closely related and the
information of one task can be utilized in the other task. Previous studies
either model the two tasks separately or only consider the single information
flow from intent to slot. None of the prior approaches model the bidirectional
connection between the two tasks simultaneously. In this paper, we propose a
Co-Interactive Transformer to consider the cross-impact between the two tasks.
Instead of adopting the self-attention mechanism in vanilla Transformer, we
propose a co-interactive module to consider the cross-impact by building a
bidirectional connection between the two related tasks. In addition, the
proposed co-interactive module can be stacked to incrementally enhance each
other with mutual features. The experimental results on two public datasets
(SNIPS and ATIS) show that our model achieves the state-of-the-art performance
with considerable improvements (+3.4% and +0.9% on overall acc). Extensive
experiments empirically verify that our model successfully captures the mutual
interaction knowledge.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 10:16:52 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 12:29:17 GMT'}]",2020-10-13,"[['Qin', 'Libo', ''], ['Liu', 'Tailu', ''], ['Che', 'Wanxiang', ''], ['Kang', 'Bingbing', ''], ['Zhao', 'Sendong', ''], ['Liu', 'Ting', '']]"
1278646,2004.13876,Marcos Vin\'icius Treviso,Marcos V. Treviso and Andr\'e F. T. Martins,"The Explanation Game: Towards Prediction Explainability through Sparse
  Communication",BlackBoxNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Explainability is a topic of growing importance in NLP. In this work, we
provide a unified perspective of explainability as a communication problem
between an explainer and a layperson about a classifier's decision. We use this
framework to compare several prior approaches for extracting explanations,
including gradient methods, representation erasure, and attention mechanisms,
in terms of their communication success. In addition, we reinterpret these
methods at the light of classical feature selection, and we use this as
inspiration to propose new embedded methods for explainability, through the use
of selective, sparse attention. Experiments in text classification, natural
language entailment, and machine translation, using different configurations of
explainers and laypeople (including both machines and humans), reveal an
advantage of attention-based explainers over gradient and erasure methods.
Furthermore, human evaluation experiments show promising results with post-hoc
explainers trained to optimize communication success and faithfulness.
","[{'version': 'v1', 'created': 'Tue, 28 Apr 2020 22:27:19 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 08:05:13 GMT'}]",2020-10-13,"[['Treviso', 'Marcos V.', ''], ['Martins', 'André F. T.', '']]"
1360428,2010.04098,Varun Gangal,"Varun Gangal, Eduard Hovy","BERTering RAMS: What and How Much does BERT Already Know About Event
  Arguments? -- A Study on the RAMS Dataset","Accepted for the BlackBoxNLP 2020 Workshop @EMNLP 2020;
  Pre-camera-ready copy",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Using the attention map based probing frame-work from (Clark et al., 2019),
we observe that, on the RAMS dataset (Ebner et al., 2020), BERT's attention
heads have modest but well above-chance ability to spot event arguments sans
any training or domain finetuning, vary-ing from a low of 17.77% for Place to a
high of 51.61% for Artifact. Next, we find that linear combinations of these
heads, estimated with approx 11% of available total event argument detection
supervision, can push performance well-higher for some roles - highest two
being Victim (68.29% Accuracy) and Artifact(58.82% Accuracy). Furthermore, we
investigate how well our methods do for cross-sentence event arguments. We
propose a procedure to isolate ""best heads"" for cross-sentence argument
detection separately of those for intra-sentence arguments. The heads thus
estimated have superior cross-sentence performance compared to their jointly
estimated equivalents, albeit only under the unrealistic assumption that we
already know the argument is present in an-other sentence. Lastly, we seek to
isolate to what extent our numbers stem from lexical frequency based
associations between gold arguments and roles. We propose NONCE, a scheme to
create adversarial test examples by replacing gold arguments with randomly
generated ""nonce"" words. We find that learnt linear combinations are robust to
NONCE, though individual best heads can be more sensitive.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 16:27:03 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 19:02:14 GMT'}]",2020-10-13,"[['Gangal', 'Varun', ''], ['Hovy', 'Eduard', '']]"
1360521,2010.04191,Abhishek Singh,Abhishek Singh,PoinT-5: Pointer Network and T-5 based Financial NarrativeSummarisation,Accepted in FNS2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Companies provide annual reports to their shareholders at the end of the
financial year that describes their operations and financial conditions. The
average length of these reports is 80, and it may extend up to 250 pages long.
In this paper, we propose our methodology PoinT-5 (the combination of Pointer
Network and T-5 (Test-to-text transfer Transformer) algorithms) that we used in
the Financial Narrative Summarisation (FNS) 2020 task. The proposed method uses
pointer networks to extract important narrative sentences from the report, and
then T-5 is used to paraphrase extracted sentences into a concise yet
informative sentence. We evaluate our method using ROUGE-N (1,2), L, and SU4.
The proposed method achieves the highest precision scores in all the metrics
and highest F1 scores in ROUGE1, and LCS and the only solution to cross the
MUSE solution baseline in ROUGE-LCS metrics.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 18:09:45 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 03:31:25 GMT'}]",2020-10-13,"[['Singh', 'Abhishek', '']]"
1360955,2010.04625,Omar Shaikh,"Omar Shaikh, Jiaao Chen, Jon Saad-Falcon, Duen Horng Chau and Diyi
  Yang",Examining the Ordering of Rhetorical Strategies in Persuasive Requests,Findings of EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Interpreting how persuasive language influences audiences has implications
across many domains like advertising, argumentation, and propaganda. Persuasion
relies on more than a message's content. Arranging the order of the message
itself (i.e., ordering specific rhetorical strategies) also plays an important
role. To examine how strategy orderings contribute to persuasiveness, we first
utilize a Variational Autoencoder model to disentangle content and rhetorical
strategies in textual requests from a large-scale loan request corpus. We then
visualize interplay between content and strategy through an attentional LSTM
that predicts the success of textual requests. We find that specific (orderings
of) strategies interact uniquely with a request's content to impact success
rate, and thus the persuasiveness of a request.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 15:10:44 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 00:37:11 GMT'}]",2020-10-13,"[['Shaikh', 'Omar', ''], ['Chen', 'Jiaao', ''], ['Saad-Falcon', 'Jon', ''], ['Chau', 'Duen Horng', ''], ['Yang', 'Diyi', '']]"
1361066,2010.04736,Chenhao Tan,"Samuel Carton, Anirudh Rathore, Chenhao Tan",Evaluating and Characterizing Human Rationales,"14 pages, 15 figures, to appear in EMNLP 2020. Code is available at
  https://github.com/BoulderDS/evaluating-human-rationales",,,,cs.CL cs.AI cs.CY cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Two main approaches for evaluating the quality of machine-generated
rationales are: 1) using human rationales as a gold standard; and 2) automated
metrics based on how rationales affect model behavior. An open question,
however, is how human rationales fare with these automatic metrics. Analyzing a
variety of datasets and models, we find that human rationales do not
necessarily perform well on these metrics. To unpack this finding, we propose
improved metrics to account for model-dependent baseline performance. We then
propose two methods to further characterize rationale quality, one based on
model retraining and one on using ""fidelity curves"" to reveal properties such
as irrelevance and redundancy. Our work leads to actionable suggestions for
evaluating and characterizing rationales.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 18:00:04 GMT'}]",2020-10-13,"[['Carton', 'Samuel', ''], ['Rathore', 'Anirudh', ''], ['Tan', 'Chenhao', '']]"
1361074,2010.04744,Christopher Chu,"Christopher Chu, Scot Fang and Kevin Knight",Learning to Pronounce Chinese Without a Pronunciation Dictionary,7 pages. To appear in EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We demonstrate a program that learns to pronounce Chinese text in Mandarin,
without a pronunciation dictionary. From non-parallel streams of Chinese
characters and Chinese pinyin syllables, it establishes a many-to-many mapping
between characters and pronunciations. Using unsupervised methods, the program
effectively deciphers writing into speech. Its token-level
character-to-syllable accuracy is 89%, which significantly exceeds the 22%
accuracy of prior work.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 18:03:49 GMT'}]",2020-10-13,"[['Chu', 'Christopher', ''], ['Fang', 'Scot', ''], ['Knight', 'Kevin', '']]"
1361076,2010.04746,Christopher Chu,"Christopher Chu, Raphael Valenti, Kevin Knight",Solving Historical Dictionary Codes with a Neural Language Model,"10 pages, 6 figures. To appear in EMNLP 2020",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We solve difficult word-based substitution codes by constructing a decoding
lattice and searching that lattice with a neural language model. We apply our
method to a set of enciphered letters exchanged between US Army General James
Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s,
obtained from the US Library of Congress. We are able to decipher 75.1% of the
cipher-word tokens correctly.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 18:04:05 GMT'}]",2020-10-13,"[['Chu', 'Christopher', ''], ['Valenti', 'Raphael', ''], ['Knight', 'Kevin', '']]"
1361077,2010.04747,Christopher Chu,"Arkady Arkhangorodsky, Amittai Axelrod, Christopher Chu, Scot Fang,
  Yiqi Huang, Ajay Nagesh, Xing Shi, Boliang Zhang and Kevin Knight","MEEP: An Open-Source Platform for Human-Human Dialog Collection and
  End-to-End Agent Training",10 pages,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We create a new task-oriented dialog platform (MEEP) where agents are given
considerable freedom in terms of utterances and API calls, but are constrained
to work within a push-button environment. We include facilities for collecting
human-human dialog corpora, and for training automatic agents in an end-to-end
fashion. We demonstrate MEEP with a dialog assistant that lets users specify
trip destinations.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 18:04:17 GMT'}]",2020-10-13,"[['Arkhangorodsky', 'Arkady', ''], ['Axelrod', 'Amittai', ''], ['Chu', 'Christopher', ''], ['Fang', 'Scot', ''], ['Huang', 'Yiqi', ''], ['Nagesh', 'Ajay', ''], ['Shi', 'Xing', ''], ['Zhang', 'Boliang', ''], ['Knight', 'Kevin', '']]"
1361085,2010.04755,Jun Yen Leung,"Jun Yen Leung, Guy Emerson, Ryan Cotterell","Investigating Cross-Linguistic Adjective Ordering Tendencies with a
  Latent-Variable Model","13 pages, 7 tables, 1 figure. To be published in EMNLP 2020
  proceedings",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Across languages, multiple consecutive adjectives modifying a noun (e.g. ""the
big red dog"") follow certain unmarked ordering rules. While explanatory
accounts have been put forward, much of the work done in this area has relied
primarily on the intuitive judgment of native speakers, rather than on corpus
data. We present the first purely corpus-driven model of multi-lingual
adjective ordering in the form of a latent-variable model that can accurately
order adjectives across 24 different languages, even when the training and
testing languages are different. We utilize this novel statistical model to
provide strong converging evidence for the existence of universal,
cross-linguistic, hierarchical adjective ordering tendencies.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 18:27:55 GMT'}]",2020-10-13,"[['Leung', 'Jun Yen', ''], ['Emerson', 'Guy', ''], ['Cotterell', 'Ryan', '']]"
1361092,2010.04762,William Huang,"William Huang, Haokun Liu, and Samuel R. Bowman","Counterfactually-Augmented SNLI Training Data Does Not Yield Better
  Generalization Than Unaugmented Data",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A growing body of work shows that models exploit annotation artifacts to
achieve state-of-the-art performance on standard crowdsourced
benchmarks---datasets collected from crowdworkers to create an evaluation
task---while still failing on out-of-domain examples for the same task. Recent
work has explored the use of counterfactually-augmented data---data built by
minimally editing a set of seed examples to yield counterfactual labels---to
augment training data associated with these benchmarks and build more robust
classifiers that generalize better. However, Khashabi et al. (2020) find that
this type of augmentation yields little benefit on reading comprehension tasks
when controlling for dataset size and cost of collection. We build upon this
work by using English natural language inference data to test model
generalization and robustness and find that models trained on a
counterfactually-augmented SNLI dataset do not generalize better than
unaugmented datasets of similar size and that counterfactual augmentation can
hurt performance, yielding models that are less robust to challenge examples.
Counterfactual augmentation of natural language understanding data through
standard crowdsourcing techniques does not appear to be an effective way of
collecting training data and further innovation is required to make this
general line of work viable.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 18:44:02 GMT'}]",2020-10-13,"[['Huang', 'William', ''], ['Liu', 'Haokun', ''], ['Bowman', 'Samuel R.', '']]"
1361121,2010.04791,Shiyue Zhang,"Shiyue Zhang, Benjamin Frey, Mohit Bansal","ChrEn: Cherokee-English Machine Translation for Endangered Language
  Revitalization",EMNLP 2020 (19 pages),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cherokee is a highly endangered Native American language spoken by the
Cherokee people. The Cherokee culture is deeply embedded in its language.
However, there are approximately only 2,000 fluent first language Cherokee
speakers remaining in the world, and the number is declining every year. To
help save this endangered language, we introduce ChrEn, a Cherokee-English
parallel dataset, to facilitate machine translation research between Cherokee
and English. Compared to some popular machine translation language pairs, ChrEn
is extremely low-resource, only containing 14k sentence pairs in total. We
split our parallel data in ways that facilitate both in-domain and
out-of-domain evaluation. We also collect 5k Cherokee monolingual data to
enable semi-supervised learning. Besides these datasets, we propose several
Cherokee-English and English-Cherokee machine translation systems. We compare
SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems;
supervised versus semi-supervised (via language model, back-translation, and
BERT/Multilingual-BERT) methods; as well as transfer learning versus
multilingual joint training with 4 other languages. Our best results are
15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr
translations, respectively, and we hope that our dataset and systems will
encourage future work by the community for Cherokee language revitalization.
Our data, code, and demo will be publicly available at
https://github.com/ZhangShiyue/ChrEn
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 20:28:06 GMT'}]",2020-10-13,"[['Zhang', 'Shiyue', ''], ['Frey', 'Benjamin', ''], ['Bansal', 'Mohit', '']]"
1361136,2010.04806,Silei Xu,"Silei Xu, Sina J. Semnani, Giovanni Campagna, Monica S. Lam","AutoQA: From Databases To QA Semantic Parsers With Only Synthetic
  Training Data",To appear in EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose AutoQA, a methodology and toolkit to generate semantic parsers
that answer questions on databases, with no manual effort. Given a database
schema and its data, AutoQA automatically generates a large set of high-quality
questions for training that covers different database operations. It uses
automatic paraphrasing combined with template-based parsing to find alternative
expressions of an attribute in different parts of speech. It also uses a novel
filtered auto-paraphraser to generate correct paraphrases of entire sentences.
We apply AutoQA to the Schema2QA dataset and obtain an average logical form
accuracy of 62.9% when tested on natural questions, which is only 6.4% lower
than a model trained with expert natural language annotations and paraphrase
data collected from crowdworkers. To demonstrate the generality of AutoQA, we
also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy,
16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower
than the same model trained with human data.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 21:06:57 GMT'}]",2020-10-13,"[['Xu', 'Silei', ''], ['Semnani', 'Sina J.', ''], ['Campagna', 'Giovanni', ''], ['Lam', 'Monica S.', '']]"
1361156,2010.04826,Prasanna Parthasarathi,Prasanna Parthasarathi and Arvind Neelakantan and Sharan Narang,On Task-Level Dialogue Composition of Generative Transformer Model,"8 pages; Accepted at Workshop on Insights from Negative Results in
  NLP",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task-oriented dialogue systems help users accomplish tasks such as booking a
movie ticket and ordering food via conversation. Generative models
parameterized by a deep neural network are widely used for next turn response
generation in such systems. It is natural for users of the system to want to
accomplish multiple tasks within the same conversation, but the ability of
generative models to compose multiple tasks is not well studied. In this work,
we begin by studying the effect of training human-human task-oriented dialogues
towards improving the ability to compose multiple tasks on Transformer
generative models. To that end, we propose and explore two solutions: (1)
creating synthetic multiple task dialogue data for training from human-human
single task dialogue and (2) forcing the encoder representation to be invariant
to single and multiple task dialogues using an auxiliary loss. The results from
our experiments highlight the difficulty of even the sophisticated variant of
transformer model in learning to compose multiple tasks from single task
dialogues.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 22:10:03 GMT'}]",2020-10-13,"[['Parthasarathi', 'Prasanna', ''], ['Neelakantan', 'Arvind', ''], ['Narang', 'Sharan', '']]"
1361159,2010.04829,Amir Cohen,"Amir DN Cohen, Shachar Rosenman, Yoav Goldberg",Relation Extraction as Two-way Span-Prediction,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The current supervised relation classification (RC) task uses a single
embedding to represent the relation between a pair of entities. We argue that a
better approach is to treat the RC task as a Question answering (QA) like span
prediction problem. We present a span-prediction based system for RC and
evaluate its performance compared to the embedding based system. We achieve
state-of-the-art results on the TACRED and SemEval task 8 datasets.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 22:23:21 GMT'}]",2020-10-13,"[['Cohen', 'Amir DN', ''], ['Rosenman', 'Shachar', ''], ['Goldberg', 'Yoav', '']]"
1361687,2010.05357,Jiahua Chen,Jiahua Chen and Shuai Wang and Sahisnu Mazumder and Bing Liu,"A Knowledge-Driven Approach to Classifying Object and Attribute
  Coreferences in Opinion Mining",Accepted to Proceedings of EMNLP 2020 (Findings),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Classifying and resolving coreferences of objects (e.g., product names) and
attributes (e.g., product aspects) in opinionated reviews is crucial for
improving the opinion mining performance. However, the task is challenging as
one often needs to consider domain-specific knowledge (e.g., iPad is a tablet
and has aspect resolution) to identify coreferences in opinionated reviews.
Also, compiling a handcrafted and curated domain-specific knowledge base for
each domain is very time consuming and arduous. This paper proposes an approach
to automatically mine and leverage domain-specific knowledge for classifying
objects and attribute coreferences. The approach extracts domain-specific
knowledge from unlabeled review data and trains a knowledgeaware neural
coreference classification model to leverage (useful) domain knowledge together
with general commonsense knowledge for the task. Experimental evaluation on
realworld datasets involving five domains (product types) shows the
effectiveness of the approach.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 22:08:43 GMT'}]",2020-10-13,"[['Chen', 'Jiahua', ''], ['Wang', 'Shuai', ''], ['Mazumder', 'Sahisnu', ''], ['Liu', 'Bing', '']]"
1361331,2010.05001,Wanqing Cui,"Wanqing Cui, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng",Beyond Language: Learning Commonsense from Images for Reasoning,Accepted to EMNLP'20 Findings,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper proposes a novel approach to learn commonsense from images,
instead of limited raw texts or costly constructed knowledge bases, for the
commonsense reasoning problem in NLP. Our motivation comes from the fact that
an image is worth a thousand words, where richer scene information could be
leveraged to help distill the commonsense knowledge, which is often hidden in
languages. Our approach, namely Loire, consists of two stages. In the first
stage, a bi-modal sequence-to-sequence approach is utilized to conduct the
scene layout generation task, based on a text representation model ViBERT. In
this way, the required visual scene knowledge, such as spatial relations, will
be encoded in ViBERT by the supervised learning process with some bi-modal data
like COCO. Then ViBERT is concatenated with a pre-trained language model to
perform the downstream commonsense reasoning tasks. Experimental results on two
commonsense reasoning problems, i.e. commonsense question answering and pronoun
resolution, demonstrate that Loire outperforms traditional language-based
methods. We also give some case studies to show what knowledge is learned from
images and explain how the generated scene layout helps the commonsense
reasoning process.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 13:47:13 GMT'}]",2020-10-13,"[['Cui', 'Wanqing', ''], ['Lan', 'Yanyan', ''], ['Pang', 'Liang', ''], ['Guo', 'Jiafeng', ''], ['Cheng', 'Xueqi', '']]"
1361319,2010.04989,Liang Ding,"Lei Zhou, Liang Ding and Koichi Takeda","Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual
  Patterns",To appear in WMT2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes our submission of the WMT 2020 Shared Task on Sentence
Level Direct Assessment, Quality Estimation (QE). In this study, we empirically
reveal the \textit{mismatching issue} when directly adopting BERTScore to QE.
Specifically, there exist lots of mismatching errors between the source
sentence and translated candidate sentence with token pairwise similarity. In
response to this issue, we propose to expose explicit cross-lingual patterns,
\textit{e.g.} word alignments and generation score, to our proposed zero-shot
models. Experiments show that our proposed QE model with explicit cross-lingual
patterns could alleviate the mismatching issue, thereby improving the
performance. Encouragingly, our zero-shot QE method could achieve comparable
performance with supervised QE method, and even outperforms the supervised
counterpart on 2 out of 6 directions. We expect our work could shed light on
the zero-shot QE model improvement.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 13:11:41 GMT'}]",2020-10-13,"[['Zhou', 'Lei', ''], ['Ding', 'Liang', ''], ['Takeda', 'Koichi', '']]"
1361333,2010.05003,Xinyu Wang,"Xinyu Wang, Kewei Tu","Second-Order Neural Dependency Parsing with Message Passing and
  End-to-End Training",Accepted to AACL 2020. 7 pages,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we propose second-order graph-based neural dependency parsing
using message passing and end-to-end neural networks. We empirically show that
our approaches match the accuracy of very recent state-of-the-art second-order
graph-based neural dependency parsers and have significantly faster speed in
both training and testing. We also empirically show the advantage of
second-order parsing over first-order parsing and observe that the usefulness
of the head-selection structured constraint vanishes when using BERT embedding.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 13:49:46 GMT'}]",2020-10-13,"[['Wang', 'Xinyu', ''], ['Tu', 'Kewei', '']]"
1361532,2010.05202,Wangchunshu Zhou,Qifei Li and Wangchunshu Zhou,Connecting the Dots Between Fact Verification and Fake News Detection,Accepted to COLING 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fact verification models have enjoyed a fast advancement in the last two
years with the development of pre-trained language models like BERT and the
release of large scale datasets such as FEVER. However, the challenging problem
of fake news detection has not benefited from the improvement of fact
verification models, which is closely related to fake news detection. In this
paper, we propose a simple yet effective approach to connect the dots between
fact verification and fake news detection. Our approach first employs a text
summarization model pre-trained on news corpora to summarize the long news
article into a short claim. Then we use a fact verification model pre-trained
on the FEVER dataset to detect whether the input news article is real or fake.
Our approach makes use of the recent success of fact verification models and
enables zero-shot fake news detection, alleviating the need of large-scale
training data to train fake news detection models. Experimental results on
FakenewsNet, a benchmark dataset for fake news detection, demonstrate the
effectiveness of our proposed approach.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 09:28:52 GMT'}]",2020-10-13,"[['Li', 'Qifei', ''], ['Zhou', 'Wangchunshu', '']]"
1361553,2010.05223,Harshil Jain,"Harshil Jain, Akshat Agarwal, Kumar Shridhar, Denis Kleyko",End to End Binarized Neural Networks for Text Classification,"14 pages. Accepted at the SustaiNLP Workshop on Simple and Efficient
  Natural Language Processing at EMNLP 2020",,,,cs.LG cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Deep neural networks have demonstrated their superior performance in almost
every Natural Language Processing task, however, their increasing complexity
raises concerns. In particular, these networks require high expenses on
computational hardware, and training budget is a concern for many. Even for a
trained network, the inference phase can be too demanding for
resource-constrained devices, thus limiting its applicability. The
state-of-the-art transformer models are a vivid example. Simplifying the
computations performed by a network is one way of relaxing the complexity
requirements. In this paper, we propose an end to end binarized neural network
architecture for the intent classification task. In order to fully utilize the
potential of end to end binarization, both input representations (vector
embeddings of tokens statistics) and the classifier are binarized. We
demonstrate the efficiency of such architecture on the intent classification of
short texts over three datasets and for text classification with a larger
dataset. The proposed architecture achieves comparable to the state-of-the-art
results on standard intent classification datasets while utilizing ~ 20-40%
lesser memory and training time. Furthermore, the individual components of the
architecture, such as binarized vector embeddings of documents or binarized
classifiers, can be used separately with not necessarily fully binary
architectures.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 11:21:53 GMT'}]",2020-10-13,"[['Jain', 'Harshil', ''], ['Agarwal', 'Akshat', ''], ['Shridhar', 'Kumar', ''], ['Kleyko', 'Denis', '']]"
1361559,2010.05229,Tanya Schmah,Aditya Ohri and Tanya Schmah,Machine Translation of Mathematical Text,"14 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We have implemented a machine translation system, the PolyMath Translator,
for LaTeX documents containing mathematical text. The current implementation
translates English LaTeX to French LaTeX, attaining a BLEU score of 53.5 on a
held-out test corpus of mathematical sentences. It produces LaTeX documents
that can be compiled to PDF without further editing. The system first converts
the body of an input LaTeX document into English sentences containing math
tokens, using the pandoc universal document converter to parse LaTeX input. We
have trained a Transformer-based translator model, using OpenNMT, on a combined
corpus containing a small proportion of domain-specific sentences. Our full
system uses both this Transformer model and Google Translate, the latter being
used as a backup to better handle linguistic features that do not appear in our
training dataset. If the Transformer model does not have confidence in its
translation, as determined by a high perplexity score, then we use Google
Translate with a custom glossary. This backup was used 26% of the time on our
test corpus of mathematical sentences. The PolyMath Translator is available as
a web service at www.polymathtrans.ai.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 11:59:40 GMT'}]",2020-10-13,"[['Ohri', 'Aditya', ''], ['Schmah', 'Tanya', '']]"
1361560,2010.05230,Xinpeng Wang,"Feifei Xu, Xinpeng Wang, Yunpu Ma, Volker Tresp, Yuyi Wang, Shanlin
  Zhou and Haizhou Du",Controllable Multi-Character Psychology-Oriented Story Generation,Accepted by CIKM2020,,10.1145/1122445.1122456,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Story generation, which aims to generate a long and coherent story
automatically based on the title or an input sentence, is an important research
area in the field of natural language generation. There is relatively little
work on story generation with appointed emotions. Most existing works focus on
using only one specific emotion to control the generation of a whole story and
ignore the emotional changes in the characters in the course of the story. In
our work, we aim to design an emotional line for each character that considers
multiple emotions common in psychological theories, with the goal of generating
stories with richer emotional changes in the characters. To the best of our
knowledge, this work is first to focuses on characters' emotional lines in
story generation. We present a novel model-based attention mechanism that we
call SoCP (Storytelling of multi-Character Psychology). We show that the
proposed model can generate stories considering the changes in the
psychological state of different characters. To take into account the
particularity of the model, in addition to commonly used evaluation
indicators(BLEU, ROUGE, etc.), we introduce the accuracy rate of psychological
state control as a novel evaluation metric. The new indicator reflects the
effect of the model on the psychological state control of story characters.
Experiments show that with SoCP, the generated stories follow the psychological
state for each character according to both automatic and human evaluations.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 12:05:00 GMT'}]",2020-10-13,"[['Xu', 'Feifei', ''], ['Wang', 'Xinpeng', ''], ['Ma', 'Yunpu', ''], ['Tresp', 'Volker', ''], ['Wang', 'Yuyi', ''], ['Zhou', 'Shanlin', ''], ['Du', 'Haizhou', '']]"
1361573,2010.05243,Debaditya Pal,"Debaditya Pal, Harsh Sharma, Kaustubh Chaudhari",Data Agnostic RoBERTa-based Natural Language to SQL Query Generation,"8 Pages, 2 figures. Submitted to IEEE/ACM Transactions on Audio,
  Speech and Language Processing",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Relational databases are among the most widely used architectures to store
massive amounts of data in the modern world. However, there is a barrier
between these databases and the average user. The user often lacks the
knowledge of a query language such as SQL required to interact with the
database. The NL2SQL task aims at finding deep learning approaches to solve
this problem by converting natural language questions into valid SQL queries.
Given the sensitive nature of some databases and the growing need for data
privacy, we have presented an approach with data privacy at its core. We have
passed RoBERTa embeddings and data-agnostic knowledge vectors into LSTM based
submodels to predict the final query. Although we have not achieved state of
the art results, we have eliminated the need for the table data, right from the
training of the model, and have achieved a test set execution accuracy of
76.7%. By eliminating the table data dependency while training we have created
a model capable of zero shot learning based on the natural language question
and table schema alone.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 13:18:46 GMT'}]",2020-10-13,"[['Pal', 'Debaditya', ''], ['Sharma', 'Harsh', ''], ['Chaudhari', 'Kaustubh', '']]"
1361578,2010.05248,Qingqing Cao,"Qingqing Cao, Aruna Balasubramanian, Niranjan Balasubramanian",Towards Accurate and Reliable Energy Measurement of NLP Models,Accepted to SustaiNLP 2020 (co-located with EMNLP 2020),,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Accurate and reliable measurement of energy consumption is critical for
making well-informed design choices when choosing and training large scale NLP
models. In this work, we show that existing software-based energy measurements
are not accurate because they do not take into account hardware differences and
how resource utilization affects energy consumption. We conduct energy
measurement experiments with four different models for a question answering
task. We quantify the error of existing software-based energy measurements by
using a hardware power meter that provides highly accurate energy measurements.
Our key takeaway is the need for a more accurate energy estimation model that
takes into account hardware variabilities and the non-linear relationship
between resource utilization and energy consumption. We release the code and
data at https://github.com/csarron/sustainlp2020-energy.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 13:44:52 GMT'}]",2020-10-13,"[['Cao', 'Qingqing', ''], ['Balasubramanian', 'Aruna', ''], ['Balasubramanian', 'Niranjan', '']]"
1361586,2010.05256,Yutai Hou,"Yutai Hou, Yongkui Lai, Yushan Wu, Wanxiang Che, Ting Liu",Few-shot Learning for Multi-label Intent Detection,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we study the few-shot multi-label classification for user
intent detection. For multi-label intent detection, state-of-the-art work
estimates label-instance relevance scores and uses a threshold to select
multiple associated intent labels. To determine appropriate thresholds with
only a few examples, we first learn universal thresholding experience on
data-rich domains, and then adapt the thresholds to certain few-shot domains
with a calibration based on nonparametric learning. For better calculation of
label-instance relevance score, we introduce label name embedding as anchor
points in representation space, which refines representations of different
classes to be well-separated from each other. Experiments on two datasets show
that the proposed model significantly outperforms strong baselines in both
one-shot and five-shot settings.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 14:42:18 GMT'}]",2020-10-13,"[['Hou', 'Yutai', ''], ['Lai', 'Yongkui', ''], ['Wu', 'Yushan', ''], ['Che', 'Wanxiang', ''], ['Liu', 'Ting', '']]"
1361595,2010.05265,Shauli Ravfogel,"Shauli Ravfogel, Yanai Elazar, Jacob Goldberger, Yoav Goldberg","Unsupervised Distillation of Syntactic Information from Contextualized
  Word Representations",Accepted in BlackboxNLP@EMNLP2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contextualized word representations, such as ELMo and BERT, were shown to
perform well on various semantic and syntactic tasks. In this work, we tackle
the task of unsupervised disentanglement between semantics and structure in
neural language representations: we aim to learn a transformation of the
contextualized vectors, that discards the lexical semantics, but keeps the
structural information. To this end, we automatically generate groups of
sentences which are structurally similar but semantically different, and use
metric-learning approach to learn a transformation that emphasizes the
structural component that is encoded in the vectors. We demonstrate that our
transformation clusters vectors in space by structural properties, rather than
by lexical semantics. Finally, we demonstrate the utility of our distilled
representations by showing that they outperform the original contextualized
representations in a few-shot parsing setting.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 15:13:18 GMT'}]",2020-10-13,"[['Ravfogel', 'Shauli', ''], ['Elazar', 'Yanai', ''], ['Goldberger', 'Jacob', ''], ['Goldberg', 'Yoav', '']]"
1361599,2010.05269,"Mika H\""am\""al\""ainen","Khalid Alnajjar, Mika H\""am\""al\""ainen, Niko Partanen, Jack Rueter",Automated Prediction of Medieval Arabic Diacritics,,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This study uses a character level neural machine translation approach trained
on a long short-term memory-based bi-directional recurrent neural network
architecture for diacritization of Medieval Arabic. The results improve from
the online tool used as a baseline. A diacritization model have been published
openly through an easy to use Python package available on PyPi and Zenodo. We
have found that context size should be considered when optimizing a feasible
prediction model.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 15:21:01 GMT'}]",2020-10-13,"[['Alnajjar', 'Khalid', ''], ['Hämäläinen', 'Mika', ''], ['Partanen', 'Niko', ''], ['Rueter', 'Jack', '']]"
1361623,2010.05293,Jared Millson,Jared Millson,A Defeasible Calculus for Zetetic Agents,,,10.12775/LLP.2020.019,,cs.AI cs.CL math.LO,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The study of defeasible reasoning unites epistemologists with those working
in AI, in part, because both are interested in epistemic rationality. While it
is traditionally thought to govern the formation and (with)holding of beliefs,
epistemic rationality may also apply to the interrogative attitudes associated
with our core epistemic practice of inquiry, such as wondering, investigating,
and curiosity. Since generally intelligent systems should be capable of
rational inquiry, AI researchers have a natural interest in the norms that
govern interrogative attitudes. Following its recent coinage, we use the term
""zetetic"" to refer to the properties and norms associated with the capacity to
inquire. In this paper, we argue that zetetic norms can be modeled via
defeasible inferences to and from questions---a.k.a erotetic inferences---in a
manner similar to the way norms of epistemic rationality are represented by
defeasible inference rules. We offer a sequent calculus that accommodates the
unique features of ""erotetic defeat"" and that exhibits the computational
properties needed to inform the design of zetetic agents. The calculus
presented here is an improved version of the one presented in Millson (2019),
extended to cover a new class of defeasible erotetic inferences.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 17:39:03 GMT'}]",2020-10-13,"[['Millson', 'Jared', '']]"
1361647,2010.05317,Sai Prabhakar Pandi Selvaraj,"Dhruvesh Patel, Sandeep Konam, Sai P. Selvaraj","Weakly Supervised Medication Regimen Extraction from Medical
  Conversations","To appear in the Proceedings of the Clinical Natural Language
  Processing Workshop, EMNLP, 2020",,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated Medication Regimen (MR) extraction from medical conversations can
not only improve recall and help patients follow through with their care plan,
but also reduce the documentation burden for doctors. In this paper, we focus
on extracting spans for frequency, route and change, corresponding to
medications discussed in the conversation. We first describe a unique dataset
of annotated doctor-patient conversations and then present a weakly supervised
model architecture that can perform span extraction using noisy classification
data. The model utilizes an attention bottleneck inside a classification model
to perform the extraction. We experiment with several variants of attention
scoring and projection functions and propose a novel transformer-based
attention scoring function (TAScore). The proposed combination of TAScore and
Fusedmax projection achieves a 10 point increase in Longest Common Substring F1
compared to the baseline of additive scoring plus softmax projection.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 18:53:03 GMT'}]",2020-10-13,"[['Patel', 'Dhruvesh', ''], ['Konam', 'Sandeep', ''], ['Selvaraj', 'Sai P.', '']]"
1361648,2010.05318,Tharindu Ranasinghe Mr,"Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov",TransQuest at WMT2020: Sentence-Level Direct Assessment,Accepted to WMT 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper presents the team TransQuest's participation in Sentence-Level
Direct Assessment shared task in WMT 2020. We introduce a simple QE framework
based on cross-lingual transformers, and we use it to implement and evaluate
two different neural architectures. The proposed methods achieve
state-of-the-art results surpassing the results obtained by OpenKiwi, the
baseline used in the shared task. We further fine tune the QE framework by
performing ensemble and data augmentation. Our approach is the winning solution
in all of the language pairs according to the WMT 2020 official results.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 18:53:05 GMT'}]",2020-10-13,"[['Ranasinghe', 'Tharindu', ''], ['Orasan', 'Constantin', ''], ['Mitkov', 'Ruslan', '']]"
1361654,2010.05324,Tharindu Ranasinghe Mr,"Tharindu Ranasinghe, Marcos Zampieri","Multilingual Offensive Language Identification with Cross-lingual
  Embeddings",Accepted to EMNLP 2020,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Offensive content is pervasive in social media and a reason for concern to
companies and government organizations. Several studies have been recently
published investigating methods to detect the various forms of such content
(e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of
these studies deal with English partially because most annotated datasets
available contain English data. In this paper, we take advantage of English
data available by applying cross-lingual contextual word embeddings and
transfer learning to make predictions in languages with less resources. We
project predictions on comparable data in Bengali, Hindi, and Spanish and we
report results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and
0.7513 F1 macro for Spanish. Finally, we show that our approach compares
favorably to the best systems submitted to recent shared tasks on these three
languages, confirming the robustness of cross-lingual contextual embeddings and
transfer learning for this task.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 19:17:24 GMT'}]",2020-10-13,"[['Ranasinghe', 'Tharindu', ''], ['Zampieri', 'Marcos', '']]"
1361657,2010.05327,Tharindu Ranasinghe Mr,"Hansi Hettiarachchi, Tharindu Ranasinghe","InfoMiner at WNUT-2020 Task 2: Transformer-based Covid-19 Informative
  Tweet Extraction","Accepted to the 6th Workshop on Noisy User-generated Text (W-NUT) at
  EMNLP 2020",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Identifying informative tweets is an important step when building information
extraction systems based on social media. WNUT-2020 Task 2 was organised to
recognise informative tweets from noise tweets. In this paper, we present our
approach to tackle the task objective using transformers. Overall, our approach
achieves 10th place in the final rankings scoring 0.9004 F1 score for the test
set.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 19:31:18 GMT'}]",2020-10-13,"[['Hettiarachchi', 'Hansi', ''], ['Ranasinghe', 'Tharindu', '']]"
1361660,2010.05330,Brielen Madureira,Brielen Madureira and David Schlangen,"Incremental Processing in the Age of Non-Incremental Encoders: An
  Empirical Assessment of Bidirectional Models for Incremental NLU",Accepted to the EMNLP 2020 conference (long paper),,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  While humans process language incrementally, the best language encoders
currently used in NLP do not. Both bidirectional LSTMs and Transformers assume
that the sequence that is to be encoded is available in full, to be processed
either forwards and backwards (BiLSTMs) or as a whole (Transformers). We
investigate how they behave under incremental interfaces, when partial output
must be provided based on partial input seen up to a certain time step, which
may happen in interactive systems. We test five models on various NLU datasets
and compare their performance using three incremental evaluation metrics. The
results support the possibility of using bidirectional encoders in incremental
mode while retaining most of their non-incremental quality. The
""omni-directional"" BERT model, which achieves better non-incremental
performance, is impacted more by the incremental access. This can be alleviated
by adapting the training regime (truncated training), or the testing procedure,
by delaying the output until some right context is available or by
incorporating hypothetical right contexts generated by a language model like
GPT-2.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 19:51:21 GMT'}]",2020-10-13,"[['Madureira', 'Brielen', ''], ['Schlangen', 'David', '']]"
1361662,2010.05332,Danielle Saunders,Danielle Saunders and Rosie Sallis and Bill Byrne,"Neural Machine Translation Doesn't Translate Gender Coreference Right
  Unless You Make It","Workshop on Gender Bias in NLP, 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural Machine Translation (NMT) has been shown to struggle with grammatical
gender that is dependent on the gender of human referents, which can cause
gender bias effects. Many existing approaches to this problem seek to control
gender inflection in the target language by explicitly or implicitly adding a
gender feature to the source sentence, usually at the sentence level.
  In this paper we propose schemes for incorporating explicit word-level gender
inflection tags into NMT. We explore the potential of this gender-inflection
controlled translation when the gender feature can be determined from a human
reference, assessing on English-to-Spanish and English-to-German translation.
  We find that simple existing approaches can over-generalize a gender-feature
to multiple entities in a sentence, and suggest an effective alternative in the
form of tagged coreference adaptation data. We also propose an extension to
assess translations of gender-neutral entities from English given a
corresponding linguistic convention in the inflected target language.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 20:05:42 GMT'}]",2020-10-13,"[['Saunders', 'Danielle', ''], ['Sallis', 'Rosie', ''], ['Byrne', 'Bill', '']]"
1361663,2010.05333,Danielle Saunders,Danielle Saunders and Bill Byrne,"Addressing Exposure Bias With Document Minimum Risk Training: Cambridge
  at the WMT20 Biomedical Translation Task",WMT20 biomedical task,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The 2020 WMT Biomedical translation task evaluated Medline abstract
translations. This is a small-domain translation task, meaning limited relevant
training data with very distinct style and vocabulary. Models trained on such
data are susceptible to exposure bias effects, particularly when training
sentence pairs are imperfect translations of each other. This can result in
poor behaviour during inference if the model learns to neglect the source
sentence.
  The UNICAM entry addresses this problem during fine-tuning using a robust
variant on Minimum Risk Training. We contrast this approach with data-filtering
to remove `problem' training examples. Under MRT fine-tuning we obtain good
results for both directions of English-German and English-Spanish biomedical
translation. In particular we achieve the best English-to-Spanish translation
result and second-best Spanish-to-English result, despite using only single
models with no ensembling.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 20:09:43 GMT'}]",2020-10-13,"[['Saunders', 'Danielle', ''], ['Byrne', 'Bill', '']]"
1361668,2010.05338,Ramy Baly,"Ramy Baly, Giovanni Da San Martino, James Glass and Preslav Nakov","We Can Detect Your Bias: Predicting the Political Ideology of News
  Articles","Political bias, bias in news, neural networks bias, adversarial
  adaptation, triplet loss, transformers, recurrent neural networks",EMNLP-2020,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We explore the task of predicting the leading political ideology or bias of
news articles. First, we collect and release a large dataset of 34,737 articles
that were manually annotated for political ideology -left, center, or right-,
which is well-balanced across both topics and media. We further use a
challenging experimental setup where the test examples come from media that
were not seen during training, which prevents the model from learning to detect
the source of the target news article instead of predicting its political
ideology. From a modeling perspective, we propose an adversarial media
adaptation, as well as a specially adapted triplet loss. We further add
background information about the source, and we show that it is quite helpful
for improving article-level prediction. Our experimental results show very
sizable improvements over using state-of-the-art pre-trained Transformers in
this challenging setup.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 20:27:55 GMT'}]",2020-10-13,"[['Baly', 'Ramy', ''], ['Martino', 'Giovanni Da San', ''], ['Glass', 'James', ''], ['Nakov', 'Preslav', '']]"
1361675,2010.05345,Xikun Zhang,"Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, Dan Roth",Do Language Embeddings Capture Scales?,Accepted at EMNLP Findings 2020 and EMNLP BlackboxNLP workshop 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained Language Models (LMs) have been shown to possess significant
linguistic, common sense, and factual knowledge. One form of knowledge that has
not been studied yet in this context is information about the scalar magnitudes
of objects. We show that pretrained language models capture a significant
amount of this information but are short of the capability required for general
common-sense reasoning. We identify contextual information in pre-training and
numeracy as two key factors affecting their performance and show that a simple
method of canonicalizing numbers can have a significant effect on the results.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 21:11:09 GMT'}]",2020-10-13,"[['Zhang', 'Xikun', ''], ['Ramachandran', 'Deepak', ''], ['Tenney', 'Ian', ''], ['Elazar', 'Yanai', ''], ['Roth', 'Dan', '']]"
1361524,2010.05194,Giannis Karamanolakis,"Ziyi Liu, Giannis Karamanolakis, Daniel Hsu, Luis Gravano","Detecting Foodborne Illness Complaints in Multiple Languages Using
  English Annotations Only","Accepted for the 11th International Workshop on Health Text Mining
  and Information Analysis (LOUHI@EMNLP 2020)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Health departments have been deploying text classification systems for the
early detection of foodborne illness complaints in social media documents such
as Yelp restaurant reviews. Current systems have been successfully applied for
documents in English and, as a result, a promising direction is to increase
coverage and recall by considering documents in additional languages, such as
Spanish or Chinese. Training previous systems for more languages, however,
would be expensive, as it would require the manual annotation of many documents
for each new target language. To address this challenge, we consider
cross-lingual learning and train multilingual classifiers using only the
annotations for English-language reviews. Recent zero-shot approaches based on
pre-trained multi-lingual BERT (mBERT) have been shown to effectively align
languages for aspects such as sentiment. Interestingly, we show that those
approaches are less effective for capturing the nuances of foodborne illness,
our public health application of interest. To improve performance without extra
annotations, we create artificial training documents in the target language
through machine translation and train mBERT jointly for the source (English)
and target language. Furthermore, we show that translating labeled documents to
multiple languages leads to additional performance improvements for some target
languages. We demonstrate the benefits of our approach through extensive
experiments with Yelp restaurant reviews in seven languages. Our classifiers
identify foodborne illness complaints in multilingual reviews from the Yelp
Challenge dataset, which highlights the potential of our general approach for
deployment in health departments.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 08:46:17 GMT'}]",2020-10-13,"[['Liu', 'Ziyi', ''], ['Karamanolakis', 'Giannis', ''], ['Hsu', 'Daniel', ''], ['Gravano', 'Luis', '']]"
1361332,2010.05002,Prafull Prakash,"Prafull Prakash, Saurabh Kumar Shashidhar, Wenlong Zhao, Subendhu
  Rongali, Haidar Khan, Michael Kayser","Compressing Transformer-Based Semantic Parsing Models using
  Compositional Code Embeddings",Accepted at EMNLP 2020 (Findings); 7 Pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The current state-of-the-art task-oriented semantic parsing models use BERT
or RoBERTa as pretrained encoders; these models have huge memory footprints.
This poses a challenge to their deployment for voice assistants such as Amazon
Alexa and Google Assistant on edge devices with limited memory budgets. We
propose to learn compositional code embeddings to greatly reduce the sizes of
BERT-base and RoBERTa-base. We also apply the technique to DistilBERT,
ALBERT-base, and ALBERT-large, three already compressed BERT variants which
attain similar state-of-the-art performances on semantic parsing with much
smaller model sizes. We observe 95.15% ~ 98.46% embedding compression rates and
20.47% ~ 34.22% encoder compression rates, while preserving greater than 97.5%
semantic parsing performances. We provide the recipe for training and analyze
the trade-off between code embedding sizes and downstream performances.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 13:47:55 GMT'}]",2020-10-13,"[['Prakash', 'Prafull', ''], ['Shashidhar', 'Saurabh Kumar', ''], ['Zhao', 'Wenlong', ''], ['Rongali', 'Subendhu', ''], ['Khan', 'Haidar', ''], ['Kayser', 'Michael', '']]"
1361523,2010.05193,Chenhui Chu,"Vipul Mishra, Chenhui Chu and Yuki Arase",Lexically Cohesive Neural Machine Translation with Copy Mechanism,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lexically cohesive translations preserve consistency in word choices in
document-level translation. We employ a copy mechanism into a context-aware
neural machine translation model to allow copying words from previous
translation outputs. Different from previous context-aware neural machine
translation models that handle all the discourse phenomena implicitly, our
model explicitly addresses the lexical cohesion problem by boosting the
probabilities to output words consistently. We conduct experiments on Japanese
to English translation using an evaluation dataset for discourse translation.
The results showed that the proposed model significantly improved lexical
cohesion compared to previous context-aware models.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 08:39:02 GMT'}]",2020-10-13,"[['Mishra', 'Vipul', ''], ['Chu', 'Chenhui', ''], ['Arase', 'Yuki', '']]"
1361515,2010.05185,Chenhui Chu,"Chenhui Chu, Yuto Takebayashi, Mishra Vipul, Yuta Nakashima",Constructing a Visual Relationship Authenticity Dataset,,,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A visual relationship denotes a relationship between two objects in an image,
which can be represented as a triplet of (subject; predicate; object). Visual
relationship detection is crucial for scene understanding in images. Existing
visual relationship detection datasets only contain true relationships that
correctly describe the content in an image. However, distinguishing false
visual relationships from true ones is also crucial for image understanding and
grounded natural language processing. In this paper, we construct a visual
relationship authenticity dataset, where both true and false relationships
among all objects appeared in the captions in the Flickr30k entities image
caption dataset are annotated. The dataset is available at
https://github.com/codecreator2053/VR_ClassifiedDataset. We hope that this
dataset can promote the study on both vision and language understanding.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 07:38:33 GMT'}]",2020-10-13,"[['Chu', 'Chenhui', ''], ['Takebayashi', 'Yuto', ''], ['Vipul', 'Mishra', ''], ['Nakashima', 'Yuta', '']]"
1361336,2010.05006,Xinyu Wang,"Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei
  Huang, Kewei Tu",Automated Concatenation of Embeddings for Structured Prediction,"We propose ACE, which achieves new SOTA for 6 NLP tasks over 23
  datasets. Under review as a conference paper at ICLR 2021. 19 pages",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained contextualized embeddings are powerful word representations for
structured prediction tasks. Recent work found that better word representations
can be obtained by concatenating different types of embeddings. However, the
selection of embeddings to form the best concatenated representation usually
varies depending on the task and the collection of candidate embeddings, and
the ever-increasing number of embedding types makes it a more difficult
problem. In this paper, we propose Automated Concatenation of Embeddings (ACE)
to automate the process of finding better concatenations of embeddings for
structured prediction tasks, based on a formulation inspired by recent progress
on neural architecture search. Specifically, a controller alternately samples a
concatenation of embeddings, according to its current belief of the
effectiveness of individual embedding types in consideration for a task, and
updates the belief based on a reward. We follow strategies in reinforcement
learning to optimize the parameters of the controller and compute the reward
based on the accuracy of a task model, which is fed with the sampled
concatenation as input and trained on a task dataset. Empirical results on 6
tasks and 23 datasets show that our approach outperforms strong baselines and
achieves state-of-the-art performance with fine-tuned embeddings in the vast
majority of evaluations.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 14:03:20 GMT'}]",2020-10-13,"[['Wang', 'Xinyu', ''], ['Jiang', 'Yong', ''], ['Bach', 'Nguyen', ''], ['Wang', 'Tao', ''], ['Huang', 'Zhongqiang', ''], ['Huang', 'Fei', ''], ['Tu', 'Kewei', '']]"
1361340,2010.05010,Xinyu Wang,"Xinyu Wang, Yong Jiang, Zhaohui Yan, Zixia Jia, Nguyen Bach, Tao Wang,
  Zhongqiang Huang, Fei Huang, Kewei Tu",Structural Knowledge Distillation,Under review as a conference paper of ICLR 2021. 15 pages,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Knowledge distillation is a critical technique to transfer knowledge between
models, typically from a large model (the teacher) to a smaller one (the
student). The objective function of knowledge distillation is typically the
cross-entropy between the teacher and the student's output distributions.
However, for structured prediction problems, the output space is exponential in
size; therefore, the cross-entropy objective becomes intractable to compute and
optimize directly. In this paper, we derive a factorized form of the knowledge
distillation objective for structured prediction, which is tractable for many
typical choices of the teacher and student models. In particular, we show the
tractability and empirical effectiveness of structural knowledge distillation
between sequence labeling and dependency parsing models under four different
scenarios: 1) the teacher and student share the same factorization form of the
output structure scoring function; 2) the student factorization produces
smaller substructures than the teacher factorization; 3) the teacher
factorization produces smaller substructures than the student factorization; 4)
the factorization forms from the teacher and the student are incompatible.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 14:19:25 GMT'}]",2020-10-13,"[['Wang', 'Xinyu', ''], ['Jiang', 'Yong', ''], ['Yan', 'Zhaohui', ''], ['Jia', 'Zixia', ''], ['Bach', 'Nguyen', ''], ['Wang', 'Tao', ''], ['Huang', 'Zhongqiang', ''], ['Huang', 'Fei', ''], ['Tu', 'Kewei', '']]"
1361420,2010.05090,Kunal Chawla,"Kunal Chawla, Diyi Yang","Semi-supervised Formality Style Transfer using Language Model
  Discriminator and Mutual Information Maximization",EMNLP 2020 Findings,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Formality style transfer is the task of converting informal sentences to
grammatically-correct formal sentences, which can be used to improve
performance of many downstream NLP tasks. In this work, we propose a
semi-supervised formality style transfer model that utilizes a language
model-based discriminator to maximize the likelihood of the output sentence
being formal, which allows us to use maximization of token-level conditional
probabilities for training. We further propose to maximize mutual information
between source and target styles as our training objective instead of
maximizing the regular likelihood that often leads to repetitive and trivial
generated responses. Experiments showed that our model outperformed previous
state-of-the-art baselines significantly in terms of both automated metrics and
human judgement. We further generalized our model to unsupervised text style
transfer task, and achieved significant improvements on two benchmark sentiment
style transfer datasets.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 21:05:56 GMT'}]",2020-10-13,"[['Chawla', 'Kunal', ''], ['Yang', 'Diyi', '']]"
1361426,2010.05096,Surabhi Datta,Surabhi Datta and Shekhar Khanpara and Roy F. Riascos and Kirk Roberts,"Leveraging Spatial Information in Radiology Reports for Ischemic Stroke
  Phenotyping",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Classifying fine-grained ischemic stroke phenotypes relies on identifying
important clinical information. Radiology reports provide relevant information
with context to determine such phenotype information. We focus on stroke
phenotypes with location-specific information: brain region affected,
laterality, stroke stage, and lacunarity. We use an existing fine-grained
spatial information extraction system--Rad-SpatialNet--to identify clinically
important information and apply simple domain rules on the extracted
information to classify phenotypes. The performance of our proposed approach is
promising (recall of 89.62% for classifying brain region and 74.11% for
classifying brain region, side, and stroke stage together). Our work
demonstrates that an information extraction system based on a fine-grained
schema can be utilized to determine complex phenotypes with the inclusion of
simple domain rules. These phenotypes have the potential to facilitate stroke
research focusing on post-stroke outcome and treatment planning based on the
stroke location.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 21:35:42 GMT'}]",2020-10-13,"[['Datta', 'Surabhi', ''], ['Khanpara', 'Shekhar', ''], ['Riascos', 'Roy F.', ''], ['Roberts', 'Kirk', '']]"
1361433,2010.05103,Stephen Mussmann,"Stephen Mussmann, Robin Jia, Percy Liang","On the Importance of Adaptive Data Collection for Extremely Imbalanced
  Pairwise Tasks",In Findings of EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many pairwise classification tasks, such as paraphrase detection and
open-domain question answering, naturally have extreme label imbalance (e.g.,
$99.99\%$ of examples are negatives). In contrast, many recent datasets
heuristically choose examples to ensure label balance. We show that these
heuristics lead to trained models that generalize poorly: State-of-the art
models trained on QQP and WikiQA each have only $2.4\%$ average precision when
evaluated on realistically imbalanced test data. We instead collect training
data with active learning, using a BERT-based embedding model to efficiently
retrieve uncertain points from a very large pool of unlabeled utterance pairs.
By creating balanced training data with more informative negative examples,
active learning greatly improves average precision to $32.5\%$ on QQP and
$20.1\%$ on WikiQA.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 21:56:27 GMT'}]",2020-10-13,"[['Mussmann', 'Stephen', ''], ['Jia', 'Robin', ''], ['Liang', 'Percy', '']]"
1361436,2010.05106,Mehrad Moradshahi,"Mehrad Moradshahi, Giovanni Campagna, Sina J. Semnani, Silei Xu,
  Monica S. Lam","Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine
  Translation",Published in EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural
Machine Translation (NMT) systems to localize a semantic parser for a new
language. Our methodology is to (1) generate training data automatically in the
target language by augmenting machine-translated datasets with local entities
scraped from public websites, (2) add a few-shot boost of human-translated
sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model
on natural utterances curated using human translators.
  We assess the effectiveness of our approach by extending the current
capabilities of Schema2QA, a system for English Question Answering (QA) on the
open web, to 10 new languages for the restaurants and hotels domains. Our
models achieve an overall test accuracy ranging between 61% and 69% for the
hotels domain and between 64% and 78% for restaurants domain, which compares
favorably to 69% and 80% obtained for English parser trained on gold English
data and a few examples from validation set. We show our approach outperforms
the previous state-of-the-art methodology by more than 30% for hotels and 40%
for restaurants with localized ontologies for the subset of languages tested.
  Our methodology enables any software developer to add a new language
capability to a QA system for a new domain, leveraging machine translation, in
less than 24 hours.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 22:03:58 GMT'}]",2020-10-13,"[['Moradshahi', 'Mehrad', ''], ['Campagna', 'Giovanni', ''], ['Semnani', 'Sina J.', ''], ['Xu', 'Silei', ''], ['Lam', 'Monica S.', '']]"
1361441,2010.05111,Shyam Subramanian,"Shyam Subramanian, Kyumin Lee","Hierarchical Evidence Set Modeling for Automated Fact Extraction and
  Verification","12 pages, 7 figures. Accepted to EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated fact extraction and verification is a challenging task that
involves finding relevant evidence sentences from a reliable corpus to verify
the truthfulness of a claim. Existing models either (i) concatenate all the
evidence sentences, leading to the inclusion of redundant and noisy
information; or (ii) process each claim-evidence sentence pair separately and
aggregate all of them later, missing the early combination of related sentences
for more accurate claim verification. Unlike the prior works, in this paper, we
propose Hierarchical Evidence Set Modeling (HESM), a framework to extract
evidence sets (each of which may contain multiple evidence sentences), and
verify a claim to be supported, refuted or not enough info, by encoding and
attending the claim and evidence sets at different levels of hierarchy. Our
experimental results show that HESM outperforms 7 state-of-the-art methods for
fact extraction and claim verification. Our source code is available at
https://github.com/ShyamSubramanian/HESM.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 22:27:17 GMT'}]",2020-10-13,"[['Subramanian', 'Shyam', ''], ['Lee', 'Kyumin', '']]"
1361520,2010.05190,Siddharth Karamcheti,"Siddharth Karamcheti, Dorsa Sadigh, Percy Liang",Learning Adaptive Language Interfaces through Decomposition,"Accepted at the 1st Workshop for Interactive and Executable Semantic
  Parsing (IntEx-SemPar) @ EMNLP 2020. 11 pages, 5 figures",,,,cs.CL cs.AI cs.LG cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Our goal is to create an interactive natural language interface that
efficiently and reliably learns from users to complete tasks in simulated
robotics settings. We introduce a neural semantic parsing system that learns
new high-level abstractions through decomposition: users interactively teach
the system by breaking down high-level utterances describing novel behavior
into low-level steps that it can understand. Unfortunately, existing methods
either rely on grammars which parse sentences with limited flexibility, or
neural sequence-to-sequence models that do not learn efficiently or reliably
from individual examples. Our approach bridges this gap, demonstrating the
flexibility of modern neural systems, as well as the one-shot reliable
generalization of grammar-based methods. Our crowdsourced interactive
experiments suggest that over time, users complete complex tasks more
efficiently while using our system by leveraging what they just taught. At the
same time, getting users to trust the system enough to be incentivized to teach
high-level utterances is still an ongoing challenge. We end with a discussion
of some of the obstacles we need to overcome to fully realize the potential of
the interactive paradigm.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 08:27:07 GMT'}]",2020-10-13,"[['Karamcheti', 'Siddharth', ''], ['Sadigh', 'Dorsa', ''], ['Liang', 'Percy', '']]"
1361969,2010.05639,Qiao Jin,"Qiao Jin, Chuanqi Tan, Mosha Chen, Xiaozhong Liu, Songfang Huang",Predicting Clinical Trial Results by Implicit Evidence Integration,EMNLP 2020 long paper,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Clinical trials provide essential guidance for practicing Evidence-Based
Medicine, though often accompanying with unendurable costs and risks. To
optimize the design of clinical trials, we introduce a novel Clinical Trial
Result Prediction (CTRP) task. In the CTRP framework, a model takes a
PICO-formatted clinical trial proposal with its background as input and
predicts the result, i.e. how the Intervention group compares with the
Comparison group in terms of the measured Outcome in the studied Population.
While structured clinical evidence is prohibitively expensive for manual
collection, we exploit large-scale unstructured sentences from medical
literature that implicitly contain PICOs and results as evidence. Specifically,
we pre-train a model to predict the disentangled results from such implicit
evidence and fine-tune the model with limited data on the downstream datasets.
Experiments on the benchmark Evidence Integration dataset show that the
proposed model outperforms the baselines by large margins, e.g., with a 10.7%
relative gain over BioBERT in macro-F1. Moreover, the performance improvement
is also validated on another dataset composed of clinical trials related to
COVID-19.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 12:25:41 GMT'}]",2020-10-13,"[['Jin', 'Qiao', ''], ['Tan', 'Chuanqi', ''], ['Chen', 'Mosha', ''], ['Liu', 'Xiaozhong', ''], ['Huang', 'Songfang', '']]"
1361459,2010.05129,Dongyeop Kang,"Dongyeop Kang, Andrew Head, Risham Sidhu, Kyle Lo, Daniel S. Weld,
  Marti A. Hearst","Document-Level Definition Detection in Scholarly Documents: Existing
  Models, Error Analyses, and Future Directions","Workshop on Scholarly Document Processing (SDP), EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task of definition detection is important for scholarly papers, because
papers often make use of technical terminology that may be unfamiliar to
readers. Despite prior work on definition detection, current approaches are far
from being accurate enough to use in real-world applications. In this paper, we
first perform in-depth error analysis of the current best performing definition
detection system and discover major causes of errors. Based on this analysis,
we develop a new definition detection system, HEDDEx, that utilizes syntactic
features, transformer encoders, and heuristic filters, and evaluate it on a
standard sentence-level benchmark. Because current benchmarks evaluate randomly
sampled sentences, we propose an alternative evaluation that assesses every
sentence within a document. This allows for evaluating recall in addition to
precision. HEDDEx outperforms the leading system on both the sentence-level and
the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively.
We note that performance on the high-recall document-level task is much lower
than in the standard evaluation approach, due to the necessity of incorporation
of document structure as features. We discuss remaining challenges in
document-level definition detection, ideas for improvements, and potential
issues for the development of reading aid applications.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 01:16:10 GMT'}]",2020-10-13,"[['Kang', 'Dongyeop', ''], ['Head', 'Andrew', ''], ['Sidhu', 'Risham', ''], ['Lo', 'Kyle', ''], ['Weld', 'Daniel S.', ''], ['Hearst', 'Marti A.', '']]"
1361471,2010.05141,Dongyeop Kang,"Dongyeop Kang, Eduard Hovy",Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the recent success of contextualized language models on various NLP
tasks, language model itself cannot capture textual coherence of a long,
multi-sentence document (e.g., a paragraph). Humans often make structural
decisions on what and how to say about before making utterances. Guiding
surface realization with such high-level decisions and structuring text in a
coherent way is essentially called a planning process. Where can the model
learn such high-level coherence? A paragraph itself contains various forms of
inductive coherence signals called self-supervision in this work, such as
sentence orders, topical keywords, rhetorical structures, and so on. Motivated
by that, this work proposes a new paragraph completion task PARCOM; predicting
masked sentences in a paragraph. However, the task suffers from predicting and
selecting appropriate topical content with respect to the given context. To
address that, we propose a self-supervised text planner SSPlanner that predicts
what to say first (content prediction), then guides the pretrained language
model (surface realization) using the predicted content. SSPlanner outperforms
the baseline generation models on the paragraph completion task in both
automatic and human evaluation. We also find that a combination of noun and
verb types of keywords is the most effective for content selection. As more
number of content keywords are provided, overall generation quality also
increases.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 02:38:21 GMT'}]",2020-10-13,"[['Kang', 'Dongyeop', ''], ['Hovy', 'Eduard', '']]"
1361473,2010.05143,Xiang Yue,Xiang Yue and Shuang Zhou,"PHICON: Improving Generalization of Clinical Text De-identification
  Models via Data Augmentation",Accepted by The 3rd ClinicalNLP Workshop at EMNLP'20,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  De-identification is the task of identifying protected health information
(PHI) in the clinical text. Existing neural de-identification models often fail
to generalize to a new dataset. We propose a simple yet effective data
augmentation method PHICON to alleviate the generalization issue. PHICON
consists of PHI augmentation and Context augmentation, which creates augmented
training corpora by replacing PHI entities with named-entities sampled from
external sources, and by changing background context with synonym replacement
or random word insertion, respectively. Experimental results on the i2b2 2006
and 2014 de-identification challenge datasets show that PHICON can help three
selected de-identification models boost F1-score (by at most 8.6%) on
cross-dataset test setting. We also discuss how much augmentation to use and
how each augmentation method influences the performance.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 02:57:11 GMT'}]",2020-10-13,"[['Yue', 'Xiang', ''], ['Zhou', 'Shuang', '']]"
1361480,2010.05150,Tsung-Yen Yang,"Tsung-Yen Yang and Michael Hu and Yinlam Chow and Peter J. Ramadge and
  Karthik Narasimhan",Safe Reinforcement Learning with Natural Language Constraints,The first two authors contributed equally,,,,cs.CL cs.AI cs.LG cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we tackle the problem of learning control policies for tasks
when provided with constraints in natural language. In contrast to instruction
following, language here is used not to specify goals, but rather to describe
situations that an agent must avoid during its exploration of the environment.
Specifying constraints in natural language also differs from the predominant
paradigm in safe reinforcement learning, where safety criteria are enforced by
hand-defined cost functions. While natural language allows for easy and
flexible specification of safety constraints and budget limitations, its
ambiguous nature presents a challenge when mapping these specifications into
representations that can be used by techniques for safe reinforcement learning.
To address this, we develop a model that contains two components: (1) a
constraint interpreter to encode natural language constraints into vector
representations capturing spatial and temporal information on forbidden states,
and (2) a policy network that uses these representations to output a policy
with minimal constraint violations. Our model is end-to-end differentiable and
we train it using a recently proposed algorithm for constrained policy
optimization. To empirically demonstrate the effectiveness of our approach, we
create a new benchmark task for autonomous navigation with crowd-sourced
free-form text specifying three different types of constraints. Our method
outperforms several baselines by achieving 6-7 times higher returns and 76%
fewer constraint violations on average. Dataset and code to reproduce our
experiments are available at https://sites.google.com/view/polco-hazard-world/.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 03:41:56 GMT'}]",2020-10-13,"[['Yang', 'Tsung-Yen', ''], ['Hu', 'Michael', ''], ['Chow', 'Yinlam', ''], ['Ramadge', 'Peter J.', ''], ['Narasimhan', 'Karthik', '']]"
1361494,2010.05164,Laurence Clarfeld,"Laurence A. Clarfeld, Robert Gramling, Donna M. Rizzo, Margaret J.
  Eppstein","A General Model of Conversational Dynamics and an Example Application in
  Serious Illness Communication","34 pages, 20 figures, submitted to PLOS One (in review)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conversation has been a primary means for the exchange of information since
ancient times. Understanding patterns of information flow in conversations is a
critical step in assessing and improving communication quality. In this paper,
we describe COnversational DYnamics Model (CODYM) analysis, a novel approach
for studying patterns of information flow in conversations. CODYMs are Markov
Models that capture sequential dependencies in the lengths of speaker turns.
The proposed method is automated and scalable, and preserves the privacy of the
conversational participants. The primary function of CODYM analysis is to
quantify and visualize patterns of information flow, concisely summarized over
sequential turns from one or more conversations. Our approach is general and
complements existing methods, providing a new tool for use in the analysis of
any type of conversation. As an important first application, we demonstrate the
model on transcribed conversations between palliative care clinicians and
seriously ill patients. These conversations are dynamic and complex, taking
place amidst heavy emotions, and include difficult topics such as end-of-life
preferences and patient values. We perform a versatile set of CODYM analyses
that (a) establish the validity of the model by confirming known patterns of
conversational turn-taking and word usage, (b) identify normative patterns of
information flow in serious illness conversations, and (c) show how these
patterns vary across narrative time and differ under expressions of anger, fear
and sadness. Potential applications of CODYMs range from assessment and
training of effective healthcare communication to comparing conversational
dynamics across language and culture, with the prospect of identifying
universal similarities and unique ""fingerprints"" of information flow.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 04:33:03 GMT'}]",2020-10-13,"[['Clarfeld', 'Laurence A.', ''], ['Gramling', 'Robert', ''], ['Rizzo', 'Donna M.', ''], ['Eppstein', 'Margaret J.', '']]"
1361501,2010.05171,Changhan Wang,"Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino",fairseq S2T: Fast Speech-to-Text Modeling with fairseq,Accepted to AACL 2020 Demo,,,,cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T)
modeling tasks such as end-to-end speech recognition and speech-to-text
translation. It follows fairseq's careful design for scalability and
extensibility. We provide end-to-end workflows from data pre-processing, model
training to offline (online) inference. We implement state-of-the-art RNN-based
as well as Transformer-based models and open-source detailed training recipes.
Fairseq's machine translation models and language models can be seamlessly
integrated into S2T workflows for multi-task learning or transfer learning.
Fairseq S2T documentation and examples are available at
https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 05:36:54 GMT'}]",2020-10-13,"[['Wang', 'Changhan', ''], ['Tang', 'Yun', ''], ['Ma', 'Xutai', ''], ['Wu', 'Anne', ''], ['Okhonko', 'Dmytro', ''], ['Pino', 'Juan', '']]"
1361452,2010.05122,Zuchao Li,"Zuchao Li, Hai Zhao, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro
  Sumita","SJTU-NICT's Supervised and Unsupervised Neural Machine Translation
  Systems for the WMT20 News Translation Task",WMT20,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we introduced our joint team SJTU-NICT 's participation in the
WMT 2020 machine translation shared task. In this shared task, we participated
in four translation directions of three language pairs: English-Chinese,
English-Polish on supervised machine translation track, German-Upper Sorbian on
low-resource and unsupervised machine translation tracks. Based on different
conditions of language pairs, we have experimented with diverse neural machine
translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language
model enhanced NMT, bidirectional translation as a pre-training, reference
language based UNMT, data-dependent gaussian prior objective, and BT-BLEU
collaborative filtering self-training. We also used the TF-IDF algorithm to
filter the training set to obtain a domain more similar set with the test set
for finetuning. In our submissions, the primary systems won the first place on
English to Chinese, Polish to English, and German to Upper Sorbian translation
directions.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 00:40:05 GMT'}]",2020-10-13,"[['Li', 'Zuchao', ''], ['Zhao', 'Hai', ''], ['Wang', 'Rui', ''], ['Chen', 'Kehai', ''], ['Utiyama', 'Masao', ''], ['Sumita', 'Eiichiro', '']]"
1278618,2004.13848,Honglei Liu,"Honglei Liu, Yan Xu, Zhiqiang Zhang, Ni Wang, Yanqun Huang, Yanjun Hu,
  Zhenghan Yang, Rui Jiang, Hui Chen","A Natural Language Processing Pipeline of Chinese Free-text Radiology
  Reports for Liver Cancer Diagnosis",,,10.1109/ACCESS.2020.3020138,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the rapid development of natural language processing (NLP)
implementation in electronic medical records (EMRs), Chinese EMRs processing
remains challenging due to the limited corpus and specific grammatical
characteristics, especially for radiology reports. In this study, we designed
an NLP pipeline for the direct extraction of clinically relevant features from
Chinese radiology reports, which is the first key step in computer-aided
radiologic diagnosis. The pipeline was comprised of named entity recognition,
synonyms normalization, and relationship extraction to finally derive the
radiological features composed of one or more terms. In named entity
recognition, we incorporated lexicon into deep learning model bidirectional
long short-term memory-conditional random field (BiLSTM-CRF), and the model
finally achieved an F1 score of 93.00%. With the extracted radiological
features, least absolute shrinkage and selection operator and machine learning
methods (support vector machine, random forest, decision tree, and logistic
regression) were used to build the classifiers for liver cancer prediction. For
liver cancer diagnosis, random forest had the highest predictive performance in
liver cancer diagnosis (F1 score 86.97%, precision 87.71%, and recall 86.25%).
This work was a comprehensive NLP study focusing on Chinese radiology reports
and the application of NLP in cancer risk prediction. The proposed NLP pipeline
for the radiological feature extraction could be easily implemented in other
kinds of Chinese clinical texts and other disease predictive tasks.
","[{'version': 'v1', 'created': 'Fri, 10 Apr 2020 09:32:07 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 12:51:42 GMT'}]",2020-10-14,"[['Liu', 'Honglei', ''], ['Xu', 'Yan', ''], ['Zhang', 'Zhiqiang', ''], ['Wang', 'Ni', ''], ['Huang', 'Yanqun', ''], ['Hu', 'Yanjun', ''], ['Yang', 'Zhenghan', ''], ['Jiang', 'Rui', ''], ['Chen', 'Hui', '']]"
1277496,2004.12726,Kawin Ethayarajh,Kawin Ethayarajh and Dorsa Sadigh,BLEU Neighbors: A Reference-less Approach to Automatic Evaluation,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Evaluation is a bottleneck in the development of natural language generation
(NLG) models. Automatic metrics such as BLEU rely on references, but for tasks
such as open-ended generation, there are no references to draw upon. Although
language diversity can be estimated using statistical measures such as
perplexity, measuring language quality requires human evaluation. However,
because human evaluation at scale is slow and expensive, it is used sparingly;
it cannot be used to rapidly iterate on NLG models, in the way BLEU is used for
machine translation. To this end, we propose BLEU Neighbors, a nearest
neighbors model for estimating language quality by using the BLEU score as a
kernel function. On existing datasets for chitchat dialogue and open-ended
sentence generation, we find that -- on average -- the quality estimation from
a BLEU Neighbors model has a lower mean squared error and higher Spearman
correlation with the ground truth than individual human annotators. Despite its
simplicity, BLEU Neighbors even outperforms state-of-the-art models on
automatically grading essays, including models that have access to a
gold-standard reference essay.
","[{'version': 'v1', 'created': 'Mon, 27 Apr 2020 11:51:28 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Apr 2020 04:54:12 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Oct 2020 21:20:37 GMT'}]",2020-10-14,"[['Ethayarajh', 'Kawin', ''], ['Sadigh', 'Dorsa', '']]"
1291881,2005.12086,Joosung Lee,Joosung Lee,"Stable Style Transformer: Delete and Generate Approach with
  Encoder-Decoder for Text Style Transfer","10 pages, 3 figures, INLG 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text style transfer is the task that generates a sentence by preserving the
content of the input sentence and transferring the style. Most existing studies
are progressing on non-parallel datasets because parallel datasets are limited
and hard to construct. In this work, we introduce a method that follows two
stages in non-parallel datasets. The first stage is to delete attribute markers
of a sentence directly through a classifier. The second stage is to generate a
transferred sentence by combining the content tokens and the target style. We
experiment on two benchmark datasets and evaluate context, style, fluency, and
semantic. It is difficult to select the best system using only these automatic
metrics, but it is possible to select stable systems. We consider only robust
systems in all automatic evaluation metrics to be the minimum conditions that
can be used in real applications. Many previous systems are difficult to use in
certain situations because performance is significantly lower in several
evaluation metrics. However, our system is stable in all automatic evaluation
metrics and has results comparable to other models. Also, we compare the
performance results of our system and the unstable system through human
evaluation. Our code and data are available at the
link~\footnote{https://github.com/rungjoo/Stable-Style-Transformer}.
","[{'version': 'v1', 'created': 'Mon, 25 May 2020 13:04:54 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 08:04:13 GMT'}]",2020-10-14,"[['Lee', 'Joosung', '']]"
1297004,2006.02490,Juan Pino,"Juan Pino and Qiantong Xu and Xutai Ma and Mohammad Javad Dousti and
  Yun Tang",Self-Training for End-to-End Speech Translation,INTERSPEECH 2020,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One of the main challenges for end-to-end speech translation is data
scarcity. We leverage pseudo-labels generated from unlabeled audio by a cascade
and an end-to-end speech translation model. This provides 8.3 and 5.7 BLEU
gains over a strong semi-supervised baseline on the MuST-C English-French and
English-German datasets, reaching state-of-the art performance. The effect of
the quality of the pseudo-labels is investigated. Our approach is shown to be
more effective than simply pre-training the encoder on the speech recognition
task. Finally, we demonstrate the effectiveness of self-training by directly
generating pseudo-labels with an end-to-end model instead of a cascade model.
","[{'version': 'v1', 'created': 'Wed, 3 Jun 2020 19:28:36 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 05:25:01 GMT'}]",2020-10-14,"[['Pino', 'Juan', ''], ['Xu', 'Qiantong', ''], ['Ma', 'Xutai', ''], ['Dousti', 'Mohammad Javad', ''], ['Tang', 'Yun', '']]"
1302946,2006.08432,Gencer Sumbul,"Gencer Sumbul, Sonali Nayak, Beg\""um Demir",SD-RSIC: Summarization Driven Deep Remote Sensing Image Captioning,"Accepted in the IEEE Transactions on Geoscience and Remote Sensing.
  For code visit: https://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC",,10.1109/TGRS.2020.3031111,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks (DNNs) have been recently found popular for image
captioning problems in remote sensing (RS). Existing DNN based approaches rely
on the availability of a training set made up of a high number of RS images
with their captions. However, captions of training images may contain redundant
information (they can be repetitive or semantically similar to each other),
resulting in information deficiency while learning a mapping from the image
domain to the language domain. To overcome this limitation, in this paper, we
present a novel Summarization Driven Remote Sensing Image Captioning (SD-RSIC)
approach. The proposed approach consists of three main steps. The first step
obtains the standard image captions by jointly exploiting convolutional neural
networks (CNNs) with long short-term memory (LSTM) networks. The second step,
unlike the existing RS image captioning methods, summarizes the ground-truth
captions of each training image into a single caption by exploiting sequence to
sequence neural networks and eliminates the redundancy present in the training
set. The third step automatically defines the adaptive weights associated to
each RS image to combine the standard captions with the summarized captions
based on the semantic content of the image. This is achieved by a novel
adaptive weighting strategy defined in the context of LSTM networks.
Experimental results obtained on the RSCID, UCM-Captions and Sydney-Captions
datasets show the effectiveness of the proposed approach compared to the
state-of-the-art RS image captioning approaches. The code of the proposed
approach is publicly available at
https://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC.
","[{'version': 'v1', 'created': 'Mon, 15 Jun 2020 14:29:12 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 10:09:15 GMT'}]",2020-10-14,"[['Sumbul', 'Gencer', ''], ['Nayak', 'Sonali', ''], ['Demir', 'Begüm', '']]"
1290980,2005.11185,Danni Liu,"Danni Liu, Gerasimos Spanakis, Jan Niehues","Low-Latency Sequence-to-Sequence Speech Recognition and Translation by
  Partial Hypothesis Selection",Interspeech 2020,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Encoder-decoder models provide a generic architecture for
sequence-to-sequence tasks such as speech recognition and translation. While
offline systems are often evaluated on quality metrics like word error rates
(WER) and BLEU, latency is also a crucial factor in many practical use-cases.
We propose three latency reduction techniques for chunk-based incremental
inference and evaluate their efficiency in terms of accuracy-latency trade-off.
On the 300-hour How2 dataset, we reduce latency by 83% to 0.8 second by
sacrificing 1% WER (6% rel.) compared to offline transcription. Although our
experiments use the Transformer, the hypothesis selection strategies are
applicable to other encoder-decoder models. To avoid expensive re-computation,
we use a unidirectionally-attending encoder. After an adaptation procedure to
partial sequences, the unidirectional model performs on-par with the original
model. We further show that our approach is also applicable to low-latency
speech translation. On How2 English-Portuguese speech translation, we reduce
latency to 0.7 second (-84% rel.) while incurring a loss of 2.4 BLEU points (5%
rel.) compared to the offline system.
","[{'version': 'v1', 'created': 'Fri, 22 May 2020 13:42:54 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 16:18:44 GMT'}]",2020-10-14,"[['Liu', 'Danni', ''], ['Spanakis', 'Gerasimos', ''], ['Niehues', 'Jan', '']]"
1362599,2010.06269,Tharindu Ranasinghe Mr,"Hansi Hettiarachchi, Tharindu Ranasinghe","BRUMS at SemEval-2020 Task 3: Contextualised Embeddings forPredicting
  the (Graded) Effect of Context in Word Similarity","Accepted to SemEval-2020 (International Workshop on Semantic
  Evaluation) at COLING 2020",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This paper presents the team BRUMS submission to SemEval-2020 Task 3: Graded
Word Similarity in Context. The system utilises state-of-the-art contextualised
word embeddings, which have some task-specific adaptations, including stacked
embeddings and average embeddings. Overall, the approach achieves good
evaluation scores across all the languages, while maintaining simplicity.
Following the final rankings, our approach is ranked within the top 5 solutions
of each language while preserving the 1st position of Finnish subtask 2.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 10:25:18 GMT'}]",2020-10-14,"[['Hettiarachchi', 'Hansi', ''], ['Ranasinghe', 'Tharindu', '']]"
1362377,2010.06047,Saturnino Luz,"Sofia de la Fuente Garcia, Craig Ritchie and Saturnino Luz","Artificial Intelligence, speech and language processing approaches to
  monitoring Alzheimer's Disease: a systematic review",Pre-print submitted to the Journal of Alzheimer's Disease,,,,cs.AI cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language is a valuable source of clinical information in Alzheimer's Disease,
as it declines concurrently with neurodegeneration. Consequently, speech and
language data have been extensively studied in connection with its diagnosis.
This paper summarises current findings on the use of artificial intelligence,
speech and language processing to predict cognitive decline in the context of
Alzheimer's Disease, detailing current research procedures, highlighting their
limitations and suggesting strategies to address them. We conducted a
systematic review of original research between 2000 and 2019, registered in
PROSPERO (reference CRD42018116606). An interdisciplinary search covered six
databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine
(PubMed and Embase) and Web of Science. Bibliographies of relevant papers were
screened until December 2019. From 3,654 search results 51 articles were
selected against the eligibility criteria. Four tables summarise their
findings: study details (aim, population, interventions, comparisons, methods
and outcomes), data details (size, type, modalities, annotation, balance,
availability and language of study), methodology (pre-processing, feature
generation, machine learning, evaluation and results) and clinical
applicability (research implications, clinical potential, risk of bias and
strengths/limitations). While promising results are reported across nearly all
51 studies, very few have been implemented in clinical research or practice. We
concluded that the main limitations of the field are poor standardisation,
limited comparability of results, and a degree of disconnect between study aims
and clinical applications. Attempts to close these gaps should support
translation of future research into clinical practice.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 21:43:04 GMT'}]",2020-10-14,"[['Garcia', 'Sofia de la Fuente', ''], ['Ritchie', 'Craig', ''], ['Luz', 'Saturnino', '']]"
1362371,2010.06041,Sina Ahmadi,"Sina Ahmadi, Mariam Masoud",Towards Machine Translation for the Kurdish Language,"12 pages - under review in the ACM Transactions on Asian and
  Low-Resource Language Information Processing (TALLIP)",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Machine translation is the task of translating texts from one language to
another using computers. It has been one of the major tasks in natural language
processing and computational linguistics and has been motivating to facilitate
human communication. Kurdish, an Indo-European language, has received little
attention in this realm due to the language being less-resourced. Therefore, in
this paper, we are addressing the main issues in creating a machine translation
system for the Kurdish language, with a focus on the Sorani dialect. We
describe the available scarce parallel data suitable for training a neural
machine translation model for Sorani Kurdish-English translation. We also
discuss some of the major challenges in Kurdish language translation and
demonstrate how fundamental text processing tasks, such as tokenization, can
improve translation performance.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 21:28:57 GMT'}]",2020-10-14,"[['Ahmadi', 'Sina', ''], ['Masoud', 'Mariam', '']]"
1362902,2010.06572,Jack Hessel,Jack Hessel and Lillian Lee,"Does my multimodal model learn cross-modal interactions? It's harder to
  tell than you might think!",,Published in EMNLP 2020,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modeling expressive cross-modal interactions seems crucial in multimodal
tasks, such as visual question answering. However, sometimes high-performing
black-box algorithms turn out to be mostly exploiting unimodal signals in the
data. We propose a new diagnostic tool, empirical multimodally-additive
function projection (EMAP), for isolating whether or not cross-modal
interactions improve performance for a given model on a given task. This
function projection modifies model predictions so that cross-modal interactions
are eliminated, isolating the additive, unimodal structure. For seven
image+text classification tasks (on each of which we set new state-of-the-art
benchmarks), we find that, in many cases, removing cross-modal interactions
results in little to no performance degradation. Surprisingly, this holds even
when expressive models, with capacity to consider interactions, otherwise
outperform less expressive models; thus, performance improvements, even when
present, often cannot be attributed to consideration of cross-modal feature
interactions. We hence recommend that researchers in multimodal machine
learning report the performance not only of unimodal baselines, but also the
EMAP of their best-performing model.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 17:45:28 GMT'}]",2020-10-14,"[['Hessel', 'Jack', ''], ['Lee', 'Lillian', '']]"
1362362,2010.06032,Kellie Webster,"Kellie Webster and Xuezhi Wang and Ian Tenney and Alex Beutel and
  Emily Pitler and Ellie Pavlick and Jilin Chen and Slav Petrov",Measuring and Reducing Gendered Correlations in Pre-trained Models,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained models have revolutionized natural language understanding.
However, researchers have found they can encode artifacts undesired in many
applications, such as professions correlating with one gender more than
another. We explore such gendered correlations as a case study for how to
address unintended correlations in pre-trained models. We define metrics and
reveal that it is possible for models with similar accuracy to encode
correlations at very different rates. We show how measured correlations can be
reduced with general-purpose techniques, and highlight the trade offs different
strategies have. With these results, we make recommendations for training
robust models: (1) carefully evaluate unintended correlations, (2) be mindful
of seemingly innocuous configuration differences, and (3) focus on general
mitigations.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 21:15:29 GMT'}]",2020-10-14,"[['Webster', 'Kellie', ''], ['Wang', 'Xuezhi', ''], ['Tenney', 'Ian', ''], ['Beutel', 'Alex', ''], ['Pitler', 'Emily', ''], ['Pavlick', 'Ellie', ''], ['Chen', 'Jilin', ''], ['Petrov', 'Slav', '']]"
1362360,2010.06030,Jiahui Yu,"Jiahui Yu, Wei Han, Anmol Gulati, Chung-Cheng Chiu, Bo Li, Tara N.
  Sainath, Yonghui Wu, Ruoming Pang","Universal ASR: Unify and Improve Streaming ASR with Full-context
  Modeling",tech report,,,,cs.CL cs.AI cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Streaming automatic speech recognition (ASR) aims to emit each hypothesized
word as quickly and accurately as possible, while full-context ASR waits for
the completion of a full speech utterance before emitting completed hypotheses.
In this work, we propose a unified framework, Universal ASR, to train a single
end-to-end ASR model with shared weights for both streaming and full-context
speech recognition. We show that the latency and accuracy of streaming ASR
significantly benefit from weight sharing and joint training of full-context
ASR, especially with inplace knowledge distillation. The Universal ASR
framework can be applied to recent state-of-the-art convolution-based and
transformer-based ASR networks. We present extensive experiments with two
state-of-the-art ASR networks, ContextNet and Conformer, on two datasets, a
widely used public dataset LibriSpeech and an internal large-scale dataset
MultiDomain. Experiments and ablation studies demonstrate that Universal ASR
not only simplifies the workflow of training and deploying streaming and
full-context ASR models, but also significantly improves both emission latency
and recognition accuracy of streaming ASR. With Universal ASR, we achieve new
state-of-the-art streaming ASR results on both LibriSpeech and MultiDomain in
terms of accuracy and latency.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 21:12:56 GMT'}]",2020-10-14,"[['Yu', 'Jiahui', ''], ['Han', 'Wei', ''], ['Gulati', 'Anmol', ''], ['Chiu', 'Chung-Cheng', ''], ['Li', 'Bo', ''], ['Sainath', 'Tara N.', ''], ['Wu', 'Yonghui', ''], ['Pang', 'Ruoming', '']]"
1362358,2010.06028,Cicero Nogueira Dos Santos,"Siamak Shakeri, Cicero Nogueira dos Santos, Henry Zhu, Patrick Ng,
  Feng Nan, Zhiguo Wang, Ramesh Nallapati, Bing Xiang","End-to-End Synthetic Data Generation for Domain Adaptation of Question
  Answering Systems",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose an end-to-end approach for synthetic QA data generation. Our model
comprises a single transformer-based encoder-decoder network that is trained
end-to-end to generate both answers and questions. In a nutshell, we feed a
passage to the encoder and ask the decoder to generate a question and an answer
token-by-token. The likelihood produced in the generation process is used as a
filtering score, which avoids the need for a separate filtering model. Our
generator is trained by fine-tuning a pretrained LM using maximum likelihood
estimation. The experimental results indicate significant improvements in the
domain adaptation of QA models outperforming current state-of-the-art methods.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 21:10:18 GMT'}]",2020-10-14,"[['Shakeri', 'Siamak', ''], ['Santos', 'Cicero Nogueira dos', ''], ['Zhu', 'Henry', ''], ['Ng', 'Patrick', ''], ['Nan', 'Feng', ''], ['Wang', 'Zhiguo', ''], ['Nallapati', 'Ramesh', ''], ['Xiang', 'Bing', '']]"
1362348,2010.06018,Tom Kocmi,"Tom Kocmi, Tomasz Limisiewicz, Gabriel Stanovsky",Gender Coreference and Bias Evaluation at WMT 2020,Accepted WMT20,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Gender bias in machine translation can manifest when choosing gender
inflections based on spurious gender correlations. For example, always
translating doctors as men and nurses as women. This can be particularly
harmful as models become more popular and deployed within commercial systems.
Our work presents the largest evidence for the phenomenon in more than 19
systems submitted to the WMT over four diverse target languages: Czech, German,
Polish, and Russian. To achieve this, we use WinoMT, a recent automatic test
suite which examines gender coreference and bias when translating from English
to languages with grammatical gender. We extend WinoMT to handle two new
languages tested in WMT: Polish and Czech. We find that all systems
consistently use spurious correlations in the data rather than meaningful
contextual information.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 20:42:21 GMT'}]",2020-10-14,"[['Kocmi', 'Tom', ''], ['Limisiewicz', 'Tomasz', ''], ['Stanovsky', 'Gabriel', '']]"
1362330,2010.06000,Sanjay Subramanian,"Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben Bogin, Madeleine
  van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi","MedICaT: A Dataset of Medical Images, Captions, and Textual References",EMNLP-Findings 2020,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding the relationship between figures and text is key to scientific
document understanding. Medical figures in particular are quite complex, often
consisting of several subfigures (75% of figures in our dataset), with detailed
text describing their content. Previous work studying figures in scientific
papers focused on classifying figure content rather than understanding how
images relate to the text. To address challenges in figure retrieval and
figure-to-text alignment, we introduce MedICaT, a dataset of medical images in
context. MedICaT consists of 217K images from 131K open access biomedical
papers, and includes captions, inline references for 74% of figures, and
manually annotated subfigures and subcaptions for a subset of figures. Using
MedICaT, we introduce the task of subfigure to subcaption alignment in compound
figures and demonstrate the utility of inline references in image-text
matching. Our data and code can be accessed at
https://github.com/allenai/medicat.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 19:56:08 GMT'}]",2020-10-14,"[['Subramanian', 'Sanjay', ''], ['Wang', 'Lucy Lu', ''], ['Mehta', 'Sachin', ''], ['Bogin', 'Ben', ''], ['van Zuylen', 'Madeleine', ''], ['Parasa', 'Sravanthi', ''], ['Singh', 'Sameer', ''], ['Gardner', 'Matt', ''], ['Hajishirzi', 'Hannaneh', '']]"
1362327,2010.05997,Xing Jie Zhong,"Xing Jie Zhong, and David Chiang","Look It Up: Bilingual and Monolingual Dictionaries Improve Neural
  Machine Translation",Accepted for publication in Proceedings of WMT 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite advances in neural machine translation (NMT) quality, rare words
continue to be problematic. For humans, the solution to the rare-word problem
has long been dictionaries, but dictionaries cannot be straightforwardly
incorporated into NMT. In this paper, we describe a new method for ""attaching""
dictionary definitions to rare words so that the network can learn the best way
to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual
dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 19:53:08 GMT'}]",2020-10-14,"[['Zhong', 'Xing Jie', ''], ['Chiang', 'David', '']]"
1362324,2010.05994,Jianqiao Li,"Guoyin Wang, Chunyuan Li, Jianqiao Li, Hao Fu, Yuh-Chen Lin, Liqun
  Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, Dinghan Shen, Qian
  Yang and Lawrence Carin",Improving Text Generation with Student-Forcing Optimal Transport,To appear at EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural language models are often trained with maximum likelihood estimation
(MLE), where the next word is generated conditioned on the ground-truth word
tokens. During testing, however, the model is instead conditioned on previously
generated tokens, resulting in what is termed exposure bias. To reduce this gap
between training and testing, we propose using optimal transport (OT) to match
the sequences generated in these two modes. An extension is further proposed to
improve the OT learning, based on the structural and contextual information of
the text sequences. The effectiveness of the proposed method is validated on
machine translation, text summarization, and text generation tasks.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 19:42:25 GMT'}]",2020-10-14,"[['Wang', 'Guoyin', ''], ['Li', 'Chunyuan', ''], ['Li', 'Jianqiao', ''], ['Fu', 'Hao', ''], ['Lin', 'Yuh-Chen', ''], ['Chen', 'Liqun', ''], ['Zhang', 'Yizhe', ''], ['Tao', 'Chenyang', ''], ['Zhang', 'Ruiyi', ''], ['Wang', 'Wenlin', ''], ['Shen', 'Dinghan', ''], ['Yang', 'Qian', ''], ['Carin', 'Lawrence', '']]"
1362323,2010.05993,Andrea Zugarini,Andrea Zugarini and Matteo Tiezzi and Marco Maggini,"Vulgaris: Analysis of a Corpus for Middle-Age Varieties of Italian
  Language",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Italian is a Romance language that has its roots in Vulgar Latin. The birth
of the modern Italian started in Tuscany around the 14th century, and it is
mainly attributed to the works of Dante Alighieri, Francesco Petrarca and
Giovanni Boccaccio, who are among the most acclaimed authors of the medieval
age in Tuscany. However, Italy has been characterized by a high variety of
dialects, which are often loosely related to each other, due to the past
fragmentation of the territory. Italian has absorbed influences from many of
these dialects, as also from other languages due to dominion of portions of the
country by other nations, such as Spain and France. In this work we present
Vulgaris, a project aimed at studying a corpus of Italian textual resources
from authors of different regions, ranging in a time period between 1200 and
1600. Each composition is associated to its author, and authors are also
grouped in families, i.e. sharing similar stylistic/chronological
characteristics. Hence, the dataset is not only a valuable resource for
studying the diachronic evolution of Italian and the differences between its
dialects, but it is also useful to investigate stylistic aspects between single
authors. We provide a detailed statistical analysis of the data, and a
corpus-driven study in dialectology and diachronic varieties.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 19:42:22 GMT'}]",2020-10-14,"[['Zugarini', 'Andrea', ''], ['Tiezzi', 'Matteo', ''], ['Maggini', 'Marco', '']]"
1362909,2010.06579,Jekaterina Novikova Dr.,"Benjamin Eyre, Aparna Balagopalan, Jekaterina Novikova","Fantastic Features and Where to Find Them: Detecting Cognitive
  Impairment with a Subsequence Classification Guided Approach",EMNLP Workshop on Noisy User-generated Text (W-NUT 2020),,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the widely reported success of embedding-based machine learning
methods on natural language processing tasks, the use of more easily
interpreted engineered features remains common in fields such as cognitive
impairment (CI) detection. Manually engineering features from noisy text is
time and resource consuming, and can potentially result in features that do not
enhance model performance. To combat this, we describe a new approach to
feature engineering that leverages sequential machine learning models and
domain knowledge to predict which features help enhance performance. We provide
a concrete example of this method on a standard data set of CI speech and
demonstrate that CI classification accuracy improves by 2.3% over a strong
baseline when using features produced by this method. This demonstration
provides an ex-ample of how this method can be used to assist classification in
fields where interpretability is important, such as health care.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 17:57:18 GMT'}]",2020-10-14,"[['Eyre', 'Benjamin', ''], ['Balagopalan', 'Aparna', ''], ['Novikova', 'Jekaterina', '']]"
1362317,2010.05987,Sean MacAvaney,"Sean MacAvaney, Arman Cohan, Nazli Goharian",SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search,EMNLP 2020. This article draws heavily from arXiv:2005.02365,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With worldwide concerns surrounding the Severe Acute Respiratory Syndrome
Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific
literature on the virus. Clinicians, researchers, and policy-makers need to be
able to search these articles effectively. In this work, we present a zero-shot
ranking algorithm that adapts to COVID-related scientific literature. Our
approach filters training data from another collection down to medical-related
queries, uses a neural re-ranking model pre-trained on scientific text
(SciBERT), and filters the target document collection. This approach ranks top
among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a
P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2
judgments. Despite not relying on TREC-COVID data, our method outperforms
models that do. As one of the first search methods to thoroughly evaluate
COVID-19 search, we hope that this serves as a strong baseline and helps in the
global crisis.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 19:28:29 GMT'}]",2020-10-14,"[['MacAvaney', 'Sean', ''], ['Cohan', 'Arman', ''], ['Goharian', 'Nazli', '']]"
1362315,2010.05985,Alexander Gutkin,Alexander Gutkin and Richard Sproat,"NEMO: Frequentist Inference Approach to Constrained Linguistic Typology
  Feature Prediction in SIGTYP 2020 Shared Task","To appear in Second Workshop on Computational Research in Linguistic
  Typology (SIGTYP 2020) at 2020 Conference on Empirical Methods in Natural
  Language Processing (EMNLP)",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper describes the NEMO submission to SIGTYP 2020 shared task which
deals with prediction of linguistic typological features for multiple languages
using the data derived from World Atlas of Language Structures (WALS). We
employ frequentist inference to represent correlations between typological
features and use this representation to train simple multi-class estimators
that predict individual features. We describe two submitted ridge
regression-based configurations which ranked second and third overall in the
constrained task. Our best configuration achieved the micro-averaged accuracy
score of 0.66 on 149 test languages.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 19:25:43 GMT'}]",2020-10-14,"[['Gutkin', 'Alexander', ''], ['Sproat', 'Richard', '']]"
1362301,2010.05971,Yanai Elazar,"Yanai Elazar, Victoria Basmov, Shauli Ravfogel, Yoav Goldberg, Reut
  Tsarfaty",The Extraordinary Failure of Complement Coercion Crowdsourcing,"Workshop on Insights from Negative Results in NLP, co-located with
  EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Crowdsourcing has eased and scaled up the collection of linguistic annotation
in recent years. In this work, we follow known methodologies of collecting
labeled data for the complement coercion phenomenon. These are constructions
with an implied action -- e.g., ""I started a new book I bought last week"",
where the implied action is reading. We aim to collect annotated data for this
phenomenon by reducing it to either of two known tasks: Explicit Completion and
Natural Language Inference. However, in both cases, crowdsourcing resulted in
low agreement scores, even though we followed the same methodologies as in
previous work. Why does the same process fail to yield high agreement scores?
We specify our modeling schemes, highlight the differences with previous work
and provide some insights about the task and possible explanations for the
failure. We conclude that specific phenomena require tailored solutions, not
only in specialized algorithms, but also in data collection methods.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 19:04:04 GMT'}]",2020-10-14,"[['Elazar', 'Yanai', ''], ['Basmov', 'Victoria', ''], ['Ravfogel', 'Shauli', ''], ['Goldberg', 'Yoav', ''], ['Tsarfaty', 'Reut', '']]"
1362297,2010.05967,Ewan Dunbar,"Ewan Dunbar and Julien Karadayi and Mathieu Bernard and Xuan-Nga Cao
  and Robin Algayres and Lucas Ondel and Laurent Besacier and Sakriani Sakti
  and Emmanuel Dupoux","The Zero Resource Speech Challenge 2020: Discovering discrete subword
  and word units",,Proceedings of Interspeech 2020,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present the Zero Resource Speech Challenge 2020, which aims at learning
speech representations from raw audio signals without any labels. It combines
the data sets and metrics from two previous benchmarks (2017 and 2019) and
features two tasks which tap into two levels of speech representation. The
first task is to discover low bit-rate subword representations that optimize
the quality of speech synthesis; the second one is to discover word-like units
from unsegmented raw speech. We present the results of the twenty submitted
models and discuss the implications of the main findings for unsupervised
speech learning.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 18:56:48 GMT'}]",2020-10-14,"[['Dunbar', 'Ewan', ''], ['Karadayi', 'Julien', ''], ['Bernard', 'Mathieu', ''], ['Cao', 'Xuan-Nga', ''], ['Algayres', 'Robin', ''], ['Ondel', 'Lucas', ''], ['Besacier', 'Laurent', ''], ['Sakti', 'Sakriani', ''], ['Dupoux', 'Emmanuel', '']]"
1362291,2010.05961,Ewan Dunbar,Juliette Millet and Ewan Dunbar,"Perceptimatic: A human speech perception benchmark for unsupervised
  subword modelling",,Proceedings of Interspeech 2020,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present a data set and methods to compare speech processing
models and human behaviour on a phone discrimination task. We provide
Perceptimatic, an open data set which consists of French and English speech
stimuli, as well as the results of 91 English- and 93 French-speaking
listeners. The stimuli test a wide range of French and English contrasts, and
are extracted directly from corpora of natural running read speech, used for
the 2017 Zero Resource Speech Challenge. We provide a method to compare humans'
perceptual space with models' representational space, and we apply it to models
previously submitted to the Challenge. We show that, unlike unsupervised models
and supervised multilingual models, a standard supervised monolingual HMM-GMM
phone recognition system, while good at discriminating phones, yields a
representational space very different from that of human native listeners.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 18:40:08 GMT'}]",2020-10-14,"[['Millet', 'Juliette', ''], ['Dunbar', 'Ewan', '']]"
1362289,2010.05959,Alexander Gutkin,Alexander Gutkin and Martin Jansche and Lucy Skidmore,Towards Induction of Structured Phoneme Inventories,"To appear in the Second Workshop on Computational Research in
  Linguistic Typology (SIGTYP 2020) at EMNLP 2020",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This extended abstract surveying the work on phonological typology was
prepared for ""SIGTYP 2020: The Second Workshop on Computational Research in
Linguistic Typology"" to be held at EMNLP 2020.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 18:39:07 GMT'}]",2020-10-14,"[['Gutkin', 'Alexander', ''], ['Jansche', 'Martin', ''], ['Skidmore', 'Lucy', '']]"
1362283,2010.05953,Jena Hwang,"Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke
  Sakaguchi, Antoine Bosselut, Yejin Choi",COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent years have brought about a renewed interest in commonsense
representation and reasoning in the field of natural language understanding.
The development of new commonsense knowledge graphs (CSKG) has been central to
these advances as their diverse facts can be used and referenced by machine
learning models for tackling new and challenging tasks. At the same time, there
remain questions about the quality and coverage of these resources due to the
massive scale required to comprehensively encompass general commonsense
knowledge.
  In this work, we posit that manually constructed CSKGs will never achieve the
coverage necessary to be applicable in all situations encountered by NLP
agents. Therefore, we propose a new evaluation framework for testing the
utility of KGs based on how effectively implicit knowledge representations can
be learned from them.
  With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose
commonsense knowledge containing knowledge that is not readily available in
pretrained language models. We evaluate its properties in comparison with other
leading CSKGs, performing the first large-scale pairwise study of commonsense
knowledge resources. Next, we show that ATOMIC 2020 is better suited for
training knowledge models that can generate accurate, representative knowledge
for new, unseen entities and events. Finally, through human evaluation, we show
that the few-shot performance of GPT-3 (175B parameters), while impressive,
remains ~12 absolute points lower than a BART-based knowledge model trained on
ATOMIC 2020 despite using over 430x fewer parameters.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 18:27:05 GMT'}]",2020-10-14,"[['Hwang', 'Jena D.', ''], ['Bhagavatula', 'Chandra', ''], ['Bras', 'Ronan Le', ''], ['Da', 'Jeff', ''], ['Sakaguchi', 'Keisuke', ''], ['Bosselut', 'Antoine', ''], ['Choi', 'Yejin', '']]"
1279294,2004.14524,Huda Khayrallah,"Huda Khayrallah, Brian Thompson, Matt Post, Philipp Koehn","Simulated Multiple Reference Training Improves Low-Resource Machine
  Translation",EMNLP 2020 camera ready,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many valid translations exist for a given sentence, yet machine translation
(MT) is trained with a single reference translation, exacerbating data sparsity
in low-resource settings. We introduce Simulated Multiple Reference Training
(SMRT), a novel MT training method that approximates the full space of possible
translations by sampling a paraphrase of the reference sentence from a
paraphraser and training the MT model to predict the paraphraser's distribution
over possible tokens. We demonstrate the effectiveness of SMRT in low-resource
settings when translating to English, with improvements of 1.2 to 7.0 BLEU. We
also find SMRT is complementary to back-translation.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 00:11:53 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 15:43:57 GMT'}]",2020-10-14,"[['Khayrallah', 'Huda', ''], ['Thompson', 'Brian', ''], ['Post', 'Matt', ''], ['Koehn', 'Philipp', '']]"
1361853,2010.05523,Xiangru Tang,"Xiangru Tang, Alan Aw","FILM: A Fast, Interpretable, and Low-rank Metric Learning Approach for
  Sentence Matching",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Detection of semantic similarity plays a vital role in sentence matching. It
requires to learn discriminative representations of natural language. Recently,
owing to more and more sophisticated model architecture, impressive progress
has been made, along with a time-consuming training process and
not-interpretable inference. To alleviate this problem, we explore a metric
learning approach, named FILM (Fast, Interpretable, and Low-rank Metric
learning) to efficiently find a high discriminative projection of the
high-dimensional data. We construct this metric learning problem as a manifold
optimization problem and solve it with the Cayley transformation method with
the Barzilai-Borwein step size. In experiments, we apply FILM with triplet loss
minimization objective to the Quora Challenge and Semantic Textual Similarity
(STS) Task. The results demonstrate that the FILM method achieves superior
performance as well as the fastest computation speed, which is consistent with
our theoretical analysis of time complexity.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 08:24:41 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 01:14:14 GMT'}]",2020-10-14,"[['Tang', 'Xiangru', ''], ['Aw', 'Alan', '']]"
1360835,2010.04505,Yu Wan,"Yu Wan, Baosong Yang, Derek F. Wong, Yikai Zhou, Lidia S. Chao, Haibo
  Zhang, Boxing Chen",Self-Paced Learning for Neural Machine Translation,Accepted by EMNLP2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have proven that the training of neural machine translation
(NMT) can be facilitated by mimicking the learning process of humans.
Nevertheless, achievements of such kind of curriculum learning rely on the
quality of artificial schedule drawn up with the handcrafted features, e.g.
sentence length or word rarity. We ameliorate this procedure with a more
flexible manner by proposing self-paced learning, where NMT model is allowed to
1) automatically quantify the learning confidence over training examples; and
2) flexibly govern its learning via regulating the loss in each iteration step.
Experimental results over multiple translation tasks demonstrate that the
proposed model yields better performance than strong baselines and those models
trained with human-designed curricula on both translation quality and
convergence speed.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 11:33:16 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 09:02:09 GMT'}]",2020-10-14,"[['Wan', 'Yu', ''], ['Yang', 'Baosong', ''], ['Wong', 'Derek F.', ''], ['Zhou', 'Yikai', ''], ['Chao', 'Lidia S.', ''], ['Zhang', 'Haibo', ''], ['Chen', 'Boxing', '']]"
1360185,2010.03855,Dong-Jin Kim,"Dong-Jin Kim, Tae-Hyun Oh, Jinsoo Choi, In So Kweon",Dense Relational Image Captioning via Multi-task Triple-Stream Networks,"Journal extension of our CVPR 2019 paper ( arXiv:1903.05942 ). Source
  code : https://github.com/Dong-JinKim/DenseRelationalCaptioning",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce dense relational captioning, a novel image captioning task which
aims to generate multiple captions with respect to relational information
between objects in a visual scene. Relational captioning provides explicit
descriptions of each relationship between object combinations. This framework
is advantageous in both diversity and amount of information, leading to a
comprehensive image understanding based on relationships, e.g., relational
proposal generation. For relational understanding between objects, the
part-of-speech (POS, i.e., subject-object-predicate categories) can be a
valuable prior information to guide the causal sequence of words in a caption.
We enforce our framework to not only learn to generate captions but also
predict the POS of each word. To this end, we propose the multi-task
triple-stream network (MTTSNet) which consists of three recurrent units
responsible for each POS which is trained by jointly predicting the correct
captions and POS for each word. In addition, we found that the performance of
MTTSNet can be improved by modulating the object embeddings with an explicit
relational module. We demonstrate that our proposed model can generate more
diverse and richer captions, via extensive experimental analysis on large scale
datasets and several metrics. We additionally extend analysis to an ablation
study, applications on holistic image captioning, scene graph generation, and
retrieval tasks.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 09:17:55 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 18:14:14 GMT'}]",2020-10-14,"[['Kim', 'Dong-Jin', ''], ['Oh', 'Tae-Hyun', ''], ['Choi', 'Jinsoo', ''], ['Kweon', 'In So', '']]"
1362383,2010.06053,Zhao Song,"Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, Sanjeev Arora",TextHide: Tackling Data Privacy in Language Understanding Tasks,Findings of EMNLP 2020,,,,cs.CL cs.CR cs.DS cs.LG stat.ML,http://creativecommons.org/licenses/by-sa/4.0/,"  An unsolved challenge in distributed or federated learning is to effectively
mitigate privacy risks without slowing down training or reducing accuracy. In
this paper, we propose TextHide aiming at addressing this challenge for natural
language understanding tasks. It requires all participants to add a simple
encryption step to prevent an eavesdropping attacker from recovering private
text data. Such an encryption step is efficient and only affects the task
performance slightly. In addition, TextHide fits well with the popular
framework of fine-tuning pre-trained language models (e.g., BERT) for any
sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and
our experiments show that TextHide can effectively defend attacks on shared
gradients or representations and the averaged accuracy reduction is only
$1.9\%$. We also present an analysis of the security of TextHide using a
conjecture about the computational intractability of a mathematical problem.
  Our code is available at https://github.com/Hazelsuko07/TextHide
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 22:22:15 GMT'}]",2020-10-14,"[['Huang', 'Yangsibo', ''], ['Song', 'Zhao', ''], ['Chen', 'Danqi', ''], ['Li', 'Kai', ''], ['Arora', 'Sanjeev', '']]"
1220338,1912.08442,Xinting Huang,"Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang",MALA: Cross-Domain Dialogue Generation with Action Learning,Update: Accepted to Proceedings of AAAI 2020,,10.1609/aaai.v34i05.6306,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Response generation for task-oriented dialogues involves two basic
components: dialogue planning and surface realization. These two components,
however, have a discrepancy in their objectives, i.e., task completion and
language quality. To deal with such discrepancy, conditioned response
generation has been introduced where the generation process is factorized into
action decision and language generation via explicit action representations. To
obtain action representations, recent studies learn latent actions in an
unsupervised manner based on the utterance lexical similarity. Such an action
learning approach is prone to diversities of language surfaces, which may
impinge task completion and language quality. To address this issue, we propose
multi-stage adaptive latent action learning (MALA) that learns semantic latent
actions by distinguishing the effects of utterances on dialogue progress. We
model the utterance effect using the transition of dialogue states caused by
the utterance and develop a semantic similarity measurement that estimates
whether utterances have similar effects. For learning semantic actions on
domains without dialogue states, MsALA extends the semantic similarity
measurement across domains progressively, i.e., from aligning shared actions to
learning domain-specific actions. Experiments using multi-domain datasets, SMD
and MultiWOZ, show that our proposed model achieves consistent improvements
over the baselines models in terms of both task completion and language
quality.
","[{'version': 'v1', 'created': 'Wed, 18 Dec 2019 08:14:10 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 13:33:38 GMT'}]",2020-10-14,"[['Huang', 'Xinting', ''], ['Qi', 'Jianzhong', ''], ['Sun', 'Yu', ''], ['Zhang', 'Rui', '']]"
1362395,2010.06065,Zonghai Yao,"Zonghai Yao, Liangliang Cao and Huapu Pan",Zero-shot Entity Linking with Efficient Long Range Sequence Modeling,"6 pages, 6 figures, Findings of EMNLP2020",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper considers the problem of zero-shot entity linking, in which a link
in the test time may not present in training. Following the prevailing
BERT-based research efforts, we find a simple yet effective way is to expand
the long-range sequence modeling. Unlike many previous methods, our method does
not require expensive pre-training of BERT with long position embedding.
Instead, we propose an efficient position embeddings initialization method
called Embedding-repeat, which initializes larger position embeddings based on
BERT-Base. On Wikia's zero-shot EL dataset, our method improves the SOTA from
76.06% to 79.08%, and for its long data, the corresponding improvement is from
74.57% to 82.14%. Our experiments suggest the effectiveness of long-range
sequence modeling without retraining the BERT model.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 22:59:18 GMT'}]",2020-10-14,"[['Yao', 'Zonghai', ''], ['Cao', 'Liangliang', ''], ['Pan', 'Huapu', '']]"
1362445,2010.06115,Yuanhe Tian,"Yuanhe Tian, Yan Song, Fei Xia","Supertagging Combinatory Categorial Grammar with Attentive Graph
  Convolutional Networks","Natural Language Processing. 8 pages, 4 figures. EMNLP-2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Supertagging is conventionally regarded as an important task for combinatory
categorial grammar (CCG) parsing, where effective modeling of contextual
information is highly important to this task. However, existing studies have
made limited efforts to leverage contextual features except for applying
powerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph
convolutional networks to enhance neural CCG supertagging through a novel
solution of leveraging contextual information. Specifically, we build the graph
from chunks (n-grams) extracted from a lexicon and apply attention over the
graph, so that different word pairs from the contexts within and across chunks
are weighted in the model and facilitate the supertagging accordingly. The
experiments performed on the CCGbank demonstrate that our approach outperforms
all previous studies in terms of both supertagging and parsing. Further
analyses illustrate the effectiveness of each component in our approach to
discriminatively learn from word pairs to enhance CCG supertagging.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 01:58:29 GMT'}]",2020-10-14,"[['Tian', 'Yuanhe', ''], ['Song', 'Yan', ''], ['Xia', 'Fei', '']]"
1362762,2010.06432,Matan Orbach,"Orith Toledo-Ronen, Matan Orbach, Yonatan Bilu, Artem Spector, Noam
  Slonim",Multilingual Argument Mining: Datasets and Analysis,"Accepted to Findings of EMNLP 2020 (Long Paper). For the associated
  multilingual arguments and evidence corpus, see
  https://www.research.ibm.com/haifa/dept/vst/debating_data.shtml#Multilingual%20Argument%20Mining",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The growing interest in argument mining and computational argumentation
brings with it a plethora of Natural Language Understanding (NLU) tasks and
corresponding datasets. However, as with many other NLU tasks, the dominant
language is English, with resources in other languages being few and far
between. In this work, we explore the potential of transfer learning using the
multilingual BERT model to address argument mining tasks in non-English
languages, based on English datasets and the use of machine translation. We
show that such methods are well suited for classifying the stance of arguments
and detecting evidence, but less so for assessing the quality of arguments,
presumably because quality is harder to preserve under translation. In
addition, focusing on the translate-train approach, we show how the choice of
languages for translation, and the relations among them, affect the accuracy of
the resultant model. Finally, to facilitate evaluation of transfer learning on
argument mining tasks, we provide a human-generated dataset with more than 10k
arguments in multiple languages, as well as machine translation of the English
datasets.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 14:49:10 GMT'}]",2020-10-14,"[['Toledo-Ronen', 'Orith', ''], ['Orbach', 'Matan', ''], ['Bilu', 'Yonatan', ''], ['Spector', 'Artem', ''], ['Slonim', 'Noam', '']]"
1362684,2010.06354,"J\""org Tiedemann","J\""org Tiedemann","The Tatoeba Translation Challenge -- Realistic Data Sets for Low
  Resource and Multilingual MT",to be appear at the 5th Conference on Machine Translation (WMT20),,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper describes the development of a new benchmark for machine
translation that provides training and test data for thousands of language
pairs covering over 500 languages and tools for creating state-of-the-art
translation models from that collection. The main goal is to trigger the
development of open translation tools and models with a much broader coverage
of the World's languages. Using the package it is possible to work on realistic
low-resource scenarios avoiding artificially reduced setups that are common
when demonstrating zero-shot or few-shot learning. For the first time, this
package provides a comprehensive collection of diverse data sets in hundreds of
languages with systematic language and script annotation and data splits to
extend the narrow coverage of existing benchmarks. Together with the data
release, we also provide a growing number of pre-trained baseline models for
individual language pairs and selected language groups.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 13:12:21 GMT'}]",2020-10-14,"[['Tiedemann', 'Jörg', '']]"
1362655,2010.06325,Guillaume Salha,"Elena V. Epure and Guillaume Salha and Manuel Moussallam and Romain
  Hennequin",Modeling the Music Genre Perception across Language-Bound Cultures,"2020 Conference on Empirical Methods in Natural Language Processing
  (EMNLP 2020)",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The music genre perception expressed through human annotations of artists or
albums varies significantly across language-bound cultures. These variations
cannot be modeled as mere translations since we also need to account for
cultural differences in the music genre perception. In this work, we study the
feasibility of obtaining relevant cross-lingual, culture-specific music genre
annotations based only on language-specific semantic representations, namely
distributed concept embeddings and ontologies. Our study, focused on six
languages, shows that unsupervised cross-lingual music genre annotation is
feasible with high accuracy, especially when combining both types of
representations. This approach of studying music genres is the most extensive
to date and has many implications in musicology and music information
retrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus
to benchmark state of the art multilingual pre-trained embedding models.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 12:20:32 GMT'}]",2020-10-14,"[['Epure', 'Elena V.', ''], ['Salha', 'Guillaume', ''], ['Moussallam', 'Manuel', ''], ['Hennequin', 'Romain', '']]"
1362766,2010.06436,Andrey Kutuzov,"Julia Rodina, Andrey Kutuzov",RuSemShift: a dataset of historical lexical semantic change in Russian,Accepted to COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present RuSemShift, a large-scale manually annotated test set for the task
of semantic change modeling in Russian for two long-term time period pairs:
from the pre-Soviet through the Soviet times and from the Soviet through the
post-Soviet times. Target words were annotated by multiple crowd-source
workers. The annotation process was organized following the DURel framework and
was based on sentence contexts extracted from the Russian National Corpus.
Additionally, we report the performance of several distributional approaches on
RuSemShift, achieving promising results, which at the same time leave room for
other researchers to improve.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 14:54:05 GMT'}]",2020-10-14,"[['Rodina', 'Julia', ''], ['Kutuzov', 'Andrey', '']]"
1362774,2010.06444,Thiago H. Silva,"Frances Santos, Thiago H Silva, Antonio A F Loureiro, Leandro Villas","Automatic Extraction of Urban Outdoor Perception from Geolocated
  Free-Texts",Paper accepted - to be published,,,,cs.SI cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The automatic extraction of urban perception shared by people on
location-based social networks (LBSNs) is an important multidisciplinary
research goal. One of the reasons is because it facilitates the understanding
of the intrinsic characteristics of urban areas in a scalable way, helping to
leverage new services. However, content shared on LBSNs is diverse,
encompassing several topics, such as politics, sports, culture, religion, and
urban perceptions, making the task of content extraction regarding a particular
topic very challenging. Considering free-text messages shared on LBSNs, we
propose an automatic and generic approach to extract people's perceptions. For
that, our approach explores opinions that are spatial-temporal and semantically
similar. We exemplify our approach in the context of urban outdoor areas in
Chicago, New York City and London. Studying those areas, we found evidence that
LBSN data brings valuable information about urban regions. To analyze and
validate our outcomes, we conducted a temporal analysis to measure the results'
robustness over time. We show that our approach can be helpful to better
understand urban areas considering different perspectives. We also conducted a
comparative analysis based on a public dataset, which contains volunteers'
perceptions regarding urban areas expressed in a controlled experiment. We
observe that both results yield a very similar level of agreement.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 14:59:46 GMT'}]",2020-10-14,"[['Santos', 'Frances', ''], ['Silva', 'Thiago H', ''], ['Loureiro', 'Antonio A F', ''], ['Villas', 'Leandro', '']]"
1362624,2010.06294,Li Liang,"Li Liang, Zheng Zhao and Bonnie Webber",Extending Implicit Discourse Relation Recognition to the PDTB-3,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The PDTB-3 contains many more Implicit discourse relations than the previous
PDTB-2. This is in part because implicit relations have now been annotated
within sentences as well as between them. In addition, some now co-occur with
explicit discourse relations, instead of standing on their own. Here we show
that while this can complicate the problem of identifying the location of
implicit discourse relations, it can in turn simplify the problem of
identifying their senses. We present data to support this claim, as well as
methods that can serve as a non-trivial baseline for future state-of-the-art
recognizers for implicit discourse relations.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 11:19:42 GMT'}]",2020-10-14,"[['Liang', 'Li', ''], ['Zhao', 'Zheng', ''], ['Webber', 'Bonnie', '']]"
1362613,2010.06283,Hendrik Schuff,"Hendrik Schuff, Heike Adel, Ngoc Thang Vu","F1 is Not Enough! Models and Evaluation Towards User-Centered
  Explainable Question Answering",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Explainable question answering systems predict an answer together with an
explanation showing why the answer has been selected. The goal is to enable
users to assess the correctness of the system and understand its reasoning
process. However, we show that current models and evaluation settings have
shortcomings regarding the coupling of answer and explanation which might cause
serious issues in user experience. As a remedy, we propose a hierarchical model
and a new regularization term to strengthen the answer-explanation coupling as
well as two evaluation scores to quantify the coupling. We conduct experiments
on the HOTPOTQA benchmark data set and perform a user study. The user study
shows that our models increase the ability of the users to judge the
correctness of the system and that scores like F1 are not enough to estimate
the usefulness of a model in a practical setting with human users. Our scores
are better aligned with user experience, making them promising candidates for
model selection.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 10:53:20 GMT'}]",2020-10-14,"[['Schuff', 'Hendrik', ''], ['Adel', 'Heike', ''], ['Vu', 'Ngoc Thang', '']]"
1362611,2010.06281,Tharindu Ranasinghe Mr,"Tharindu Ranasinghe, Alistair Plum, Constantin Orasan, Ruslan Mitkov",RGCL at SemEval-2020 Task 6: Neural Approaches to Definition Extraction,"Accepted to SemEval-2020 (International Workshop on Semantic
  Evaluation) at COLING 2020",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This paper presents the RGCL team submission to SemEval 2020 Task 6:
DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence
and token levels. It utilises state-of-the-art neural network architectures,
which have some task-specific adaptations, including an automatically extended
training set. Overall, the approach achieves acceptable evaluation scores,
while maintaining flexibility in architecture selection.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 10:48:15 GMT'}]",2020-10-14,"[['Ranasinghe', 'Tharindu', ''], ['Plum', 'Alistair', ''], ['Orasan', 'Constantin', ''], ['Mitkov', 'Ruslan', '']]"
1362608,2010.06278,Tharindu Ranasinghe Mr,"Tharindu Ranasinghe, Hansi Hettiarachchi","BRUMS at SemEval-2020 Task 12 : Transformer based Multilingual Offensive
  Language Identification in Social Media","Accepted to SemEval-2020 (International Workshop on Semantic
  Evaluation) at COLING 2020",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we describe the team \textit{BRUMS} entry to OffensEval 2:
Multilingual Offensive Language Identification in Social Media in SemEval-2020.
The OffensEval organizers provided participants with annotated datasets
containing posts from social media in Arabic, Danish, English, Greek and
Turkish. We present a multilingual deep learning model to identify offensive
language in social media. Overall, the approach achieves acceptable evaluation
scores, while maintaining flexibility between languages.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 10:39:14 GMT'}]",2020-10-14,"[['Ranasinghe', 'Tharindu', ''], ['Hettiarachchi', 'Hansi', '']]"
1362797,2010.06467,Jimmy Lin,"Jimmy Lin, Rodrigo Nogueira, and Andrew Yates",Pretrained Transformers for Text Ranking: BERT and Beyond,,,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The goal of text ranking is to generate an ordered list of texts retrieved
from a corpus in response to a query. Although the most common formulation of
text ranking is search, instances of the task can also be found in many natural
language processing applications. This survey provides an overview of text
ranking with neural network architectures known as transformers, of which BERT
is the best-known example. The combination of transformers and self-supervised
pretraining has, without exaggeration, revolutionized the fields of natural
language processing (NLP), information retrieval (IR), and beyond. In this
survey, we provide a synthesis of existing work as a single point of entry for
practitioners who wish to gain a better understanding of how to apply
transformers to text ranking problems and researchers who wish to pursue work
in this area. We cover a wide range of modern techniques, grouped into two
high-level categories: transformer models that perform reranking in multi-stage
ranking architectures and learned dense representations that attempt to perform
ranking directly. There are two themes that pervade our survey: techniques for
handling long documents, beyond the typical sentence-by-sentence processing
approaches used in NLP, and techniques for addressing the tradeoff between
effectiveness (result quality) and efficiency (query latency). Although
transformer architectures and pretraining techniques are recent innovations,
many aspects of how they are applied to text ranking are relatively well
understood and represent mature techniques. However, there remain many open
research questions, and thus in addition to laying out the foundations of
pretrained transformers for text ranking, this survey also attempts to
prognosticate where the field is heading.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 15:20:32 GMT'}]",2020-10-14,"[['Lin', 'Jimmy', ''], ['Nogueira', 'Rodrigo', ''], ['Yates', 'Andrew', '']]"
1362583,2010.06253,Peng Cui,"Peng Cui, Le Hu, and Yuanchao Liu","Enhancing Extractive Text Summarization with Topic-Aware Graph Neural
  Networks",Accepted by COLING(2020),,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Text summarization aims to compress a textual document to a short summary
while keeping salient information. Extractive approaches are widely used in
text summarization because of their fluency and efficiency. However, most of
existing extractive models hardly capture inter-sentence relationships,
particularly in long documents. They also often ignore the effect of topical
information on capturing important contents. To address these issues, this
paper proposes a graph neural network (GNN)-based extractive summarization
model, enabling to capture inter-sentence relationships efficiently via
graph-structured document representation. Moreover, our model integrates a
joint neural topic model (NTM) to discover latent topics, which can provide
document-level features for sentence selection. The experimental results
demonstrate that our model not only substantially achieves state-of-the-art
results on CNN/DM and NYT datasets but also considerably outperforms existing
approaches on scientific paper datasets consisting of much longer documents,
indicating its better robustness in document genres and lengths. Further
discussions show that topical information can help the model preselect salient
contents from an entire document, which interprets its effectiveness in long
document summarization.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 09:30:04 GMT'}]",2020-10-14,"[['Cui', 'Peng', ''], ['Hu', 'Le', ''], ['Liu', 'Yuanchao', '']]"
1362802,2010.06472,Aaron Mueller,"Aaron Mueller, Zach Wood-Doughty, Silvio Amir, Mark Dredze, Alicia L.
  Nobles","Demographic Representation and Collective Storytelling in the Me Too
  Twitter Hashtag Activism Movement",27 pages (incl. 5 for references). Submitted to CSCW 2021,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The #MeToo movement on Twitter has drawn attention to the pervasive nature of
sexual harassment and violence. While #MeToo has been praised for providing
support for self-disclosures of harassment or violence and shifting societal
response, it has also been criticized for exemplifying how women of color have
been discounted for their historical contributions to and excluded from
feminist movements. Through an analysis of over 600,000 tweets from over
256,000 unique users, we examine online #MeToo conversations across gender and
racial/ethnic identities and the topics that each demographic emphasized. We
found that tweets authored by white women were overrepresented in the movement
compared to other demographics, aligning with criticism of unequal
representation. We found that intersected identities contributed differing
narratives to frame the movement, co-opted the movement to raise visibility in
parallel ongoing movements, employed the same hashtags both critically and
supportively, and revived and created new hashtags in response to pivotal
moments. Notably, tweets authored by black women often expressed emotional
support and were critical about differential treatment in the justice system
and by police. In comparison, tweets authored by white women and men often
highlighted sexual harassment and violence by public figures and weaved in more
general political discussions. We discuss the implications of work for digital
activism research and design including suggestions to raise visibility by those
who were under-represented in this hashtag activism movement. Content warning:
this article discusses issues of sexual harassment and violence.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 15:25:33 GMT'}]",2020-10-14,"[['Mueller', 'Aaron', ''], ['Wood-Doughty', 'Zach', ''], ['Amir', 'Silvio', ''], ['Dredze', 'Mark', ''], ['Nobles', 'Alicia L.', '']]"
1362543,2010.06213,Maxime Peyrard,"Maxime Peyrard, Robert West",KLearn: Background Knowledge Inference from Summarization Data,Accepted at Findings of EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The goal of text summarization is to compress documents to the relevant
information while excluding background information already known to the
receiver. So far, summarization researchers have given considerably more
attention to relevance than to background knowledge. In contrast, this work
puts background knowledge in the foreground. Building on the realization that
the choices made by human summarizers and annotators contain implicit
information about their background knowledge, we develop and compare techniques
for inferring background knowledge from summarization data. Based on this
framework, we define summary scoring functions that explicitly model background
knowledge, and show that these scoring functions fit human judgments
significantly better than baselines. We illustrate some of the many potential
applications of our framework. First, we provide insights into human
information importance priors. Second, we demonstrate that averaging the
background knowledge of multiple, potentially biased annotators or corpora
greatly improves summary-scoring performance. Finally, we discuss potential
applications of our framework beyond summarization.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 07:42:25 GMT'}]",2020-10-14,"[['Peyrard', 'Maxime', ''], ['West', 'Robert', '']]"
1362808,2010.06478,Tommaso Pasini,"Alessandro Raganato, Tommaso Pasini, Jose Camacho-Collados, Mohammad
  Taher Pilehvar","XL-WiC: A Multilingual Benchmark for Evaluating Semantic
  Contextualization",EMNLP2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The ability to correctly model distinct meanings of a word is crucial for the
effectiveness of semantic representation techniques. However, most existing
evaluation benchmarks for assessing this criterion are tied to sense
inventories (usually WordNet), restricting their usage to a small subset of
knowledge-based representation techniques. The Word-in-Context dataset (WiC)
addresses the dependence on sense inventories by reformulating the standard
disambiguation task as a binary classification problem; but, it is limited to
the English language. We put forward a large multilingual benchmark, XL-WiC,
featuring gold standards in 12 new languages from varied language families and
with different degrees of resource availability, opening room for evaluation
scenarios such as zero-shot cross-lingual transfer. We perform a series of
experiments to determine the reliability of the datasets and to set performance
baselines for several recent contextualized multilingual models. Experimental
results show that even when no tagged instances are available for a target
language, models trained solely on the English data can attain competitive
performance in the task of distinguishing different meanings of a word, even
for distant languages. XL-WiC is available at
https://pilehvar.github.io/xlwic/.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 15:32:00 GMT'}]",2020-10-14,"[['Raganato', 'Alessandro', ''], ['Pasini', 'Tommaso', ''], ['Camacho-Collados', 'Jose', ''], ['Pilehvar', 'Mohammad Taher', '']]"
1362526,2010.06196,Zitao Liu,"Tianqiao Liu, Qian Fang, Wenbiao Ding, Zhongqin Wu, Zitao Liu","Mathematical Word Problem Generation from Commonsense Knowledge Graph
  and Equations",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is an increasing interest in the use of automatic mathematical word
problem (MWP) generation in educational assessment. Different from standard
natural question generation, MWP generation needs to maintain the underlying
mathematical operations between quantities and variables, while at the same
time ensuring the relevance between the output and the given topic. To address
above problem we develop an end-to-end neural model to generate personalized
and diverse MWPs in real-world scenarios from commonsense knowledge graph and
equations. The proposed model (1) learns both representations from
edge-enhanced Levi graphs of symbolic equations and commonsense knowledge; (2)
automatically fuses equation and commonsense knowledge information via a
self-planning module when generating the MWPs. Experiments on an educational
gold-standard set and a large-scale generated MWP set show that our approach is
superior on the MWP generation task, and it outperforms the state-of-the-art
models in terms of both automatic evaluation metrics, i.e., BLEU-4, ROUGE-L,
Self-BLEU, and human evaluation metrics, i.e, equation relevance, topic
relevance, and language coherence.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 06:31:53 GMT'}]",2020-10-14,"[['Liu', 'Tianqiao', ''], ['Fang', 'Qian', ''], ['Ding', 'Wenbiao', ''], ['Wu', 'Zhongqin', ''], ['Liu', 'Zitao', '']]"
1362515,2010.06185,Avishai Gretz,"Shai Gretz, Yonatan Bilu, Edo Cohen-Karlik and Noam Slonim","The workweek is the best time to start a family -- A Study of GPT-2
  Based Claim Generation",Accepted to Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Argument generation is a challenging task whose research is timely
considering its potential impact on social media and the dissemination of
information. Here we suggest a pipeline based on GPT-2 for generating coherent
claims, and explore the types of claims that it produces, and their veracity,
using an array of manual and automatic assessments. In addition, we explore the
interplay between this task and the task of Claim Retrieval, showing how they
can complement one another.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 05:22:30 GMT'}]",2020-10-14,"[['Gretz', 'Shai', ''], ['Bilu', 'Yonatan', ''], ['Cohen-Karlik', 'Edo', ''], ['Slonim', 'Noam', '']]"
1362480,2010.06150,Nan Ding,"Xi Chen, Nan Ding, Tomer Levinboim, Radu Soricut","Improving Text Generation Evaluation with Batch Centering and Tempered
  Word Mover Distance",EMNLP 2020 Eval4NLP Workshop,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in automatic evaluation metrics for text have shown that deep
contextualized word representations, such as those generated by BERT encoders,
are helpful for designing metrics that correlate well with human judgements. At
the same time, it has been argued that contextualized word representations
exhibit sub-optimal statistical properties for encoding the true similarity
between words or sentences. In this paper, we present two techniques for
improving encoding representations for similarity metrics: a batch-mean
centering strategy that improves statistical properties; and a computationally
efficient tempered Word Mover Distance, for better fusion of the information in
the contextualized word representations. We conduct numerical experiments that
demonstrate the robustness of our techniques, reporting results over various
BERT-backbone learned metrics and achieving state of the art correlation with
human ratings on several benchmarks.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 03:46:25 GMT'}]",2020-10-14,"[['Chen', 'Xi', ''], ['Ding', 'Nan', ''], ['Levinboim', 'Tomer', ''], ['Soricut', 'Radu', '']]"
1362468,2010.06138,Junliang Guo,"Junliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei, Boxing Chen, Enhong
  Chen",Incorporating BERT into Parallel Sequence Decoding with Adapters,NeurIPS 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While large scale pre-trained language models such as BERT have achieved
great success on various natural language understanding tasks, how to
efficiently and effectively incorporate them into sequence-to-sequence models
and the corresponding text generation tasks remains a non-trivial problem. In
this paper, we propose to address this problem by taking two different BERT
models as the encoder and decoder respectively, and fine-tuning them by
introducing simple and lightweight adapter modules, which are inserted between
BERT layers and tuned on the task-specific dataset. In this way, we obtain a
flexible and efficient model which is able to jointly leverage the information
contained in the source-side and target-side BERT models, while bypassing the
catastrophic forgetting problem. Each component in the framework can be
considered as a plug-in unit, making the framework flexible and task agnostic.
Our framework is based on a parallel sequence decoding algorithm named
Mask-Predict considering the bi-directional and conditional independent nature
of BERT, and can be adapted to traditional autoregressive decoding easily. We
conduct extensive experiments on neural machine translation tasks where the
proposed method consistently outperforms autoregressive baselines while
reducing the inference latency by half, and achieves $36.49$/$33.57$ BLEU
scores on IWSLT14 German-English/WMT14 German-English translation. When adapted
to autoregressive decoding, the proposed method achieves $30.60$/$43.56$ BLEU
scores on WMT14 English-German/English-French translation, on par with the
state-of-the-art baseline models.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 03:25:15 GMT'}]",2020-10-14,"[['Guo', 'Junliang', ''], ['Zhang', 'Zhirui', ''], ['Xu', 'Linli', ''], ['Wei', 'Hao-Ran', ''], ['Chen', 'Boxing', ''], ['Chen', 'Enhong', '']]"
1362467,2010.06137,Farjana Sultana Mim,"Farjana Sultana Mim, Naoya Inoue, Paul Reisert, Hiroki Ouchi and
  Kentaro Inui","Corruption Is Not All Bad: Incorporating Discourse Structure into
  Pre-training via Corruption for Essay Scoring",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing approaches for automated essay scoring and document representation
learning typically rely on discourse parsers to incorporate discourse structure
into text representation. However, the performance of parsers is not always
adequate, especially when they are used on noisy texts, such as student essays.
In this paper, we propose an unsupervised pre-training approach to capture
discourse structure of essays in terms of coherence and cohesion that does not
require any discourse parser or annotation. We introduce several types of
token, sentence and paragraph-level corruption techniques for our proposed
pre-training approach and augment masked language modeling pre-training with
our pre-training method to leverage both contextualized and discourse
information. Our proposed unsupervised approach achieves new state-of-the-art
result on essay Organization scoring task.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 03:17:34 GMT'}]",2020-10-14,"[['Mim', 'Farjana Sultana', ''], ['Inoue', 'Naoya', ''], ['Reisert', 'Paul', ''], ['Ouchi', 'Hiroki', ''], ['Inui', 'Kentaro', '']]"
1362463,2010.06133,XiaoKang Liu,"Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng Xu, Min Yang and
  Yaohong Jin","BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth
  Mover's Distance",EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models (e.g., BERT) have achieved significant success in
various natural language processing (NLP) tasks. However, high storage and
computational costs obstruct pre-trained language models to be effectively
deployed on resource-constrained devices. In this paper, we propose a novel
BERT distillation method based on many-to-many layer mapping, which allows each
intermediate student layer to learn from any intermediate teacher layers. In
this way, our model can learn from different teacher layers adaptively for
various NLP tasks. %motivated by the intuition that different NLP tasks require
different levels of linguistic knowledge contained in the intermediate layers
of BERT. In addition, we leverage Earth Mover's Distance (EMD) to compute the
minimum cumulative cost that must be paid to transform knowledge from teacher
network to student network. EMD enables the effective matching for many-to-many
layer mapping. %EMD can be applied to network layers with different sizes and
effectively measures semantic distance between the teacher network and student
network. Furthermore, we propose a cost attention mechanism to learn the layer
weights used in EMD automatically, which is supposed to further improve the
model's performance and accelerate convergence time. Extensive experiments on
GLUE benchmark demonstrate that our model achieves competitive performance
compared to strong competitors in terms of both accuracy and model compression.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 02:53:52 GMT'}]",2020-10-14,"[['Li', 'Jianquan', ''], ['Liu', 'Xiaokang', ''], ['Zhao', 'Honghong', ''], ['Xu', 'Ruifeng', ''], ['Yang', 'Min', ''], ['Jin', 'Yaohong', '']]"
1362457,2010.06127,Yang Chen,Yang Chen and Alan Ritter,"Model Selection for Cross-Lingual Transfer using a Learned Scoring
  Function",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformers that are pre-trained on multilingual text corpora, such as,
mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer learning
results. In the zero-shot cross-lingual transfer setting, only English training
data is assumed, and the fine-tuned model is evaluated on another target
language. No target-language validation data is assumed in this setting,
however substantial variance has been observed in target language performance
between different fine-tuning runs. Prior work has relied on English
validation/development data to select among models that are fine-tuned with
different learning rates, number of steps and other hyperparameters, often
resulting in suboptimal choices. To address this challenge, we propose a
meta-learning approach to model selection that uses the fine-tuned model's own
internal representations to predict its cross-lingual capabilities. In
extensive experiments we find that our approach consistently selects better
models than English validation data across five languages and five well-studied
NLP tasks, achieving results that are comparable to small amounts of target
language development data.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 02:36:48 GMT'}]",2020-10-14,"[['Chen', 'Yang', ''], ['Ritter', 'Alan', '']]"
1362452,2010.06122,Clara Vania,"Clara Vania, Ruijie Chen, Samuel R. Bowman","Asking Crowdworkers to Write Entailment Examples: The Best of Bad
  Options",AACL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale natural language inference (NLI) datasets such as SNLI or MNLI
have been created by asking crowdworkers to read a premise and write three new
hypotheses, one for each possible semantic relationships (entailment,
contradiction, and neutral). While this protocol has been used to create useful
benchmark data, it remains unclear whether the writing-based annotation
protocol is optimal for any purpose, since it has not been evaluated directly.
Furthermore, there is ample evidence that crowdworker writing can introduce
artifacts in the data. We investigate two alternative protocols which
automatically create candidate (premise, hypothesis) pairs for annotators to
label. Using these protocols and a writing-based baseline, we collect several
new English NLI datasets of over 3k examples each, each using a fixed amount of
annotator time, but a varying number of examples to fit that time budget. Our
experiments on NLI and transfer learning show negative results: None of the
alternative protocols outperforms the baseline in evaluations of generalization
within NLI or on transfer to outside target tasks. We conclude that crowdworker
writing still the best known option for entailment data, highlighting the need
for further data collection work to focus on improving writing-based annotation
processes.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 02:27:05 GMT'}]",2020-10-14,"[['Vania', 'Clara', ''], ['Chen', 'Ruijie', ''], ['Bowman', 'Samuel R.', '']]"
1362449,2010.06119,Qingyun Wang,"Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Heng Ji, Nazneen
  Fatema Rajani","ReviewRobot: Explainable Paper Review Generation based on Knowledge
  Synthesis",11 pages. Accepted by INLG 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To assist human review process, we build a novel ReviewRobot to automatically
assign a review score and write comments for multiple categories. A good review
needs to be knowledgeable, namely that the comments should be constructive and
informative to help improve the paper; and explainable by providing detailed
evidence. ReviewRobot achieves these goals via three steps: (1) We perform
domain-specific Information Extraction to construct a knowledge graph (KG) from
the target paper under review, a related work KG from the papers cited by the
target paper, and a background KG from a large collection of previous papers in
the domain. (2) By comparing these three KGs we predict a review score and
detailed structured knowledge as evidence for each review category. (3) We
carefully select and generalize human review sentences into templates, and
apply these templates to transform the review scores and evidence into natural
language comments. Experimental results show that our review score predictor
reaches 71.4-100% accuracy. Human assessment by domain experts shows that
41.7%-70.5% of the comments generated by ReviewRobot are valid and
constructive, and better than human-written ones 20% of the time. Thus,
ReviewRobot can serve as an assistant for paper reviewers, program chairs and
authors.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 02:17:58 GMT'}]",2020-10-14,"[['Wang', 'Qingyun', ''], ['Zeng', 'Qi', ''], ['Huang', 'Lifu', ''], ['Knight', 'Kevin', ''], ['Ji', 'Heng', ''], ['Rajani', 'Nazneen Fatema', '']]"
1362851,2010.06521,Michael Kruse,"Michael Kruse, Hal Finkel, Xingfu Wu",Autotuning Search Space for Loop Transformations,LLVM-in-HPC 2020 preprint,,,,cs.DC cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One of the challenges for optimizing compilers is to predict whether applying
an optimization will improve its execution speed. Programmers may override the
compiler's profitability heuristic using optimization directives such as
pragmas in the source code. Machine learning in the form of autotuning can
assist users in finding the best optimizations for each platform.
  In this paper we propose a loop transformation search space that takes the
form of a tree, in contrast to previous approaches that usually use vector
spaces to represent loop optimization configurations. We implemented a simple
autotuner exploring the search space and applied it to a selected set of
PolyBench kernels. While the autotuner is capable of representing every
possible sequence of loop transformations and their relations, the results
motivate the use of better search strategies such as Monte Carlo tree search to
find sophisticated loop transformations such as multilevel tiling.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 16:26:57 GMT'}]",2020-10-14,"[['Kruse', 'Michael', ''], ['Finkel', 'Hal', ''], ['Wu', 'Xingfu', '']]"
1359519,2010.03189,BalaSundaraRaman Lakshmanan,BalaSundaraRaman Lakshmanan and Sanjeeth Kumar Ravindranath,"Theedhum Nandrum@Dravidian-CodeMix-FIRE2020: A Sentiment Polarity
  Classifier for YouTube Comments with Code-switching between Tamil, Malayalam
  and English","FIRE 2020, December 16-20, 2020, Hyderabad, India",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Theedhum Nandrum is a sentiment polarity detection system using two
approaches--a Stochastic Gradient Descent (SGD) based classifier and a Long
Short-term Memory (LSTM) based Classifier. Our approach utilises language
features like use of emoji, choice of scripts and code mixing which appeared
quite marked in the datasets specified for the Dravidian Codemix - FIRE 2020
task. The hyperparameters for the SGD were tuned using GridSearchCV. Our system
was ranked 4th in Tamil-English with a weighted average F1 score of 0.62 and
9th in Malayalam-English with a score of 0.65. We achieved a weighted average
F1 score of 0.77 for Tamil-English using a Logistic Regression based model
after the task deadline. This performance betters the top ranked classifier on
this dataset by a wide margin. Our use of language-specific Soundex to
harmonise the spelling variants in code-mixed data appears to be a novel
application of Soundex. Our complete code is published in github at
https://github.com/oligoglot/theedhum-nandrum.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 05:40:25 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 09:27:35 GMT'}]",2020-10-14,"[['Lakshmanan', 'BalaSundaraRaman', ''], ['Ravindranath', 'Sanjeeth Kumar', '']]"
1362725,2010.06395,Malte Ostendorff,"Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, Georg Rehm",Aspect-based Document Similarity for Research Papers,Accepted for publication at COLING 2020,,,,cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Traditional document similarity measures provide a coarse-grained distinction
between similar and dissimilar documents. Typically, they do not consider in
what aspects two documents are similar. This limits the granularity of
applications like recommender systems that rely on document similarity. In this
paper, we extend similarity with aspect information by performing a pairwise
document classification task. We evaluate our aspect-based document similarity
for research papers. Paper citations indicate the aspect-based similarity,
i.e., the section title in which a citation occurs acts as a label for the pair
of citing and cited paper. We apply a series of Transformer models such as
RoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM
baseline. We perform our experiments on two newly constructed datasets of
172,073 research paper pairs from the ACL Anthology and CORD-19 corpus. Our
results show SciBERT as the best performing system. A qualitative examination
validates our quantitative results. Our findings motivate future research of
aspect-based document similarity and the development of a recommender system
based on the evaluated techniques. We make our datasets, code, and trained
models publicly available.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 13:51:21 GMT'}]",2020-10-14,"[['Ostendorff', 'Malte', ''], ['Ruas', 'Terry', ''], ['Blume', 'Till', ''], ['Gipp', 'Bela', ''], ['Rehm', 'Georg', '']]"
1349618,2009.08115,Yichi Zhang,"Yichi Zhang, Zhijian Ou, Huixin Wang, Junlan Feng","A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief
  States towards Semi-Supervised Learning",Accepted by EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Structured belief states are crucial for user goal tracking and database
query in task-oriented dialog systems. However, training belief trackers often
requires expensive turn-level annotations of every user utterance. In this
paper we aim at alleviating the reliance on belief state labels in building
end-to-end dialog systems, by leveraging unlabeled dialog data towards
semi-supervised learning. We propose a probabilistic dialog model, called the
LAtent BElief State (LABES) model, where belief states are represented as
discrete latent variables and jointly modeled with system responses given user
inputs. Such latent variable modeling enables us to develop semi-supervised
learning under the principled variational learning framework. Furthermore, we
introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of
LABES. In supervised experiments, LABES-S2S obtains strong results on three
benchmark datasets of different scales. In utilizing unlabeled dialog data,
semi-supervised LABES-S2S significantly outperforms both supervised-only and
semi-supervised baselines. Remarkably, we can reduce the annotation demands to
50% without performance loss on MultiWOZ.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 07:26:37 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 06:43:09 GMT'}, {'version': 'v3', 'created': 'Tue, 13 Oct 2020 14:18:09 GMT'}]",2020-10-14,"[['Zhang', 'Yichi', ''], ['Ou', 'Zhijian', ''], ['Wang', 'Huixin', ''], ['Feng', 'Junlan', '']]"
1354773,2009.13270,Rajiv Movva,"Rajiv Movva, Jason Y. Zhao","Dissecting Lottery Ticket Transformers: Structural and Behavioral Study
  of Sparse Neural Machine Translation",Camera-ready for BlackboxNLP @ EMNLP 2020,,,,cs.CL cs.LG stat.ML,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recent work on the lottery ticket hypothesis has produced highly sparse
Transformers for NMT while maintaining BLEU. However, it is unclear how such
pruning techniques affect a model's learned representations. By probing
Transformers with more and more low-magnitude weights pruned away, we find that
complex semantic information is first to be degraded. Analysis of internal
activations reveals that higher layers diverge most over the course of pruning,
gradually becoming less complex than their dense counterparts. Meanwhile, early
layers of sparse models begin to perform more encoding. Attention mechanisms
remain remarkably consistent as sparsity increases.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 02:08:45 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 18:55:22 GMT'}]",2020-10-14,"[['Movva', 'Rajiv', ''], ['Zhao', 'Jason Y.', '']]"
1346268,2009.04765,Damian Pascual,"Nicolas Affolter, Beni Egressy, Damian Pascual, Roger Wattenhofer",Brain2Word: Decoding Brain Activity for Language Generation,,,,,cs.CL cs.LG q-bio.NC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Brain decoding, understood as the process of mapping brain activities to the
stimuli that generated them, has been an active research area in the last
years. In the case of language stimuli, recent studies have shown that it is
possible to decode fMRI scans into an embedding of the word a subject is
reading. However, such word embeddings are designed for natural language
processing tasks rather than for brain decoding. Therefore, they limit our
ability to recover the precise stimulus. In this work, we propose to directly
classify an fMRI scan, mapping it to the corresponding word within a fixed
vocabulary. Unlike existing work, we evaluate on scans from previously unseen
subjects. We argue that this is a more realistic setup and we present a model
that can decode fMRI data from unseen subjects. Our model achieves 5.22% Top-1
and 13.59% Top-5 accuracy in this challenging task, significantly outperforming
all the considered competitive baselines. Furthermore, we use the decoded words
to guide language generation with the GPT-2 model. This way, we advance the
quest for a system that translates brain activities into coherent text.
","[{'version': 'v1', 'created': 'Thu, 10 Sep 2020 10:47:36 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 08:05:08 GMT'}]",2020-10-14,"[['Affolter', 'Nicolas', ''], ['Egressy', 'Beni', ''], ['Pascual', 'Damian', ''], ['Wattenhofer', 'Roger', '']]"
1267191,2004.02421,Deng Cai,"Zibo Lin, Deng Cai, Yan Wang, Xiaojiang Liu, Hai-Tao Zheng, Shuming
  Shi","The World is Not Binary: Learning to Rank with Grayscale Data for
  Dialogue Response Selection",EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Response selection plays a vital role in building retrieval-based
conversation systems. Despite that response selection is naturally a
learning-to-rank problem, most prior works take a point-wise view and train
binary classifiers for this task: each response candidate is labeled either
relevant (one) or irrelevant (zero). On the one hand, this formalization can be
sub-optimal due to its ignorance of the diversity of response quality. On the
other hand, annotating grayscale data for learning-to-rank can be prohibitively
expensive and challenging. In this work, we show that grayscale data can be
automatically constructed without human effort. Our method employs
off-the-shelf response retrieval models and response generation models as
automatic grayscale data generators. With the constructed grayscale data, we
propose multi-level ranking objectives for training, which can (1) teach a
matching model to capture more fine-grained context-response relevance
difference and (2) reduce the train-test discrepancy in terms of distractor
strength. Our method is simple, effective, and universal. Experiments on three
benchmark datasets and four state-of-the-art matching models show that the
proposed approach brings significant and consistent performance improvements.
","[{'version': 'v1', 'created': 'Mon, 6 Apr 2020 06:34:54 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Apr 2020 02:39:39 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Sep 2020 14:08:23 GMT'}, {'version': 'v4', 'created': 'Tue, 13 Oct 2020 07:08:07 GMT'}]",2020-10-14,"[['Lin', 'Zibo', ''], ['Cai', 'Deng', ''], ['Wang', 'Yan', ''], ['Liu', 'Xiaojiang', ''], ['Zheng', 'Hai-Tao', ''], ['Shi', 'Shuming', '']]"
1141880,1906.09694,Frank Z. Xing,Haodong Bai and Frank Z. Xing and Erik Cambria and Win-Bin Huang,"Business Taxonomy Construction Using Concept-Level Hierarchical
  Clustering","Accepted to The First Workshop on Financial Technology and Natural
  Language Processing (FinNLP@IJCAI-19)",,,,cs.CL q-fin.PM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Business taxonomies are indispensable tools for investors to do equity
research and make professional decisions. However, to identify the structure of
industry sectors in an emerging market is challenging for two reasons. First,
existing taxonomies are designed for mature markets, which may not be the
appropriate classification for small companies with innovative business models.
Second, emerging markets are fast-developing, thus the static business
taxonomies cannot promptly reflect the new features. In this article, we
propose a new method to construct business taxonomies automatically from the
content of corporate annual reports. Extracted concepts are hierarchically
clustered using greedy affinity propagation. Our method requires less
supervision and is able to discover new terms. Experiments and evaluation on
the Chinese National Equities Exchange and Quotations (NEEQ) market show
several advantages of the business taxonomy we build. Our results provide an
effective tool for understanding and investing in the new growth companies.
","[{'version': 'v1', 'created': 'Mon, 24 Jun 2019 02:59:22 GMT'}]",2020-10-14,"[['Bai', 'Haodong', ''], ['Xing', 'Frank Z.', ''], ['Cambria', 'Erik', ''], ['Huang', 'Win-Bin', '']]"
1268260,2004.03490,Priyanka Sen,Priyanka Sen and Amir Saffari,What do Models Learn from Question Answering Datasets?,Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While models have reached superhuman performance on popular question
answering (QA) datasets such as SQuAD, they have yet to outperform humans on
the task of question answering itself. In this paper, we investigate if models
are learning reading comprehension from QA datasets by evaluating BERT-based
models across five datasets. We evaluate models on their generalizability to
out-of-domain examples, responses to missing or incorrect data, and ability to
handle question variations. We find that no single dataset is robust to all of
our experiments and identify shortcomings in both datasets and evaluation
methods. Following our analysis, we make recommendations for building future QA
datasets that better evaluate the task of question answering through reading
comprehension. We also release code to convert QA datasets to a shared format
for easier experimentation at
https://github.com/amazon-research/qa-dataset-converter.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 15:41:55 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 13:02:44 GMT'}]",2020-10-14,"[['Sen', 'Priyanka', ''], ['Saffari', 'Amir', '']]"
1268429,2004.03659,Elena Tutubalina Dr.,"Elena Tutubalina, Ilseyar Alimova, Zulfat Miftahutdinov, Andrey
  Sakhovskiy, Valentin Malykh and Sergey Nikolenko","The Russian Drug Reaction Corpus and Neural Models for Drug Reactions
  and Effectiveness Detection in User Reviews","9 pages, 9 tables, 4 figures","Bioinformatics, 2020",10.1093/bioinformatics/btaa675,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Russian Drug Reaction Corpus (RuDReC) is a new partially annotated corpus
of consumer reviews in Russian about pharmaceutical products for the detection
of health-related named entities and the effectiveness of pharmaceutical
products. The corpus itself consists of two parts, the raw one and the labelled
one. The raw part includes 1.4 million health-related user-generated texts
collected from various Internet sources, including social media. The labelled
part contains 500 consumer reviews about drug therapy with drug- and
disease-related information. Labels for sentences include health-related issues
or their absence. The sentences with one are additionally labelled at the
expression level for identification of fine-grained subtypes such as drug
classes and drug forms, drug indications, and drug reactions. Further, we
present a baseline model for named entity recognition (NER) and multi-label
sentence classification tasks on this corpus. The macro F1 score of 74.85% in
the NER task was achieved by our RuDR-BERT model. For the sentence
classification task, our model achieves the macro F1 score of 68.82% gaining
7.47% over the score of BERT model trained on Russian data. We make the RuDReC
corpus and pretrained weights of domain-specific BERT models freely available
at https://github.com/cimm-kzn/RuDReC
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 19:26:13 GMT'}]",2020-10-14,"[['Tutubalina', 'Elena', ''], ['Alimova', 'Ilseyar', ''], ['Miftahutdinov', 'Zulfat', ''], ['Sakhovskiy', 'Andrey', ''], ['Malykh', 'Valentin', ''], ['Nikolenko', 'Sergey', '']]"
1268638,2004.03868,Dieuwke Hupkes,"Diana Rodr\'iguez Luna, Edoardo Maria Ponti, Dieuwke Hupkes, Elia
  Bruni","Internal and external pressures on language emergence: least effort,
  object constancy and frequency",Accepted for EMNLP-findings,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In previous work, artificial agents were shown to achieve almost perfect
accuracy in referential games where they have to communicate to identify
images. Nevertheless, the resulting communication protocols rarely display
salient features of natural languages, such as compositionality. In this paper,
we propose some realistic sources of pressure on communication that avert this
outcome. More specifically, we formalise the principle of least effort through
an auxiliary objective. Moreover, we explore several game variants, inspired by
the principle of object constancy, in which we alter the frequency, position,
and luminosity of the objects in the images. We perform an extensive analysis
on their effect through compositionality metrics, diagnostic classifiers, and
zero-shot evaluation. Our findings reveal that the proposed sources of pressure
result in emerging languages with less redundancy, more focus on high-level
conceptual information, and better abilities of generalisation. Overall, our
contributions reduce the gap between emergent and natural languages.
","[{'version': 'v1', 'created': 'Wed, 8 Apr 2020 08:12:41 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Jul 2020 17:48:06 GMT'}, {'version': 'v3', 'created': 'Tue, 13 Oct 2020 09:29:44 GMT'}]",2020-10-14,"[['Luna', 'Diana Rodríguez', ''], ['Ponti', 'Edoardo Maria', ''], ['Hupkes', 'Dieuwke', ''], ['Bruni', 'Elia', '']]"
1280561,2005.00766,Nora Kassner,"Nora Kassner and Hinrich Sch\""utze","BERT-kNN: Adding a kNN Search Component to Pretrained Language Models
  for Better QA",to appear in EMNLP Findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve
language model performance. We show that this idea is beneficial for
open-domain question answering (QA). To improve the recall of facts encountered
during training, we combine BERT (Devlin et al., 2019) with a traditional
information retrieval step (IR) and a kNN search over a large datastore of an
embedded text collection. Our contributions are as follows: i) BERT-kNN
outperforms BERT on cloze-style QA by large margins without any further
training. ii) We show that BERT often identifies the correct response category
(e.g., US city), but only kNN recovers the factually correct answer (e.g.,
""Miami""). iii) Compared to BERT, BERT-kNN excels for rare facts. iv) BERT-kNN
can easily handle facts not covered by BERT's training set, e.g., recent
events.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 09:34:42 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 18:44:05 GMT'}]",2020-10-14,"[['Kassner', 'Nora', ''], ['Schütze', 'Hinrich', '']]"
1279942,2005.00147,Yasumasa Onoe,Yasumasa Onoe and Greg Durrett,Interpretable Entity Representations through Large-Scale Typing,Findings of EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In standard methodology for natural language processing, entities in text are
typically embedded in dense vector spaces with pre-trained models. The
embeddings produced this way are effective when fed into downstream models, but
they require end-task fine-tuning and are fundamentally difficult to interpret.
In this paper, we present an approach to creating entity representations that
are human readable and achieve high performance on entity-related tasks out of
the box. Our representations are vectors whose values correspond to posterior
probabilities over fine-grained entity types, indicating the confidence of a
typing model's decision that the entity belongs to the corresponding type. We
obtain these representations using a fine-grained entity typing model, trained
either on supervised ultra-fine entity typing data (Choi et al. 2018) or
distantly-supervised examples from Wikipedia. On entity probing tasks involving
recognizing entity identity, our embeddings used in parameter-free downstream
models achieve competitive performance with ELMo- and BERT-based embeddings in
trained models. We also show that it is possible to reduce the size of our type
set in a learning-based way for particular domains. Finally, we show that these
embeddings can be post-hoc modified through a small number of rules to
incorporate domain knowledge and improve performance.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 23:58:03 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 01:18:13 GMT'}]",2020-10-14,"[['Onoe', 'Yasumasa', ''], ['Durrett', 'Greg', '']]"
1347871,2009.06368,Yanjun  Qi Dr.,"Jin Yong Yoo, John X. Morris, Eli Lifland, Yanjun Qi","Searching for a Search Method: Benchmarking Search Algorithms for
  Generating NLP Adversarial Examples","14 pages, 5 figures, 4 tables; Accepted by EMNLP BlackBox NLP
  Workshop 2020 @ https://blackboxnlp.github.io/cfp.html",,,,cs.CL cs.AI cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the behavior of several black-box search algorithms used for
generating adversarial examples for natural language processing (NLP) tasks. We
perform a fine-grained analysis of three elements relevant to search: search
algorithm, search space, and search budget. When new search algorithms are
proposed in past work, the attack search space is often modified alongside the
search algorithm. Without ablation studies benchmarking the search algorithm
change with the search space held constant, one cannot tell if an increase in
attack success rate is a result of an improved search algorithm or a less
restrictive search space. Additionally, many previous studies fail to properly
consider the search algorithms' run-time cost, which is essential for
downstream tasks like adversarial training. Our experiments provide a
reproducible benchmark of search algorithms across a variety of search spaces
and query budgets to guide future research in adversarial NLP. Based on our
experiments, we recommend greedy attacks with word importance ranking when
under a time constraint or attacking long inputs, and either beam search or
particle swarm optimization otherwise. Code implementation shared via
https://github.com/QData/TextAttack-Search-Benchmark
","[{'version': 'v1', 'created': 'Wed, 9 Sep 2020 17:04:42 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 19:46:36 GMT'}]",2020-10-14,"[['Yoo', 'Jin Yong', ''], ['Morris', 'John X.', ''], ['Lifland', 'Eli', ''], ['Qi', 'Yanjun', '']]"
1348813,2009.07310,Ozan Caglayan,"Ozan Caglayan, Julia Ive, Veneta Haralampieva, Pranava Madhyastha,
  Lo\""ic Barrault and Lucia Specia",Simultaneous Machine Translation with Visual Context,"Long paper accepted to EMNLP 2020, Camera-ready version",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Simultaneous machine translation (SiMT) aims to translate a continuous input
text stream into another language with the lowest latency and highest quality
possible. The translation thus has to start with an incomplete source text,
which is read progressively, creating the need for anticipation. In this paper,
we seek to understand whether the addition of visual information can compensate
for the missing source context. To this end, we analyse the impact of different
multimodal approaches and visual features on state-of-the-art SiMT frameworks.
Our results show that visual context is helpful and that visually-grounded
models based on explicit object region information are much better than
commonly used global features, reaching up to 3 BLEU points improvement under
low latency scenarios. Our qualitative analysis illustrates cases where only
the multimodal systems are able to translate correctly from English into
gender-marked languages, as well as deal with differences in word order, such
as adjective-noun placement between English and French.
","[{'version': 'v1', 'created': 'Tue, 15 Sep 2020 18:19:11 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Sep 2020 10:27:15 GMT'}, {'version': 'v3', 'created': 'Tue, 13 Oct 2020 10:45:18 GMT'}]",2020-10-14,"[['Caglayan', 'Ozan', ''], ['Ive', 'Julia', ''], ['Haralampieva', 'Veneta', ''], ['Madhyastha', 'Pranava', ''], ['Barrault', 'Loïc', ''], ['Specia', 'Lucia', '']]"
1082699,1902.01615,Prakhar Ganesh,"Prakhar Ganesh, Saket Dingliwal","Restructuring Conversations using Discourse Relations for Zero-shot
  Abstractive Dialogue Summarization",4 pages + supplementary,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Dialogue summarization is a challenging problem due to the informal and
unstructured nature of conversational data. Recent advances in abstractive
summarization have been focused on data-hungry neural models and adapting these
models to a new domain requires the availability of domain-specific manually
annotated corpus created by linguistic experts. We propose a zero-shot
abstractive dialogue summarization method that uses discourse relations to
provide structure to conversations, and then uses an out-of-the-box document
summarization model to create final summaries. Experiments on the AMI and ICSI
meeting corpus, with document summarization models like PGN and BART, shows
that our method improves the ROGUE score by up to 3 points, and even performs
competitively against other state-of-the-art methods.
","[{'version': 'v1', 'created': 'Tue, 5 Feb 2019 09:50:47 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 07:04:22 GMT'}]",2020-10-14,"[['Ganesh', 'Prakhar', ''], ['Dingliwal', 'Saket', '']]"
1348956,2009.07453,Se Jung Kwon,"Insoo Chung, Byeongwook Kim, Yoonjung Choi, Se Jung Kwon, Yongkweon
  Jeon, Baeseong Park, Sangha Kim and Dongsoo Lee","Extremely Low Bit Transformer Quantization for On-Device Neural Machine
  Translation",Findings of EMNLP 2020,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The deployment of widely used Transformer architecture is challenging because
of heavy computation load and memory overhead during inference, especially when
the target device is limited in computational resources such as mobile or edge
devices. Quantization is an effective technique to address such challenges. Our
analysis shows that for a given number of quantization bits, each block of
Transformer contributes to translation quality and inference computations in
different manners. Moreover, even inside an embedding block, each word presents
vastly different contributions. Correspondingly, we propose a mixed precision
quantization strategy to represent Transformer weights by an extremely low
number of bits (e.g., under 3 bits). For example, for each word in an embedding
block, we assign different quantization bits based on statistical property. Our
quantized Transformer model achieves 11.8$\times$ smaller model size than the
baseline model, with less than -0.5 BLEU. We achieve 8.3$\times$ reduction in
run-time memory footprints and 3.5$\times$ speed up (Galaxy N10+) such that our
proposed compression strategy enables efficient implementation for on-device
NMT.
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 03:58:01 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 05:23:31 GMT'}]",2020-10-14,"[['Chung', 'Insoo', ''], ['Kim', 'Byeongwook', ''], ['Choi', 'Yoonjung', ''], ['Kwon', 'Se Jung', ''], ['Jeon', 'Yongkweon', ''], ['Park', 'Baeseong', ''], ['Kim', 'Sangha', ''], ['Lee', 'Dongsoo', '']]"
1349046,2009.07543,Hengyi Cai,"Hengyi Cai, Hongshen Chen, Yonghao Song, Zhuoye Ding, Yongjun Bao,
  Weipeng Yan, Xiaofang Zhao",Group-wise Contrastive Learning for Neural Dialogue Generation,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural dialogue response generation has gained much popularity in recent
years. Maximum Likelihood Estimation (MLE) objective is widely adopted in
existing dialogue model learning. However, models trained with MLE objective
function are plagued by the low-diversity issue when it comes to the
open-domain conversational setting. Inspired by the observation that humans not
only learn from the positive signals but also benefit from correcting behaviors
of undesirable actions, in this work, we introduce contrastive learning into
dialogue generation, where the model explicitly perceives the difference
between the well-chosen positive and negative utterances. Specifically, we
employ a pretrained baseline model as a reference. During contrastive learning,
the target dialogue model is trained to give higher conditional probabilities
for the positive samples, and lower conditional probabilities for those
negative samples, compared to the reference model. To manage the multi-mapping
relations prevailed in human conversation, we augment contrastive dialogue
learning with group-wise dual sampling. Extensive experimental results show
that the proposed group-wise contrastive learning framework is suited for
training a wide range of neural dialogue generation models with very favorable
performance over the baseline training approaches.
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 08:28:30 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 04:12:07 GMT'}]",2020-10-14,"[['Cai', 'Hengyi', ''], ['Chen', 'Hongshen', ''], ['Song', 'Yonghao', ''], ['Ding', 'Zhuoye', ''], ['Bao', 'Yongjun', ''], ['Yan', 'Weipeng', ''], ['Zhao', 'Xiaofang', '']]"
1201947,1911.03353,Tan Yan,"Tan Yan, Heyan Huang, Xian-Ling Mao","SEPT: Improving Scientific Named Entity Recognition with Span
  Representation",This work is outdated. The result should not be trusted,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a new scientific named entity recognizer called SEPT, which
stands for Span Extractor with Pre-trained Transformers. In recent papers, span
extractors have been demonstrated to be a powerful model compared with sequence
labeling models. However, we discover that with the development of pre-trained
language models, the performance of span extractors appears to become similar
to sequence labeling models. To keep the advantages of span representation, we
modified the model by under-sampling to balance the positive and negative
samples and reduce the search space. Furthermore, we simplify the origin
network architecture to combine the span extractor with BERT. Experiments
demonstrate that even simplified architecture achieves the same performance and
SEPT achieves a new state of the art result in scientific named entity
recognition even without relation information involved.
","[{'version': 'v1', 'created': 'Fri, 8 Nov 2019 16:19:26 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 04:25:25 GMT'}]",2020-10-14,"[['Yan', 'Tan', ''], ['Huang', 'Heyan', ''], ['Mao', 'Xian-Ling', '']]"
1286820,2005.07025,Kun Zhou,"Kun Zhou, Berrak Sisman, Mingyang Zhang and Haizhou Li","Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice
  Conversion",Accepted by Interspeech 2020,,,,cs.SD cs.AI cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Emotional voice conversion aims to convert the emotion of speech from one
state to another while preserving the linguistic content and speaker identity.
The prior studies on emotional voice conversion are mostly carried out under
the assumption that emotion is speaker-dependent. We consider that there is a
common code between speakers for emotional expression in a spoken language,
therefore, a speaker-independent mapping between emotional states is possible.
In this paper, we propose a speaker-independent emotional voice conversion
framework, that can convert anyone's emotion without the need for parallel
data. We propose a VAW-GAN based encoder-decoder structure to learn the
spectrum and prosody mapping. We perform prosody conversion by using continuous
wavelet transform (CWT) to model the temporal dependencies. We also investigate
the use of F0 as an additional input to the decoder to improve emotion
conversion performance. Experiments show that the proposed speaker-independent
framework achieves competitive results for both seen and unseen speakers.
","[{'version': 'v1', 'created': 'Wed, 13 May 2020 13:36:34 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Aug 2020 13:37:48 GMT'}, {'version': 'v3', 'created': 'Tue, 13 Oct 2020 06:07:16 GMT'}]",2020-10-14,"[['Zhou', 'Kun', ''], ['Sisman', 'Berrak', ''], ['Zhang', 'Mingyang', ''], ['Li', 'Haizhou', '']]"
1328678,2008.00956,Paul Tarau,Paul Tarau and Eduardo Blanco,Interactive Text Graph Mining with a Prolog-based Dialog Engine,"Under consideration in Theory and Practice of Logic Programming
  (TPLP). arXiv admin note: substantial text overlap with arXiv:1909.09742",,10.1017/S1471068420000137,,cs.CL cs.LO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  On top of a neural network-based dependency parser and a graph-based natural
language processing module we design a Prolog-based dialog engine that explores
interactively a ranked fact database extracted from a text document.
  We reorganize dependency graphs to focus on the most relevant content
elements of a sentence and integrate sentence identifiers as graph nodes.
  Additionally, after ranking the graph we take advantage of the implicit
semantic information that dependency links and WordNet bring in the form of
subject-verb-object, is-a and part-of relations.
  Working on the Prolog facts and their inferred consequences, the dialog
engine specializes the text graph with respect to a query and reveals
interactively the document's most relevant content elements.
  The open-source code of the integrated system is available at
https://github.com/ptarau/DeepRank .
  Under consideration in Theory and Practice of Logic Programming (TPLP).
","[{'version': 'v1', 'created': 'Fri, 31 Jul 2020 03:29:49 GMT'}]",2020-10-14,"[['Tarau', 'Paul', ''], ['Blanco', 'Eduardo', '']]"
1277067,2004.12297,Liu Yang,"Liu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, Marc Najork","Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical
  Encoder for Long-Form Document Matching",Accepted as a full paper in CIKM 2020,,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many natural language processing and information retrieval problems can be
formalized as the task of semantic matching. Existing work in this area has
been largely focused on matching between short texts (e.g., question
answering), or between a short and a long text (e.g., ad-hoc retrieval).
Semantic matching between long-form documents, which has many important
applications like news recommendation, related article recommendation and
document clustering, is relatively less explored and needs more research
effort. In recent years, self-attention based models like Transformers and BERT
have achieved state-of-the-art performance in the task of text matching. These
models, however, are still limited to short text like a few sentences or one
paragraph due to the quadratic computational complexity of self-attention with
respect to input text length. In this paper, we address the issue by proposing
the Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for
long-form document matching. Our model contains several innovations to adapt
self-attention models for longer text input. In order to better capture
sentence level semantic relations within a document, we pre-train the model
with a novel masked sentence block language modeling task in addition to the
masked word language modeling task used by BERT. Our experimental results on
several benchmark datasets for long-form document matching show that our
proposed SMITH model outperforms the previous state-of-the-art models including
hierarchical attention, multi-depth attention-based hierarchical recurrent
neural network, and BERT. Comparing to BERT based baselines, our model is able
to increase maximum input text length from 512 to 2048. We will open source a
Wikipedia based benchmark dataset, code and a pre-trained checkpoint to
accelerate future research on long-form document matching.
","[{'version': 'v1', 'created': 'Sun, 26 Apr 2020 07:04:08 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 01:48:52 GMT'}]",2020-10-14,"[['Yang', 'Liu', ''], ['Zhang', 'Mingyang', ''], ['Li', 'Cheng', ''], ['Bendersky', 'Michael', ''], ['Najork', 'Marc', '']]"
1208490,1911.09896,Robert Hawkins,"Robert D. Hawkins, Minae Kwon, Dorsa Sadigh, Noah D. Goodman",Continual adaptation for efficient machine communication,Accepted at CoNLL,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To communicate with new partners in new contexts, humans rapidly form new
linguistic conventions. Recent neural language models are able to comprehend
and produce the existing conventions present in their training data, but are
not able to flexibly and interactively adapt those conventions on the fly as
humans do. We introduce an interactive repeated reference task as a benchmark
for models of adaptation in communication and propose a regularized continual
learning framework that allows an artificial agent initialized with a generic
language model to more accurately and efficiently communicate with a partner
over time. We evaluate this framework through simulations on COCO and in
real-time reference game experiments with human partners.
","[{'version': 'v1', 'created': 'Fri, 22 Nov 2019 07:26:40 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 09:39:21 GMT'}]",2020-10-14,"[['Hawkins', 'Robert D.', ''], ['Kwon', 'Minae', ''], ['Sadigh', 'Dorsa', ''], ['Goodman', 'Noah D.', '']]"
1353806,2009.12303,Prasetya Ajie Utama,"Prasetya Ajie Utama, Nafise Sadat Moosavi, Iryna Gurevych",Towards Debiasing NLU Models from Unknown Biases,Accepted at EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  NLU models often exploit biases to achieve high dataset-specific performance
without properly learning the intended task. Recently proposed debiasing
methods are shown to be effective in mitigating this tendency. However, these
methods rely on a major assumption that the types of bias should be known
a-priori, which limits their application to many NLU tasks and datasets. In
this work, we present the first step to bridge this gap by introducing a
self-debiasing framework that prevents models from mainly utilizing biases
without knowing them in advance. The proposed framework is general and
complementary to the existing debiasing methods. We show that it allows these
existing methods to retain the improvement on the challenge datasets (i.e.,
sets of examples designed to expose models' reliance on biases) without
specifically targeting certain biases. Furthermore, the evaluation suggests
that applying the framework results in improved overall robustness.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 15:49:39 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 11:00:39 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Oct 2020 11:52:22 GMT'}, {'version': 'v4', 'created': 'Tue, 13 Oct 2020 12:37:27 GMT'}]",2020-10-14,"[['Utama', 'Prasetya Ajie', ''], ['Moosavi', 'Nafise Sadat', ''], ['Gurevych', 'Iryna', '']]"
1363188,2010.06858,Paul O'Leary McCann,Paul McCann,"fugashi, a Tool for Tokenizing Japanese in Python",Accepted at EMNLP2020's NLP-OSS workshop,,,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  Recent years have seen an increase in the number of large-scale multilingual
NLP projects. However, even in such projects, languages with special processing
requirements are often excluded. One such language is Japanese. Japanese is
written without spaces, tokenization is non-trivial, and while high quality
open source tokenizers exist they can be hard to use and lack English
documentation. This paper introduces fugashi, a MeCab wrapper for Python, and
gives an introduction to tokenizing Japanese.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 07:52:47 GMT'}]",2020-10-15,"[['McCann', 'Paul', '']]"
1363221,2010.06891,Qingyang Wu,"Qingyang Wu, Zhenzhong Lan, Jing Gu, Zhou Yu",Memformer: The Memory-Augmented Transformer,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer models have obtained remarkable accomplishments in various NLP
tasks. However, these models have efficiency issues on long sequences, as the
complexity of their self-attention module scales quadratically with the
sequence length. To remedy the limitation, we present Memformer, a novel
language model that utilizes a single unified memory to encode and retrieve
past information. It includes a new optimization scheme, Memory Replay
Back-Propagation, which promotes long-range back-propagation through time with
a significantly reduced memory requirement. Memformer achieves $\mathcal{O}(n)$
time complexity and $\mathcal{O}(1)$ space complexity in processing long
sequences, meaning that the model can handle an infinite length sequence during
inference. Our model is also compatible with other self-supervised tasks to
further improve the performance on language modeling. Experimental results show
that Memformer outperforms the previous long-range sequence models on
WikiText-103, including Transformer-XL and compressive Transformer.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 09:03:36 GMT'}]",2020-10-15,"[['Wu', 'Qingyang', ''], ['Lan', 'Zhenzhong', ''], ['Gu', 'Jing', ''], ['Yu', 'Zhou', '']]"
1363045,2010.06715,Liam Fowl,"Liam Fowl, Micah Goldblum, Arjun Gupta, Amr Sharaf, Tom Goldstein","Random Network Distillation as a Diversity Metric for Both Image and
  Text Generation",,,,,cs.LG cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative models are increasingly able to produce remarkably high quality
images and text. The community has developed numerous evaluation metrics for
comparing generative models. However, these metrics do not effectively quantify
data diversity. We develop a new diversity metric that can readily be applied
to data, both synthetic and natural, of any type. Our method employs random
network distillation, a technique introduced in reinforcement learning. We
validate and deploy this metric on both images and text. We further explore
diversity in few-shot image generation, a setting which was previously
difficult to evaluate.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 22:03:52 GMT'}]",2020-10-15,"[['Fowl', 'Liam', ''], ['Goldblum', 'Micah', ''], ['Gupta', 'Arjun', ''], ['Sharaf', 'Amr', ''], ['Goldstein', 'Tom', '']]"
1363165,2010.06835,Zhucheng Tu,"Svitlana Vakulenko, Shayne Longpre, Zhucheng Tu, Raviteja Anantha","A Wrong Answer or a Wrong Question? An Intricate Relationship between
  Question Reformulation and Answer Selection in Conversational Question
  Answering","Accepted at the Workshop on Search-Oriented Conversational AI (SCAI)
  2020. arXiv admin note: text overlap with arXiv:2004.14652",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The dependency between an adequate question formulation and correct answer
selection is a very intriguing but still underexplored area. In this paper, we
show that question rewriting (QR) of the conversational context allows to shed
more light on this phenomenon and also use it to evaluate robustness of
different answer selection approaches. We introduce a simple framework that
enables an automated analysis of the conversational question answering (QA)
performance using question rewrites, and present the results of this analysis
on the TREC CAsT and QuAC (CANARD) datasets. Our experiments uncover
sensitivity to question formulation of the popular state-of-the-art models for
reading comprehension and passage ranking. Our results demonstrate that the
reading comprehension model is insensitive to question formulation, while the
passage ranking changes dramatically with a little variation in the input
question. The benefit of QR is that it allows us to pinpoint and group such
cases automatically. We show how to use this methodology to verify whether QA
models are really learning the task or just finding shortcuts in the dataset,
and better understand the frequent types of error they make.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 06:29:51 GMT'}]",2020-10-15,"[['Vakulenko', 'Svitlana', ''], ['Longpre', 'Shayne', ''], ['Tu', 'Zhucheng', ''], ['Anantha', 'Raviteja', '']]"
1363153,2010.06823,JingHui Qin,"Jinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang, Liang Lin","Semantically-Aligned Universal Tree-Structured Solver for Math Word
  Problems",EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A practical automatic textual math word problems (MWPs) solver should be able
to solve various textual MWPs while most existing works only focused on
one-unknown linear MWPs. Herein, we propose a simple but efficient method
called Universal Expression Tree (UET) to make the first attempt to represent
the equations of various MWPs uniformly. Then a semantically-aligned universal
tree-structured solver (SAU-Solver) based on an encoder-decoder framework is
proposed to resolve multiple types of MWPs in a unified model, benefiting from
our UET representation. Our SAU-Solver generates a universal expression tree
explicitly by deciding which symbol to generate according to the generated
symbols' semantic meanings like human solving MWPs. Besides, our SAU-Solver
also includes a novel subtree-level semanticallyaligned regularization to
further enforce the semantic constraints and rationality of the generated
expression tree by aligning with the contextual information. Finally, to
validate the universality of our solver and extend the research boundary of
MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP),
consisting of three types of MWPs. Experimental results on several MWPs
datasets show that our model can solve universal types of MWPs and outperforms
several state-of-the-art models.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 06:27:07 GMT'}]",2020-10-15,"[['Qin', 'Jinghui', ''], ['Lin', 'Lihui', ''], ['Liang', 'Xiaodan', ''], ['Zhang', 'Rumin', ''], ['Lin', 'Liang', '']]"
1363134,2010.06804,Ankur Goswami,"Ankur Goswami, Akshata Bhat, Hadar Ohana, Theodoros Rekatsinas","Unsupervised Relation Extraction from Language Models using Constrained
  Cloze Completion","14 pages, 5 figures, Accepted to Findings of EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We show that state-of-the-art self-supervised language models can be readily
used to extract relations from a corpus without the need to train a fine-tuned
extractive head. We introduce RE-Flex, a simple framework that performs
constrained cloze completion over pretrained language models to perform
unsupervised relation extraction. RE-Flex uses contextual matching to ensure
that language model predictions matches supporting evidence from the input
corpus that is relevant to a target relation. We perform an extensive
experimental study over multiple relation extraction benchmarks and demonstrate
that RE-Flex outperforms competing unsupervised relation extraction methods
based on pretrained language models by up to 27.8 $F_1$ points compared to the
next-best method. Our results show that constrained inference queries against a
language model can enable accurate unsupervised relation extraction.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 04:21:57 GMT'}]",2020-10-15,"[['Goswami', 'Ankur', ''], ['Bhat', 'Akshata', ''], ['Ohana', 'Hadar', ''], ['Rekatsinas', 'Theodoros', '']]"
1362777,2010.06447,Dan John Velasco,Dan John Velasco,"Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource
  Language","5 pages, 3 tables, 1 figure. in Filipino language; typos corrected,
  rephrased sentences, thoughts and results unchanged",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Low-resource languages such as Filipino suffer from data scarcity which makes
it challenging to develop NLP applications for Filipino language. The use of
Transfer Learning (TL) techniques alleviates this problem in low-resource
setting. In recent years, transformer-based models are proven to be effective
in low-resource tasks but faces challenges in accessibility due to its high
compute and memory requirements. For this reason, there's a need for a cheaper
but effective alternative. This paper has three contributions. First, release a
pre-trained AWD-LSTM language model for Filipino language. Second, benchmark
AWD-LSTM in the Hate Speech classification task and show that it performs on
par with transformer-based models. Third, analyze the the performance of
AWD-LSTM in low-resource setting using degradation test and compare it with
transformer-based models.
  -----
  Ang mga low-resource languages tulad ng Filipino ay gipit sa accessible na
datos kaya't mahirap gumawa ng mga applications sa wikang ito. Ang mga Transfer
Learning (TL) techniques ay malaking tulong para sa low-resource setting o mga
pagkakataong gipit sa datos. Sa mga nagdaang taon, nanaig ang mga
transformer-based TL techniques pagdating sa low-resource tasks ngunit ito ay
mataas na compute and memory requirements kaya nangangailangan ng mas mura pero
epektibong alternatibo. Ang papel na ito ay may tatlong kontribusyon. Una,
maglabas ng pre-trained AWD-LSTM language model sa wikang Filipino upang maging
tuntungan sa pagbuo ng mga NLP applications sa wikang Filipino. Pangalawa, mag
benchmark ng AWD-LSTM sa Hate Speech classification task at ipakita na kayang
nitong makipagsabayan sa mga transformer-based models. Pangatlo, suriin ang
performance ng AWD-LSTM sa low-resource setting gamit ang degradation test at
ikumpara ito sa mga transformer-based models.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 15:06:07 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 04:50:06 GMT'}]",2020-10-15,"[['Velasco', 'Dan John', '']]"
1363131,2010.06801,Ming Gong,"Xingyao Zhang, Linjun Shou, Jian Pei, Ming Gong, Lijie Wen, Daxin
  Jiang","A Graph Representation of Semi-structured Data for Web Question
  Answering",Accepted as long paper in COLING 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The abundant semi-structured data on the Web, such as HTML-based tables and
lists, provide commercial search engines a rich information source for question
answering (QA). Different from plain text passages in Web documents, Web tables
and lists have inherent structures, which carry semantic correlations among
various elements in tables and lists. Many existing studies treat tables and
lists as flat documents with pieces of text and do not make good use of
semantic information hidden in structures. In this paper, we propose a novel
graph representation of Web tables and lists based on a systematic
categorization of the components in semi-structured data as well as their
relations. We also develop pre-training and reasoning techniques on the graph
model for the QA task. Extensive experiments on several real datasets collected
from a commercial engine verify the effectiveness of our approach. Our method
improves F1 score by 3.90 points over the state-of-the-art baselines.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 04:01:54 GMT'}]",2020-10-15,"[['Zhang', 'Xingyao', ''], ['Shou', 'Linjun', ''], ['Pei', 'Jian', ''], ['Gong', 'Ming', ''], ['Wen', 'Lijie', ''], ['Jiang', 'Daxin', '']]"
1363054,2010.06724,Muhao Chen,"Muhao Chen, Hongming Zhang, Haoyu Wang, Dan Roth","""What Are You Trying to Do?"" Semantic Typing of Event Processes",CoNLL 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper studies a new cognitively motivated semantic typing task,
multi-axis event process typing, that, given an event process, attempts to
infer free-form type labels describing (i) the type of action made by the
process and (ii) the type of object the process seeks to affect. This task is
inspired by computational and cognitive studies of event understanding, which
suggest that understanding processes of events is often directed by recognizing
the goals, plans or intentions of the protagonist(s). We develop a large
dataset containing over 60k event processes, featuring ultra fine-grained
typing on both the action and object type axes with very large ($10^3\sim
10^4$) label vocabularies. We then propose a hybrid learning framework, P2GT,
which addresses the challenging typing problem with indirect supervision from
glosses1and a joint learning-to-rank framework. As our experiments indicate,
P2GT supports identifying the intent of processes, as well as the fine semantic
type of the affected object. It also demonstrates the capability of handling
few-shot cases, and strong generalizability on out-of-domain event processes.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 22:37:29 GMT'}]",2020-10-15,"[['Chen', 'Muhao', ''], ['Zhang', 'Hongming', ''], ['Wang', 'Haoyu', ''], ['Roth', 'Dan', '']]"
1363116,2010.06786,Fereshteh Jafariakinabad,"Fereshteh Jafariakinabad, Kien A. Hua","A Self-supervised Representation Learning of Sentence Structure for
  Authorship Attribution",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Syntactic structure of sentences in a document substantially informs about
its authorial writing style. Sentence representation learning has been widely
explored in recent years and it has been shown that it improves the
generalization of different downstream tasks across many domains. Even though
utilizing probing methods in several studies suggests that these learned
contextual representations implicitly encode some amount of syntax, explicit
syntactic information further improves the performance of deep neural models in
the domain of authorship attribution. These observations have motivated us to
investigate the explicit representation learning of syntactic structure of
sentences. In this paper, we propose a self-supervised framework for learning
structural representations of sentences. The self-supervised network contains
two components; a lexical sub-network and a syntactic sub-network which take
the sequence of words and their corresponding structural labels as the input,
respectively. Due to the n-to-1 mapping of words to their structural labels,
each word will be embedded into a vector representation which mainly carries
structural information. We evaluate the learned structural representations of
sentences using different probing tasks, and subsequently utilize them in the
authorship attribution task. Our experimental results indicate that the
structural embeddings significantly improve the classification tasks when
concatenated with the existing pre-trained word embeddings.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 02:57:10 GMT'}]",2020-10-15,"[['Jafariakinabad', 'Fereshteh', ''], ['Hua', 'Kien A.', '']]"
1363108,2010.06778,Alexander Gutkin,"Alena Butryna and Shan-Hui Cathy Chu and Isin Demirsahin and Alexander
  Gutkin and Linne Ha and Fei He and Martin Jansche and Cibu Johny and Anna
  Katanova and Oddur Kjartansson and Chenfang Li and Tatiana Merkulova and Yin
  May Oo and Knot Pipatsrisawat and Clara Rivera and Supheakmungkol Sarin and
  Pasindu de Silva and Keshan Sodimana and Richard Sproat and Theeraphol
  Wattanavekin and Jaka Aris Eko Wibawa","Google Crowdsourced Speech Corpora and Related Open-Source Resources for
  Low-Resource Languages and Dialects: An Overview","Appeared in 2019 UNESCO International Conference Language
  Technologies for All (LT4All): Enabling Linguistic Diversity and
  Multilingualism Worldwide, 4-6 December, Paris, France",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper presents an overview of a program designed to address the growing
need for developing freely available speech resources for under-represented
languages. At present we have released 38 datasets for building text-to-speech
and automatic speech recognition applications for languages and dialects of
South and Southeast Asia, Africa, Europe and South America. The paper describes
the methodology used for developing such corpora and presents some of our
findings that could benefit under-represented language communities.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 02:24:04 GMT'}]",2020-10-15,"[['Butryna', 'Alena', ''], ['Chu', 'Shan-Hui Cathy', ''], ['Demirsahin', 'Isin', ''], ['Gutkin', 'Alexander', ''], ['Ha', 'Linne', ''], ['He', 'Fei', ''], ['Jansche', 'Martin', ''], ['Johny', 'Cibu', ''], ['Katanova', 'Anna', ''], ['Kjartansson', 'Oddur', ''], ['Li', 'Chenfang', ''], ['Merkulova', 'Tatiana', ''], ['Oo', 'Yin May', ''], ['Pipatsrisawat', 'Knot', ''], ['Rivera', 'Clara', ''], ['Sarin', 'Supheakmungkol', ''], ['de Silva', 'Pasindu', ''], ['Sodimana', 'Keshan', ''], ['Sproat', 'Richard', ''], ['Wattanavekin', 'Theeraphol', ''], ['Wibawa', 'Jaka Aris Eko', '']]"
1363051,2010.06721,Steven Reich,"Steven Reich, David Mueller, Nicholas Andrews","Ensemble Distillation for Structured Prediction: Calibrated, Accurate,
  Fast---Choose Three",EMNLP 2020,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern neural networks do not always produce well-calibrated predictions,
even when trained with a proper scoring function such as cross-entropy. In
classification settings, simple methods such as isotonic regression or
temperature scaling may be used in conjunction with a held-out dataset to
calibrate model outputs. However, extending these methods to structured
prediction is not always straightforward or effective; furthermore, a held-out
calibration set may not always be available. In this paper, we study ensemble
distillation as a general framework for producing well-calibrated structured
prediction models while avoiding the prohibitive inference-time cost of
ensembles. We validate this framework on two tasks: named-entity recognition
and machine translation. We find that, across both tasks, ensemble distillation
produces models which retain much of, and occasionally improve upon, the
performance and calibration benefits of ensembles, while only requiring a
single model during test-time.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 22:30:06 GMT'}]",2020-10-15,"[['Reich', 'Steven', ''], ['Mueller', 'David', ''], ['Andrews', 'Nicholas', '']]"
1363187,2010.06857,Hatem Haddad,"Abir Messaoudi and Hatem Haddad and Moez Ben HajHmida and Chayma
  Fourati and Abderrazak Ben Hamida",Learning Word Representations for Tunisian Sentiment Analysis,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Tunisians on social media tend to express themselves in their local dialect
using Latin script (TUNIZI). This raises an additional challenge to the process
of exploring and recognizing online opinions. To date, very little work has
addressed TUNIZI sentiment analysis due to scarce resources for training an
automated system. In this paper, we focus on the Tunisian dialect sentiment
analysis used on social media. Most of the previous work used machine learning
techniques combined with handcrafted features. More recently, Deep Neural
Networks were widely used for this task, especially for the English language.
In this paper, we explore the importance of various unsupervised word
representations (word2vec, BERT) and we investigate the use of Convolutional
Neural Networks and Bidirectional Long Short-Term Memory. Without using any
kind of handcrafted features, our experimental results on two publicly
available datasets showed comparable performances to other languages.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 07:47:33 GMT'}]",2020-10-15,"[['Messaoudi', 'Abir', ''], ['Haddad', 'Hatem', ''], ['HajHmida', 'Moez Ben', ''], ['Fourati', 'Chayma', ''], ['Hamida', 'Abderrazak Ben', '']]"
1363236,2010.06906,Debanjana Kar,"Debanjana Kar, Mohit Bhardwaj, Suranjana Samanta, Amar Prakash Azad","No Rumours Please! A Multi-Indic-Lingual Approach for COVID Fake-Tweet
  Detection","6 pages, 4 figures",,,,cs.CL cs.LG cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The sudden widespread menace created by the present global pandemic COVID-19
has had an unprecedented effect on our lives. Man-kind is going through
humongous fear and dependence on social media like never before. Fear
inevitably leads to panic, speculations, and the spread of misinformation. Many
governments have taken measures to curb the spread of such misinformation for
public well being. Besides global measures, to have effective outreach, systems
for demographically local languages have an important role to play in this
effort. Towards this, we propose an approach to detect fake news about COVID-19
early on from social media, such as tweets, for multiple Indic-Languages
besides English. In addition, we also create an annotated dataset of Hindi and
Bengali tweet for fake news detection. We propose a BERT based model augmented
with additional relevant features extracted from Twitter to identify fake
tweets. To expand our approach to multiple Indic languages, we resort to mBERT
based model which is fine-tuned over created dataset in Hindi and Bengali. We
also propose a zero-shot learning approach to alleviate the data scarcity issue
for such low resource languages. Through rigorous experiments, we show that our
approach reaches around 89% F-Score in fake tweet detection which supercedes
the state-of-the-art (SOTA) results. Moreover, we establish the first benchmark
for two Indic-Languages, Hindi and Bengali. Using our annotated data, our model
achieves about 79% F-Score in Hindi and 81% F-Score for Bengali Tweets. Our
zero-shot model achieves about 81% F-Score in Hindi and 78% F-Score for Bengali
Tweets without any annotated data, which clearly indicates the efficacy of our
approach.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 09:37:51 GMT'}]",2020-10-15,"[['Kar', 'Debanjana', ''], ['Bhardwaj', 'Mohit', ''], ['Samanta', 'Suranjana', ''], ['Azad', 'Amar Prakash', '']]"
1363046,2010.06716,Oleg Vasilyev,"Oleg Vasilyev, Vedant Dharnidharka, Nicholas Egan, Charlene Chambliss,
  John Bohannon",Sensitivity of BLANC to human-scored qualities of text summaries,"6 pages, 3 figures, 2 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We explore the sensitivity of a document summary quality estimator, BLANC, to
human assessment of qualities for the same summaries. In our human evaluations,
we distinguish five summary qualities, defined by how fluent, understandable,
informative, compact, and factually correct the summary is. We make the case
for optimal BLANC parameters, at which the BLANC sensitivity to almost all of
summary qualities is about as good as the sensitivity of a human annotator.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 22:08:11 GMT'}]",2020-10-15,"[['Vasilyev', 'Oleg', ''], ['Dharnidharka', 'Vedant', ''], ['Egan', 'Nicholas', ''], ['Chambliss', 'Charlene', ''], ['Bohannon', 'John', '']]"
1363044,2010.06714,Jiaxin Huang,"Jiaxin Huang, Yiqing Xie, Yu Meng, Yunyi Zhang, Jiawei Han","CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and
  Relation Transferring",KDD 2020 Research Track,,10.1145/3394486.3403244,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Taxonomy is not only a fundamental form of knowledge representation, but also
crucial to vast knowledge-rich applications, such as question answering and web
search. Most existing taxonomy construction methods extract hypernym-hyponym
entity pairs to organize a ""universal"" taxonomy. However, these generic
taxonomies cannot satisfy user's specific interest in certain areas and
relations. Moreover, the nature of instance taxonomy treats each node as a
single word, which has low semantic coverage. In this paper, we propose a
method for seed-guided topical taxonomy construction, which takes a corpus and
a seed taxonomy described by concept names as input, and constructs a more
complete taxonomy based on user's interest, wherein each node is represented by
a cluster of coherent terms. Our framework, CoRel, has two modules to fulfill
this goal. A relation transferring module learns and transfers the user's
interested relation along multiple paths to expand the seed taxonomy structure
in width and depth. A concept learning module enriches the semantics of each
concept node by jointly embedding the taxonomy and text. Comprehensive
experiments conducted on real-world datasets show that Corel generates
high-quality topical taxonomies and outperforms all the baselines
significantly.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 22:00:31 GMT'}]",2020-10-15,"[['Huang', 'Jiaxin', ''], ['Xie', 'Yiqing', ''], ['Meng', 'Yu', ''], ['Zhang', 'Yunyi', ''], ['Han', 'Jiawei', '']]"
1363609,2010.07279,Khyathi Raghavi Chandu,Khyathi Raghavi Chandu and Alan W Black,Dissecting the components and factors of Neural Text Generation,15 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural text generation metamorphosed into several critical natural language
applications ranging from text completion to free form narrative generation.
Generating natural language has fundamentally been a human attribute and the
advent of ubiquitous NLP applications and virtual agents marks the need to
impart this skill to machines. There has been a colossal research effort in
various frontiers of neural text generation including machine translation,
summarization, image captioning, storytelling etc., We believe that this is an
excellent juncture to retrospect on the directions of the field. Specifically,
this paper surveys the fundamental factors and components relaying task
agnostic impacts across various generation tasks such as storytelling,
summarization, translation etc., In specific, we present an abstraction of the
imperative techniques with respect to learning paradigms, pretraining, modeling
approaches, decoding and the key challenges. Thereby, we hope to deliver a
one-stop destination for researchers in the field to facilitate a perspective
on where to situate their work and how it impacts other closely related tasks.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 17:54:42 GMT'}]",2020-10-15,"[['Chandu', 'Khyathi Raghavi', ''], ['Black', 'Alan W', '']]"
1363040,2010.06710,Diego Amancio,Jorge A. V. Tohalino and Diego R. Amancio,Language Networks: a Practical Approach,,,,,cs.CL cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This manuscript provides a short and practical introduction to the topic of
language networks. This text aims at assisting researchers with no practical
experience in text and/or network analysis. We provide a practical tutorial on
how to model and characterize texts using network-based features. In this
tutorial, we also include examples of pre-processing and network
representations. A brief description of the main tasks allying network science
and text analysis is also provided. A further development of this text shall
include a practical description of network classification via machine learning
methods.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 21:51:14 GMT'}]",2020-10-15,"[['Tohalino', 'Jorge A. V.', ''], ['Amancio', 'Diego R.', '']]"
1363575,2010.07245,Yu Meng,"Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao
  Zhang, Jiawei Han","Text Classification Using Label Names Only: A Language Model
  Self-Training Approach",EMNLP 2020. (Code: https://github.com/yumeng5/LOTClass),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current text classification methods typically require a good number of
human-labeled documents as training data, which can be costly and difficult to
obtain in real applications. Humans can perform classification without seeing
any labeled examples but only based on a small set of words describing the
categories to be classified. In this paper, we explore the potential of only
using the label name of each class to train classification models on unlabeled
data, without using any labeled documents. We use pre-trained neural language
models both as general linguistic knowledge sources for category understanding
and as representation learning models for document classification. Our method
(1) associates semantically related words with the label names, (2) finds
category-indicative words and trains the model to predict their implied
categories, and (3) generalizes the model via self-training. We show that our
model achieves around 90% accuracy on four benchmark datasets including topic
and sentiment classification without using any labeled documents but learning
from unlabeled data supervised by at most 3 words (1 in most cases) per class
as the label name.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 17:06:41 GMT'}]",2020-10-15,"[['Meng', 'Yu', ''], ['Zhang', 'Yunyi', ''], ['Huang', 'Jiaxin', ''], ['Xiong', 'Chenyan', ''], ['Ji', 'Heng', ''], ['Zhang', 'Chao', ''], ['Han', 'Jiawei', '']]"
1363542,2010.07212,Debajyoti Datta,"Debajyoti Datta, Shashwat Kumar, Laura Barnes, Tom Fletcher",Geometry matters: Exploring language examples at the decision boundary,Preprint: Under Review,,,,cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A growing body of recent evidence has highlighted the limitations of natural
language processing (NLP) datasets and classifiers. These include the presence
of annotation artifacts in datasets, classifiers relying on shallow features
like a single word (e.g., if a movie review has the word ""romantic"", the review
tends to be positive), or unnecessary words (e.g., learning a proper noun to
classify a movie as positive or negative). The presence of such artifacts has
subsequently led to the development of challenging datasets to force the model
to generalize better. While a variety of heuristic strategies, such as
counterfactual examples and contrast sets, have been proposed, the theoretical
justification about what makes these examples difficult is often lacking or
unclear. In this paper, using tools from information geometry, we propose a
theoretical way to quantify the difficulty of an example in NLP. Using our
approach, we explore difficult examples for two popular NLP architectures. We
discover that both BERT and CNN are susceptible to single word substitutions in
high difficulty examples. Consequently, examples with low difficulty scores
tend to be robust to multiple word substitutions. Our analysis shows that
perturbations like contrast sets and counterfactual examples are not
necessarily difficult for the model, and they may not be accomplishing the
intended goal. Our approach is simple, architecture agnostic, and easily
extendable to other datasets. All the code used will be made publicly
available, including a tool to explore the difficult examples for other
datasets.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 16:26:13 GMT'}]",2020-10-15,"[['Datta', 'Debajyoti', ''], ['Kumar', 'Shashwat', ''], ['Barnes', 'Laura', ''], ['Fletcher', 'Tom', '']]"
1363001,2010.06671,Pedram Hosseini,"Lily Li, Or Levi, Pedram Hosseini, David A. Broniatowski",A Multi-Modal Method for Satire Detection using Textual and Visual Cues,"Accepted to the Third Workshop on NLP for Internet Freedom (NLP4IF):
  Censorship, Disinformation, and Propaganda. Co-located with COLING 2020",,,,cs.CL cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Satire is a form of humorous critique, but it is sometimes misinterpreted by
readers as legitimate news, which can lead to harmful consequences. We observe
that the images used in satirical news articles often contain absurd or
ridiculous content and that image manipulation is used to create fictional
scenarios. While previous work have studied text-based methods, in this work we
propose a multi-modal approach based on state-of-the-art visiolinguistic model
ViLBERT. To this end, we create a new dataset consisting of images and
headlines of regular and satirical news for the task of satire detection. We
fine-tune ViLBERT on the dataset and train a convolutional neural network that
uses an image forensics technique. Evaluation on the dataset shows that our
proposed multi-modal approach outperforms image-only, text-only, and simple
fusion baselines.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 20:08:29 GMT'}]",2020-10-15,"[['Li', 'Lily', ''], ['Levi', 'Or', ''], ['Hosseini', 'Pedram', ''], ['Broniatowski', 'David A.', '']]"
1362996,2010.06666,Devin Johnson,"Devin Johnson, Denise Mak, Drew Barker, Lexi Loessberg-Zahl","Probing for Multilingual Numerical Understanding in Transformer-Based
  Language Models",BlackboxNLP (EMNLP 2020),,,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  Natural language numbers are an example of compositional structures, where
larger numbers are composed of operations on smaller numbers. Given that
compositional reasoning is a key to natural language understanding, we propose
novel multilingual probing tasks tested on DistilBERT, XLM, and BERT to
investigate for evidence of compositional reasoning over numerical data in
various natural language number systems. By using both grammaticality judgment
and value comparison classification tasks in English, Japanese, Danish, and
French, we find evidence that the information encoded in these pretrained
models' embeddings is sufficient for grammaticality judgments but generally not
for value comparisons. We analyze possible reasons for this and discuss how our
tasks could be extended in further studies.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 19:56:02 GMT'}]",2020-10-15,"[['Johnson', 'Devin', ''], ['Mak', 'Denise', ''], ['Barker', 'Drew', ''], ['Loessberg-Zahl', 'Lexi', '']]"
1363504,2010.07174,Benjamin Newman,"Benjamin Newman, John Hewitt, Percy Liang, Christopher D. Manning",The EOS Decision and Length Extrapolation,"16 page, 7 Figures, 9 Tables, Blackbox NLP Workshop at EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Extrapolation to unseen sequence lengths is a challenge for neural generative
models of language. In this work, we characterize the effect on length
extrapolation of a modeling decision often overlooked: predicting the end of
the generative process through the use of a special end-of-sequence (EOS)
vocabulary item. We study an oracle setting - forcing models to generate to the
correct sequence length at test time - to compare the length-extrapolative
behavior of networks trained to predict EOS (+EOS) with networks not trained to
(-EOS). We find that -EOS substantially outperforms +EOS, for example
extrapolating well to lengths 10 times longer than those seen at training time
in a bracket closing task, as well as achieving a 40% improvement over +EOS in
the difficult SCAN dataset length generalization task. By comparing the hidden
states and dynamics of -EOS and +EOS models, we observe that +EOS models fail
to generalize because they (1) unnecessarily stratify their hidden states by
their linear position is a sequence (structures we call length manifolds) or
(2) get stuck in clusters (which we refer to as length attractors) once the EOS
token is the highest-probability prediction.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 15:46:17 GMT'}]",2020-10-15,"[['Newman', 'Benjamin', ''], ['Hewitt', 'John', ''], ['Liang', 'Percy', ''], ['Manning', 'Christopher D.', '']]"
1362987,2010.06657,Hancheng Cao,"Hancheng Cao, Mengjie Cheng, Zhepeng Cen, Daniel A. McFarland, Xiang
  Ren","Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer
  of Scientific Concepts across Text Corpora",EMNLP 2020 Findings,,,,cs.CY cs.CL cs.DL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  What kind of basic research ideas are more likely to get applied in practice?
There is a long line of research investigating patterns of knowledge transfer,
but it generally focuses on documents as the unit of analysis and follow their
transfer into practice for a specific scientific domain. Here we study
translational research at the level of scientific concepts for all scientific
fields. We do this through text mining and predictive modeling using three
corpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28
million clinical trials. We extract scientific concepts (i.e., phrases) from
corpora as instantiations of ""research ideas"", create concept-level features as
motivated by literature, and then follow the trajectories of over 450,000 new
concepts (emerged from 1995-2014) to identify factors that lead only a small
proportion of these ideas to be used in inventions and drug trials. Results
from our analysis suggest several mechanisms that distinguish which scientific
concept will be adopted in practice, and which will not. We also demonstrate
that our derived features can be used to explain and predict knowledge transfer
with high accuracy. Our work provides greater understanding of knowledge
transfer for researchers, practitioners, and government agencies interested in
encouraging translational research.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 19:46:59 GMT'}]",2020-10-15,"[['Cao', 'Hancheng', ''], ['Cheng', 'Mengjie', ''], ['Cen', 'Zhepeng', ''], ['McFarland', 'Daniel A.', ''], ['Ren', 'Xiang', '']]"
1363460,2010.07130,Pradeep R,"Pradeep Rangan, Sundeep Teki, and Hemant Misra","Exploiting Spectral Augmentation for Code-Switched Spoken Language
  Identification","5 pages, 3 figures, Accepted for INTERSPEECH-2020 - ""First Workshop
  on Speech Technologies for Code-switching in Multilingual Communities 2020""",,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Spoken language Identification (LID) systems are needed to identify the
language(s) present in a given audio sample, and typically could be the first
step in many speech processing related tasks such as automatic speech
recognition (ASR). Automatic identification of the languages present in a
speech signal is not only scientifically interesting, but also of practical
importance in a multilingual country such as India. In many of the Indian
cities, when people interact with each other, as many as three languages may
get mixed. These may include the official language of that province, Hindi and
English (at times the languages of the neighboring provinces may also get mixed
during these interactions). This makes the spoken LID task extremely
challenging in Indian context. While quite a few LID systems in the context of
Indian languages have been implemented, most such systems have used small scale
speech data collected internally within an organization. In the current work,
we perform spoken LID on three Indian languages (Gujarati, Telugu, and Tamil)
code-mixed with English. This task was organized by the Microsoft research team
as a spoken LID challenge. In our work, we modify the usual spectral
augmentation approach and propose a language mask that discriminates the
language ID pairs, which leads to a noise robust spoken LID system. The
proposed method gives a relative improvement of approximately 3-5% in the LID
accuracy over a baseline system proposed by Microsoft on the three language
pairs for two shared tasks suggested in the challenge.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 14:37:03 GMT'}]",2020-10-15,"[['Rangan', 'Pradeep', ''], ['Teki', 'Sundeep', ''], ['Misra', 'Hemant', '']]"
1363439,2010.07109,Zihan Zhao,"Zihan Zhao, Yuncong Liu, Lu Chen, Qi Liu, Rao Ma and Kai Yu","An Investigation on Different Underlying Quantization Schemes for
  Pre-trained Language Models",Accepted to NLPCC 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, pre-trained language models like BERT have shown promising
performance on multiple natural language processing tasks. However, the
application of these models has been limited due to their huge size. To reduce
its size, a popular and efficient way is quantization. Nevertheless, most of
the works focusing on BERT quantization adapted primary linear clustering as
the quantization scheme, and few works try to upgrade it. That limits the
performance of quantization significantly. In this paper, we implement k-means
quantization and compare its performance on the fix-precision quantization of
BERT with linear quantization. Through the comparison, we verify that the
effect of the underlying quantization scheme upgrading is underestimated and
there is a huge development potential of k-means quantization. Besides, we also
compare the two quantization schemes on ALBERT models to explore the robustness
differences between different pre-trained models.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 14:05:06 GMT'}]",2020-10-15,"[['Zhao', 'Zihan', ''], ['Liu', 'Yuncong', ''], ['Chen', 'Lu', ''], ['Liu', 'Qi', ''], ['Ma', 'Rao', ''], ['Yu', 'Kai', '']]"
1362925,2010.06595,Dallas Card,"Dallas Card and Peter Henderson and Urvashi Khandelwal and Robin Jia
  and Kyle Mahowald and Dan Jurafsky",With Little Power Comes Great Responsibility,To appear at EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite its importance to experimental design, statistical power (the
probability that, given a real effect, an experiment will reject the null
hypothesis) has largely been ignored by the NLP community. Underpowered
experiments make it more difficult to discern the difference between
statistical noise and meaningful model improvements, and increase the chances
of exaggerated findings. By meta-analyzing a set of existing NLP papers and
datasets, we characterize typical power for a variety of settings and conclude
that underpowered experiments are common in the NLP literature. In particular,
for several tasks in the popular GLUE benchmark, small test sets mean that most
attempted comparisons to state of the art models will not be adequately
powered. Similarly, based on reasonable assumptions, we find that the most
typical experimental design for human rating studies will be underpowered to
detect small model differences, of the sort that are frequently studied. For
machine translation, we find that typical test sets of 2000 sentences have
approximately 75% power to detect differences of 1 BLEU point. To improve the
situation going forward, we give an overview of best practices for power
analysis in NLP and release a series of notebooks to assist with future power
analyses.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 18:00:02 GMT'}]",2020-10-15,"[['Card', 'Dallas', ''], ['Henderson', 'Peter', ''], ['Khandelwal', 'Urvashi', ''], ['Jia', 'Robin', ''], ['Mahowald', 'Kyle', ''], ['Jurafsky', 'Dan', '']]"
1363431,2010.07101,Xu Zhao,"Xu Zhao, Zihao Wang, Hao Wu, Yong Zhang",Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction,"12 pages, 2 figures, 6 tables, accepted as long paper by EMNLP2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semi-supervision is a promising paradigm for Bilingual Lexicon Induction
(BLI) with limited annotations. However, previous semisupervised methods do not
fully utilize the knowledge hidden in annotated and nonannotated data, which
hinders further improvement of their performance. In this paper, we propose a
new semi-supervised BLI framework to encourage the interaction between the
supervised signal and unsupervised alignment. We design two message-passing
mechanisms to transfer knowledge between annotated and non-annotated data,
named prior optimal transport and bi-directional lexicon update respectively.
Then, we perform semi-supervised learning based on a cyclic or a parallel
parameter feeding routine to update our models. Our framework is a general
framework that can incorporate any supervised and unsupervised BLI methods
based on optimal transport. Experimental results on MUSE and VecMap datasets
show significant improvement of our models. Ablation study also proves that the
two-way interaction between the supervised signal and unsupervised alignment
accounts for the gain of the overall performance. Results on distant language
pairs further illustrate the advantage and robustness of our proposed method.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 13:59:07 GMT'}]",2020-10-15,"[['Zhao', 'Xu', ''], ['Wang', 'Zihao', ''], ['Wu', 'Hao', ''], ['Zhang', 'Yong', '']]"
1363430,2010.07100,Manik Bhandari,"Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu and Graham
  Neubig",Re-evaluating Evaluation in Text Summarization,Accepted at EMNLP 2020,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated evaluation metrics as a stand-in for manual evaluation are an
essential part of the development of text-generation tasks such as text
summarization. However, while the field has progressed, our standard metrics
have not -- for nearly 20 years ROUGE has been the standard evaluation in most
summarization papers. In this paper, we make an attempt to re-evaluate the
evaluation method for text summarization: assessing the reliability of
automatic metrics using top-scoring system outputs, both abstractive and
extractive, on recently popular datasets for both system-level and
summary-level evaluation settings. We find that conclusions about evaluation
metrics on older datasets do not necessarily hold on modern datasets and
systems.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 13:58:53 GMT'}]",2020-10-15,"[['Bhandari', 'Manik', ''], ['Gour', 'Pranav', ''], ['Ashfaq', 'Atabak', ''], ['Liu', 'Pengfei', ''], ['Neubig', 'Graham', '']]"
1363425,2010.07095,Xu Zhao,"Xu Zhao, Zihao Wang, Hao Wu, Yong Zhang",A Relaxed Matching Procedure for Unsupervised BLI,"6 pages,1 figures, accepted as short paper by ACL2020",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently unsupervised Bilingual Lexicon Induction (BLI) without any parallel
corpus has attracted much research interest. One of the crucial parts in
methods for the BLI task is the matching procedure. Previous works impose a too
strong constraint on the matching and lead to many counterintuitive translation
pairings. Thus, We propose a relaxed matching procedure to find a more precise
matching between two languages. We also find that aligning source and target
language embedding space bidirectionally will bring significant improvement. We
follow the previous iterative framework to conduct experiments. Results on
standard benchmark demonstrate the effectiveness of our proposed method, which
substantially outperforms previous unsupervised methods.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 13:53:08 GMT'}]",2020-10-15,"[['Zhao', 'Xu', ''], ['Wang', 'Zihao', ''], ['Wu', 'Hao', ''], ['Zhang', 'Yong', '']]"
1363405,2010.07075,Yujing Wang,"Yiren Chen, Yaming Yang, Hong Sun, Yujing Wang, Yu Xu, Wei Shen, Rong
  Zhou, Yunhai Tong, Jing Bai, Ruofei Zhang",AutoADR: Automatic Model Design for Ad Relevance,CIKM 2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale pre-trained models have attracted extensive attention in the
research community and shown promising results on various tasks of natural
language processing. However, these pre-trained models are memory and
computation intensive, hindering their deployment into industrial online
systems like Ad Relevance. Meanwhile, how to design an effective yet efficient
model architecture is another challenging problem in online Ad Relevance.
Recently, AutoML shed new lights on architecture design, but how to integrate
it with pre-trained language models remains unsettled. In this paper, we
propose AutoADR (Automatic model design for AD Relevance) -- a novel end-to-end
framework to address this challenge, and share our experience to ship these
cutting-edge techniques into online Ad Relevance system at Microsoft Bing.
Specifically, AutoADR leverages a one-shot neural architecture search algorithm
to find a tailored network architecture for Ad Relevance. The search process is
simultaneously guided by knowledge distillation from a large pre-trained
teacher model (e.g. BERT), while taking the online serving constraints (e.g.
memory and latency) into consideration. We add the model designed by AutoADR as
a sub-model into the production Ad Relevance model. This additional sub-model
improves the Precision-Recall AUC (PR AUC) on top of the original Ad Relevance
model by 2.65X of the normalized shipping bar. More importantly, adding this
automatically designed sub-model leads to a statistically significant 4.6%
Bad-Ad ratio reduction in online A/B testing. This model has been shipped into
Microsoft Bing Ad Relevance Production model.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 13:24:43 GMT'}]",2020-10-15,"[['Chen', 'Yiren', ''], ['Yang', 'Yaming', ''], ['Sun', 'Hong', ''], ['Wang', 'Yujing', ''], ['Xu', 'Yu', ''], ['Shen', 'Wei', ''], ['Zhou', 'Rong', ''], ['Tong', 'Yunhai', ''], ['Bai', 'Jing', ''], ['Zhang', 'Ruofei', '']]"
1363404,2010.07074,Jiwei Li,"Xiaofei Sun, Chun Fan, Zijun Sun, Yuxian Meng, Fei Wu and Jiwei Li","Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical
  Supervision from Extractive Summaries",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Long-text generation remains a challenge. The difficulty of generating
coherent long texts lies in the fact that existing models overwhelmingly focus
on the tasks of local word prediction, and cannot make high level plans on what
to generate or capture the high-level discourse dependencies between chunks of
texts. Inspired by how humans write, where a list of bullet points or a catalog
is first outlined, and then each bullet point is expanded to form the whole
article, we propose {\it SOE}, a pipelined system that involves of summarizing,
outlining and elaborating for long text generation: the model first outlines
the summaries for different segments of long texts, and then elaborates on each
bullet point to generate the corresponding segment. To avoid the
labor-intensive process of summary soliciting, we propose the {\it
reconstruction} strategy, which extracts segment summaries in an unsupervised
manner by selecting its most informative part to reconstruct the segment.The
proposed generation system comes with the following merits: (1) the summary
provides high-level guidances for text generation and avoids the local minimum
of individual word predictions; (2) the high-level discourse dependencies are
captured in the conditional dependencies between summaries and are preserved
during the summary expansion process and (3) additionally, we are able to
consider significantly more contexts by representing contexts as concise
summaries. Extensive experiments demonstrate that SOE produces long texts with
significantly better quality, along with faster convergence speed.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 13:22:20 GMT'}]",2020-10-15,"[['Sun', 'Xiaofei', ''], ['Fan', 'Chun', ''], ['Sun', 'Zijun', ''], ['Meng', 'Yuxian', ''], ['Wu', 'Fei', ''], ['Li', 'Jiwei', '']]"
1363378,2010.07048,Jipeng Qiang,"Jipeng Qiang and Xinyu Lu and Yun Li and Yunhao Yuan and Yang Shi and
  Xindong Wu",Chinese Lexical Simplification,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lexical simplification has attracted much attention in many languages, which
is the process of replacing complex words in a given sentence with simpler
alternatives of equivalent meaning. Although the richness of vocabulary in
Chinese makes the text very difficult to read for children and non-native
speakers, there is no research work for Chinese lexical simplification (CLS)
task. To circumvent difficulties in acquiring annotations, we manually create
the first benchmark dataset for CLS, which can be used for evaluating the
lexical simplification systems automatically. In order to acquire more thorough
comparison, we present five different types of methods as baselines to generate
substitute candidates for the complex word that include synonym-based approach,
word embedding-based approach, pretrained language model-based approach,
sememe-based approach, and a hybrid approach. Finally, we design the
experimental evaluation of these baselines and discuss their advantages and
disadvantages. To our best knowledge, this is the first study for CLS task.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 12:55:36 GMT'}]",2020-10-15,"[['Qiang', 'Jipeng', ''], ['Lu', 'Xinyu', ''], ['Li', 'Yun', ''], ['Yuan', 'Yunhao', ''], ['Shi', 'Yang', ''], ['Wu', 'Xindong', '']]"
1363347,2010.07017,Christopher Wild,"Wesley Burr, Fanny Chevalier, Christopher Collins, Alison L Gibbs,
  Raymond Ng, Chris Wild",Computational Skills by Stealth in Secondary School Data Science,"38 pages, 8 figures",,,,cs.CY cs.CL stat.OT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The unprecedented growth in the availability of data of all types and
qualities and the emergence of the field of data science has provided an
impetus to finally realizing the implementation of the full breadth of the
Nolan and Temple Lang proposed integration of computing concepts into
statistics curricula at all levels in statistics and new data science programs
and courses. Moreover, data science, implemented carefully, opens accessible
pathways to stem for students for whom neither mathematics nor computer science
are natural affinities, and who would traditionally be excluded. We discuss a
proposal for the stealth development of computational skills in students' first
exposure to data science through careful, scaffolded exposure to computation
and its power. The intent of this approach is to support students, regardless
of interest and self-efficacy in coding, in becoming data-driven learners, who
are capable of asking complex questions about the world around them, and then
answering those questions through the use of data-driven inquiry. This
discussion is presented in the context of the International Data Science in
Schools Project which recently published computer science and statistics
consensus curriculum frameworks for a two-year secondary school data science
program, designed to make data science accessible to all.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 09:11:51 GMT'}]",2020-10-15,"[['Burr', 'Wesley', ''], ['Chevalier', 'Fanny', ''], ['Collins', 'Christopher', ''], ['Gibbs', 'Alison L', ''], ['Ng', 'Raymond', ''], ['Wild', 'Chris', '']]"
1363333,2010.07003,Gyuwan Kim,Gyuwan Kim and Kyunghyun Cho,"Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime
  with Search","11 pages, 4 figures",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although transformers have achieved impressive accuracies in various tasks in
natural language processing, they often come with a prohibitive computational
cost, that prevents their use in scenarios with limited computational resources
for inference. This need for computational efficiency in inference has been
addressed by for instance PoWER-BERT (Goyal et al., 2020) which gradually
decreases the length of a sequence as it is passed through layers. These
approaches however often assume that the target computational complexity is
known in advance at the time of training. This implies that a separate model
must be trained for each inference scenario with its distinct computational
budget. In this paper, we extend PoWER-BERT to address this issue of
inefficiency and redundancy. The proposed extension enables us to train a
large-scale transformer, called Length-Adaptive Transformer, once and uses it
for various inference scenarios without re-training it. To do so, we train a
transformer with LengthDrop, a structural variant of dropout, which
stochastically determines the length of a sequence at each layer. We then use a
multi-objective evolutionary search to find a length configuration that
maximizes the accuracy and minimizes the computational complexity under any
given computational budget. Additionally, we significantly extend the
applicability of PoWER-BERT beyond sequence-level classification into
token-level classification such as span-based question-answering, by
introducing the idea of Drop-and-Restore. With Drop-and-Restore, word-vectors
are dropped temporarily in intermediate layers and restored at the last layer
if necessary. We empirically verify the utility of the proposed approach by
demonstrating the superior accuracy-efficiency trade-off under various setups,
including SQuAD 1.1, MNLI-m, and SST-2. Code is available at
https://github.com/clovaai/length-adaptive-transformer.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 12:28:08 GMT'}]",2020-10-15,"[['Kim', 'Gyuwan', ''], ['Cho', 'Kyunghyun', '']]"
1363323,2010.06993,Pavel Kalaidin,Artem Chumachenko and Daniil Gavrilov and Pavel Kalaidin,Weight Squeezing: Reparameterization for Compression and Fast Inference,,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we present a novel approach for simultaneous knowledge transfer
and model compression called Weight Squeezing. With this method, we perform
knowledge transfer from a pre-trained teacher model by learning the mapping
from its weights to smaller student model weights, without significant loss of
model accuracy.
  We applied Weight Squeezing combined with Knowledge Distillation to a
pre-trained text classification model, and compared it to various knowledge
transfer and model compression methods on several downstream text
classification tasks. We observed that our approach produces better results
than Knowledge Distillation methods without any loss in inference speed. We
also compared Weight Squeezing with Low Rank Factorization methods and observed
that our method is significantly faster at inference while being competitive in
terms of accuracy.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 12:13:28 GMT'}]",2020-10-15,"[['Chumachenko', 'Artem', ''], ['Gavrilov', 'Daniil', ''], ['Kalaidin', 'Pavel', '']]"
1363305,2010.06975,Shaoxiong Ji,Shaoxiong Ji and Shirui Pan and Pekka Marttinen,Medical Code Assignment with Gated Convolution and Note-Code Interaction,,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Medical code assignment from clinical text is a fundamental task in clinical
information system management. As medical notes are typically lengthy and the
medical coding system's code space is large, this task is a long-standing
challenge. Recent work applies deep neural network models to encode the medical
notes and assign medical codes to clinical documents. However, these methods
are still ineffective as they do not fully encode and capture the lengthy and
rich semantic information of medical notes nor explicitly exploit the
interactions between the notes and codes. We propose a novel method, gated
convolutional neural networks, and a note-code interaction (GatedCNN-NCI), for
automatic medical code assignment to overcome these challenges. Our methods
capture the rich semantic information of the lengthy clinical text for better
representation by utilizing embedding injection and gated information
propagation in the medical note encoding module. With a novel note-code
interaction design and a graph message passing mechanism, we explicitly capture
the underlying dependency between notes and codes, enabling effective code
prediction. A weight sharing scheme is further designed to decrease the number
of trainable parameters. Empirical experiments on real-world clinical datasets
show that our proposed model outperforms state-of-the-art models in most cases,
and our model size is on par with light-weighted baselines.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 11:37:24 GMT'}]",2020-10-15,"[['Ji', 'Shaoxiong', ''], ['Pan', 'Shirui', ''], ['Marttinen', 'Pekka', '']]"
1363303,2010.06973,James Thorne,"James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,
  Sebastian Riedel, Alon Halevy",Neural Databases,Submitted to PVLDB vol 14,,,,cs.CL cs.DB cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, neural networks have shown impressive performance gains on
long-standing AI problems, and in particular, answering queries from natural
language text. These advances raise the question of whether they can be
extended to a point where we can relax the fundamental assumption of database
management, namely, that our data is represented as fields of a pre-defined
schema.
  This paper presents a first step in answering that question. We describe
NeuralDB, a database system with no pre-defined schema, in which updates and
queries are given in natural language. We develop query processing techniques
that build on the primitives offered by the state of the art Natural Language
Processing methods.
  We begin by demonstrating that at the core, recent NLP transformers, powered
by pre-trained language models, can answer select-project-join queries if they
are given the exact set of relevant facts. However, they cannot scale to
non-trivial databases and cannot perform aggregation queries. Based on these
findings, we describe a NeuralDB architecture that runs multiple Neural SPJ
operators in parallel, each with a set of database sentences that can produce
one of the answers to the query. The result of these operators is fed to an
aggregation operator if needed. We describe an algorithm that learns how to
create the appropriate sets of facts to be fed into each of the Neural SPJ
operators. Importantly, this algorithm can be trained by the Neural SPJ
operator itself. We experimentally validate the accuracy of NeuralDB and its
components, showing that we can answer queries over thousands of sentences with
very high accuracy.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 11:31:53 GMT'}]",2020-10-15,"[['Thorne', 'James', ''], ['Yazdani', 'Majid', ''], ['Saeidi', 'Marzieh', ''], ['Silvestri', 'Fabrizio', ''], ['Riedel', 'Sebastian', ''], ['Halevy', 'Alon', '']]"
1363273,2010.06943,Jiwei Li,"Yuxian Meng, Chun Fan, Zijun Sun, Eduard Hovy, Fei Wu and Jiwei Li","Pair the Dots: Jointly Examining Training History and Test Stimuli for
  Model Interpretability",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Any prediction from a model is made by a combination of learning history and
test stimuli. This provides significant insights for improving model
interpretability: {\it because of which part(s) of which training example(s),
the model attends to which part(s) of a test example}. Unfortunately, existing
methods to interpret a model's predictions are only able to capture a single
aspect of either test stimuli or learning history, and evidences from both are
never combined or integrated. In this paper, we propose an efficient and
differentiable approach to make it feasible to interpret a model's prediction
by jointly examining training history and test stimuli. Test stimuli is first
identified by gradient-based methods, signifying {\it the part of a test
example that the model attends to}. The gradient-based saliency scores are then
propagated to training examples using influence functions to identify {\it
which part(s) of which training example(s)} make the model attends to the test
stimuli. The system is differentiable and time efficient: the adoption of
saliency scores from gradient-based methods allows us to efficiently trace a
model's prediction through test stimuli, and then back to training examples
through influence functions. We demonstrate that the proposed methodology
offers clear explanations about neural model decisions, along with being useful
for performing error analysis, crafting adversarial examples and fixing
erroneously classified examples.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 10:45:01 GMT'}]",2020-10-15,"[['Meng', 'Yuxian', ''], ['Fan', 'Chun', ''], ['Sun', 'Zijun', ''], ['Hovy', 'Eduard', ''], ['Wu', 'Fei', ''], ['Li', 'Jiwei', '']]"
1363255,2010.06925,Chuhan Wu,"Chuhan Wu, Fangzhao Wu, Yongfeng Huang",DA-Transformer: Distance-aware Transformer,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer has achieved great success in the NLP field by composing various
advanced models like BERT and GPT. However, Transformer and its existing
variants may not be optimal in capturing token distances because the position
or distance embeddings used by these methods usually cannot keep the precise
information of real distances, which may not be beneficial for modeling the
orders and relations of contexts. In this paper, we propose DA-Transformer,
which is a distance-aware Transformer that can exploit the real distance. We
propose to incorporate the real distances between tokens to re-scale the raw
self-attention weights, which are computed by the relevance between attention
query and key. Concretely, in different self-attention heads the relative
distance between each pair of tokens is weighted by different learnable
parameters, which control the different preferences on long- or short-term
information of these heads. Since the raw weighted real distances may not be
optimal for adjusting self-attention weights, we propose a learnable sigmoid
function to map them into re-scaled coefficients that have proper ranges. We
first clip the raw self-attention weights via the ReLU function to keep
non-negativity and introduce sparsity, and then multiply them with the
re-scaled coefficients to encode real distance information into self-attention.
Extensive experiments on five benchmark datasets show that DA-Transformer can
effectively improve the performance of many tasks and outperform the vanilla
Transformer and its several variants.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 10:09:01 GMT'}]",2020-10-15,"[['Wu', 'Chuhan', ''], ['Wu', 'Fangzhao', ''], ['Huang', 'Yongfeng', '']]"
1362879,2010.06549,Ghazi Felhi,"Ghazi Felhi, Joseph Leroux, Djam\'e Seddah","Controlling the Interaction Between Generation and Inference in
  Semi-Supervised Variational Autoencoders Using Importance Weighting",,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Even though Variational Autoencoders (VAEs) are widely used for
semi-supervised learning, the reason why they work remains unclear. In fact,
the addition of the unsupervised objective is most often vaguely described as a
regularization. The strength of this regularization is controlled by
down-weighting the objective on the unlabeled part of the training set. Through
an analysis of the objective of semi-supervised VAEs, we observe that they use
the posterior of the learned generative model to guide the inference model in
learning the partially observed latent variable. We show that given this
observation, it is possible to gain finer control on the effect of the
unsupervised objective on the training procedure. Using importance weighting,
we derive two novel objectives that prioritize either one of the partially
observed latent variable, or the unobserved latent variable. Experiments on the
IMDB english sentiment analysis dataset and on the AG News topic classification
dataset show the improvements brought by our prioritization mechanism and
exhibit a behavior that is inline with our description of the inner working of
Semi-Supervised VAEs.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 17:01:40 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 13:04:47 GMT'}]",2020-10-15,"[['Felhi', 'Ghazi', ''], ['Leroux', 'Joseph', ''], ['Seddah', 'Djamé', '']]"
1363035,2010.06705,Jiaxin Huang,"Jiaxin Huang, Yu Meng, Fang Guo, Heng Ji, Jiawei Han","Weakly-Supervised Aspect-Based Sentiment Analysis via Joint
  Aspect-Sentiment Topic Embedding",accepted to EMNLP 2020,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect-based sentiment analysis of review texts is of great value for
understanding user feedback in a fine-grained manner. It has in general two
sub-tasks: (i) extracting aspects from each review, and (ii) classifying
aspect-based reviews by sentiment polarity. In this paper, we propose a
weakly-supervised approach for aspect-based sentiment analysis, which uses only
a few keywords describing each aspect/sentiment without using any labeled
examples. Existing methods are either designed only for one of the sub-tasks,
neglecting the benefit of coupling both, or are based on topic models that may
contain overlapping concepts. We propose to first learn <sentiment, aspect>
joint topic embeddings in the word embedding space by imposing regularizations
to encourage topic distinctiveness, and then use neural models to generalize
the word-level discriminative information by pre-training the classifiers with
embedding-based predictions and self-training them on unlabeled data. Our
comprehensive performance analysis shows that our method generates quality
joint topics and outperforms the baselines significantly (7.4% and 5.1%
F1-score gain on average for aspect and sentiment classification respectively)
on benchmark datasets. Our code and data are available at
https://github.com/teapot123/JASen.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 21:33:24 GMT'}]",2020-10-15,"[['Huang', 'Jiaxin', ''], ['Meng', 'Yu', ''], ['Guo', 'Fang', ''], ['Ji', 'Heng', ''], ['Han', 'Jiawei', '']]"
1363057,2010.06727,Muhao Chen,"Haoyu Wang, Muhao Chen, Hongming Zhang, Dan Roth",Joint Constrained Learning for Event-Event Relation Extraction,EMNLP 2020,,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding natural language involves recognizing how multiple event
mentions structurally and temporally interact with each other. In this process,
one can induce event complexes that organize multi-granular events with
temporal order and membership relations interweaving among them. Due to the
lack of jointly labeled data for these relational phenomena and the restriction
on the structures they articulate, we propose a joint constrained learning
framework for modeling event-event relations. Specifically, the framework
enforces logical constraints within and across multiple temporal and subevent
relations by converting these constraints into differentiable learning
objectives. We show that our joint constrained learning approach effectively
compensates for the lack of jointly labeled data, and outperforms SOTA methods
on benchmarks for both temporal relation extraction and event hierarchy
construction, replacing a commonly used but more expensive global inference
process. We also present a promising case study showing the effectiveness of
our approach in inducing event complexes on an external corpus.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 22:45:28 GMT'}]",2020-10-15,"[['Wang', 'Haoyu', ''], ['Chen', 'Muhao', ''], ['Zhang', 'Hongming', ''], ['Roth', 'Dan', '']]"
1106724,1904.01684,Jekaterina Novikova Dr.,"Aparna Balagopalan, Ksenia Shkaruta, Jekaterina Novikova","Impact of ASR on Alzheimer's Disease Detection: All Errors are Equal,
  but Deletions are More Equal than Others",EMNLP Workshop on Noisy User-generated Text (W-NUT 2020),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic Speech Recognition (ASR) is a critical component of any
fully-automated speech-based dementia detection model. However, despite years
of speech recognition research, little is known about the impact of ASR
accuracy on dementia detection. In this paper, we experiment with controlled
amounts of artificially generated ASR errors and investigate their influence on
dementia detection. We find that deletion errors affect detection performance
the most, due to their impact on the features of syntactic complexity and
discourse representation in speech. We show the trend to be generalisable
across two different datasets for cognitive impairment detection. As a
conclusion, we propose optimising the ASR to reflect a higher penalty for
deletion errors in order to improve dementia detection performance.
","[{'version': 'v1', 'created': 'Tue, 2 Apr 2019 21:59:35 GMT'}, {'version': 'v2', 'created': 'Mon, 8 Apr 2019 00:44:41 GMT'}, {'version': 'v3', 'created': 'Tue, 13 Oct 2020 18:08:35 GMT'}]",2020-10-15,"[['Balagopalan', 'Aparna', ''], ['Shkaruta', 'Ksenia', ''], ['Novikova', 'Jekaterina', '']]"
1358659,2010.02329,Boxin Wang,"Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li,
  Jingjing Liu","InfoBERT: Improving Robustness of Language Models from An Information
  Theoretic Perspective","20 pages, 8 tables, 2 figures",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale language models such as BERT have achieved state-of-the-art
performance across a wide range of NLP tasks. Recent studies, however, show
that such BERT-based models are vulnerable facing the threats of textual
adversarial attacks. We aim to address this problem from an
information-theoretic perspective, and propose InfoBERT, a novel learning
framework for robust fine-tuning of pre-trained language models. InfoBERT
contains two mutual-information-based regularizers for model training: (i) an
Information Bottleneck regularizer, which suppresses noisy mutual information
between the input and the feature representation; and (ii) a Robust Feature
regularizer, which increases the mutual information between local robust
features and global features. We provide a principled way to theoretically
analyze and improve the robustness of representation learning for language
models in both standard and adversarial training. Extensive experiments
demonstrate that InfoBERT achieves state-of-the-art robust accuracy over
several adversarial datasets on Natural Language Inference (NLI) and Question
Answering (QA) tasks.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 20:49:26 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 13:24:03 GMT'}]",2020-10-15,"[['Wang', 'Boxin', ''], ['Wang', 'Shuohang', ''], ['Cheng', 'Yu', ''], ['Gan', 'Zhe', ''], ['Jia', 'Ruoxi', ''], ['Li', 'Bo', ''], ['Liu', 'Jingjing', '']]"
1359414,2010.03084,Xiaoyu Yang,"Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang Chen, Xiaodan Zhu","Program Enhanced Fact Verification with Verbalization and Graph
  Attention Network",16 pages (EMNLP 2019),,,,cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Performing fact verification based on structured data is important for many
real-life applications and is a challenging research problem, particularly when
it involves both symbolic operations and informal inference based on language
understanding. In this paper, we present a Program-enhanced Verbalization and
Graph Attention Network (ProgVGAT) to integrate programs and execution into
textual inference models. Specifically, a verbalization with program execution
model is proposed to accumulate evidences that are embedded in operations over
the tables. Built on that, we construct the graph attention verification
networks, which are designed to fuse different sources of evidences from
verbalized program execution, program structures, and the original statements
and tables, to make the final verification decision. To support the above
framework, we propose a program selection module optimized with a new training
strategy based on margin loss, to produce more accurate programs, which is
shown to be effective in enhancing the final verification results. Experimental
results show that the proposed framework achieves the new state-of-the-art
performance, a 74.4% accuracy, on the benchmark dataset TABFACT.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 23:29:08 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 16:43:38 GMT'}, {'version': 'v3', 'created': 'Wed, 14 Oct 2020 00:49:15 GMT'}]",2020-10-15,"[['Yang', 'Xiaoyu', ''], ['Nie', 'Feng', ''], ['Feng', 'Yufei', ''], ['Liu', 'Quan', ''], ['Chen', 'Zhigang', ''], ['Zhu', 'Xiaodan', '']]"
1358759,2010.02429,Heeyoung Kwon,"Heeyoung Kwon, Mahnaz Koupaee, Pratyush Singh, Gargi Sawhney, Anmol
  Shukla, Keerthi Kumar Kallur, Nathanael Chambers and Niranjan Balasubramanian",Modeling Preconditions in Text with a Crowd-sourced Dataset,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Preconditions provide a form of logical connection between events that
explains why some events occur together and information that is complementary
to the more widely studied relations such as causation, temporal ordering,
entailment, and discourse relations. Modeling preconditions in text has been
hampered in part due to the lack of large scale labeled data grounded in text.
This paper introduces PeKo, a crowd-sourced annotation of preconditions between
event pairs in newswire, an order of magnitude larger than prior text
annotations. To complement this new corpus, we also introduce two challenge
tasks aimed at modeling preconditions: (i) Precondition Identification -- a
standard classification task defined over pairs of event mentions, and (ii)
Precondition Generation -- a generative task aimed at testing a more general
ability to reason about a given event. Evaluation on both tasks shows that
modeling preconditions is challenging even for today's large language models
(LM). This suggests that precondition knowledge is not easily accessible in
LM-derived representations alone. Our generation results show that fine-tuning
an LM on PeKo yields better conditional relations than when trained on raw text
or temporally-ordered corpora.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 01:52:34 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Oct 2020 02:20:14 GMT'}, {'version': 'v3', 'created': 'Wed, 14 Oct 2020 17:56:03 GMT'}]",2020-10-15,"[['Kwon', 'Heeyoung', ''], ['Koupaee', 'Mahnaz', ''], ['Singh', 'Pratyush', ''], ['Sawhney', 'Gargi', ''], ['Shukla', 'Anmol', ''], ['Kallur', 'Keerthi Kumar', ''], ['Chambers', 'Nathanael', ''], ['Balasubramanian', 'Niranjan', '']]"
1280601,2005.00806,Qinyuan Ye,"Qinyuan Ye, Xiao Huang, Elizabeth Boschee, Xiang Ren",Teaching Machine Comprehension with Compositional Explanations,"Accepted to EMNLP 2020 Findings. Camera-ready version. Project page:
  http://inklab.usc.edu/mrc-explanation-project/",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Advances in machine reading comprehension (MRC) rely heavily on the
collection of large scale human-annotated examples in the form of (question,
paragraph, answer) triples. In contrast, humans are typically able to
generalize with only a few examples, relying on deeper underlying world
knowledge, linguistic sophistication, and/or simply superior deductive powers.
In this paper, we focus on ""teaching"" machines reading comprehension, using a
small number of semi-structured explanations that explicitly inform machines
why answer spans are correct. We extract structured variables and rules from
explanations and compose neural module teachers that annotate instances for
training downstream MRC models. We use learnable neural modules and soft logic
to handle linguistic variation and overcome sparse coverage; the modules are
jointly optimized with the MRC model to improve final performance. On the SQuAD
dataset, our proposed method achieves 70.14% F1 score with supervision from 26
explanations, comparable to plain supervised learning using 1,100 labeled
instances, yielding a 12x speed up.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 11:54:34 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Sep 2020 20:13:10 GMT'}, {'version': 'v3', 'created': 'Tue, 13 Oct 2020 19:28:50 GMT'}]",2020-10-15,"[['Ye', 'Qinyuan', ''], ['Huang', 'Xiao', ''], ['Boschee', 'Elizabeth', ''], ['Ren', 'Xiang', '']]"
1293058,2005.13263,Tanner Bohn,"Tanner Bohn, Charles X. Ling",Catching Attention with Automatic Pull Quote Selection,"Accepted to COLING-2020. 15 pages (~9 for content + refs + appendix),
  6 figures, 5 tables (+ 5 appendix tables)",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To advance understanding on how to engage readers, we advocate the novel task
of automatic pull quote selection. Pull quotes are a component of articles
specifically designed to catch the attention of readers with spans of text
selected from the article and given more salient presentation. This task
differs from related tasks such as summarization and clickbait identification
by several aspects. We establish a spectrum of baseline approaches to the task,
ranging from handcrafted features to a neural mixture-of-experts to cross-task
models. By examining the contributions of individual features and embedding
dimensions from these models, we uncover unexpected properties of pull quotes
to help answer the important question of what engages readers. Human evaluation
also supports the uniqueness of this task and the suitability of our selection
models. The benefits of exploring this problem further are clear: pull quotes
increase enjoyment and readability, shape reader perceptions, and facilitate
learning. Code to reproduce this work is available at
https://github.com/tannerbohn/AutomaticPullQuoteSelection.
","[{'version': 'v1', 'created': 'Wed, 27 May 2020 09:59:34 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 02:49:35 GMT'}]",2020-10-15,"[['Bohn', 'Tanner', ''], ['Ling', 'Charles X.', '']]"
1360093,2010.03763,Lang Yu,Lang Yu and Allyson Ettinger,Assessing Phrasal Representation and Composition in Transformers,Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep transformer models have pushed performance on NLP tasks to new limits,
suggesting sophisticated treatment of complex linguistic inputs, such as
phrases. However, we have limited understanding of how these models handle
representation of phrases, and whether this reflects sophisticated composition
of phrase meaning like that done by humans. In this paper, we present
systematic analysis of phrasal representations in state-of-the-art pre-trained
transformers. We use tests leveraging human judgments of phrase similarity and
meaning shift, and compare results before and after control of word overlap, to
tease apart lexical effects versus composition effects. We find that phrase
representation in these models relies heavily on word content, with little
evidence of nuanced composition. We also identify variations in phrase
representation quality across models, layers, and representation types, and
make corresponding recommendations for usage of representations from these
models.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 04:59:39 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 02:25:52 GMT'}]",2020-10-15,"[['Yu', 'Lang', ''], ['Ettinger', 'Allyson', '']]"
1164139,1908.05969,Ruotian Ma,"Ruotian Ma, Minlong Peng, Qi Zhang, Xuanjing Huang",Simplify the Usage of Lexicon in Chinese NER,ACL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, many works have tried to augment the performance of Chinese named
entity recognition (NER) using word lexicons. As a representative, Lattice-LSTM
(Zhang and Yang, 2018) has achieved new benchmark results on several public
Chinese NER datasets. However, Lattice-LSTM has a complex model architecture.
This limits its application in many industrial areas where real-time NER
responses are needed.
  In this work, we propose a simple but effective method for incorporating the
word lexicon into the character representations. This method avoids designing a
complicated sequence modeling architecture, and for any neural NER model, it
requires only subtle adjustment of the character representation layer to
introduce the lexicon information. Experimental studies on four benchmark
Chinese NER datasets show that our method achieves an inference speed up to
6.15 times faster than those of state-ofthe-art methods, along with a better
performance. The experimental results also show that the proposed method can be
easily incorporated with pre-trained models like BERT.
","[{'version': 'v1', 'created': 'Fri, 16 Aug 2019 13:35:24 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 08:06:12 GMT'}]",2020-10-15,"[['Ma', 'Ruotian', ''], ['Peng', 'Minlong', ''], ['Zhang', 'Qi', ''], ['Huang', 'Xuanjing', '']]"
1333236,2008.05514,Roger Hsiao,"Roger Hsiao, Dogan Can, Tim Ng, Ruchir Travadi and Arnab Ghoshal","Online Automatic Speech Recognition with Listen, Attend and Spell Model","5 pages, 4 figures, this version is submitted to IEEE Signal
  Processing Letters",,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Listen, Attend and Spell (LAS) model and other attention-based automatic
speech recognition (ASR) models have known limitations when operated in a fully
online mode. In this paper, we analyze the online operation of LAS models to
demonstrate that these limitations stem from the handling of silence regions
and the reliability of online attention mechanism at the edge of input buffers.
We propose a novel and simple technique that can achieve fully online
recognition while meeting accuracy and latency targets. For the Mandarin
dictation task, our proposed approach can achieve a character error rate in
online operation that is within 4% relative to an offline LAS model. The
proposed online LAS model operates at 12% lower latency relative to a
conventional neural network hidden Markov model hybrid of comparable accuracy.
We have validated the proposed method through a production scale deployment,
which, to the best of our knowledge, is the first such deployment of a fully
online LAS model.
","[{'version': 'v1', 'created': 'Wed, 12 Aug 2020 18:18:54 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 18:40:28 GMT'}]",2020-10-15,"[['Hsiao', 'Roger', ''], ['Can', 'Dogan', ''], ['Ng', 'Tim', ''], ['Travadi', 'Ruchir', ''], ['Ghoshal', 'Arnab', '']]"
1362370,2010.06040,Dinghan Shen,"Mingzhi Zheng, Dinghan Shen, Yelong Shen, Weizhu Chen, Lin Xiao","Improving Self-supervised Pre-training via a Fully-Explored Masked
  Language Model",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Masked Language Model (MLM) framework has been widely adopted for
self-supervised language pre-training. In this paper, we argue that randomly
sampled masks in MLM would lead to undesirably large gradient variance. Thus,
we theoretically quantify the gradient variance via correlating the gradient
covariance with the Hamming distance between two different masks (given a
certain text sequence). To reduce the variance due to the sampling of masks, we
propose a fully-explored masking strategy, where a text sequence is divided
into a certain number of non-overlapping segments. Thereafter, the tokens
within one segment are masked for training. We prove, from a theoretical
perspective, that the gradients derived from this new masking schema have a
smaller variance and can lead to more efficient self-supervised training. We
conduct extensive experiments on both continual pre-training and general
pre-training from scratch. Empirical results confirm that this new masking
strategy can consistently outperform standard random masking. Detailed
efficiency analysis and ablation studies further validate the advantages of our
fully-explored masking strategy under the MLM framework.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 21:28:14 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 04:45:59 GMT'}]",2020-10-15,"[['Zheng', 'Mingzhi', ''], ['Shen', 'Dinghan', ''], ['Shen', 'Yelong', ''], ['Chen', 'Weizhu', ''], ['Xiao', 'Lin', '']]"
1343222,2009.01719,Felix Hill Mr,"Felix Hill, Olivier Tieleman, Tamara von Glehn, Nathaniel Wong, Hamza
  Merzic, Stephen Clark",Grounded Language Learning Fast and Slow,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has shown that large text-based neural language models, trained
with conventional supervised learning objectives, acquire a surprising
propensity for few- and one-shot learning. Here, we show that an embodied agent
situated in a simulated 3D world, and endowed with a novel dual-coding external
memory, can exhibit similar one-shot word learning when trained with
conventional reinforcement learning algorithms. After a single introduction to
a novel object via continuous visual perception and a language prompt (""This is
a dax""), the agent can re-identify the object and manipulate it as instructed
(""Put the dax on the bed""). In doing so, it seamlessly integrates short-term,
within-episode knowledge of the appropriate referent for the word ""dax"" with
long-term lexical and motor knowledge acquired across episodes (i.e. ""bed"" and
""putting""). We find that, under certain training conditions and with a
particular memory writing mechanism, the agent's one-shot word-object binding
generalizes to novel exemplars within the same ShapeNet category, and is
effective in settings with unfamiliar numbers of objects. We further show how
dual-coding memory can be exploited as a signal for intrinsic motivation,
stimulating the agent to seek names for objects that may be useful for later
executing instructions. Together, the results demonstrate that deep neural
networks can exploit meta-learning, episodic memory and an explicitly
multi-modal environment to account for 'fast-mapping', a fundamental pillar of
human cognitive development and a potentially transformative capacity for
agents that interact with human users.
","[{'version': 'v1', 'created': 'Thu, 3 Sep 2020 14:52:03 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Sep 2020 13:25:12 GMT'}, {'version': 'v3', 'created': 'Tue, 15 Sep 2020 10:56:08 GMT'}, {'version': 'v4', 'created': 'Wed, 14 Oct 2020 14:38:58 GMT'}]",2020-10-15,"[['Hill', 'Felix', ''], ['Tieleman', 'Olivier', ''], ['von Glehn', 'Tamara', ''], ['Wong', 'Nathaniel', ''], ['Merzic', 'Hamza', ''], ['Clark', 'Stephen', '']]"
1362399,2010.06069,Shiran Dudy,Shiran Dudy and Steven Bedrick,Are Some Words Worth More than Others?,EMNLP 2020 Eval4NLP Workshop,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current evaluation metrics for language modeling and generation rely heavily
on the accuracy of predicted (or generated) words as compared to a reference
ground truth. While important, token-level accuracy only captures one aspect of
a language model's behavior, and ignores linguistic properties of words that
may allow some mis-predicted tokens to be useful in practice. Furthermore,
statistics directly tied to prediction accuracy (including perplexity) may be
confounded by the Zipfian nature of written language, as the majority of the
prediction attempts will occur with frequently-occurring types. A model's
performance may vary greatly between high- and low-frequency words, which in
practice could lead to failure modes such as repetitive and dull generated text
being produced by a downstream consumer of a language model. To address this,
we propose two new intrinsic evaluation measures within the framework of a
simple word prediction task that are designed to give a more holistic picture
of a language model's performance. We evaluate several commonly-used large
English language models using our proposed metrics, and demonstrate that our
approach reveals functional differences in performance between the models that
are obscured by more traditional metrics.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 23:12:11 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 03:39:59 GMT'}]",2020-10-15,"[['Dudy', 'Shiran', ''], ['Bedrick', 'Steven', '']]"
1327754,2008.00032,Cristina Zuheros,"Cristina Zuheros, Eugenio Mart\'inez-C\'amara, Enrique Herrera-Viedma,
  and Francisco Herrera","Sentiment Analysis based Multi-person Multi-criteria Decision Making
  Methodology using Natural Language Processing and Deep Learning for Smarter
  Decision Aid. Case study of restaurant choice using TripAdvisor reviews",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Decision making models are constrained by taking the expert evaluations with
pre-defined numerical or linguistic terms. We claim that the use of sentiment
analysis will allow decision making models to consider expert evaluations in
natural language. Accordingly, we propose the Sentiment Analysis based
Multi-person Multi-criteria Decision Making (SA-MpMcDM) methodology for smarter
decision aid, which builds the expert evaluations from their natural language
reviews, and even from their numerical ratings if they are available. The
SA-MpMcDM methodology incorporates an end-to-end multi-task deep learning model
for aspect based sentiment analysis, named DOC-ABSADeepL model, able to
identify the aspect categories mentioned in an expert review, and to distill
their opinions and criteria. The individual evaluations are aggregated via the
procedure named criteria weighting through the attention of the experts. We
evaluate the methodology in a case study of restaurant choice using TripAdvisor
reviews, hence we build, manually annotate, and release the TripR-2020 dataset
of restaurant reviews. We analyze the SA-MpMcDM methodology in different
scenarios using and not using natural language and numerical evaluations. The
analysis shows that the combination of both sources of information results in a
higher quality preference vector.
","[{'version': 'v1', 'created': 'Fri, 31 Jul 2020 18:45:52 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 14:18:41 GMT'}]",2020-10-15,"[['Zuheros', 'Cristina', ''], ['Martínez-Cámara', 'Eugenio', ''], ['Herrera-Viedma', 'Enrique', ''], ['Herrera', 'Francisco', '']]"
1362640,2010.06310,Yue Wang,"Yue Wang, Zhuo Xu, Lu Bai, Yao Wan, Lixin Cui, Qian Zhao, Edwin R.
  Hancock, Philip S. Yu","Cross-Supervised Joint-Event-Extraction with Heterogeneous Information
  Networks",Accepted by ICPR 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Joint-event-extraction, which extracts structural information (i.e., entities
or triggers of events) from unstructured real-world corpora, has attracted more
and more research attention in natural language processing. Most existing works
do not fully address the sparse co-occurrence relationships between entities
and triggers, which loses this important information and thus deteriorates the
extraction performance. To mitigate this issue, we first define the
joint-event-extraction as a sequence-to-sequence labeling task with a tag set
composed of tags of triggers and entities. Then, to incorporate the missing
information in the aforementioned co-occurrence relationships, we propose a
Cross-Supervised Mechanism (CSM) to alternately supervise the extraction of
either triggers or entities based on the type distribution of each other.
Moreover, since the connected entities and triggers naturally form a
heterogeneous information network (HIN), we leverage the latent pattern along
meta-paths for a given corpus to further improve the performance of our
proposed method. To verify the effectiveness of our proposed method, we conduct
extensive experiments on four real-world datasets as well as compare our method
with state-of-the-art methods. Empirical results and analysis show that our
approach outperforms the state-of-the-art methods in both entity and trigger
extraction.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 11:51:17 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 02:11:09 GMT'}]",2020-10-15,"[['Wang', 'Yue', ''], ['Xu', 'Zhuo', ''], ['Bai', 'Lu', ''], ['Wan', 'Yao', ''], ['Cui', 'Lixin', ''], ['Zhao', 'Qian', ''], ['Hancock', 'Edwin R.', ''], ['Yu', 'Philip S.', '']]"
1302856,2006.08342,Lukas Muttenthaler,Lukas Muttenthaler,"Subjective Question Answering: Deciphering the inner workings of
  Transformers in the realm of subjectivity","80 pages, Master's thesis in Computer Science (CS)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding subjectivity demands reasoning skills beyond the realm of
common knowledge. It requires a machine learning model to process sentiment and
to perform opinion mining. In this work, I've exploited a recently released
dataset for span-selection Question Answering, namely SubjQA. SubjQA is the
first QA dataset that contains questions that ask for subjective opinions
corresponding to review paragraphs from six different domains. Hence, to answer
these subjective questions, a learner must extract opinions and process
sentiment for various domains, and additionally, align the knowledge extracted
from a paragraph with the natural language utterances in the corresponding
question, which together enhance the difficulty of a QA task. The primary goal
of this thesis was to investigate the inner workings (i.e., latent
representations) of a Transformer-based architecture to contribute to a better
understanding of these not yet well understood ""black-box"" models.
Transformer's hidden representations, concerning the true answer span, are
clustered more closely in vector space than those representations corresponding
to erroneous predictions. This observation holds across the top three
Transformer layers for both objective and subjective questions and generally
increases as a function of layer dimensions. Moreover, the probability to
achieve a high cosine similarity among hidden representations in latent space
concerning the true answer span tokens is significantly higher for correct
compared to incorrect answer span predictions. These results have decisive
implications for down-stream applications, where it is crucial to know about
why a neural network made mistakes, and in which point, in space and time the
mistake has happened (e.g., to automatically predict correctness of an answer
span prediction without the necessity of labeled data).
","[{'version': 'v1', 'created': 'Tue, 2 Jun 2020 13:48:14 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 07:47:52 GMT'}]",2020-10-15,"[['Muttenthaler', 'Lukas', '']]"
1362390,2010.06060,Hoo Chang Shin,"Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa
  Patwary, Mohammad Shoeybi, Raghav Mani",BioMegatron: Larger Biomedical Domain Language Model,Accepted for publication at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There has been an influx of biomedical domain-specific language models,
showing language models pre-trained on biomedical text perform better on
biomedical domain benchmarks than those trained on general domain text corpora
such as Wikipedia and Books. Yet, most works do not study the factors affecting
each domain language application deeply. Additionally, the study of model size
on domain-specific models has been mostly missing. We empirically study and
evaluate several factors that can affect performance on domain language
applications, such as the sub-word vocabulary set, model size, pre-training
corpus, and domain transfer. We show consistent improvements on benchmarks with
our larger BioMegatron model trained on a larger domain corpus, contributing to
our understanding of domain language model applications. We demonstrate
noticeable improvements over the previous state-of-the-art (SOTA) on standard
biomedical NLP benchmarks of named entity recognition, relation extraction, and
question answering. Model checkpoints and code are available at
[https://ngc.nvidia.com] and [https://github.com/NVIDIA/NeMo].
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 22:46:10 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 02:02:55 GMT'}]",2020-10-15,"[['Shin', 'Hoo-Chang', ''], ['Zhang', 'Yang', ''], ['Bakhturina', 'Evelina', ''], ['Puri', 'Raul', ''], ['Patwary', 'Mostofa', ''], ['Shoeybi', 'Mohammad', ''], ['Mani', 'Raghav', '']]"
1363105,2010.06775,Hao Tan,"Hao Tan, Mohit Bansal","Vokenization: Improving Language Understanding with Contextualized,
  Visual-Grounded Supervision",EMNLP 2020 (15 pages),,,,cs.CL cs.AI cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Humans learn language by listening, speaking, writing, reading, and also, via
interaction with the multimodal real world. Existing language pre-training
frameworks show the effectiveness of text-only self-supervision while we
explore the idea of a visually-supervised language model in this paper. We find
that the main reason hindering this exploration is the large divergence in
magnitude and distributions between the visually-grounded language datasets and
pure-language corpora. Therefore, we develop a technique named ""vokenization""
that extrapolates multimodal alignments to language-only data by contextually
mapping language tokens to their related images (which we call ""vokens""). The
""vokenizer"" is trained on relatively small image captioning datasets and we
then apply it to generate vokens for large language corpora. Trained with these
contextually generated vokens, our visually-supervised language models show
consistent improvements over self-supervised alternatives on multiple
pure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models
publicly available at https://github.com/airsplay/vokenization
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 02:11:51 GMT'}]",2020-10-15,"[['Tan', 'Hao', ''], ['Bansal', 'Mohit', '']]"
1302033,2006.07519,Kyle Dent,Kyle Dent and Kalai Ramea,Conversational User Interfaces for Blind Knowledge Workers: A Case Study,,,,,cs.HC cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern trends in interface design for office equipment using controls on
touch surfaces create greater obstacles for blind and visually impaired users
and contribute to an environment of dependency in work settings. We believe
that \textit{conversational user interfaces} (CUIs) offer a reasonable
alternative to touchscreen interactions enabling more access and most
importantly greater independence for blind knowledge workers. We present a case
study of our work to develop a conversational user interface for accessibility
for multifunction printers. We also describe our approach to conversational
interfaces in general, which emphasizes task-based collaborative interactions
between people and intelligent agents, and we detail the specifics of the
solution we created for multifunction printers. To guide our design, we worked
with a group of blind and visually impaired individuals starting with focus
group sessions to ascertain the challenges our target users face in their
professional lives. We followed our technology development with a user study to
assess the solution and direct our future efforts. We present our findings and
conclusions from the study.
","[{'version': 'v1', 'created': 'Sat, 13 Jun 2020 00:27:14 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 21:06:00 GMT'}]",2020-10-15,"[['Dent', 'Kyle', ''], ['Ramea', 'Kalai', '']]"
1362581,2010.06251,Mariana Neves,Mariana Neves and Jurica Seva,Annotationsaurus: A Searchable Directory of Annotation Tools,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Manual annotation of textual documents is a necessary task when constructing
benchmark corpora for training and evaluating machine learning algorithms. We
created a comprehensive directory of annotation tools that currently includes
93 tools. We analyzed the tools over a set of 31 features and implemented
simple scripts and a Web application that filters the tools based on chosen
criteria. We present two use cases using the directory and propose ideas for
its maintenance. The directory, source codes for scripts, and link to the Web
application are available at: https://github.com/mariananeves/annotation-tools
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 09:22:48 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 06:41:06 GMT'}]",2020-10-15,"[['Neves', 'Mariana', ''], ['Seva', 'Jurica', '']]"
1358160,2010.01830,Mark Anderson,Mark Anderson and Carlos G\'omez-Rodr\'iguez,On the Frailty of Universal POS Tags for Neural UD Parsers,"To be published in proceedings of the 24th SIGNLL Conference on
  Computational Natural Language Learning (CoNLL). Be aware of long appendix:
  please don't print all 28 pages",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present an analysis on the effect UPOS accuracy has on parsing
performance. Results suggest that leveraging UPOS tags as features for neural
parsers requires a prohibitively high tagging accuracy and that the use of gold
tags offers a non-linear increase in performance, suggesting some sort of
exceptionality. We also investigate what aspects of predicted UPOS tags impact
parsing accuracy the most, highlighting some potentially meaningful linguistic
facets of the problem.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 07:40:35 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 07:42:54 GMT'}, {'version': 'v3', 'created': 'Wed, 14 Oct 2020 05:47:29 GMT'}]",2020-10-15,"[['Anderson', 'Mark', ''], ['Gómez-Rodríguez', 'Carlos', '']]"
1363765,2010.07435,Maryam Hashemzadeh,"Maryam Hashemzadeh, Greta Kaufeld, Martha White, Andrea E. Martin,
  Alona Fyshe","From Language to Language-ish: How Brain-Like is an LSTM's
  Representation of Nonsensical Language Stimuli?",12 pages,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The representations generated by many models of language (word embeddings,
recurrent neural networks and transformers) correlate to brain activity
recorded while people read. However, these decoding results are usually based
on the brain's reaction to syntactically and semantically sound language
stimuli. In this study, we asked: how does an LSTM (long short term memory)
language model, trained (by and large) on semantically and syntactically intact
language, represent a language sample with degraded semantic or syntactic
information? Does the LSTM representation still resemble the brain's reaction?
We found that, even for some kinds of nonsensical language, there is a
statistically significant relationship between the brain's activity and the
representations of an LSTM. This indicates that, at least in some instances,
LSTMs and the human brain handle nonsensical data similarly.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 23:26:28 GMT'}]",2020-10-16,"[['Hashemzadeh', 'Maryam', ''], ['Kaufeld', 'Greta', ''], ['White', 'Martha', ''], ['Martin', 'Andrea E.', ''], ['Fyshe', 'Alona', '']]"
1363762,2010.07432,Alex Tamkin,"Alex Tamkin, Mike Wu, Noah Goodman","Viewmaker Networks: Learning Views for Unsupervised Representation
  Learning",,,,,cs.LG cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many recent methods for unsupervised representation learning involve training
models to be invariant to different ""views,"" or transformed versions of an
input. However, designing these views requires considerable human expertise and
experimentation, hindering widespread adoption of unsupervised representation
learning methods across domains and modalities. To address this, we propose
viewmaker networks: generative models that learn to produce input-dependent
views for contrastive learning. We train this network jointly with an encoder
network to produce adversarial $\ell_p$ perturbations for an input, which
yields challenging yet useful views without extensive human tuning. Our learned
views, when applied to CIFAR-10, enable comparable transfer accuracy to the the
well-studied augmentations used for the SimCLR model. Our views significantly
outperforming baseline augmentations in speech (+9% absolute) and wearable
sensor (+17% absolute) domains. We also show how viewmaker views can be
combined with handcrafted views to improve robustness to common image
corruptions. Our method demonstrates that learned views are a promising way to
reduce the amount of expertise and effort needed for unsupervised learning,
potentially extending its benefits to a much wider set of domains.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 23:03:31 GMT'}]",2020-10-16,"[['Tamkin', 'Alex', ''], ['Wu', 'Mike', ''], ['Goodman', 'Noah', '']]"
1363744,2010.07414,Isar Nejadgholi,Isar Nejadgholi and Svetlana Kiritchenko,On Cross-Dataset Generalization in Automatic Detection of Online Abuse,"13 pages, 3 figures, accepted to WOAH-2020 (The 4th Workshop on
  Online Abuse and Harms)",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  NLP research has attained high performances in abusive language detection as
a supervised classification task. While in research settings, training and test
datasets are usually obtained from similar data samples, in practice systems
are often applied on data that are different from the training set in topic and
class distributions. Also, the ambiguity in class definitions inherited in this
task aggravates the discrepancies between source and target datasets. We
explore the topic bias and the task formulation bias in cross-dataset
generalization. We show that the benign examples in the Wikipedia Detox dataset
are biased towards platform-specific topics. We identify these examples using
unsupervised topic modeling and manual inspection of topics' keywords. Removing
these topics increases cross-dataset generalization, without reducing in-domain
classification performance. For a robust dataset design, we suggest applying
inexpensive unsupervised methods to inspect the collected data and downsize the
non-generalizable content before manually annotating for class labels.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 21:47:03 GMT'}]",2020-10-16,"[['Nejadgholi', 'Isar', ''], ['Kiritchenko', 'Svetlana', '']]"
1363770,2010.07440,Elena Del Olmo Su\'arez,"Elena del Olmo Su\'arez and Ana Mar\'ia Fern\'andez-Pampill\'on
  Cesteros","A new approach for extracting the conceptual schema of texts based on
  the linguistic Thematic Progression theory",,"Del Olmo, E.; Fern\'andez-Pampill\'on, A. A new approach for
  extracting the conceptual schema of texts based on the linguistic Thematic
  Progression theory. Proceedings of the Workshop on Hybrid Intelligence for
  NLP Tasks (ECAI-2020), 23-27",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The purpose of this article is to present a new approach for the discovery
and labelling of the implicit conceptual schema of texts through the
application of the Thematic Progression theory. The underlying conceptual
schema is the core component for the generation of summaries that are genuinely
consistent with the semantics of the text.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 23:50:25 GMT'}]",2020-10-16,"[['Suárez', 'Elena del Olmo', ''], ['Cesteros', 'Ana María Fernández-Pampillón', '']]"
1358472,2010.02142,Anshul Wadhawan,"Janvijay Singh, Anshul Wadhawan","PublishInCovid19 at WNUT 2020 Shared Task-1: Entity Recognition in Wet
  Lab Protocols using Structured Learning Ensemble and Contextualised
  Embeddings",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we describe the approach that we employed to address the task
of Entity Recognition over Wet Lab Protocols -- a shared task in EMNLP
WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase,
we experiment with various contextualised word embeddings (like Flair,
BERT-based) and a BiLSTM-CRF model to arrive at the best-performing
architecture. In the second phase, we create an ensemble composed of eleven
BiLSTM-CRF models. The individual models are trained on random train-validation
splits of the complete dataset. Here, we also experiment with different output
merging schemes, including Majority Voting and Structured Learning Ensembling
(SLE). Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for
the partial and exact match of the entity spans, respectively. We were ranked
first and second, in terms of partial and exact match, respectively.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 16:45:30 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 08:33:45 GMT'}]",2020-10-16,"[['Singh', 'Janvijay', ''], ['Wadhawan', 'Anshul', '']]"
1363740,2010.07410,Ilan Price,"Ilan Price, Jordan Gifford-Moore, Jory Flemming, Saul Musker, Maayan
  Roichman, Guillaume Sylvain, Nithum Thain, Lucas Dixon, Jeffrey Sorensen",Six Attributes of Unhealthy Conversation,Appearing in the 4th Workshop on Online Abuse and Harms (2020),,,,cs.CL cs.SI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We present a new dataset of approximately 44000 comments labeled by
crowdworkers. Each comment is labelled as either 'healthy' or 'unhealthy', in
addition to binary labels for the presence of six potentially 'unhealthy'
sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or
trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic;
and/or (6) an unfair generalisation. Each label also has an associated
confidence score. We argue that there is a need for datasets which enable
research based on a broad notion of 'unhealthy online conversation'. We build
this typology to encompass a substantial proportion of the individual comments
which contribute to unhealthy online conversation. For some of these
attributes, this is the first publicly available dataset of this scale. We
explore the quality of the dataset, present some summary statistics and initial
models to illustrate the utility of this data, and highlight limitations and
directions for further research.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 21:28:06 GMT'}]",2020-10-16,"[['Price', 'Ilan', ''], ['Gifford-Moore', 'Jordan', ''], ['Flemming', 'Jory', ''], ['Musker', 'Saul', ''], ['Roichman', 'Maayan', ''], ['Sylvain', 'Guillaume', ''], ['Thain', 'Nithum', ''], ['Dixon', 'Lucas', ''], ['Sorensen', 'Jeffrey', '']]"
1363796,2010.07466,Hoang Nguyen Hung Van,"Hoang Van, Ahmad Musa, Mihai Surdeanu and Stephen Kobourov","The Language of Food during the Pandemic: Hints about the Dietary
  Effects of Covid-19","9 page of main contents plus 1 page of references. 4 figures and 9
  tables",,,,cs.CL cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the language of food on Twitter during the pandemic lockdown in the
United States, focusing on the two month period of March 15 to May 15, 2020.
Specifically, we analyze over770,000 tweets published during the lockdown and
the equivalent period in the five previous years and highlight several worrying
trends. First, we observe that during the lockdown there was a notable shift
from mentions of healthy foods to unhealthy foods. Second, we show an increased
pointwise mutual information of depression hashtags with food-related tweets
posted during the lockdown and an increased association between depression
hashtags and unhealthy foods, tobacco, and alcohol during the lockdown.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 01:33:05 GMT'}]",2020-10-16,"[['Van', 'Hoang', ''], ['Musa', 'Ahmad', ''], ['Surdeanu', 'Mihai', ''], ['Kobourov', 'Stephen', '']]"
1363622,2010.07292,Hancheng Cao,"Hancheng Cao, Vivian Yang, Victor Chen, Yu Jin Lee, Lydia Stone,
  N'godjigui Junior Diarrassouba, Mark E. Whiting, Michael S. Bernstein","My Team Will Go On: Differentiating High and Low Viability Teams through
  Team Interaction",CSCW 2020 Honorable Mention Award,,,,cs.CY cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding team viability -- a team's capacity for sustained and future
success -- is essential for building effective teams. In this study, we
aggregate features drawn from the organizational behavior literature to train a
viability classification model over a dataset of 669 10-minute text
conversations of online teams. We train classifiers to identify teams at the
top decile (most viable teams), 50th percentile (above a median split), and
bottom decile (least viable teams), then characterize the attributes of teams
at each of these viability levels. We find that a lasso regression model
achieves an accuracy of .74--.92 AUC ROC under different thresholds of
classifying viability scores. From these models, we identify the use of
exclusive language such as `but' and `except', and the use of second person
pronouns, as the most predictive features for detecting the most viable teams,
suggesting that active engagement with others' ideas is a crucial signal of a
viable team. Only a small fraction of the 10-minute discussion, as little as 70
seconds, is required for predicting the viability of team interaction. This
work suggests opportunities for teams to assess, track, and visualize their own
viability in real time as they collaborate.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 21:33:36 GMT'}]",2020-10-16,"[['Cao', 'Hancheng', ''], ['Yang', 'Vivian', ''], ['Chen', 'Victor', ''], ['Lee', 'Yu Jin', ''], ['Stone', 'Lydia', ''], ['Diarrassouba', ""N'godjigui Junior"", ''], ['Whiting', 'Mark E.', ''], ['Bernstein', 'Michael S.', '']]"
1363591,2010.07261,Makesh Narsimhan Sreedhar,"Makesh Narsimhan Sreedhar, Kun Ni, Siva Reddy","Learning Improvised Chatbots from Adversarial Modifications of Natural
  Language Feedback",Accepted for publication at Findings of EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The ubiquitous nature of chatbots and their interaction with users generate
an enormous amount of data. Can we improve chatbots using this data? A
self-feeding chatbot improves itself by asking natural language feedback when a
user is dissatisfied with its response and uses this feedback as an additional
training sample. However, user feedback in most cases contains extraneous
sequences hindering their usefulness as a training sample. In this work, we
propose a generative adversarial model that converts noisy feedback into a
plausible natural response in a conversation. The generator's goal is to
convert the feedback into a response that answers the user's previous utterance
and to fool the discriminator which distinguishes feedback from natural
responses. We show that augmenting original training data with these modified
feedback responses improves the original chatbot performance from 69.94% to
75.96% in ranking correct responses on the Personachat dataset, a large
improvement given that the original model is already trained on 131k samples.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 17:33:37 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 02:19:13 GMT'}]",2020-10-16,"[['Sreedhar', 'Makesh Narsimhan', ''], ['Ni', 'Kun', ''], ['Reddy', 'Siva', '']]"
1363777,2010.07447,Michal Lukasik,"Michal Lukasik, Himanshu Jain, Aditya Krishna Menon, Seungyeon Kim,
  Srinadh Bhojanapalli, Felix Yu, Sanjiv Kumar",Semantic Label Smoothing for Sequence to Sequence Problems,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Label smoothing has been shown to be an effective regularization strategy in
classification, that prevents overfitting and helps in label de-noising.
However, extending such methods directly to seq2seq settings, such as Machine
Translation, is challenging: the large target output space of such problems
makes it intractable to apply label smoothing over all possible outputs. Most
existing approaches for seq2seq settings either do token level smoothing, or
smooth over sequences generated by randomly substituting tokens in the target
sequence. Unlike these works, in this paper, we propose a technique that
smooths over \emph{well formed} relevant sequences that not only have
sufficient n-gram overlap with the target sequence, but are also
\emph{semantically similar}. Our method shows a consistent and significant
improvement over the state-of-the-art techniques on different datasets.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 00:31:15 GMT'}]",2020-10-16,"[['Lukasik', 'Michal', ''], ['Jain', 'Himanshu', ''], ['Menon', 'Aditya Krishna', ''], ['Kim', 'Seungyeon', ''], ['Bhojanapalli', 'Srinadh', ''], ['Yu', 'Felix', ''], ['Kumar', 'Sanjiv', '']]"
1279976,2005.00181,Yi Luan,"Yi Luan, Jacob Eisenstein, Kristina Toutanova, Michael Collins","Sparse, Dense, and Attentional Representations for Text Retrieval","To appear in TACL 2020. The arXiv version is a pre-MIT Press
  publication version",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dual encoders perform retrieval by encoding documents and queries into dense
lowdimensional vectors, scoring each document by its inner product with the
query. We investigate the capacity of this architecture relative to sparse
bag-of-words models and attentional neural networks. Using both theoretical and
empirical analysis, we establish connections between the encoding dimension,
the margin between gold and lower-ranked documents, and the document length,
suggesting limitations in the capacity of fixed-length encodings to support
precise retrieval of long documents. Building on these insights, we propose a
simple neural model that combines the efficiency of dual encoders with some of
the expressiveness of more costly attentional architectures, and explore
sparse-dense hybrids to capitalize on the precision of sparse retrieval. These
models outperform strong alternatives in large-scale retrieval.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 02:21:17 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 19:12:42 GMT'}]",2020-10-16,"[['Luan', 'Yi', ''], ['Eisenstein', 'Jacob', ''], ['Toutanova', 'Kristina', ''], ['Collins', 'Michael', '']]"
1356624,2010.00294,Anshul Wadhawan,Anshul Wadhawan,"Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID
  Twitter BERT and Bagging Ensemble Technique based on Plurality Voting",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents the approach that we employed to tackle the EMNLP
WNUT-2020 Shared Task 2 : Identification of informative COVID-19 English
Tweets. The task is to develop a system that automatically identifies whether
an English Tweet related to the novel coronavirus (COVID-19) is informative or
not. We solve the task in three stages. The first stage involves pre-processing
the dataset by filtering only relevant information. This is followed by
experimenting with multiple deep learning models like CNNs, RNNs and
Transformer based models. In the last stage, we propose an ensemble of the best
model trained on different subsets of the provided dataset. Our final approach
achieved an F1-score of 0.9037 and we were ranked sixth overall with F1-score
as the evaluation criteria.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 10:54:54 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Oct 2020 09:43:34 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Oct 2020 08:35:00 GMT'}]",2020-10-16,"[['Wadhawan', 'Anshul', '']]"
1356283,2009.14780,Yevgeni Berzak,"Jonathan Malmaud, Roger Levy, Yevgeni Berzak","Bridging Information-Seeking Human Gaze and Machine Reading
  Comprehension",CoNLL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we analyze how human gaze during reading comprehension is
conditioned on the given reading comprehension question, and whether this
signal can be beneficial for machine reading comprehension. To this end, we
collect a new eye-tracking dataset with a large number of participants engaging
in a multiple choice reading comprehension task. Our analysis of this data
reveals increased fixation times over parts of the text that are most relevant
for answering the question. Motivated by this finding, we propose making
automated reading comprehension more human-like by mimicking human
information-seeking reading behavior during reading comprehension. We
demonstrate that this approach leads to performance gains on multiple choice
question answering in English for a state-of-the-art reading comprehension
model.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 16:34:27 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 16:08:02 GMT'}]",2020-10-16,"[['Malmaud', 'Jonathan', ''], ['Levy', 'Roger', ''], ['Berzak', 'Yevgeni', '']]"
1279393,2004.14623,Christopher Potts,"Atticus Geiger, Kyle Richardson, and Christopher Potts","Neural Natural Language Inference Models Partially Embed Theories of
  Lexical Entailment and Negation",To appear in Proceedings of BlackBoxNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We address whether neural models for Natural Language Inference (NLI) can
learn the compositional interactions between lexical entailment and negation,
using four methods: the behavioral evaluation methods of (1) challenge test
sets and (2) systematic generalization tasks, and the structural evaluation
methods of (3) probes and (4) interventions. To facilitate this holistic
evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset
focused on lexical entailment and negation. In our behavioral evaluations, we
find that models trained on general-purpose NLI datasets fail systematically on
MoNLI examples containing negation, but that MoNLI fine-tuning addresses this
failure. In our structural evaluations, we look for evidence that our
top-performing BERT-based model has learned to implement the monotonicity
algorithm behind MoNLI. Probes yield evidence consistent with this conclusion,
and our intervention experiments bolster this, showing that the causal dynamics
of the model mirror the causal dynamics of this algorithm on subsets of MoNLI.
This suggests that the BERT model at least partially embeds a theory of lexical
entailment and negation at an algorithmic level.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 07:53:20 GMT'}, {'version': 'v2', 'created': 'Tue, 14 Jul 2020 20:51:04 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Oct 2020 17:04:12 GMT'}]",2020-10-16,"[['Geiger', 'Atticus', ''], ['Richardson', 'Kyle', ''], ['Potts', 'Christopher', '']]"
1355391,2009.13888,Kawin Ethayarajh,"Kawin Ethayarajh, Dan Jurafsky",Utility is in the Eye of the User: A Critique of NLP Leaderboards,EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Benchmarks such as GLUE have helped drive advances in NLP by incentivizing
the creation of more accurate models. While this leaderboard paradigm has been
remarkably successful, a historical focus on performance-based evaluation has
been at the expense of other qualities that the NLP community values in models,
such as compactness, fairness, and energy efficiency. In this opinion paper, we
study the divergence between what is incentivized by leaderboards and what is
useful in practice through the lens of microeconomic theory. We frame both the
leaderboard and NLP practitioners as consumers and the benefit they get from a
model as its utility to them. With this framing, we formalize how leaderboards
-- in their current form -- can be poor proxies for the NLP community at large.
For example, a highly inefficient model would provide less utility to
practitioners but not to a leaderboard, since it is a cost that only the former
must bear. To allow practitioners to better estimate a model's utility to them,
we advocate for more transparency on leaderboards, such as the reporting of
statistics that are of practical concern (e.g., model size, energy efficiency,
and inference latency).
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 09:25:31 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 23:04:29 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Oct 2020 00:40:13 GMT'}]",2020-10-16,"[['Ethayarajh', 'Kawin', ''], ['Jurafsky', 'Dan', '']]"
1278835,2004.14065,Hila Gonen,Hila Gonen and Kellie Webster,"Automatically Identifying Gender Issues in Machine Translation using
  Perturbations",Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The successful application of neural methods to machine translation has
realized huge quality advances for the community. With these improvements, many
have noted outstanding challenges, including the modeling and treatment of
gendered language. While previous studies have identified issues using
synthetic examples, we develop a novel technique to mine examples from real
world data to explore challenges for deployed systems. We use our method to
compile an evaluation benchmark spanning examples for four languages from three
language families, which we publicly release to facilitate research. The
examples in our benchmark expose where model representations are gendered, and
the unintended consequences these gendered representations can have in
downstream application.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 10:38:09 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 19:01:59 GMT'}]",2020-10-16,"[['Gonen', 'Hila', ''], ['Webster', 'Kellie', '']]"
1363805,2010.07475,Wanjun Zhong,"Wanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou,
  Jiahai Wang, Jian Yin",Neural Deepfake Detection with Factual Structure of Text,EMNLP2020;10 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deepfake detection, the task of automatically discriminating
machine-generated text, is increasingly critical with recent advances in
natural language generative models. Existing approaches to deepfake detection
typically represent documents with coarse-grained representations. However,
they struggle to capture factual structures of documents, which is a
discriminative factor between machine-generated and human-written text
according to our statistical analysis. To address this, we propose a
graph-based model that utilizes the factual structure of a document for
deepfake detection of text. Our approach represents the factual structure of a
given document as an entity graph, which is further utilized to learn sentence
representations with a graph neural network. Sentence representations are then
composed to a document representation for making predictions, where consistent
relations between neighboring sentences are sequentially modeled. Results of
experiments on two public deepfake datasets show that our approach
significantly improves strong base models built with RoBERTa. Model analysis
further indicates that our model can distinguish the difference in the factual
structure between machine-generated text and human-written text.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 02:35:31 GMT'}]",2020-10-16,"[['Zhong', 'Wanjun', ''], ['Tang', 'Duyu', ''], ['Xu', 'Zenan', ''], ['Wang', 'Ruize', ''], ['Duan', 'Nan', ''], ['Zhou', 'Ming', ''], ['Wang', 'Jiahai', ''], ['Yin', 'Jian', '']]"
1352298,2009.10795,Swabha Swayamdipta,"Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang,
  Hannaneh Hajishirzi, Noah A. Smith, Yejin Choi","Dataset Cartography: Mapping and Diagnosing Datasets with Training
  Dynamics",Proceedings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large datasets have become commonplace in NLP research. However, the
increased emphasis on data quantity has made it challenging to assess the
quality of data. We introduce Data Maps---a model-based tool to characterize
and diagnose datasets. We leverage a largely ignored source of information: the
behavior of the model on individual instances during training (training
dynamics) for building data maps. This yields two intuitive measures for each
example---the model's confidence in the true class, and the variability of this
confidence across epochs---obtained in a single run of training. Experiments
across four datasets show that these model-dependent measures reveal three
distinct regions in the data map, each with pronounced characteristics. First,
our data maps show the presence of ""ambiguous"" regions with respect to the
model, which contribute the most towards out-of-distribution generalization.
Second, the most populous regions in the data are ""easy to learn"" for the
model, and play an important role in model optimization. Finally, data maps
uncover a region with instances that the model finds ""hard to learn""; these
often correspond to labeling errors. Our results indicate that a shift in focus
from quantity to quality of data could lead to robust models and improved
out-of-distribution generalization.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 20:19:41 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 05:53:46 GMT'}]",2020-10-16,"[['Swayamdipta', 'Swabha', ''], ['Schwartz', 'Roy', ''], ['Lourie', 'Nicholas', ''], ['Wang', 'Yizhong', ''], ['Hajishirzi', 'Hannaneh', ''], ['Smith', 'Noah A.', ''], ['Choi', 'Yejin', '']]"
1287604,2005.07809,Zhuohao Chen,"Zhuohao Chen, Nikolaos Flemotomos, Victor Ardulov, Torrey A. Creed,
  Zac E. Imel, David C. Atkins, Shrikanth Narayanan","Feature Fusion Strategies for End-to-End Evaluation of Cognitive
  Behavior Therapy Sessions",,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cognitive Behavioral Therapy (CBT) is a goal-oriented psychotherapy for
mental health concerns implemented in a conversational setting with broad
empirical support for its effectiveness across a range of presenting problems
and client populations. The quality of a CBT session is typically assessed by
trained human raters who manually assign pre-defined session-level behavioral
codes. In this paper, we develop an end-to-end pipeline that converts speech
audio to diarized and transcribed text and extracts linguistic features to code
the CBT sessions automatically. We investigate both word-level and
utterance-level features and propose feature fusion strategies to combine them.
The utterance level features include dialog act tags as well as behavioral
codes drawn from another well-known talk psychotherapy called Motivational
Interviewing (MI). We propose a novel method to augment the word-based features
with the utterance level tags for subsequent CBT code estimation. Experiments
show that our new fusion strategy outperforms all the studied features, both
when used individually and when fused by direct concatenation. We also find
that incorporating a sentence segmentation module can further improve the
overall system given the preponderance of multi-utterance conversational turns
in CBT sessions.
","[{'version': 'v1', 'created': 'Fri, 15 May 2020 22:26:58 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 20:53:36 GMT'}]",2020-10-16,"[['Chen', 'Zhuohao', ''], ['Flemotomos', 'Nikolaos', ''], ['Ardulov', 'Victor', ''], ['Creed', 'Torrey A.', ''], ['Imel', 'Zac E.', ''], ['Atkins', 'David C.', ''], ['Narayanan', 'Shrikanth', '']]"
1258247,2003.07892,Shrey Desai,Shrey Desai and Greg Durrett,Calibration of Pre-trained Transformers,Accepted to EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained Transformers are now ubiquitous in natural language processing,
but despite their high end-task performance, little is known empirically about
whether they are calibrated. Specifically, do these models' posterior
probabilities provide an accurate empirical measure of how likely the model is
to be correct on a given example? We focus on BERT and RoBERTa in this work,
and analyze their calibration across three tasks: natural language inference,
paraphrase detection, and commonsense reasoning. For each task, we consider
in-domain as well as challenging out-of-domain settings, where models face more
examples they should be uncertain about. We show that: (1) when used
out-of-the-box, pre-trained models are calibrated in-domain, and compared to
baselines, their calibration error out-of-domain can be as much as 3.5x lower;
(2) temperature scaling is effective at further reducing calibration error
in-domain, and using label smoothing to deliberately increase empirical
uncertainty helps calibrate posteriors out-of-domain.
","[{'version': 'v1', 'created': 'Tue, 17 Mar 2020 18:58:44 GMT'}, {'version': 'v2', 'created': 'Fri, 20 Mar 2020 21:35:54 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Oct 2020 17:04:21 GMT'}]",2020-10-16,"[['Desai', 'Shrey', ''], ['Durrett', 'Greg', '']]"
1258552,2003.08197,Paul Pu Liang,"Paul Pu Liang, Manzil Zaheer, Yuan Wang, Amr Ahmed",Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies,,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning continuous representations of discrete objects such as text, users,
movies, and URLs lies at the heart of many applications including language and
user modeling. When using discrete objects as input to neural networks, we
often ignore the underlying structures (e.g. natural groupings and
similarities) and embed the objects independently into individual vectors. As a
result, existing methods do not scale to large vocabulary sizes. In this paper,
we design a simple and efficient embedding algorithm that learns a small set of
anchor embeddings and a sparse transformation matrix. We call our method Anchor
& Transform (ANT) as the embeddings of discrete objects are a sparse linear
combination of the anchors, weighted according to the transformation matrix.
ANT is scalable, flexible, and end-to-end trainable. We further provide a
statistical interpretation of our algorithm as a Bayesian nonparametric prior
for embeddings that encourages sparsity and leverages natural groupings among
objects. By deriving an approximate inference algorithm based on Small Variance
Asymptotics, we obtain a natural extension that automatically learns the
optimal number of anchors instead of having to tune it as a hyperparameter. On
text classification, language modeling, and movie recommendation benchmarks, we
show that ANT is particularly suitable for large vocabulary sizes and
demonstrates stronger performance with fewer parameters (up to 40x compression)
as compared to existing compression baselines.
","[{'version': 'v1', 'created': 'Wed, 18 Mar 2020 13:07:51 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Jul 2020 03:51:17 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Oct 2020 04:43:51 GMT'}]",2020-10-16,"[['Liang', 'Paul Pu', ''], ['Zaheer', 'Manzil', ''], ['Wang', 'Yuan', ''], ['Ahmed', 'Amr', '']]"
1335494,2008.07772,Xiaodong Liu,"Xiaodong Liu, Kevin Duh, Liyuan Liu and Jianfeng Gao",Very Deep Transformers for Neural Machine Translation,"6 pages, 3 figures and 4 tables. V2 includes the back-translation
  results",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We explore the application of very deep Transformer models for Neural Machine
Translation (NMT). Using a simple yet effective initialization technique that
stabilizes training, we show that it is feasible to build standard
Transformer-based models with up to 60 encoder layers and 12 decoder layers.
These deep models outperform their baseline 6-layer counterparts by as much as
2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14
English-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14
English-German (30.1 BLEU).The code and trained models will be publicly
available at: https://github.com/namisan/exdeep-nmt.
","[{'version': 'v1', 'created': 'Tue, 18 Aug 2020 07:14:54 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 22:56:32 GMT'}]",2020-10-16,"[['Liu', 'Xiaodong', ''], ['Duh', 'Kevin', ''], ['Liu', 'Liyuan', ''], ['Gao', 'Jianfeng', '']]"
1339371,2008.11649,Masataro Asai,"Masataro Asai, Zilu Tang",Discrete Word Embedding for Logical Natural Language Understanding,equal contribution,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose an unsupervised neural model for learning a discrete embedding of
words. Unlike existing discrete embeddings, our binary embedding supports
vector arithmetic operations similar to continuous embeddings. Our embedding
represents each word as a set of propositional statements describing a
transition rule in classical/STRIPS planning formalism. This makes the
embedding directly compatible with symbolic, state of the art classical
planning solvers.
","[{'version': 'v1', 'created': 'Wed, 26 Aug 2020 16:15:18 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 14:37:59 GMT'}]",2020-10-16,"[['Asai', 'Masataro', ''], ['Tang', 'Zilu', '']]"
1356640,2010.00310,Anshul Wadhawan,"Akshita Aggarwal, Anshul Wadhawan, Anshima Chaudhary, Kavita Maurya","""Did you really mean what you said?"" : Sarcasm Detection in
  Hindi-English Code-Mixed Data using Bilingual Word Embeddings",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the increased use of social media platforms by people across the world,
many new interesting NLP problems have come into existence. One such being the
detection of sarcasm in the social media texts. We present a corpus of tweets
for training custom word embeddings and a Hinglish dataset labelled for sarcasm
detection. We propose a deep learning based approach to address the issue of
sarcasm detection in Hindi-English code mixed tweets using bilingual word
embeddings derived from FastText and Word2Vec approaches. We experimented with
various deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with
and without attention). We were able to outperform all state-of-the-art
performances with our deep learning models, with attention based Bi-directional
LSTMs giving the best performance exhibiting an accuracy of 78.49%.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 11:41:44 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 08:32:33 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Oct 2020 08:32:09 GMT'}]",2020-10-16,"[['Aggarwal', 'Akshita', ''], ['Wadhawan', 'Anshul', ''], ['Chaudhary', 'Anshima', ''], ['Maurya', 'Kavita', '']]"
1304103,2006.09589,Zijian Wang,"Elisa Kreiss, Zijian Wang, Christopher Potts",Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives,CoNLL 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Crime reporting is a prevalent form of journalism with the power to shape
public perceptions and social policies. How does the language of these reports
act on readers? We seek to address this question with the SuspectGuilt Corpus
of annotated crime stories from English-language newspapers in the U.S. For
SuspectGuilt, annotators read short crime articles and provided text-level
ratings concerning the guilt of the main suspect as well as span-level
annotations indicating which parts of the story they felt most influenced their
ratings. SuspectGuilt thus provides a rich picture of how linguistic choices
affect subjective guilt judgments. In addition, we use SuspectGuilt to train
and assess predictive models, and show that these models benefit from genre
pretraining and joint supervision from the text-level ratings and span-level
annotations. Such models might be used as tools for understanding the societal
effects of crime reporting.
","[{'version': 'v1', 'created': 'Wed, 17 Jun 2020 01:21:19 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 22:38:44 GMT'}]",2020-10-16,"[['Kreiss', 'Elisa', ''], ['Wang', 'Zijian', ''], ['Potts', 'Christopher', '']]"
1363827,2010.07497,Jianheng Tang,"Wenge Liu, Jianheng Tang, Jinghui Qin, Lin Xu, Zhen Li, Xiaodan Liang","MedDG: A Large-scale Medical Consultation Dataset for Building Medical
  Dialogue System",Data and code are available at https://github.com/lwgkzl/MedDG,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Developing conversational agents to interact with patients and provide
primary clinical advice has attracted increasing attention due to its huge
application potential, especially in the time of COVID-19 Pandemic. However,
the training of end-to-end neural-based medical dialogue system is restricted
by an insufficient quantity of medical dialogue corpus. In this work, we make
the first attempt to build and release a large-scale high-quality Medical
Dialogue dataset related to 12 types of common Gastrointestinal diseases named
MedDG, with more than 17K conversations collected from the online health
consultation community. Five different categories of entities, including
diseases, symptoms, attributes, tests, and medicines, are annotated in each
conversation of MedDG as additional labels. To push forward the future research
on building expert-sensitive medical dialogue system, we proposes two kinds of
medical dialogue tasks based on MedDG dataset. One is the next entity
prediction and the other is the doctor response generation. To acquire a clear
comprehension on these two medical dialogue tasks, we implement several
state-of-the-art benchmarks, as well as design two dialogue models with a
further consideration on the predicted entities. Experimental results show that
the pre-train language models and other baselines struggle on both tasks with
poor performance in our dataset, and the response quality can be enhanced with
the help of auxiliary entity information. From human evaluation, the simple
retrieval model outperforms several state-of-the-art generative models,
indicating that there still remains a large room for improvement on generating
medically meaningful responses.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 03:34:33 GMT'}]",2020-10-16,"[['Liu', 'Wenge', ''], ['Tang', 'Jianheng', ''], ['Qin', 'Jinghui', ''], ['Xu', 'Lin', ''], ['Li', 'Zhen', ''], ['Liang', 'Xiaodan', '']]"
1363845,2010.07515,John Hewitt,"John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, Christopher D.
  Manning",RNNs can generate bounded hierarchical languages with optimal memory,EMNLP2020 + appendix typo fixes,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recurrent neural networks empirically generate natural language with high
syntactic fidelity. However, their success is not well-understood
theoretically. We provide theoretical insight into this success, proving in a
finite-precision setting that RNNs can efficiently generate bounded
hierarchical languages that reflect the scaffolding of natural language syntax.
We introduce Dyck-($k$,$m$), the language of well-nested brackets (of $k$
types) and $m$-bounded nesting depth, reflecting the bounded memory needs and
long-distance dependencies of natural language syntax. The best known results
use $O(k^{\frac{m}{2}})$ memory (hidden units) to generate these languages. We
prove that an RNN with $O(m \log k)$ hidden units suffices, an exponential
reduction in memory, by an explicit construction. Finally, we show that no
algorithm, even with unbounded computation, can suffice with $o(m \log k)$
hidden units.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 04:42:29 GMT'}]",2020-10-16,"[['Hewitt', 'John', ''], ['Hahn', 'Michael', ''], ['Ganguli', 'Surya', ''], ['Liang', 'Percy', ''], ['Manning', 'Christopher D.', '']]"
1364103,2010.07773,Shubhanker Banerjee,"Shubhanker Banerjee, Arun Jayapal and Sajeetha Thavareesan","NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of
  Code-Mixed Dravidian text using XLNet",7 pages,,,,cs.CL cs.AI cs.LG cs.NE,http://creativecommons.org/publicdomain/zero/1.0/,"  Social media has penetrated into multilingual societies, however most of them
use English to be a preferred language for communication. So it looks natural
for them to mix their cultural language with English during conversations
resulting in abundance of multilingual data, call this code-mixed data,
available in todays' world.Downstream NLP tasks using such data is challenging
due to the semantic nature of it being spread across multiple languages.One
such Natural Language Processing task is sentiment analysis, for this we use an
auto-regressive XLNet model to perform sentiment analysis on code-mixed
Tamil-English and Malayalam-English datasets.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 14:09:02 GMT'}]",2020-10-16,"[['Banerjee', 'Shubhanker', ''], ['Jayapal', 'Arun', ''], ['Thavareesan', 'Sajeetha', '']]"
1363833,2010.07503,Sho Takase,Sho Takase and Naoaki Okazaki,Multi-Task Learning for Cross-Lingual Abstractive Summarization,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a multi-task learning framework for cross-lingual abstractive
summarization to augment training data. Recent studies constructed pseudo
cross-lingual abstractive summarization data to train their neural
encoder-decoders. Meanwhile, we introduce existing genuine data such as
translation pairs and monolingual abstractive summarization data into training.
Our proposed method, Transum, attaches a special token to the beginning of the
input sentence to indicate the target task. The special token enables us to
incorporate the genuine data into the training data easily. The experimental
results show that Transum achieves better performance than the model trained
with only pseudo cross-lingual summarization data. In addition, we achieve the
top ROUGE score on Chinese-English and Arabic-English abstractive
summarization. Moreover, Transum also has a positive effect on machine
translation. Experimental results indicate that Transum improves the
performance from the strong baseline, Transformer, in Chinese-English,
Arabic-English, and English-Japanese translation datasets.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 04:03:00 GMT'}]",2020-10-16,"[['Takase', 'Sho', ''], ['Okazaki', 'Naoaki', '']]"
1364115,2010.07785,Weishi Wang,"Weishi Wang, Shafiq Joty, Steven C.H. Hoi","Response Selection for Multi-Party Conversations with Dynamic Topic
  Tracking","9 pages, EMNLP2020",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While participants in a multi-party multi-turn conversation simultaneously
engage in multiple conversation topics, existing response selection methods are
developed mainly focusing on a two-party single-conversation scenario. Hence,
the prolongation and transition of conversation topics are ignored by current
methods. In this work, we frame response selection as a dynamic topic tracking
task to match the topic between the response and relevant conversation context.
With this new formulation, we propose a novel multi-task learning framework
that supports efficient encoding through large pretrained models with only two
utterances at once to perform dynamic topic disentanglement and response
selection. We also propose Topic-BERT an essential pretraining step to embed
topic information into BERT with self-supervised learning. Experimental results
on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response
selection and topic disentanglement tasks outperforming existing methods by a
good margin.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 14:21:38 GMT'}]",2020-10-16,"[['Wang', 'Weishi', ''], ['Joty', 'Shafiq', ''], ['Hoi', 'Steven C. H.', '']]"
1364122,2010.07792,Yinuo Guo,"Yinuo Guo, Zeqi Lin, Jian-Guang Lou, Dongmei Zhang",Hierarchical Poset Decoding for Compositional Generalization in Language,Accepted by Neurips 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We formalize human language understanding as a structured prediction task
where the output is a partially ordered set (poset). Current encoder-decoder
architectures do not take the poset structure of semantics into account
properly, thus suffering from poor compositional generalization ability. In
this paper, we propose a novel hierarchical poset decoding paradigm for
compositional generalization in language. Intuitively: (1) the proposed
paradigm enforces partial permutation invariance in semantics, thus avoiding
overfitting to bias ordering information; (2) the hierarchical mechanism allows
to capture high-level structures of posets. We evaluate our proposed decoder on
Compositional Freebase Questions (CFQ), a large and realistic natural language
question answering dataset that is specifically designed to measure
compositional generalization. Results show that it outperforms current
decoders.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 14:34:26 GMT'}]",2020-10-16,"[['Guo', 'Yinuo', ''], ['Lin', 'Zeqi', ''], ['Lou', 'Jian-Guang', ''], ['Zhang', 'Dongmei', '']]"
1364146,2010.07816,Georgios MIchalopoulos,"George Michalopoulos, Helen Chen, Alexander Wong","Where's the Question? A Multi-channel Deep Convolutional Neural Network
  for Question Identification in Textual Data","12 pages, 4 figures, to be published in The 3rd Clinical Natural
  Language Processing Workshop",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In most clinical practice settings, there is no rigorous reviewing of the
clinical documentation, resulting in inaccurate information captured in the
patient medical records. The gold standard in clinical data capturing is
achieved via ""expert-review"", where clinicians can have a dialogue with a
domain expert (reviewers) and ask them questions about data entry rules.
Automatically identifying ""real questions"" in these dialogues could uncover
ambiguities or common problems in data capturing in a given clinical setting.
  In this study, we proposed a novel multi-channel deep convolutional neural
network architecture, namely Quest-CNN, for the purpose of separating real
questions that expect an answer (information or help) about an issue from
sentences that are not questions, as well as from questions referring to an
issue mentioned in a nearby sentence (e.g., can you clarify this?), which we
will refer as ""c-questions"". We conducted a comprehensive performance
comparison analysis of the proposed multi-channel deep convolutional neural
network against other deep neural networks. Furthermore, we evaluated the
performance of traditional rule-based and learning-based methods for detecting
question sentences. The proposed Quest-CNN achieved the best F1 score both on a
dataset of data entry-review dialogue in a dialysis care setting, and on a
general domain dataset.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 15:11:22 GMT'}]",2020-10-16,"[['Michalopoulos', 'George', ''], ['Chen', 'Helen', ''], ['Wong', 'Alexander', '']]"
1364195,2010.07865,Vladislav Lyalin,"Vladislav Lialin, Rahul Goel, Andrey Simanovsky, Anna Rumshisky,
  Rushin Shah",Continual Learning for Neural Semantic Parsing,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A semantic parsing model is crucial to natural language processing
applications such as goal-oriented dialogue systems. Such models can have
hundreds of classes with a highly non-uniform distribution. In this work, we
show how to efficiently (in terms of computational budget) improve model
performance given a new portion of labeled data for a specific low-resource
class or a set of classes. We demonstrate that a simple approach with a
specific fine-tuning procedure for the old model can reduce the computational
costs by ~90% compared to the training of a new model. The resulting
performance is on-par with a model trained from scratch on a full dataset. We
showcase the efficacy of our approach on two popular semantic parsing datasets,
Facebook TOP, and SNIPS.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 16:37:41 GMT'}]",2020-10-16,"[['Lialin', 'Vladislav', ''], ['Goel', 'Rahul', ''], ['Simanovsky', 'Andrey', ''], ['Rumshisky', 'Anna', ''], ['Shah', 'Rushin', '']]"
1364091,2010.07761,Phillip Keung,"Phillip Keung, Julian Salazar, Yichao Lu, Noah A. Smith","Unsupervised Bitext Mining and Translation via Self-trained Contextual
  Embeddings","To appear in the Transactions of the Association for Computational
  Linguistics",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We describe an unsupervised method to create pseudo-parallel corpora for
machine translation (MT) from unaligned text. We use multilingual BERT to
create source and target sentence embeddings for nearest-neighbor search and
adapt the model via self-training. We validate our technique by extracting
parallel sentence pairs on the BUCC 2017 bitext mining task and observe up to a
24.5 point increase (absolute) in F1 scores over previous unsupervised methods.
We then improve an XLM-based unsupervised neural MT system pre-trained on
Wikipedia by supplementing it with pseudo-parallel text mined from the same
corpus, boosting unsupervised translation performance by up to 3.5 BLEU on the
WMT'14 French-English and WMT'16 German-English tasks and outperforming the
previous state-of-the-art. Finally, we enrich the IWSLT'15 English-Vietnamese
corpus with pseudo-parallel Wikipedia sentence pairs, yielding a 1.2 BLEU
improvement on the low-resource MT task. We demonstrate that unsupervised
bitext mining is an effective way of augmenting MT datasets and complements
existing techniques like initializing with pre-trained contextual embeddings.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 14:04:03 GMT'}]",2020-10-16,"[['Keung', 'Phillip', ''], ['Salazar', 'Julian', ''], ['Lu', 'Yichao', ''], ['Smith', 'Noah A.', '']]"
1364204,2010.07874,Peter Belc\'ak,Peter Belcak,The LL(finite) strategy for optimal LL(k) parsing,,,,,cs.PL cs.CL cs.FL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The LL(finite) parsing strategy for parsing of LL(k) grammars where k needs
not to be known is presented. The strategy parses input in linear time, uses
arbitrary but always minimal lookahead necessary to disambiguate between
alternatives of nonterminals, and it is optimal in the number of lookahead
terminal scans performed. Modifications to the algorithm are shown that allow
for resolution of grammar ambiguities by precedence -- effectively interpreting
the input as a parsing expression grammar -- as well as for the use of
predicates, and a proof of concept, the open-source parser generator Astir,
employs the LL(finite) strategy in the output it generates.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 16:52:29 GMT'}]",2020-10-16,"[['Belcak', 'Peter', '']]"
1364212,2010.07882,Jiacheng Xu,"Jiacheng Xu, Shrey Desai, Greg Durrett",Understanding Neural Abstractive Summarization Models via Uncertainty,"To appear in EMNLP 2020; code available at
  https://github.com/jiacheng-xu/text-sum-uncertainty",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  An advantage of seq2seq abstractive summarization models is that they
generate text in a free-form manner, but this flexibility makes it difficult to
interpret model behavior. In this work, we analyze summarization decoders in
both blackbox and whitebox ways by studying on the entropy, or uncertainty, of
the model's token-level predictions. For two strong pre-trained models, PEGASUS
and BART on two summarization datasets, we find a strong correlation between
low prediction entropy and where the model copies tokens rather than generating
novel text. The decoder's uncertainty also connects to factors like sentence
position and syntactic distance between adjacent pairs of tokens, giving a
sense of what factors make a context particularly selective for the model's
next output token. Finally, we study the relationship of decoder uncertainty
and attention behavior to understand how attention gives rise to these observed
effects in the model. We show that uncertainty is a useful perspective for
analyzing summarization and text generation models more broadly.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 16:57:27 GMT'}]",2020-10-16,"[['Xu', 'Jiacheng', ''], ['Desai', 'Shrey', ''], ['Durrett', 'Greg', '']]"
1364216,2010.07886,Shrey Desai,Shrey Desai and Jiacheng Xu and Greg Durrett,Compressive Summarization with Plausibility and Salience Modeling,Accepted to EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Compressive summarization systems typically rely on a crafted set of
syntactic rules to determine what spans of possible summary sentences can be
deleted, then learn a model of what to actually delete by optimizing for
content selection (ROUGE). In this work, we propose to relax the rigid
syntactic constraints on candidate spans and instead leave compression
decisions to two data-driven criteria: plausibility and salience. Deleting a
span is plausible if removing it maintains the grammaticality and factuality of
a sentence, and spans are salient if they contain important information from
the summary. Each of these is judged by a pre-trained Transformer model, and
only deletions that are both plausible and not salient can be applied. When
integrated into a simple extraction-compression pipeline, our method achieves
strong in-domain results on benchmark summarization datasets, and human
evaluation shows that the plausibility model generally selects for grammatical
and factual deletions. Furthermore, the flexibility of our approach allows it
to generalize cross-domain: our system fine-tuned on only 500 samples from a
new domain can match or exceed an in-domain extractive model trained on much
more data.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 17:07:10 GMT'}]",2020-10-16,"[['Desai', 'Shrey', ''], ['Xu', 'Jiacheng', ''], ['Durrett', 'Greg', '']]"
1360576,2010.04246,Shang-Yu Su,"Shang-Yu Su, Yung-Sung Chuang, Yun-Nung Chen",Dual Inference for Improving Language Understanding and Generation,"Published in Findings of EMNLP 2020. The first two authors
  contributed to this paper equally",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language understanding (NLU) and Natural language generation (NLG)
tasks hold a strong dual relationship, where NLU aims at predicting semantic
labels based on natural language utterances and NLG does the opposite. The
prior work mainly focused on exploiting the duality in model training in order
to obtain the models with better performance. However, regarding the
fast-growing scale of models in the current NLP area, sometimes we may have
difficulty retraining whole NLU and NLG models. To better address the issue,
this paper proposes to leverage the duality in the inference stage without the
need of retraining. The experiments on three benchmark datasets demonstrate the
effectiveness of the proposed method in both NLU and NLG, providing the great
potential of practical usage.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 20:14:41 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 02:10:48 GMT'}]",2020-10-16,"[['Su', 'Shang-Yu', ''], ['Chuang', 'Yung-Sung', ''], ['Chen', 'Yun-Nung', '']]"
1361252,2010.04922,Zhengxuan Wu,"Zhengxuan Wu, Thanh-Son Nguyen, Desmond C. Ong",Structured Self-Attention Weights Encode Semantics in Sentiment Analysis,10 pages,BlackBoxNLP Workshop at EMNLP 2020,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural attention, especially the self-attention made popular by the
Transformer, has become the workhorse of state-of-the-art natural language
processing (NLP) models. Very recent work suggests that the self-attention in
the Transformer encodes syntactic information; Here, we show that
self-attention scores encode semantics by considering sentiment analysis tasks.
In contrast to gradient-based feature attribution methods, we propose a simple
and effective Layer-wise Attention Tracing (LAT) method to analyze structured
attention weights. We apply our method to Transformer models trained on two
tasks that have surface dissimilarities, but share common semantics---sentiment
analysis of movie reviews and time-series valence prediction in life story
narratives. Across both tasks, words with high aggregated attention weights
were rich in emotional semantics, as quantitatively validated by an emotion
lexicon labeled by human annotators. Our results show that structured attention
weights encode rich semantics in sentiment analysis, and match human
interpretations of semantics.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 06:49:25 GMT'}]",2020-10-16,"[['Wu', 'Zhengxuan', ''], ['Nguyen', 'Thanh-Son', ''], ['Ong', 'Desmond C.', '']]"
1040351,1810.09164,Alberto Cetoli,"Alberto Cetoli, Mohammad Akbari, Stefano Bragaglia, Andrew D.
  O'Harney, Marc Sloan",Named Entity Disambiguation using Deep Learning on Graphs,,,10.1007/978-3-030-15719-7_10,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We tackle \ac{NED} by comparing entities in short sentences with \wikidata{}
graphs. Creating a context vector from graphs through deep learning is a
challenging problem that has never been applied to \ac{NED}. Our main
contribution is to present an experimental study of recent neural techniques,
as well as a discussion about which graph features are most important for the
disambiguation task. In addition, a new dataset (\wikidatadisamb{}) is created
to allow a clean and scalable evaluation of \ac{NED} with \wikidata{} entries,
and to be used as a reference in future research. In the end our results show
that a \ac{Bi-LSTM} encoding of the graph triplets performs best, improving
upon the baseline models and scoring an \rm{F1} value of $91.6\%$ on the
\wikidatadisamb{} test set
","[{'version': 'v1', 'created': 'Mon, 22 Oct 2018 10:16:07 GMT'}]",2020-10-16,"[['Cetoli', 'Alberto', ''], ['Akbari', 'Mohammad', ''], ['Bragaglia', 'Stefano', ''], [""O'Harney"", 'Andrew D.', ''], ['Sloan', 'Marc', '']]"
1362689,2010.06359,Eleftherios Avramidis,"Eleftherios Avramidis, Vivien Macketanz, Ursula Strohriegel, Aljoscha
  Burchardt and Sebastian M\""oller","Fine-grained linguistic evaluation for state-of-the-art Machine
  Translation","11 pages, 1 figure, Fifth Conference of Machine Translation, WMT20",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper describes a test suite submission providing detailed statistics of
linguistic performance for the state-of-the-art German-English systems of the
Fifth Conference of Machine Translation (WMT20). The analysis covers 107
phenomena organized in 14 categories based on about 5,500 test items, including
a manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan)
appear to have significantly better test suite accuracy than the others,
although the best system of WMT20 is not significantly better than the one from
WMT19 in a macro-average. Additionally, we identify some linguistic phenomena
where all systems suffer (such as idioms, resultative predicates and
pluperfect), but we are also able to identify particular weaknesses for
individual systems (such as quotation marks, lexical ambiguity and sluicing).
Most of the systems of WMT19 which submitted new versions this year show
improvements.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 13:14:37 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 19:50:28 GMT'}]",2020-10-16,"[['Avramidis', 'Eleftherios', ''], ['Macketanz', 'Vivien', ''], ['Strohriegel', 'Ursula', ''], ['Burchardt', 'Aljoscha', ''], ['Möller', 'Sebastian', '']]"
1364208,2010.07878,Matthias Hertel,"Hannah Bast, Matthias Hertel, Mostafa M. Mohamed",Tokenization Repair in the Presence of Spelling Errors,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the following tokenization repair problem: Given a natural
language text with any combination of missing or spurious spaces, correct
these. Spelling errors can be present, but it's not part of the problem to
correct them. For example, given: ""Tispa per isabout token izaionrep air"",
compute ""Tis paper is about tokenizaion repair"". It is tempting to think of
this problem as a special case of spelling correction or to treat the two
problems together. We make a case that tokenization repair and spelling
correction should and can be treated as separate problems. We investigate a
variety of neural models as well as a number of strong baselines. We identify
three main ingredients to high-quality tokenization repair: deep language
models with a bidirectional component, training the models on text with
spelling errors, and making use of the space information already present. Our
best methods can repair all tokenization errors on 97.5% of the correctly
spelled test sentences and on 96.0% of the misspelled test sentences. With all
spaces removed from the given text (the scenario from previous work), the
accuracy falls to 94.5% and 90.1%, respectively. We conduct a detailed error
analysis.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 16:55:45 GMT'}]",2020-10-16,"[['Bast', 'Hannah', ''], ['Hertel', 'Matthias', ''], ['Mohamed', 'Mostafa M.', '']]"
1364041,2010.07711,Yile Wang,"Yile Wang, Leyang Cui, Yue Zhang",Does Chinese BERT Encode Word Structure?,Accepted by COLING2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contextualized representations give significantly improved results for a wide
range of NLP tasks. Much work has been dedicated to analyzing the features
captured by representative models such as BERT. Existing work finds that
syntactic, semantic and word sense knowledge are encoded in BERT. However,
little work has investigated word features for character-based languages such
as Chinese. We investigate Chinese BERT using both attention weight
distribution statistics and probing tasks, finding that (1) word information is
captured by BERT; (2) word-level features are mostly in the middle
representation layers; (3) downstream tasks make different use of word features
in BERT, with POS tagging and chunking relying the most on word features, and
natural language inference relying the least on such features.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 12:40:56 GMT'}]",2020-10-16,"[['Wang', 'Yile', ''], ['Cui', 'Leyang', ''], ['Zhang', 'Yue', '']]"
1363705,2010.07375,Aaron Mueller,"Alexandra DeLucia, Aaron Mueller, Xiang Lisa Li, Jo\~ao Sedoc",Decoding Methods for Neural Narrative Generation,20 pages. Submitted to INLG 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Narrative generation is an open-ended NLP task in which a model generates a
story given a prompt. The task is similar to neural response generation for
chatbots; however, innovations in response generation are often not applied to
narrative generation, despite the similarity between these tasks. We aim to
bridge this gap by applying and evaluating advances in decoding methods for
neural response generation to neural narrative generation. In particular, we
employ GPT-2 and perform ablations across nucleus sampling thresholds and
diverse decoding hyperparameters---specifically, maximum mutual
information---analyzing results over multiple criteria with automatic and human
evaluation. We find that (1) nucleus sampling is generally best within $0.7
\leq p \leq 0.9$; (2) a maximum mutual information objective can improve the
quality of generated stories; and (3) established automatic metrics do not
correlate well with human judgments of narrative quality on any qualitative
metric.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 19:32:56 GMT'}]",2020-10-16,"[['DeLucia', 'Alexandra', ''], ['Mueller', 'Aaron', ''], ['Li', 'Xiang Lisa', ''], ['Sedoc', 'João', '']]"
1363904,2010.07574,Simon Flachs,"Simon Flachs, Oph\'elie Lacroix, Helen Yannakoudakis, Marek Rei,
  Anders S{\o}gaard","Grammatical Error Correction in Low Error Density Domains: A New
  Benchmark and Analyses",Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Evaluation of grammatical error correction (GEC) systems has primarily
focused on essays written by non-native learners of English, which however is
only part of the full spectrum of GEC applications. We aim to broaden the
target domain of GEC and release CWEB, a new benchmark for GEC consisting of
website text generated by English speakers of varying levels of proficiency.
Website data is a common and important domain that contains far fewer
grammatical errors than learner essays, which we show presents a challenge to
state-of-the-art GEC systems. We demonstrate that a factor behind this is the
inability of systems to rely on a strong internal language model in low error
density domains. We hope this work shall facilitate the development of
open-domain GEC models that generalize to different topics and genres.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 07:52:01 GMT'}]",2020-10-16,"[['Flachs', 'Simon', ''], ['Lacroix', 'Ophélie', ''], ['Yannakoudakis', 'Helen', ''], ['Rei', 'Marek', ''], ['Søgaard', 'Anders', '']]"
1358935,2010.02605,Ekaterina Artemova,"Taisia Glushkova and Alexey Machnev and Alena Fenogenova and Tatiana
  Shavrina and Ekaterina Artemova and Dmitry I. Ignatov",DaNetQA: a yes/no Question Answering Dataset for the Russian Language,"Analysis of Images, Social Networks and Texts - 9 th International
  Conference, AIST 2020, Skolkovo, Russia, October 15-16, 2020, Revised
  Selected Papers. Lecture Notes in Computer Science
  (https://dblp.org/db/series/lncs/index.html), Springer 2020",,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  DaNetQA, a new question-answering corpus, follows (Clark et. al, 2019)
design: it comprises natural yes/no questions. Each question is paired with a
paragraph from Wikipedia and an answer, derived from the paragraph. The task is
to take both the question and a paragraph as input and come up with a yes/no
answer, i.e. to produce a binary output. In this paper, we present a
reproducible approach to DaNetQA creation and investigate transfer learning
methods for task and language transferring. For task transferring we leverage
three similar sentence modelling tasks: 1) a corpus of paraphrases,
Paraphraser, 2) an NLI task, for which we use the Russian part of XNLI, 3)
another question answering task, SberQUAD. For language transferring we use
English to Russian translation together with multilingual language fine-tuning.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 10:30:48 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 10:36:06 GMT'}]",2020-10-16,"[['Glushkova', 'Taisia', ''], ['Machnev', 'Alexey', ''], ['Fenogenova', 'Alena', ''], ['Shavrina', 'Tatiana', ''], ['Artemova', 'Ekaterina', ''], ['Ignatov', 'Dmitry I.', '']]"
1364006,2010.07676,Guanhua Zhang,"Guanhua Zhang, Bing Bai, Jian Liang, Kun Bai, Conghui Zhu, Tiejun Zhao","Reliable Evaluations for Natural Language Inference based on a Unified
  Cross-dataset Benchmark",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies show that crowd-sourced Natural Language Inference (NLI)
datasets may suffer from significant biases like annotation artifacts. Models
utilizing these superficial clues gain mirage advantages on the in-domain
testing set, which makes the evaluation results over-estimated. The lack of
trustworthy evaluation settings and benchmarks stalls the progress of NLI
research. In this paper, we propose to assess a model's trustworthy
generalization performance with cross-datasets evaluation. We present a new
unified cross-datasets benchmark with 14 NLI datasets, and re-evaluate 9
widely-used neural network-based NLI models as well as 5 recently proposed
debiasing methods for annotation artifacts. Our proposed evaluation scheme and
experimental baselines could provide a basis to inspire future reliable NLI
research.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 11:50:12 GMT'}]",2020-10-16,"[['Zhang', 'Guanhua', ''], ['Bai', 'Bing', ''], ['Liang', 'Jian', ''], ['Bai', 'Kun', ''], ['Zhu', 'Conghui', ''], ['Zhao', 'Tiejun', '']]"
1363906,2010.07576,Yu Cao,"Yu Cao, Wei Bi, Meng Fang, Dacheng Tao","Pretrained Language Models for Dialogue Generation with Multiple Input
  Sources","9 pages (containing 4 pages of references and appendix), accepted to
  EMNLP2020-Findings",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale pretrained language models have achieved outstanding performance
on natural language understanding tasks. However, it is still under
investigating how to apply them to dialogue generation tasks, especially those
with responses conditioned on multiple sources. Previous work simply
concatenates all input sources or averages information from different input
sources. In this work, we study dialogue models with multiple input sources
adapted from the pretrained language model GPT2. We explore various methods to
fuse multiple separate attention information corresponding to different
sources. Our experimental results show that proper fusion methods deliver
higher relevance with dialogue history than simple fusion baselines.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 07:53:28 GMT'}]",2020-10-16,"[['Cao', 'Yu', ''], ['Bi', 'Wei', ''], ['Fang', 'Meng', ''], ['Tao', 'Dacheng', '']]"
1363936,2010.07606,Liang Li,"Liang Li, Can Ma, Yinliang Yue, Linjun Shou and Dayong Hu",Learning Better Representation for Tables by Self-Supervised Tasks,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Table-to-text generation aims at automatically generating natural text to
help people to conveniently obtain the important information in tables.
Although neural models for table-to-text have achieved remarkable progress,
some problems still overlooked. The first is that the values recorded in many
tables are mostly numbers in practice. The existing approaches do not do
special treatment for these, and still regard these as words in natural
language text. Secondly, the target texts in training dataset may contain
redundant information or facts do not exist in the input tables. These may give
wrong supervision signals to some methods based on content selection and
planning and auxiliary supervision. To solve these problems, we propose two
self-supervised tasks, Number Ordering and Significance Ordering, to help to
learn better table representation. The former works on the column dimension to
help to incorporate the size property of numbers into table representation. The
latter acts on row dimension and help to learn a significance-aware table
representation. We test our methods on the widely used dataset ROTOWIRE which
consists of NBA game statistic and related news. The experimental results
demonstrate that the model trained together with these two self-supervised
tasks can generate text that contains more salient and well-organized facts,
even without modeling context selection and planning. And we achieve the
state-of-the-art performance on automatic metrics.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 09:03:38 GMT'}]",2020-10-16,"[['Li', 'Liang', ''], ['Ma', 'Can', ''], ['Yue', 'Yinliang', ''], ['Shou', 'Linjun', ''], ['Hu', 'Dayong', '']]"
1363967,2010.07637,Yuzhao Mao,"Yuzhao Mao, Qi Sun, Guang Liu, Xiaojie Wang, Weiguo Gao, Xuan Li,
  Jianping Shen","DialogueTRM: Exploring the Intra- and Inter-Modal Emotional Behaviors in
  the Conversation",,,,,cs.CL cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Emotion Recognition in Conversations (ERC) is essential for building
empathetic human-machine systems. Existing studies on ERC primarily focus on
summarizing the context information in a conversation, however, ignoring the
differentiated emotional behaviors within and across different modalities.
Designing appropriate strategies that fit the differentiated multi-modal
emotional behaviors can produce more accurate emotional predictions. Thus, we
propose the DialogueTransformer to explore the differentiated emotional
behaviors from the intra- and inter-modal perspectives. For intra-modal, we
construct a novel Hierarchical Transformer that can easily switch between
sequential and feed-forward structures according to the differentiated context
preference within each modality. For inter-modal, we constitute a novel
Multi-Grained Interactive Fusion that applies both neuron- and vector-grained
feature interactions to learn the differentiated contributions across all
modalities. Experimental results show that DialogueTRM outperforms the
state-of-the-art by a significant margin on three benchmark datasets.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 10:10:41 GMT'}]",2020-10-16,"[['Mao', 'Yuzhao', ''], ['Sun', 'Qi', ''], ['Liu', 'Guang', ''], ['Wang', 'Xiaojie', ''], ['Gao', 'Weiguo', ''], ['Li', 'Xuan', ''], ['Shen', 'Jianping', '']]"
1363856,2010.07526,Ana Marasovi\'c,"Ana Marasovi\'c, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras,
  Noah A. Smith, Yejin Choi","Natural Language Rationales with Full-Stack Visual Reasoning: From
  Pixels to Semantic Frames to Commonsense Graphs",Accepted to Findings of EMNLP,,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language rationales could provide intuitive, higher-level
explanations that are easily understandable by humans, complementing the more
broadly studied lower-level explanations based on gradients or attention
weights. We present the first study focused on generating natural language
rationales across several complex visual reasoning tasks: visual commonsense
reasoning, visual-textual entailment, and visual question answering. The key
challenge of accurate rationalization is comprehensive image understanding at
all levels: not just their explicit content at the pixel level, but their
contextual contents at the semantic and pragmatic levels. We present
Rationale^VT Transformer, an integrated model that learns to generate free-text
rationales by combining pretrained language models with object recognition,
grounded visual semantic frames, and visual commonsense graphs. Our experiments
show that the base pretrained language model benefits from visual adaptation
and that free-text rationalization is a promising research direction to
complement model interpretability for complex visual-textual reasoning tasks.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 05:08:56 GMT'}]",2020-10-16,"[['Marasović', 'Ana', ''], ['Bhagavatula', 'Chandra', ''], ['Park', 'Jae Sung', ''], ['Bras', 'Ronan Le', ''], ['Smith', 'Noah A.', ''], ['Choi', 'Yejin', '']]"
1363995,2010.07665,Hareesh Bahuleyan,Hareesh Bahuleyan and Layla El Asri,Diverse Keyphrase Generation with Neural Unlikelihood Training,Accepted to COLING 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we study sequence-to-sequence (S2S) keyphrase generation
models from the perspective of diversity. Recent advances in neural natural
language generation have made possible remarkable progress on the task of
keyphrase generation, demonstrated through improvements on quality metrics such
as F1-score. However, the importance of diversity in keyphrase generation has
been largely ignored. We first analyze the extent of information redundancy
present in the outputs generated by a baseline model trained using maximum
likelihood estimation (MLE). Our findings show that repetition of keyphrases is
a major issue with MLE training. To alleviate this issue, we adopt neural
unlikelihood (UL) objective for training the S2S model. Our version of UL
training operates at (1) the target token level to discourage the generation of
repeating tokens; (2) the copy token level to avoid copying repetitive tokens
from the source text. Further, to encourage better model planning during the
decoding process, we incorporate K-step ahead token prediction objective that
computes both MLE and UL losses on future tokens as well. Through extensive
experiments on datasets from three different domains we demonstrate that the
proposed approach attains considerably large diversity gains, while maintaining
competitive output quality.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 11:12:26 GMT'}]",2020-10-16,"[['Bahuleyan', 'Hareesh', ''], ['Asri', 'Layla El', '']]"
1363998,2010.07668,Peng Cui,"Peng Cui, Le Hu, Yuanchao Liu","Inducing Alignment Structure with Gated Graph Attention Networks for
  Sentence Matching",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentence matching is a fundamental task of natural language processing with
various applications. Most recent approaches adopt attention-based neural
models to build word- or phrase-level alignment between two sentences. However,
these models usually ignore the inherent structure within the sentences and
fail to consider various dependency relationships among text units. To address
these issues, this paper proposes a graph-based approach for sentence matching.
First, we represent a sentence pair as a graph with several carefully design
strategies. We then employ a novel gated graph attention network to encode the
constructed graph for sentence matching. Experimental results demonstrate that
our method substantially achieves state-of-the-art performance on two datasets
across tasks of natural language and paraphrase identification. Further
discussions show that our model can learn meaningful graph structure,
indicating its superiority on improved interpretability.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 11:25:54 GMT'}]",2020-10-16,"[['Cui', 'Peng', ''], ['Hu', 'Le', ''], ['Liu', 'Yuanchao', '']]"
1363853,2010.07523,Zhengxuan Wu,"Zhengxuan Wu, Desmond C. Ong",Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect-based sentiment analysis (ABSA) and Targeted ASBA (TABSA) allow
finer-grained inferences about sentiment to be drawn from the same text,
depending on context. For example, a given text can have different targets
(e.g., neighborhoods) and different aspects (e.g., price or safety), with
different sentiment associated with each target-aspect pair. In this paper, we
investigate whether adding context to self-attention models improves
performance on (T)ABSA. We propose two variants of Context-Guided BERT
(CG-BERT) that learn to distribute attention under different contexts. We first
adapt a context-aware Transformer to produce a CG-BERT that uses context-guided
softmax-attention. Next, we propose an improved Quasi-Attention CG-BERT model
that learns a compositional attention that supports subtractive attention. We
train both models with pretrained BERT on two (T)ABSA datasets: SentiHood and
SemEval-2014 (Task 4). Both models achieve new state-of-the-art results with
our QACG-BERT model having the best performance. Furthermore, we provide
analyses of the impact of context in the our proposed models. Our work provides
more evidence for the utility of adding context-dependencies to pretrained
self-attention-based language models for context-based natural language tasks.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 05:01:20 GMT'}]",2020-10-16,"[['Wu', 'Zhengxuan', ''], ['Ong', 'Desmond C.', '']]"
1363852,2010.07522,Youmi Ma,"Youmi Ma, Tatsuya Hiraoka, Naoaki Okazaki","Named Entity Recognition and Relation Extraction using Enhanced Table
  Filling by Contextualized Representations",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this study, a novel method for extracting named entities and relations
from unstructured text based on the table representation is presented. By using
contextualized word embeddings, the proposed method computes representations
for entity mentions and long-range dependencies without complicated
hand-crafted features or neural-network architectures. We also adapt a tensor
dot-product to predict relation labels all at once without resorting to
history-based predictions or search strategies. These advances significantly
simplify the model and algorithm for the extraction of named entities and
relations. Despite its simplicity, the experimental results demonstrate that
the proposed method outperforms the state-of-the-art methods on the CoNLL04 and
ACE05 English datasets. We also confirm that the proposed method achieves a
comparable performance with the state-of-the-art NER models on the ACE05
datasets when multiple sentences are provided for context aggregation.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 04:58:23 GMT'}]",2020-10-16,"[['Ma', 'Youmi', ''], ['Hiraoka', 'Tatsuya', ''], ['Okazaki', 'Naoaki', '']]"
1363968,2010.07638,Prathyusha Jwalapuram,"Prathyusha Jwalapuram, Shafiq Joty, Youlin Shen",Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Popular Neural Machine Translation model training uses strategies like
backtranslation to improve BLEU scores, requiring large amounts of additional
data and training. We introduce a class of conditional
generative-discriminative hybrid losses that we use to fine-tune a trained
machine translation model. Through a combination of targeted fine-tuning
objectives and intuitive re-use of the training data the model has failed to
adequately learn from, we improve the model performance of both a
sentence-level and a contextual model without using any additional data. We
target the improvement of pronoun translations through our fine-tuning and
evaluate our models on a pronoun benchmark testset. Our sentence-level model
shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets,
while our contextual model achieves the best results, improving from 31.81 to
32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En
testset, with corresponding improvements in pronoun translation. We further
show the generalizability of our method by reproducing the improvements on two
additional language pairs, Fr-En and Cs-En. Code available at
<https://github.com/ntunlp/pronoun-finetuning>.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 10:11:40 GMT'}]",2020-10-16,"[['Jwalapuram', 'Prathyusha', ''], ['Joty', 'Shafiq', ''], ['Shen', 'Youlin', '']]"
1363873,2010.07543,Yuanhe Tian,"Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang",Improving Constituency Parsing with Span Attention,"Natural Language Processing. 13 pages, 6 figures. Findings of
  EMNLP-2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Constituency parsing is a fundamental and important task for natural language
understanding, where a good representation of contextual information can help
this task. N-grams, which is a conventional type of feature for contextual
information, have been demonstrated to be useful in many tasks, and thus could
also be beneficial for constituency parsing if they are appropriately modeled.
In this paper, we propose span attention for neural chart-based constituency
parsing to leverage n-gram information. Considering that current chart-based
parsers with Transformer-based encoder represent spans by subtraction of the
hidden states at the span boundaries, which may cause information loss
especially for long spans, we incorporate n-grams into span representations by
weighting them according to their contributions to the parsing process.
Moreover, we propose categorical span attention to further enhance the model by
weighting n-grams within different length categories, and thus benefit
long-sentence parsing. Experimental results on three widely used benchmark
datasets demonstrate the effectiveness of our approach in parsing Arabic,
Chinese, and English, where state-of-the-art performance is obtained by our
approach on all of them.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 06:36:39 GMT'}]",2020-10-16,"[['Tian', 'Yuanhe', ''], ['Song', 'Yan', ''], ['Xia', 'Fei', ''], ['Zhang', 'Tong', '']]"
1100246,1903.07860,Guangneng Hu,Guangneng Hu,Personalized Neural Embeddings for Collaborative Filtering with Text,"NAACL 2019 short papers, oral presentation",NAACL 2019,,,cs.IR cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Collaborative filtering (CF) is a core technique for recommender systems.
Traditional CF approaches exploit user-item relations (e.g., clicks, likes, and
views) only and hence they suffer from the data sparsity issue. Items are
usually associated with unstructured text such as article abstracts and product
reviews. We develop a Personalized Neural Embedding (PNE) framework to exploit
both interactions and words seamlessly. We learn such embeddings of users,
items, and words jointly, and predict user preferences on items based on these
learned representations. PNE estimates the probability that a user will like an
item by two terms---behavior factors and semantic factors. On two real-world
datasets, PNE shows better performance than four state-of-the-art baselines in
terms of three metrics. We also show that PNE learns meaningful word embeddings
by visualization.
","[{'version': 'v1', 'created': 'Tue, 19 Mar 2019 07:05:59 GMT'}]",2020-10-19,"[['Hu', 'Guangneng', '']]"
1341932,2009.00429,Jean-Marc Luck,"Anita Mehta, Jean-Marc Luck",Hearings and mishearings: decrypting the spoken word,"21 pages, 4 figures, 3 tables. To appear in Advances in Complex
  Systems","Adv. Complex Systems 23, 2050008 (2020)",10.1142/S0219525920500083,,cs.CL cond-mat.stat-mech,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a model of the speech perception of individual words in the
presence of mishearings. This phenomenological approach is based on concepts
used in linguistics, and provides a formalism that is universal across
languages. We put forward an efficient two-parameter form for the word length
distribution, and introduce a simple representation of mishearings, which we
use in our subsequent modelling of word recognition. In a context-free
scenario, word recognition often occurs via anticipation when, part-way into a
word, we can correctly guess its full form. We give a quantitative estimate of
this anticipation threshold when no mishearings occur, in terms of model
parameters. As might be expected, the whole anticipation effect disappears when
there are sufficiently many mishearings. Our global approach to the problem of
speech perception is in the spirit of an optimisation problem. We show for
instance that speech perception is easy when the word length is less than a
threshold, to be identified with a static transition, and hard otherwise. We
extend this to the dynamics of word recognition, proposing an intuitive
approach highlighting the distinction between individual, isolated mishearings
and clusters of contiguous mishearings. At least in some parameter range, a
dynamical transition is manifest well before the static transition is reached,
as is the case for many other examples of complex systems.
","[{'version': 'v1', 'created': 'Tue, 1 Sep 2020 13:58:51 GMT'}]",2020-10-19,"[['Mehta', 'Anita', ''], ['Luck', 'Jean-Marc', '']]"
1299072,2006.04558,Yi Ren,"Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu",FastSpeech 2: Fast and High-Quality End-to-End Text to Speech,,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Non-autoregressive text to speech (TTS) models such as FastSpeech can
synthesize speech significantly faster than previous autoregressive models with
comparable quality. The training of FastSpeech model relies on an
autoregressive teacher model for duration prediction (to provide more
information as input) and knowledge distillation (to simplify the data
distribution in output), which can ease the one-to-many mapping problem (i.e.,
multiple speech variations correspond to the same text) in TTS. However,
FastSpeech has several disadvantages: 1) the teacher-student distillation
pipeline is complicated and time-consuming, 2) the duration extracted from the
teacher model is not accurate enough, and the target mel-spectrograms distilled
from teacher model suffer from information loss due to data simplification,
both of which limit the voice quality. In this paper, we propose FastSpeech 2,
which addresses the issues in FastSpeech and better solves the one-to-many
mapping problem in TTS by 1) directly training the model with ground-truth
target instead of the simplified output from teacher, and 2) introducing more
variation information of speech (e.g., pitch, energy and more accurate
duration) as conditional inputs. Specifically, we extract duration, pitch and
energy from speech waveform and directly take them as conditional inputs in
training and use predicted values in inference. We further design FastSpeech
2s, which is the first attempt to directly generate speech waveform from text
in parallel, enjoying the benefit of fully end-to-end inference. Experimental
results show that 1) FastSpeech 2 achieves a 3x training speed-up over
FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech
2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even
surpass autoregressive models. Audio samples are available at
https://fastspeech2.github.io/fastspeech2/.
","[{'version': 'v1', 'created': 'Mon, 8 Jun 2020 13:05:40 GMT'}, {'version': 'v2', 'created': 'Tue, 9 Jun 2020 09:33:54 GMT'}, {'version': 'v3', 'created': 'Mon, 22 Jun 2020 05:30:06 GMT'}, {'version': 'v4', 'created': 'Fri, 16 Oct 2020 14:34:02 GMT'}]",2020-10-19,"[['Ren', 'Yi', ''], ['Hu', 'Chenxu', ''], ['Tan', 'Xu', ''], ['Qin', 'Tao', ''], ['Zhao', 'Sheng', ''], ['Zhao', 'Zhou', ''], ['Liu', 'Tie-Yan', '']]"
1341259,2008.13537,He Zhao,"He Zhao, Dinh Phung, Viet Huynh, Trung Le, Wray Buntine",Neural Topic Model via Optimal Transport,,,,,cs.IR cs.CL cs.LG stat.ML,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recently, Neural Topic Models (NTMs) inspired by variational autoencoders
have obtained increasingly research interest due to their promising results on
text analysis. However, it is usually hard for existing NTMs to achieve good
document representation and coherent/diverse topics at the same time. Moreover,
they often degrade their performance severely on short documents. The
requirement of reparameterisation could also comprise their training quality
and model flexibility. To address these shortcomings, we present a new neural
topic model via the theory of optimal transport (OT). Specifically, we propose
to learn the topic distribution of a document by directly minimising its OT
distance to the document's word distributions. Importantly, the cost matrix of
the OT distance models the weights between topics and words, which is
constructed by the distances between topics and words in an embedding space.
Our proposed model can be trained efficiently with a differentiable loss.
Extensive experiments show that our framework significantly outperforms the
state-of-the-art NTMs on discovering more coherent and diverse topics and
deriving better document representations for both regular and short texts.
","[{'version': 'v1', 'created': 'Wed, 12 Aug 2020 06:37:09 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 01:49:09 GMT'}]",2020-10-19,"[['Zhao', 'He', ''], ['Phung', 'Dinh', ''], ['Huynh', 'Viet', ''], ['Le', 'Trung', ''], ['Buntine', 'Wray', '']]"
1358168,2010.01838,Yifan Gao,"Yifan Gao, Chien-Sheng Wu, Jingjing Li, Shafiq Joty, Steven C.H. Hoi,
  Caiming Xiong, Irwin King, Michael R. Lyu","Discern: Discourse-Aware Entailment Reasoning Network for Conversational
  Machine Reading","EMNLP 2020 main conference, 11 pages, 3 Figures",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Document interpretation and dialog understanding are the two major challenges
for conversational machine reading. In this work, we propose Discern, a
discourse-aware entailment reasoning network to strengthen the connection and
enhance the understanding for both document and dialog. Specifically, we split
the document into clause-like elementary discourse units (EDU) using a
pre-trained discourse segmentation model, and we train our model in a
weakly-supervised manner to predict whether each EDU is entailed by the user
feedback in a conversation. Based on the learned EDU and entailment
representations, we either reply to the user our final decision
""yes/no/irrelevant"" of the initial question, or generate a follow-up question
to inquiry more information. Our experiments on the ShARC benchmark (blind,
held-out test set) show that Discern achieves state-of-the-art results of 78.3%
macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question
generation. Code and models are released at
https://github.com/Yifan-Gao/Discern.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 07:49:51 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 07:16:32 GMT'}, {'version': 'v3', 'created': 'Fri, 16 Oct 2020 10:06:46 GMT'}]",2020-10-19,"[['Gao', 'Yifan', ''], ['Wu', 'Chien-Sheng', ''], ['Li', 'Jingjing', ''], ['Joty', 'Shafiq', ''], ['Hoi', 'Steven C. H.', ''], ['Xiong', 'Caiming', ''], ['King', 'Irwin', ''], ['Lyu', 'Michael R.', '']]"
1297716,2006.03202,Sharon Levy,Sharon Levy and William Yang Wang,Cross-lingual Transfer Learning for COVID-19 Outbreak Alignment,,,,,cs.CL cs.LG cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The spread of COVID-19 has become a significant and troubling aspect of
society in 2020. With millions of cases reported across countries, new
outbreaks have occurred and followed patterns of previously affected areas.
Many disease detection models do not incorporate the wealth of social media
data that can be utilized for modeling and predicting its spread. In this case,
it is useful to ask, can we utilize this knowledge in one country to model the
outbreak in another? To answer this, we propose the task of cross-lingual
transfer learning for epidemiological alignment. Utilizing both macro and micro
text features, we train on Italy's early COVID-19 outbreak through Twitter and
transfer to several other countries. Our experiments show strong results with
up to 0.85 Spearman correlation in cross-country predictions.
","[{'version': 'v1', 'created': 'Fri, 5 Jun 2020 02:04:25 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 22:37:43 GMT'}]",2020-10-19,"[['Levy', 'Sharon', ''], ['Wang', 'William Yang', '']]"
1350069,2009.08566,Tejas Gokhale,Tejas Gokhale and Pratyay Banerjee and Chitta Baral and Yezhou Yang,"MUTANT: A Training Paradigm for Out-of-Distribution Generalization in
  Visual Question Answering","Accepted to EMNLP 2020, Long Papers",,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While progress has been made on the visual question answering leaderboards,
models often utilize spurious correlations and priors in datasets under the
i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples
has emerged as a proxy for generalization. In this paper, we present MUTANT, a
training paradigm that exposes the model to perceptually similar, yet
semantically distinct mutations of the input, to improve OOD generalization,
such as the VQA-CP challenge. Under this paradigm, models utilize a
consistency-constrained training objective to understand the effect of semantic
changes in input (question-image pair) on the output (answer). Unlike existing
methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of
train and test answer distributions. MUTANT establishes a new state-of-the-art
accuracy on VQA-CP with a $10.57\%$ improvement. Our work opens up avenues for
the use of semantic input mutations for OOD generalization in question
answering.
","[{'version': 'v1', 'created': 'Fri, 18 Sep 2020 00:22:54 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 01:53:08 GMT'}]",2020-10-19,"[['Gokhale', 'Tejas', ''], ['Banerjee', 'Pratyay', ''], ['Baral', 'Chitta', ''], ['Yang', 'Yezhou', '']]"
1354605,2009.13102,Xian Li,"Xian Li, Asa Cooper Stickland, Yuqing Tang, and Xiang Kong",Deep Transformers with Latent Depth,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Transformer model has achieved state-of-the-art performance in many
sequence modeling tasks. However, how to leverage model capacity with large or
variable depths is still an open challenge. We present a probabilistic
framework to automatically learn which layer(s) to use by learning the
posterior distributions of layer selection. As an extension of this framework,
we propose a novel method to train one shared Transformer network for
multilingual machine translation with different layer selection posteriors for
each language pair. The proposed method alleviates the vanishing gradient issue
and enables stable training of deep Transformers (e.g. 100 layers). We evaluate
on WMT English-German machine translation and masked language modeling tasks,
where our method outperforms existing approaches for training deeper
Transformers. Experiments on multilingual machine translation demonstrate that
this approach can effectively leverage increased model capacity and bring
universal improvement for both many-to-one and one-to-many translation with
diverse language pairs.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 07:13:23 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 03:50:56 GMT'}]",2020-10-19,"[['Li', 'Xian', ''], ['Stickland', 'Asa Cooper', ''], ['Tang', 'Yuqing', ''], ['Kong', 'Xiang', '']]"
1358705,2010.02375,Robert Hawkins,"Robert D. Hawkins, Takateru Yamakoshi, Thomas L. Griffiths, Adele E.
  Goldberg",Investigating representations of verb bias in neural language models,Accepted to EMNLP,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Languages typically provide more than one grammatical construction to express
certain types of messages. A speaker's choice of construction is known to
depend on multiple factors, including the choice of main verb -- a phenomenon
known as \emph{verb bias}. Here we introduce DAIS, a large benchmark dataset
containing 50K human judgments for 5K distinct sentence pairs in the English
dative alternation. This dataset includes 200 unique verbs and systematically
varies the definiteness and length of arguments. We use this dataset, as well
as an existing corpus of naturally occurring data, to evaluate how well recent
neural language models capture human preferences. Results show that larger
models perform better than smaller models, and transformer architectures (e.g.
GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under
comparable parameter and training settings. Additional analyses of internal
feature representations suggest that transformers may better integrate specific
lexical information with grammatical constructions.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 22:39:08 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 19:37:48 GMT'}]",2020-10-19,"[['Hawkins', 'Robert D.', ''], ['Yamakoshi', 'Takateru', ''], ['Griffiths', 'Thomas L.', ''], ['Goldberg', 'Adele E.', '']]"
1180389,1909.10351,Yichun Yin,"Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin
  Li, Fang Wang and Qun Liu",TinyBERT: Distilling BERT for Natural Language Understanding,"Findings of EMNLP 2020; results have been updated; code and model:
  https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language model pre-training, such as BERT, has significantly improved the
performances of many natural language processing tasks. However, pre-trained
language models are usually computationally expensive, so it is difficult to
efficiently execute them on resource-restricted devices. To accelerate
inference and reduce model size while maintaining accuracy, we first propose a
novel Transformer distillation method that is specially designed for knowledge
distillation (KD) of the Transformer-based models. By leveraging this new KD
method, the plenty of knowledge encoded in a large teacher BERT can be
effectively transferred to a small student Tiny-BERT. Then, we introduce a new
two-stage learning framework for TinyBERT, which performs Transformer
distillation at both the pretraining and task-specific learning stages. This
framework ensures that TinyBERT can capture he general-domain as well as the
task-specific knowledge in BERT.
  TinyBERT with 4 layers is empirically effective and achieves more than 96.8%
the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x
smaller and 9.4x faster on inference. TinyBERT with 4 layers is also
significantly better than 4-layer state-of-the-art baselines on BERT
distillation, with only about 28% parameters and about 31% inference time of
them. Moreover, TinyBERT with 6 layers performs on-par with its teacher
BERTBASE.
","[{'version': 'v1', 'created': 'Mon, 23 Sep 2019 13:05:35 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Sep 2019 12:39:36 GMT'}, {'version': 'v3', 'created': 'Tue, 3 Dec 2019 01:29:39 GMT'}, {'version': 'v4', 'created': 'Wed, 4 Dec 2019 01:50:34 GMT'}, {'version': 'v5', 'created': 'Fri, 16 Oct 2020 02:12:46 GMT'}]",2020-10-19,"[['Jiao', 'Xiaoqi', ''], ['Yin', 'Yichun', ''], ['Shang', 'Lifeng', ''], ['Jiang', 'Xin', ''], ['Chen', 'Xiao', ''], ['Li', 'Linlin', ''], ['Wang', 'Fang', ''], ['Liu', 'Qun', '']]"
1308060,2006.13546,Stefan Heinrich,"Stefan Heinrich, Yuan Yao, Tobias Hinz, Zhiyuan Liu, Thomas Hummel,
  Matthias Kerzel, Cornelius Weber, and Stefan Wermter",Crossmodal Language Grounding in an Embodied Neurocognitive Model,,"Frontiers in Neurorobotics, vol 14(52), 2020",10.3389/fnbot.2020.00052,,cs.NE cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Human infants are able to acquire natural language seemingly easily at an
early age. Their language learning seems to occur simultaneously with learning
other cognitive functions as well as with playful interactions with the
environment and caregivers. From a neuroscientific perspective, natural
language is embodied, grounded in most, if not all, sensory and sensorimotor
modalities, and acquired by means of crossmodal integration. However,
characterising the underlying mechanisms in the brain is difficult and
explaining the grounding of language in crossmodal perception and action
remains challenging. In this paper, we present a neurocognitive model for
language grounding which reflects bio-inspired mechanisms such as an implicit
adaptation of timescales as well as end-to-end multimodal abstraction. It
addresses developmental robotic interaction and extends its learning
capabilities using larger-scale knowledge-based data. In our scenario, we
utilise the humanoid robot NICO in obtaining the EMIL data collection, in which
the cognitive robot interacts with objects in a children's playground
environment while receiving linguistic labels from a caregiver. The model
analysis shows that crossmodally integrated representations are sufficient for
acquiring language merely from sensory input through interaction with objects
in an environment. The representations self-organise hierarchically and embed
temporal and spatial information through composition and decomposition. This
model can also provide the basis for further crossmodal integration of
perceptually grounded cognitive representations.
","[{'version': 'v1', 'created': 'Wed, 24 Jun 2020 08:12:09 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 08:27:34 GMT'}]",2020-10-19,"[['Heinrich', 'Stefan', ''], ['Yao', 'Yuan', ''], ['Hinz', 'Tobias', ''], ['Liu', 'Zhiyuan', ''], ['Hummel', 'Thomas', ''], ['Kerzel', 'Matthias', ''], ['Weber', 'Cornelius', ''], ['Wermter', 'Stefan', '']]"
1359966,2010.03636,Anthony Chen,"Anthony Chen, Gabriel Stanovsky, Sameer Singh and Matt Gardner","MOCHA: A Dataset for Training and Evaluating Generative Reading
  Comprehension Metrics",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Posing reading comprehension as a generation problem provides a great deal of
flexibility, allowing for open-ended questions with few restrictions on
possible answers. However, progress is impeded by existing generation metrics,
which rely on token overlap and are agnostic to the nuances of reading
comprehension. To address this, we introduce a benchmark for training and
evaluating generative reading comprehension metrics: MOdeling Correctness with
Human Annotations. MOCHA contains 40K human judgement scores on model outputs
from 6 diverse question answering datasets and an additional set of minimal
pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for
Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms
baseline metrics by 10 to 36 absolute Pearson points on held-out annotations.
When we evaluate robustness on minimal pairs, LERC achieves 80% accuracy,
outperforming baselines by 14 to 26 absolute percentage points while leaving
significant room for improvement. MOCHA presents a challenging problem for
developing accurate and robust generative reading comprehension metrics.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 20:22:54 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 18:23:18 GMT'}]",2020-10-19,"[['Chen', 'Anthony', ''], ['Stanovsky', 'Gabriel', ''], ['Singh', 'Sameer', ''], ['Gardner', 'Matt', '']]"
1346468,2009.04965,Meng-Jiun Chiou,"Meng-Jiun Chiou, Roger Zimmermann, Jiashi Feng","Visual Relationship Detection with Visual-Linguistic Knowledge from
  Multimodal Representations","10 pages, 5 figures, 4 tables",,,,cs.CV cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Visual relationship detection aims to reason over relationships among salient
objects in images, which has drawn increasing attention over the past few
years. Inspired by human reasoning mechanism, it is believed that external
visual commonsense knowledge is beneficial for reasoning visual relationships
of objects in images, which is however rarely considered in existing methods.
In this paper, we propose a novel approach named Relational Visual-Linguistic
Bidirectional Encoder Representations from Transformers (RVL-BERT), which
performs relational reasoning with both visual and language commonsense
knowledge learned via self-supervised pre-training with multimodal
representations. RVL-BERT also uses an effective spatial module and a novel
mask attention module to explicitly capture spatial information among the
objects. Moreover, our model decouples object detection from visual
relationship recognition by taking in object names directly, enabling it to be
used on top of any object detection system. We show through quantitative and
qualitative experiments that, with the transferred knowledge and novel modules,
RVL-BERT achieves competitive results on two challenging visual relationship
detection datasets. The source code will be publicly available soon.
","[{'version': 'v1', 'created': 'Thu, 10 Sep 2020 16:15:09 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 14:01:49 GMT'}]",2020-10-19,"[['Chiou', 'Meng-Jiun', ''], ['Zimmermann', 'Roger', ''], ['Feng', 'Jiashi', '']]"
1279108,2004.14338,Jack Hessel,"Jack Hessel, Zhenhai Zhu, Bo Pang, Radu Soricut","Beyond Instructional Videos: Probing for More Diverse Visual-Textual
  Grounding on YouTube",11 pages including supplementary materials,Published in EMNLP 2020,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretraining from unlabelled web videos has quickly become the de-facto means
of achieving high performance on many video understanding tasks. Features are
learned via prediction of grounded relationships between visual content and
automatic speech recognition (ASR) tokens. However, prior pretraining work has
been limited to only instructional videos; a priori, we expect this domain to
be relatively ""easy:"" speakers in instructional videos will often reference the
literal objects/actions being depicted. We ask: can similar models be trained
on more diverse video corpora? And, if so, what types of videos are ""grounded""
and what types are not? We fit a representative pretraining model to the
diverse YouTube8M dataset, and study its success and failure cases. We find
that visual-textual grounding is indeed possible across previously unexplored
video categories, and that pretraining on a more diverse set results in
representations that generalize to both non-instructional and instructional
domains.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 17:10:10 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 17:30:51 GMT'}]",2020-10-19,"[['Hessel', 'Jack', ''], ['Zhu', 'Zhenhai', ''], ['Pang', 'Bo', ''], ['Soricut', 'Radu', '']]"
1364317,2010.07987,Nadezhda Chirkova,"Nadezhda Chirkova, Sergey Troshin",Empirical Study of Transformers for Source Code,,,,,cs.LG cs.CL cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Initially developed for natural language processing (NLP), Transformers are
now widely used for source code processing, due to the format similarity
between source code and text. In contrast to natural language, source code is
strictly structured, i. e. follows the syntax of the programming language.
Several recent works develop Transformer modifications for capturing syntactic
information in source code. The drawback of these works is that they do not
compare to each other and all consider different tasks. In this work, we
conduct a thorough empirical study of the capabilities of Transformers to
utilize syntactic information in different tasks. We consider three tasks (code
completion, function naming and bug fixing) and re-implement different
syntax-capturing modifications in a unified framework. We show that
Transformers are able to make meaningful predictions based purely on syntactic
information and underline the best practices of taking the syntactic
information into account for improving the performance of the model.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 19:09:15 GMT'}]",2020-10-19,"[['Chirkova', 'Nadezhda', ''], ['Troshin', 'Sergey', '']]"
1364653,2010.08323,Kuldeep Singh,"Saeedeh Shekarpour, Abhishek Nadgeri and Kuldeep Singh","QA2Explanation: Generating and Evaluating Explanations for Question
  Answering Systems over Knowledge Graph","Accepted in IntEx-SemPar: Interactive and Executable Semantic Parsing
  EMNLP 2020 Workshop",EMNLP2020 Workshop:IntEx-SemPar,10.18653/v1/P17,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the era of Big Knowledge Graphs, Question Answering (QA) systems have
reached a milestone in their performance and feasibility. However, their
applicability, particularly in specific domains such as the biomedical domain,
has not gained wide acceptance due to their ""black box"" nature, which hinders
transparency, fairness, and accountability of QA systems. Therefore, users are
unable to understand how and why particular questions have been answered,
whereas some others fail. To address this challenge, in this paper, we develop
an automatic approach for generating explanations during various stages of a
pipeline-based QA system. Our approach is a supervised and automatic approach
which considers three classes (i.e., success, no answer, and wrong answer) for
annotating the output of involved QA components. Upon our prediction, a
template explanation is chosen and integrated into the output of the
corresponding component. To measure the effectiveness of the approach, we
conducted a user survey as to how non-expert users perceive our generated
explanations. The results of our study show a significant increase in the four
dimensions of the human factor from the Human-computer interaction community.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 11:32:12 GMT'}]",2020-10-19,"[['Shekarpour', 'Saeedeh', ''], ['Nadgeri', 'Abhishek', ''], ['Singh', 'Kuldeep', '']]"
967894,1804.06201,Herbert Hu,"Guangneng Hu, Yu Zhang, Qiang Yang","LCMR: Local and Centralized Memories for Collaborative Filtering with
  Unstructured Text",,,,,cs.IR cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Collaborative filtering (CF) is the key technique for recommender systems.
Pure CF approaches exploit the user-item interaction data (e.g., clicks, likes,
and views) only and suffer from the sparsity issue. Items are usually
associated with content information such as unstructured text (e.g., abstracts
of articles and reviews of products). CF can be extended to leverage text. In
this paper, we develop a unified neural framework to exploit interaction data
and content information seamlessly. The proposed framework, called LCMR, is
based on memory networks and consists of local and centralized memories for
exploiting content information and interaction data, respectively. By modeling
content information as local memories, LCMR attentively learns what to exploit
with the guidance of user-item interaction. On real-world datasets, LCMR shows
better performance by comparing with various baselines in terms of the hit
ratio and NDCG metrics. We further conduct analyses to understand how local and
centralized memories work for the proposed framework.
","[{'version': 'v1', 'created': 'Tue, 17 Apr 2018 12:32:23 GMT'}, {'version': 'v2', 'created': 'Fri, 20 Apr 2018 16:23:00 GMT'}]",2020-10-19,"[['Hu', 'Guangneng', ''], ['Zhang', 'Yu', ''], ['Yang', 'Qiang', '']]"
1364572,2010.08242,Shusheng Xu,"Shusheng Xu, Xingxing Zhang, Yi Wu, Furu Wei and Ming Zhou","Unsupervised Extractive Summarization by Pre-training Hierarchical
  Transformers","9 pages, 4 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unsupervised extractive document summarization aims to select important
sentences from a document without using labeled summaries during training.
Existing methods are mostly graph-based with sentences as nodes and edge
weights measured by sentence similarities. In this work, we find that
transformer attentions can be used to rank sentences for unsupervised
extractive summarization. Specifically, we first pre-train a hierarchical
transformer model using unlabeled documents only. Then we propose a method to
rank sentences using sentence-level self-attentions and pre-training
objectives. Experiments on CNN/DailyMail and New York Times datasets show our
model achieves state-of-the-art performance on unsupervised summarization. We
also find in experiments that our model is less dependent on sentence
positions. When using a linear combination of our model and a recent
unsupervised model explicitly modeling sentence positions, we obtain even
better results.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 08:44:09 GMT'}]",2020-10-19,"[['Xu', 'Shusheng', ''], ['Zhang', 'Xingxing', ''], ['Wu', 'Yi', ''], ['Wei', 'Furu', ''], ['Zhou', 'Ming', '']]"
1364595,2010.08265,Qiang Wang,"Qiang Wang, Tong Xiao, Jingbo Zhu","Training Flexible Depth Model by Multi-Task Learning for Neural Machine
  Translation",Accepted at Findings of EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The standard neural machine translation model can only decode with the same
depth configuration as training. Restricted by this feature, we have to deploy
models of various sizes to maintain the same translation latency, because the
hardware conditions on different terminal devices (e.g., mobile phones) may
vary greatly. Such individual training leads to increased model maintenance
costs and slower model iterations, especially for the industry. In this work,
we propose to use multi-task learning to train a flexible depth model that can
adapt to different depth configurations during inference. Experimental results
show that our approach can simultaneously support decoding in 24 depth
configurations and is superior to the individual training and another flexible
depth model training method -- LayerDrop.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 09:37:27 GMT'}]",2020-10-19,"[['Wang', 'Qiang', ''], ['Xiao', 'Tong', ''], ['Zhu', 'Jingbo', '']]"
1364599,2010.08269,Mark Berger,"Mark Berger, Jakub Zavrel, Paul Groth",Effective Distributed Representations for Academic Expert Search,"To be published in the Scholarly Document Processing 2020 Workshop @
  EMNLP 2020 proceedings",,,,cs.IR cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Expert search aims to find and rank experts based on a user's query. In
academia, retrieving experts is an efficient way to navigate through a large
amount of academic knowledge. Here, we study how different distributed
representations of academic papers (i.e. embeddings) impact academic expert
retrieval. We use the Microsoft Academic Graph dataset and experiment with
different configurations of a document-centric voting model for retrieval. In
particular, we explore the impact of the use of contextualized embeddings on
search performance. We also present results for paper embeddings that
incorporate citation information through retrofitting. Additionally,
experiments are conducted using different techniques for assigning author
weights based on author order. We observe that using contextual embeddings
produced by a transformer model trained for sentence similarity tasks produces
the most effective paper representations for document-centric expert retrieval.
However, retrofitting the paper embeddings and using elaborate author
contribution weighting strategies did not improve retrieval performance.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 09:43:18 GMT'}]",2020-10-19,"[['Berger', 'Mark', ''], ['Zavrel', 'Jakub', ''], ['Groth', 'Paul', '']]"
1364605,2010.08275,Hila Gonen,"Hila Gonen, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg","It's not Greek to mBERT: Inducing Word-Level Translations from
  Multilingual BERT",BlackboxNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent works have demonstrated that multilingual BERT (mBERT) learns rich
cross-lingual representations, that allow for transfer across languages. We
study the word-level translation information embedded in mBERT and present two
simple methods that expose remarkable translation capabilities with no
fine-tuning. The results suggest that most of this information is encoded in a
non-linear way, while some of it can also be recovered with purely linear
tools. As part of our analysis, we test the hypothesis that mBERT learns
representations which contain both a language-encoding component and an
abstract, cross-lingual component, and explicitly identify an empirical
language-identity subspace within mBERT representations.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 09:49:32 GMT'}]",2020-10-19,"[['Gonen', 'Hila', ''], ['Ravfogel', 'Shauli', ''], ['Elazar', 'Yanai', ''], ['Goldberg', 'Yoav', '']]"
1364648,2010.08318,Andrew Moore,Andrew Moore and Jeremy Barnes,"Multi-task Learning of Negation and Speculation for Targeted Sentiment
  Classification",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The majority of work in targeted sentiment analysis has concentrated on
finding better methods to improve the overall results. Within this paper we
show that these models are not robust to linguistic phenomena, specifically
negation and speculation. In this paper, we propose a multi-task learning
method to incorporate information from syntactic and semantic auxiliary tasks,
including negation and speculation scope detection, to create models that are
more robust to these phenomena. Further we create two challenge datasets to
evaluate model performance on negated and speculative samples. We find that
multi-task models and transfer learning from a language model can improve
performance on these challenge datasets. However the results indicate that
there is still much room for improvement in making our models more robust to
linguistic phenomena such as negation and speculation.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 11:20:03 GMT'}]",2020-10-19,"[['Moore', 'Andrew', ''], ['Barnes', 'Jeremy', '']]"
1364649,2010.08319,Tim Nugent,"Tim Nugent, Nicole Stelea and Jochen L. Leidner","Detecting ESG topics using domain-specific language models and data
  augmentation approaches","11 pages, 5 tables, 1 figure",,,,cs.CL cs.IR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Despite recent advances in deep learning-based language modelling, many
natural language processing (NLP) tasks in the financial domain remain
challenging due to the paucity of appropriately labelled data. Other issues
that can limit task performance are differences in word distribution between
the general corpora - typically used to pre-train language models - and
financial corpora, which often exhibit specialized language and symbology.
Here, we investigate two approaches that may help to mitigate these issues.
Firstly, we experiment with further language model pre-training using large
amounts of in-domain data from business and financial news. We then apply
augmentation approaches to increase the size of our dataset for model
fine-tuning. We report our findings on an Environmental, Social and Governance
(ESG) controversies dataset and demonstrate that both approaches are beneficial
to accuracy in classification tasks.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 11:20:07 GMT'}]",2020-10-19,"[['Nugent', 'Tim', ''], ['Stelea', 'Nicole', ''], ['Leidner', 'Jochen L.', '']]"
1279640,2004.14870,Samson Tan,"Samson Tan, Shafiq Joty, Lav R. Varshney, Min-Yen Kan","Mind Your Inflections! Improving NLP for Non-Standard Englishes with
  Base-Inflection Encoding","To appear in the Proceedings of the 2020 Conference on Empirical
  Methods in Natural Language Processing",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inflectional variation is a common feature of World Englishes such as
Colloquial Singapore English and African American Vernacular English. Although
comprehension by human readers is usually unimpaired by non-standard
inflections, current NLP systems are not yet robust. We propose Base-Inflection
Encoding (BITE), a method to tokenize English text by reducing inflected words
to their base forms before reinjecting the grammatical information as special
symbols. Fine-tuning pretrained NLP models for downstream tasks using our
encoding defends against inflectional adversaries while maintaining performance
on clean data. Models using BITE generalize better to dialects with
non-standard inflections without explicit training and translation models
converge faster when trained with BITE. Finally, we show that our encoding
improves the vocabulary efficiency of popular data-driven subword tokenizers.
Since there has been no prior work on quantitatively evaluating vocabulary
efficiency, we propose metrics to do so.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 15:15:40 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Oct 2020 18:54:40 GMT'}, {'version': 'v3', 'created': 'Fri, 16 Oct 2020 05:20:28 GMT'}]",2020-10-19,"[['Tan', 'Samson', ''], ['Joty', 'Shafiq', ''], ['Varshney', 'Lav R.', ''], ['Kan', 'Min-Yen', '']]"
1364676,2010.08346,Vili Hatonen,"Vili H\""at\""onen and Fiona Melzer","From Talk to Action with Accountability: Monitoring the Public
  Discussion of Finnish Decision-Makers with Deep Neural Networks and Topic
  Modelling","Submitted to NeurIPS 2020 Workshop Tackling Climate Change with
  Machine Learning",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Decades of research on climate have provided a consensus that human activity
has changed the climate and we are currently heading into a climate crisis.
Many tools and methods, some of which utilize machine learning, have been
developed to monitor, evaluate, and predict the changing climate and its
effects on societies. However, the mere existence of tools and increased
awareness have not led to swift action to reduce emissions and mitigate climate
change. Politicians and other policy makers lack the initiative to move from
talking about the climate to concrete climate action. In this work, we
contribute to the efforts of holding decision makers accountable by describing
a system which digests politicians' speeches and statements into a topic
summary. We propose a multi-source hybrid latent Dirichlet allocation model
which can process the large number of publicly available reports, social media
posts, speeches, and other documents of Finnish politicians, providing
transparency and accountability towards the general public.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 12:21:01 GMT'}]",2020-10-19,"[['Hätönen', 'Vili', ''], ['Melzer', 'Fiona', '']]"
1364742,2010.08412,Rumen Dangovski,"Matthew Khoury and Rumen Dangovski and Longwu Ou and Preslav Nakov and
  Yichen Shen and Li Jing","Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for
  Low-Latency Inference in NLP Applications","To appear at the 2020 Conference on Empirical Methods in Natural
  Language Processing (EMNLP '20), November 16-20, 2020, NMT, AI accelerators,
  co-design, TPU, OPU, 10 pages, 3 figures, 4 tables",,,,cs.CL cs.AR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks have become the standard approach to building reliable
Natural Language Processing (NLP) applications, ranging from Neural Machine
Translation (NMT) to dialogue systems. However, improving accuracy by
increasing the model size requires a large number of hardware computations,
which can slow down NLP applications significantly at inference time. To
address this issue, we propose a novel vector-vector-matrix architecture
(VVMA), which greatly reduces the latency at inference time for NMT. This
architecture takes advantage of specialized hardware that has low-latency
vector-vector operations and higher-latency vector-matrix operations. It also
reduces the number of parameters and FLOPs for virtually all models that rely
on efficient matrix multipliers without significantly impacting accuracy. We
present empirical results suggesting that our framework can reduce the latency
of sequence-to-sequence and Transformer models used for NMT by a factor of
four. Finally, we show evidence suggesting that our VVMA extends to other
domains, and we discuss novel hardware for its efficient use.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 16:54:08 GMT'}]",2020-10-19,"[['Khoury', 'Matthew', ''], ['Dangovski', 'Rumen', ''], ['Ou', 'Longwu', ''], ['Nakov', 'Preslav', ''], ['Shen', 'Yichen', ''], ['Jing', 'Li', '']]"
1364752,2010.08422,Wissam Siblini,"Wissam Siblini, Mohamed Challal and Charlotte Pasqual","Delaying Interaction Layers in Transformer-based Encoders for Efficient
  Open Domain Question Answering",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open Domain Question Answering (ODQA) on a large-scale corpus of documents
(e.g. Wikipedia) is a key challenge in computer science. Although
transformer-based language models such as Bert have shown on SQuAD the ability
to surpass humans for extracting answers in small passages of text, they suffer
from their high complexity when faced to a much larger search space. The most
common way to tackle this problem is to add a preliminary Information Retrieval
step to heavily filter the corpus and only keep the relevant passages. In this
paper, we propose a more direct and complementary solution which consists in
applying a generic change in the architecture of transformer-based models to
delay the attention between subparts of the input and allow a more efficient
management of computations. The resulting variants are competitive with the
original models on the extractive task and allow, on the ODQA setting, a
significant speedup and even a performance improvement in many cases.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 14:36:38 GMT'}]",2020-10-19,"[['Siblini', 'Wissam', ''], ['Challal', 'Mohamed', ''], ['Pasqual', 'Charlotte', '']]"
1364762,2010.08432,Haozhou Wang,"Haozhou Wang, James Henderson, Paola Merlo",Multi-Adversarial Learning for Cross-Lingual Word Embeddings,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative adversarial networks (GANs) have succeeded in inducing
cross-lingual word embeddings -- maps of matching words across languages --
without supervision. Despite these successes, GANs' performance for the
difficult case of distant languages is still not satisfactory. These
limitations have been explained by GANs' incorrect assumption that source and
target embedding spaces are related by a single linear mapping and are
approximately isomorphic. We assume instead that, especially across distant
languages, the mapping is only piece-wise linear, and propose a
multi-adversarial learning method. This novel method induces the seed
cross-lingual dictionary through multiple mappings, each induced to fit the
mapping for one subspace. Our experiments on unsupervised bilingual lexicon
induction show that this method improves performance over previous
single-mapping methods, especially for distant languages.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 14:54:28 GMT'}]",2020-10-19,"[['Wang', 'Haozhou', ''], ['Henderson', 'James', ''], ['Merlo', 'Paola', '']]"
1364855,2010.08525,Hongming Zhang,"Hongming Zhang, Muhao Chen, Haoyu Wang, Yangqiu Song, Dan Roth",Analogous Process Structure Induction for Sub-event Sequence Prediction,Accepted by EMNLP 2020,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Computational and cognitive studies of event understanding suggest that
identifying, comprehending, and predicting events depend on having structured
representations of a sequence of events and on conceptualizing (abstracting)
its components into (soft) event categories. Thus, knowledge about a known
process such as ""buying a car"" can be used in the context of a new but
analogous process such as ""buying a house"". Nevertheless, most event
understanding work in NLP is still at the ground level and does not consider
abstraction. In this paper, we propose an Analogous Process Structure Induction
APSI framework, which leverages analogies among processes and conceptualization
of sub-event instances to predict the whole sub-event sequence of previously
unseen open-domain processes. As our experiments and analysis indicate, APSI
supports the generation of meaningful sub-event sequences for unseen processes
and can help predict missing events.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 17:35:40 GMT'}]",2020-10-19,"[['Zhang', 'Hongming', ''], ['Chen', 'Muhao', ''], ['Wang', 'Haoyu', ''], ['Song', 'Yangqiu', ''], ['Roth', 'Dan', '']]"
1364870,2010.08540,Kyle Gorman,Angie Waller and Kyle Gorman,Detecting Objectifying Language in Online Professor Reviews,"To appear at the 6th Workshop on Noisy User-generated Text, a
  workshop of EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Student reviews often make reference to professors' physical appearances.
Until recently RateMyProfessors.com, the website of this study's focus, used a
design feature to encourage a ""hot or not"" rating of college professors. In the
wake of recent #MeToo and #TimesUp movements, social awareness of the
inappropriateness of these reviews has grown; however, objectifying comments
remain and continue to be posted in this online context. We describe two
supervised text classifiers for detecting objectifying commentary in professor
reviews. We then ensemble these classifiers and use the resulting model to
track objectifying commentary at scale. We measure correlations between
objectifying commentary, changes to the review website interface, and teacher
gender across a ten-year period.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 17:49:59 GMT'}]",2020-10-19,"[['Waller', 'Angie', ''], ['Gorman', 'Kyle', '']]"
1364872,2010.08542,Adrian de Wynter,Adrian de Wynter,Mischief: A Simple Black-Box Attack Against Transformer Architectures,Technical report,,,,cs.CL cs.CR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We introduce Mischief, a simple and lightweight method to produce a class of
human-readable, realistic adversarial examples for language models. We perform
exhaustive experimentations of our algorithm on four transformer-based
architectures, across a variety of downstream tasks, as well as under varying
concentrations of said examples. Our findings show that the presence of
Mischief-generated adversarial samples in the test set significantly degrades
(by up to $20\%$) the performance of these models with respect to their
reported baselines. Nonetheless, we also demonstrate that, by including similar
examples in the training set, it is possible to restore the baseline scores on
the adversarial test set. Moreover, for certain tasks, the models trained with
Mischief set show a modest increase on performance with respect to their
original, non-adversarial baseline.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 17:52:06 GMT'}]",2020-10-19,"[['de Wynter', 'Adrian', '']]"
1280484,2005.00689,Ziyu Yao,"Ziyu Yao, Yiqi Tang, Wen-tau Yih, Huan Sun, Yu Su",An Imitation Game for Learning Semantic Parsers from User Interaction,"Accepted to EMNLP 2020. 21 pages, 6 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the widely successful applications, bootstrapping and fine-tuning
semantic parsers are still a tedious process with challenges such as costly
data annotation and privacy risks. In this paper, we suggest an alternative,
human-in-the-loop methodology for learning semantic parsers directly from
users. A semantic parser should be introspective of its uncertainties and
prompt for user demonstration when uncertain. In doing so it also gets to
imitate the user behavior and continue improving itself autonomously with the
hope that eventually it may become as good as the user in interpreting their
questions. To combat the sparsity of demonstration, we propose a novel
annotation-efficient imitation learning algorithm, which iteratively collects
new datasets by mixing demonstrated states and confident predictions and
re-trains the semantic parser in a Dataset Aggregation fashion (Ross et al.,
2011). We provide a theoretical analysis of its cost bound and also empirically
demonstrate its promising performance on the text-to-SQL problem. Code will be
available at https://github.com/sunlab-osu/MISP.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 03:30:49 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 03:46:18 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Oct 2020 18:31:26 GMT'}]",2020-10-19,"[['Yao', 'Ziyu', ''], ['Tang', 'Yiqi', ''], ['Yih', 'Wen-tau', ''], ['Sun', 'Huan', ''], ['Su', 'Yu', '']]"
1261900,2003.11545,Fernando Alonso-Fernandez,"Nicole Mariah Sharon Belvisi, Naveed Muhammad, Fernando
  Alonso-Fernandez","Forensic Authorship Analysis of Microblogging Texts Using N-Grams and
  Stylometric Features","Accepted for publication at 8th International Workshop on Biometrics
  and Forensics, IWBF 2020","Proc. 8th International Workshop on Biometrics and Forensics,
  IWBF, Porto, Portugal, April 29-30, 2020",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, messages and text posted on the Internet are used in
criminal investigations. Unfortunately, the authorship of many of them remains
unknown. In some channels, the problem of establishing authorship may be even
harder, since the length of digital texts is limited to a certain number of
characters. In this work, we aim at identifying authors of tweet messages,
which are limited to 280 characters. We evaluate popular features employed
traditionally in authorship attribution which capture properties of the writing
style at different levels. We use for our experiments a self-captured database
of 40 users, with 120 to 200 tweets per user. Results using this small set are
promising, with the different features providing a classification accuracy
between 92% and 98.5%. These results are competitive in comparison to existing
studies which employ short texts such as tweets or SMS.
","[{'version': 'v1', 'created': 'Tue, 24 Mar 2020 19:32:11 GMT'}]",2020-10-19,"[['Belvisi', 'Nicole Mariah Sharon', ''], ['Muhammad', 'Naveed', ''], ['Alonso-Fernandez', 'Fernando', '']]"
1255517,2003.05162,Tejas Gokhale,"Zhiyuan Fang, Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou
  Yang","Video2Commonsense: Generating Commonsense Descriptions to Enrich Video
  Captioning","Accepted to EMNLP, Long Papers",,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Captioning is a crucial and challenging task for video understanding. In
videos that involve active agents such as humans, the agent's actions can bring
about myriad changes in the scene. Observable changes such as movements,
manipulations, and transformations of the objects in the scene, are reflected
in conventional video captioning. Unlike images, actions in videos are also
inherently linked to social aspects such as intentions (why the action is
taking place), effects (what changes due to the action), and attributes that
describe the agent. Thus for video understanding, such as when captioning
videos or when answering questions about videos, one must have an understanding
of these commonsense aspects. We present the first work on generating
commonsense captions directly from videos, to describe latent aspects such as
intentions, effects, and attributes. We present a new dataset
""Video-to-Commonsense (V2C)"" that contains $\sim9k$ videos of human agents
performing various actions, annotated with 3 types of commonsense descriptions.
Additionally we explore the use of open-ended video-based commonsense question
answering (V2C-QA) as a way to enrich our captions. Both the generation task
and the QA task can be used to enrich video captions.
","[{'version': 'v1', 'created': 'Wed, 11 Mar 2020 08:42:57 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Mar 2020 05:16:13 GMT'}, {'version': 'v3', 'created': 'Fri, 16 Oct 2020 02:08:26 GMT'}]",2020-10-19,"[['Fang', 'Zhiyuan', ''], ['Gokhale', 'Tejas', ''], ['Banerjee', 'Pratyay', ''], ['Baral', 'Chitta', ''], ['Yang', 'Yezhou', '']]"
1283718,2005.03923,Puhai Yang,"Puhai Yang, Heyan Huang, and Xian-Ling Mao","Context-Sensitive Generation Network for Handing Unknown Slot Values in
  Dialogue State Tracking",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As a key component in a dialogue system, dialogue state tracking plays an
important role. It is very important for dialogue state tracking to deal with
the problem of unknown slot values. As far as we known, almost all existing
approaches depend on pointer network to solve the unknown slot value problem.
These pointer network-based methods usually have a hidden assumption that there
is at most one out-of-vocabulary word in an unknown slot value because of the
character of a pointer network. However, often, there are multiple
out-of-vocabulary words in an unknown slot value, and it makes the existing
methods perform bad. To tackle the problem, in this paper, we propose a novel
Context-Sensitive Generation network (CSG) which can facilitate the
representation of out-of-vocabulary words when generating the unknown slot
value. Extensive experiments show that our proposed method performs better than
the state-of-the-art baselines.
","[{'version': 'v1', 'created': 'Fri, 8 May 2020 09:22:33 GMT'}, {'version': 'v2', 'created': 'Sat, 27 Jun 2020 05:49:59 GMT'}, {'version': 'v3', 'created': 'Fri, 16 Oct 2020 09:31:38 GMT'}]",2020-10-19,"[['Yang', 'Puhai', ''], ['Huang', 'Heyan', ''], ['Mao', 'Xian-Ling', '']]"
1364562,2010.08232,Dat Quoc Nguyen,"Dat Quoc Nguyen, Thanh Vu, Afshin Rahimi, Mai Hoang Dao, Linh The
  Nguyen and Long Doan",WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets,In Proceedings of the 6th Workshop on Noisy User-generated Text,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we provide an overview of the WNUT-2020 shared task on the
identification of informative COVID-19 English Tweets. We describe how we
construct a corpus of 10K Tweets and organize the development and evaluation
phases for this task. In addition, we also present a brief summary of results
obtained from the final system evaluation submissions of 55 teams, finding that
(i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the
majority of the submissions achieve substantially higher results than the
baseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained
language models on relevant language data followed by supervised training
performs well in this task.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 08:28:05 GMT'}]",2020-10-19,"[['Nguyen', 'Dat Quoc', ''], ['Vu', 'Thanh', ''], ['Rahimi', 'Afshin', ''], ['Dao', 'Mai Hoang', ''], ['Nguyen', 'Linh The', ''], ['Doan', 'Long', '']]"
1364543,2010.08213,Yanghoon Kim,"Yanghoon Kim, Seungpil Won, Seunghyun Yoon and Kyomin Jung","Collaborative Training of GANs in Continuous and Discrete Spaces for
  Text Generation",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Applying generative adversarial networks (GANs) to text-related tasks is
challenging due to the discrete nature of language. One line of research
resolves this issue by employing reinforcement learning (RL) and optimizing the
next-word sampling policy directly in a discrete action space. Such methods
compute the rewards from complete sentences and avoid error accumulation due to
exposure bias. Other approaches employ approximation techniques that map the
text to continuous representation in order to circumvent the non-differentiable
discrete process. Particularly, autoencoder-based methods effectively produce
robust representations that can model complex discrete structures. In this
paper, we propose a novel text GAN architecture that promotes the collaborative
training of the continuous-space and discrete-space methods. Our method employs
an autoencoder to learn an implicit data manifold, providing a learning
objective for adversarial training in a continuous space. Furthermore, the
complete textual output is directly evaluated and updated via RL in a discrete
space. The collaborative interplay between the two adversarial trainings
effectively regularize the text representations in different spaces. The
experimental results on three standard benchmark datasets show that our model
substantially outperforms state-of-the-art text GANs with respect to quality,
diversity, and global consistency.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 07:51:16 GMT'}]",2020-10-19,"[['Kim', 'Yanghoon', ''], ['Won', 'Seungpil', ''], ['Yoon', 'Seunghyun', ''], ['Jung', 'Kyomin', '']]"
1364570,2010.08240,Nandan Thakur,"Nandan Thakur, Nils Reimers, Johannes Daxenberger, Iryna Gurevych","Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for
  Pairwise Sentence Scoring Tasks",,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  There are two approaches for pairwise sentence scoring: Cross-encoders, which
perform full-attention over the input pair, and Bi-encoders, which map each
input independently to a dense vector space. While cross-encoders often achieve
higher performance, they are too slow for many practical use cases.
Bi-encoders, on the other hand, require substantial training data and
fine-tuning over the target task to achieve competitive performance. We present
a simple yet efficient data augmentation strategy called Augmented SBERT, where
we use the cross-encoder to label a larger set of input pairs to augment the
training data for the bi-encoder. We show that, in this process, selecting the
sentence pairs is non-trivial and crucial for the success of the method. We
evaluate our approach on multiple tasks (in-domain) as well as on a domain
adaptation task. Augmented SBERT achieves an improvement of up to 6 points for
in-domain and of up to 37 points for domain adaptation tasks compared to the
original bi-encoder performance.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 08:43:27 GMT'}]",2020-10-19,"[['Thakur', 'Nandan', ''], ['Reimers', 'Nils', ''], ['Daxenberger', 'Johannes', ''], ['Gurevych', 'Iryna', '']]"
1364527,2010.08197,Boyan Wan,"Boyan Wan, Zhuo Tang, Li Yang","Lexicon-constrained Copying Network for Chinese Abstractive
  Summarization",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Copy mechanism allows sequence-to-sequence models to choose words from the
input and put them directly into the output, which is finding increasing use in
abstractive summarization. However, since there is no explicit delimiter in
Chinese sentences, most existing models for Chinese abstractive summarization
can only perform character copy, resulting in inefficient. To solve this
problem, we propose a lexicon-constrained copying network that models
multi-granularity in both encoder and decoder. On the source side, words and
characters are aggregated into the same input memory using a Transformerbased
encoder. On the target side, the decoder can copy either a character or a
multi-character word at each time step, and the decoding process is guided by a
word-enhanced search algorithm that facilitates the parallel computation and
encourages the model to copy more words. Moreover, we adopt a word selector to
integrate keyword information. Experiments results on a Chinese social media
dataset show that our model can work standalone or with the word selector. Both
forms can outperform previous character-based models and achieve competitive
performances.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 06:59:34 GMT'}]",2020-10-19,"[['Wan', 'Boyan', ''], ['Tang', 'Zhuo', ''], ['Yang', 'Li', '']]"
1310666,2006.16152,David Beauchemin,"Marouane Yassine, David Beauchemin, Fran\c{c}ois Laviolette, Luc
  Lamontagne",Leveraging Subword Embeddings for Multinational Address Parsing,Accepted to IEEE CiSt'20,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Address parsing consists of identifying the segments that make up an address
such as a street name or a postal code. Because of its importance for tasks
like record linkage, address parsing has been approached with many techniques.
Neural network methods defined a new state-of-the-art for address parsing.
While this approach yielded notable results, previous work has only focused on
applying neural networks to achieve address parsing of addresses from one
source country. We propose an approach in which we employ subword embeddings
and a Recurrent Neural Network architecture to build a single model capable of
learning to parse addresses from multiple countries at the same time while
taking into account the difference in languages and address formatting systems.
We achieved accuracies around 99% on the countries used for training with no
pre-processing nor post-processing needed. We explore the possibility of
transferring the address parsing knowledge obtained by training on some
countries' addresses to others with no further training in a zero-shot transfer
learning setting. We achieve good results for 80% of the countries (33 out of
41), almost 50% of which (20 out of 41) is near state-of-the-art performance.
In addition, we propose an open-source Python implementation of our trained
models.
","[{'version': 'v1', 'created': 'Mon, 29 Jun 2020 16:14:27 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 16:03:49 GMT'}]",2020-10-19,"[['Yassine', 'Marouane', ''], ['Beauchemin', 'David', ''], ['Laviolette', 'François', ''], ['Lamontagne', 'Luc', '']]"
1329255,2008.01533,Fernando Alonso-Fernandez,"Fernando Alonso-Fernandez, Nicole Mariah Sharon Belvisi, Kevin
  Hernandez-Diaz, Naveed Muhammad, Josef Bigun",Forensic Writer Identification Using Microblogging Texts,,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Establishing the authorship of online texts is a fundamental issue to combat
several cybercrimes. Unfortunately, some platforms limit the length of the
text, making the challenge harder. Here, we aim at identifying the author of
Twitter messages limited to 140 characters. We evaluate popular stylometric
features, widely used in traditional literary analysis, which capture the
writing style at different levels (character, word, and sentence). We use a
public database of 93 users, containing 1142 to 3209 Tweets per user. We also
evaluate the influence of the number of Tweets per user for enrolment and
testing. If the amount is sufficient (>500), a Rank 1 of 97-99% is achieved. If
data is scarce (e.g. 20 Tweets for testing), the Rank 1 with the best
individual feature method ranges from 54.9% (100 Tweets for enrolment) to 70.6%
(1000 Tweets). By combining the available features, a substantial improvement
is observed, reaching a Rank 1 of 70% when using 100 Tweets for enrolment and
only 20 for testing. With a bigger hit list size, accuracy of the latter case
increases to 86.4% (Rank 5) or 95% (Rank 20). This demonstrates the feasibility
of identifying writers of digital texts, even with few data available.
","[{'version': 'v1', 'created': 'Fri, 31 Jul 2020 00:23:18 GMT'}]",2020-10-19,"[['Alonso-Fernandez', 'Fernando', ''], ['Belvisi', 'Nicole Mariah Sharon', ''], ['Hernandez-Diaz', 'Kevin', ''], ['Muhammad', 'Naveed', ''], ['Bigun', 'Josef', '']]"
1363887,2010.07557,"Laura Oberl\""ander","Laura Oberl\""ander, Roman Klinger","Token Sequence Labeling vs. Clause Classification for English Emotion
  Stimulus Detection",accepted at *SEM 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Emotion stimulus detection is the task of finding the cause of an emotion in
a textual description, similar to target or aspect detection for sentiment
analysis. Previous work approached this in three ways, namely (1) as text
classification into an inventory of predefined possible stimuli (""Is the
stimulus category A or B?""), (2) as sequence labeling of tokens (""Which tokens
describe the stimulus?""), and (3) as clause classification (""Does this clause
contain the emotion stimulus?""). So far, setting (3) has been evaluated broadly
on Mandarin and (2) on English, but no comparison has been performed.
Therefore, we aim to answer whether clause classification or sequence labeling
is better suited for emotion stimulus detection in English. To accomplish that,
we propose an integrated framework which enables us to evaluate the two
different approaches comparably, implement models inspired by state-of-the-art
approaches in Mandarin, and test them on four English data sets from different
domains. Our results show that sequence labeling is superior on three out of
four datasets, in both clause-based and sequence-based evaluation. The only
case in which clause classification performs better is one data set with a high
density of clause annotations. Our error analysis further confirms
quantitatively and qualitatively that clauses are not the appropriate stimulus
unit in English.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 07:11:04 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 07:37:02 GMT'}]",2020-10-19,"[['Oberländer', 'Laura', ''], ['Klinger', 'Roman', '']]"
1363950,2010.07620,Yao Zhang,"Yao Zhang, Xu Zhang, Jun Wang, Hongru Liang, Adam Jatowt, Wenqiang
  Lei, Zhenglu Yang",GMH: A General Multi-hop Reasoning Model for KG Completion,"11 pages, 5 figures and 4 tables",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graphs are essential for numerous downstream natural language
processing applications, but are typically incomplete with many facts missing.
This results in research efforts on multi-hop reasoning task, which can be
formulated as a search process and current models typically perform short
distance reasoning. However, the long-distance reasoning is also vital with the
ability to connect the superficially unrelated entities. To the best of our
knowledge, there lacks a general framework that approaches multi-hop reasoning
in both short and long scenarios. We argue that there are two key issues for
long distance reasoning: i) which edge to select, and ii) when to stop the
search. In this work, we propose a general model which resolves the issues with
three modules: 1) the local-global knowledge module to estimate the possible
paths, 2) the differentiated action dropout module to explore a diverse set of
paths, and 3) the adaptive stopping search module to avoid over searching. The
comprehensive results on three datasets demonstrate the superiority of our
model with significant improvements against baselines in both short and long
distance reasoning scenarios.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 09:30:46 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 13:48:28 GMT'}]",2020-10-19,"[['Zhang', 'Yao', ''], ['Zhang', 'Xu', ''], ['Wang', 'Jun', ''], ['Liang', 'Hongru', ''], ['Jatowt', 'Adam', ''], ['Lei', 'Wenqiang', ''], ['Yang', 'Zhenglu', '']]"
1364530,2010.08200,Guangyu Zheng,"Wanyun Cui, Guangyu Zheng, Wei Wang","Unsupervised Natural Language Inference via Decoupled Multimodal
  Contrastive Learning",Published at EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose to solve the natural language inference problem without any
supervision from the inference labels via task-agnostic multimodal pretraining.
Although recent studies of multimodal self-supervised learning also represent
the linguistic and visual context, their encoders for different modalities are
coupled. Thus they cannot incorporate visual information when encoding plain
text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled
learning (MACD) network. MACD forces the decoupled text encoder to represent
the visual information via contrastive learning. Therefore, it embeds visual
knowledge even for plain text inference. We conducted comprehensive experiments
over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD
even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 07:12:53 GMT'}]",2020-10-19,"[['Cui', 'Wanyun', ''], ['Zheng', 'Guangyu', ''], ['Wang', 'Wei', '']]"
1364302,2010.07972,Junjie Hu,"Junjie Hu and Melvin Johnson and Orhan Firat and Aditya Siddhant and
  Graham Neubig",Explicit Alignment Objectives for Multilingual Bidirectional Encoders,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and
XLMR (Conneau et al., 2020) have proven to be impressively effective at
enabling transfer-learning of NLP systems from high-resource languages to
low-resource languages. This success comes despite the fact that there is no
explicit objective to align the contextual embeddings of words/sentences with
similar meanings across languages together in the same space. In this paper, we
present a new method for learning multilingual encoders, AMBER (Aligned
Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel
data using two explicit alignment objectives that align the multilingual
representations at different granularities. We conduct experiments on zero-shot
cross-lingual transfer learning for different tasks including sequence tagging,
sentence retrieval and sentence classification. Experimental results show that
AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to
27.3 average accuracy on retrieval over the XLMR-large model which has 4.6x the
parameters of AMBER.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 18:34:13 GMT'}]",2020-10-19,"[['Hu', 'Junjie', ''], ['Johnson', 'Melvin', ''], ['Firat', 'Orhan', ''], ['Siddhant', 'Aditya', ''], ['Neubig', 'Graham', '']]"
1364318,2010.07988,Harish Tayyar Madabushi PhD,Calum Perrio and Harish Tayyar Madabushi,"CXP949 at WNUT-2020 Task 2: Extracting Informative COVID-19 Tweets --
  RoBERTa Ensembles and The Continued Relevance of Handcrafted Features",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents our submission to Task 2 of the Workshop on Noisy
User-generated Text. We explore improving the performance of a pre-trained
transformer-based language model fine-tuned for text classification through an
ensemble implementation that makes use of corpus level information and a
handcrafted feature. We test the effectiveness of including the aforementioned
features in accommodating the challenges of a noisy data set centred on a
specific subject outside the remit of the pre-training data. We show that
inclusion of additional features can improve classification results and achieve
a score within 2 points of the top performing team.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 19:12:52 GMT'}]",2020-10-19,"[['Perrio', 'Calum', ''], ['Madabushi', 'Harish Tayyar', '']]"
1364329,2010.07999,Jie Lei,"Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal","What is More Likely to Happen Next? Video-and-Language Future Event
  Prediction",EMNLP 2020 (17 pages),,,,cs.CL cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Given a video with aligned dialogue, people can often infer what is more
likely to happen next. Making such predictions requires not only a deep
understanding of the rich dynamics underlying the video and dialogue, but also
a significant amount of commonsense knowledge. In this work, we explore whether
AI models are able to learn to make such multimodal commonsense next-event
predictions. To support research in this direction, we collect a new dataset,
named Video-and-Language Event Prediction (VLEP), with 28,726 future event
prediction examples (along with their rationales) from 10,234 diverse TV Show
and YouTube Lifestyle Vlog video clips. In order to promote the collection of
non-trivial challenging examples, we employ an adversarial
human-and-model-in-the-loop data collection procedure. We also present a strong
baseline incorporating information from video, dialogue, and commonsense
knowledge. Experiments show that each type of information is useful for this
challenging task, and that compared to the high human performance on VLEP, our
model provides a good starting point but leaves large room for future work. Our
dataset and code are available at:
https://github.com/jayleicn/VideoLanguageFuturePred
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 19:56:47 GMT'}]",2020-10-19,"[['Lei', 'Jie', ''], ['Yu', 'Licheng', ''], ['Berg', 'Tamara L.', ''], ['Bansal', 'Mohit', '']]"
1364284,2010.07954,Alexander Ku,"Alexander Ku and Peter Anderson and Roma Patel and Eugene Ie and Jason
  Baldridge","Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense
  Spatiotemporal Grounding",EMNLP 2020,,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation
(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger
(more paths and instructions) than other VLN datasets. It emphasizes the role
of language in VLN by addressing known biases in paths and eliciting more
references to visible entities. Furthermore, each word in an instruction is
time-aligned to the virtual poses of instruction creators and validators. We
establish baseline scores for monolingual and multilingual settings and
multitask learning when including Room-to-Room annotations. We also provide
results for a model that learns from synchronized pose traces by focusing only
on portions of the panorama attended to in human demonstrations. The size,
scope and detail of RxR dramatically expands the frontier for research on
embodied language agents in simulated, photo-realistic environments.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 18:01:15 GMT'}]",2020-10-19,"[['Ku', 'Alexander', ''], ['Anderson', 'Peter', ''], ['Patel', 'Roma', ''], ['Ie', 'Eugene', ''], ['Baldridge', 'Jason', '']]"
1364351,2010.08021,Udit Arora,"Aman Khullar, Udit Arora","MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical
  Attention","To appear in the first EMNLP Workshop on NLP Beyond Text, 2020. Aman
  Khullar and Udit Arora have equal contribution",,,,cs.CL cs.CV cs.LG cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents MAST, a new model for Multimodal Abstractive Text
Summarization that utilizes information from all three modalities -- text,
audio and video -- in a multimodal video. Prior work on multimodal abstractive
text summarization only utilized information from the text and video
modalities. We examine the usefulness and challenges of deriving information
from the audio modality and present a sequence-to-sequence trimodal
hierarchical attention-based model that overcomes these challenges by letting
the model pay more attention to the text modality. MAST outperforms the current
state of the art model (video-text) by 2.51 points in terms of Content F1 score
and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal
language understanding.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 21:08:20 GMT'}]",2020-10-19,"[['Khullar', 'Aman', ''], ['Arora', 'Udit', '']]"
1364521,2010.08191,Jing Liu,"Yingqi Qu Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao,
  Daxiang Dong, Hua Wu, Haifeng Wang","RocketQA: An Optimized Training Approach to Dense Passage Retrieval for
  Open-Domain Question Answering",,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In open-domain question answering, dense passage retrieval has become a new
paradigm to retrieve relevant passages for answer finding. Typically, the
dual-encoder architecture is adopted to learn dense representations of
questions and passages for matching. However, it is difficult to train an
effective dual-encoder due to the challenges including the discrepancy between
training and inference, the existence of unlabeled positives and limited
training data. To address these challenges, we propose an optimized training
approach, called RocketQA, to improving dense passage retrieval. We make three
major technical contributions in RocketQA, namely cross-batch negatives,
denoised negative sampling and data augmentation. Extensive experiments show
that RocketQA significantly outperforms previous state-of-the-art models on
both MSMARCO and Natural Questions. Besides, built upon RocketQA, we achieve
the first rank at the leaderboard of MSMARCO Passage Ranking Task.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 06:54:05 GMT'}]",2020-10-19,"[['Ding', 'Yingqi Qu Yuchen', ''], ['Liu', 'Jing', ''], ['Liu', 'Kai', ''], ['Ren', 'Ruiyang', ''], ['Zhao', 'Xin', ''], ['Dong', 'Daxiang', ''], ['Wu', 'Hua', ''], ['Wang', 'Haifeng', '']]"
1364517,2010.08187,Guang-Neng Hu,"Guangneng Hu, Qiang Yang","PrivNet: Safeguarding Private Attributes in Transfer Learning for
  Recommendation",Findings of EMNLP 2020,,,,cs.AI cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transfer learning is an effective technique to improve a target recommender
system with the knowledge from a source domain. Existing research focuses on
the recommendation performance of the target domain while ignores the privacy
leakage of the source domain. The transferred knowledge, however, may
unintendedly leak private information of the source domain. For example, an
attacker can accurately infer user demographics from their historical purchase
provided by a source domain data owner. This paper addresses the above
privacy-preserving issue by learning a privacy-aware neural representation by
improving target performance while protecting source privacy. The key idea is
to simulate the attacks during the training for protecting unseen users'
privacy in the future, modeled by an adversarial game, so that the transfer
learning model becomes robust to attacks. Experiments show that the proposed
PrivNet model can successfully disentangle the knowledge benefitting the
transfer from leaking the privacy.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 06:33:45 GMT'}]",2020-10-19,"[['Hu', 'Guangneng', ''], ['Yang', 'Qiang', '']]"
1364344,2010.08014,Zi-Yi Dou,"Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig",GSum: A General Framework for Guided Neural Abstractive Summarization,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural abstractive summarization models are flexible and can produce coherent
summaries, but they are sometimes unfaithful and can be difficult to control.
While previous studies attempt to provide different types of guidance to
control the output and increase faithfulness, it is not clear how these
strategies compare and contrast to each other. In this paper, we propose a
general and extensible guided summarization framework (GSum) that can
effectively take different kinds of external guidance as input, and we perform
experiments across several different varieties. Experiments demonstrate that
this model is effective, achieving state-of-the-art performance according to
ROUGE on 4 popular summarization datasets when using highlighted sentences as
guidance. In addition, we show that our guided model can generate more faithful
summaries and demonstrate how different types of guidance generate
qualitatively different summaries, lending a degree of controllability to the
learned models.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 20:46:14 GMT'}]",2020-10-19,"[['Dou', 'Zi-Yi', ''], ['Liu', 'Pengfei', ''], ['Hayashi', 'Hiroaki', ''], ['Jiang', 'Zhengbao', ''], ['Neubig', 'Graham', '']]"
1364508,2010.08178,Xuanfu Wu,"Xuanfu Wu, Yang Feng, Chenze Shao",Generating Diverse Translation from Model Distribution with Dropout,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the improvement of translation quality, neural machine translation
(NMT) often suffers from the lack of diversity in its generation. In this
paper, we propose to generate diverse translations by deriving a large number
of possible models with Bayesian modelling and sampling models from them for
inference. The possible models are obtained by applying concrete dropout to the
NMT model and each of them has specific confidence for its prediction, which
corresponds to a posterior model distribution under specific training data in
the principle of Bayesian modeling. With variational inference, the posterior
model distribution can be approximated with a variational distribution, from
which the final models for inference are sampled. We conducted experiments on
Chinese-English and English-German translation tasks and the results shows that
our method makes a better trade-off between diversity and accuracy.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 05:50:00 GMT'}]",2020-10-19,"[['Wu', 'Xuanfu', ''], ['Feng', 'Yang', ''], ['Shao', 'Chenze', '']]"
1364515,2010.08185,Tanfang Chen,"Tanfang Chen, Weiwei Wang, Wenyang Wei, Xing Shi, Xiangang Li, Jieping
  Ye, Kevin Knight",DiDi's Machine Translation System for WMT2020,Accepted at WMT 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes DiDi AI Labs' submission to the WMT2020 news translation
shared task. We participate in the translation direction of Chinese->English.
In this direction, we use the Transformer as our baseline model, and integrate
several techniques for model enhancement, including data filtering, data
selection, back-translation, fine-tuning, model ensembling, and re-ranking. As
a result, our submission achieves a BLEU score of $36.6$ in Chinese->English.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 06:25:48 GMT'}]",2020-10-19,"[['Chen', 'Tanfang', ''], ['Wang', 'Weiwei', ''], ['Wei', 'Wenyang', ''], ['Shi', 'Xing', ''], ['Li', 'Xiangang', ''], ['Ye', 'Jieping', ''], ['Knight', 'Kevin', '']]"
1364444,2010.08114,Lingbing Guo,"Lingbing Guo, Weiqing Wang, Zequn Sun, Chenghao Liu, Wei Hu",Decentralized Knowledge Graph Representation Learning,submitted to ICLR 2021,,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graph (KG) representation learning methods have achieved
competitive performance in many KG-oriented tasks, among which the best ones
are usually based on graph neural networks (GNNs), a powerful family of
networks that learns the representation of an entity by aggregating the
features of its neighbors and itself. However, many KG representation learning
scenarios only provide the structure information that describes the
relationships among entities, causing that entities have no input features. In
this case, existing aggregation mechanisms are incapable of inducing embeddings
of unseen entities as these entities have no pre-defined features for
aggregation. In this paper, we present a decentralized KG representation
learning approach, decentRL, which encodes each entity from and only from the
embeddings of its neighbors. For optimization, we design an algorithm to
distill knowledge from the model itself such that the output embeddings can
continuously gain knowledge from the corresponding original embeddings.
Extensive experiments show that the proposed approach performed better than
many cutting-edge models on the entity alignment task, and achieved competitive
performance on the entity prediction task. Furthermore, under the inductive
setting, it significantly outperformed all baselines on both tasks.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 02:31:22 GMT'}]",2020-10-19,"[['Guo', 'Lingbing', ''], ['Wang', 'Weiqing', ''], ['Sun', 'Zequn', ''], ['Liu', 'Chenghao', ''], ['Hu', 'Wei', '']]"
1364420,2010.08090,Chelsea Tanchip,"Chelsea Tanchip, Lei Yu, Aotao Xu, Yang Xu",Inferring symmetry in natural language,"10 pages, 4 figures, Findings of EMNLP",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a methodological framework for inferring symmetry of verb
predicates in natural language. Empirical work on predicate symmetry has taken
two main approaches. The feature-based approach focuses on linguistic features
pertaining to symmetry. The context-based approach denies the existence of
absolute symmetry but instead argues that such inference is context dependent.
We develop methods that formalize these approaches and evaluate them against a
novel symmetry inference sentence (SIS) dataset comprised of 400 naturalistic
usages of literature-informed verbs spanning the spectrum of
symmetry-asymmetry. Our results show that a hybrid transfer learning model that
integrates linguistic features with contextualized language models most
faithfully predicts the empirical data. Our work integrates existing approaches
to symmetry in natural language and suggests how symmetry inference can improve
systematicity in state-of-the-art language models.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 01:25:01 GMT'}]",2020-10-19,"[['Tanchip', 'Chelsea', ''], ['Yu', 'Lei', ''], ['Xu', 'Aotao', ''], ['Xu', 'Yang', '']]"
1364397,2010.08067,Gene Louis Kim,Gene Louis Kim and Aaron Steven White,Montague Grammar Induction,"18 pages, 2 figures, to be published in SALT 30",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a computational modeling framework for inducing combinatory
categorial grammars from arbitrary behavioral data. This framework provides the
analyst fine-grained control over the assumptions that the induced grammar
should conform to: (i) what the primitive types are; (ii) how complex types are
constructed; (iii) what set of combinators can be used to combine types; and
(iv) whether (and to what) the types of some lexical items should be fixed. In
a proof-of-concept experiment, we deploy our framework for use in
distributional analysis. We focus on the relationship between
s(emantic)-selection and c(ategory)-selection, using as input a lexicon-scale
acceptability judgment dataset focused on English verbs' syntactic distribution
(the MegaAcceptability dataset) and enforcing standard assumptions from the
semantics literature on the induced grammar.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 23:25:01 GMT'}]",2020-10-19,"[['Kim', 'Gene Louis', ''], ['White', 'Aaron Steven', '']]"
1283770,2005.03975,Yan Xu,"Dan Su, Yan Xu, Tiezheng Yu, Farhad Bin Siddique, Elham J. Barezi,
  Pascale Fung","CAiRE-COVID: A Question Answering and Query-focused Multi-Document
  Summarization System for COVID-19 Scholarly Information Management",Accepted EMNLP NLP-COVID Workshop,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The outbreak of COVID-19 raises attention from the researchers from various
communities. While many scientific articles have been published, a system that
can provide reliable information to COVID-19 related questions from the latest
academic resources is crucial, especially for the medical community in the
current time-critical race to treat patients and to find a cure for the virus.
To address the requests, we propose our CAiRE-COVID, a neural-based system that
uses open-domain question answering (QA) techniques combined with summarization
techniques for mining the available scientific literature. It leverages the
Information Retrieval (IR) system and QA models to extract relevant snippets
from existing literature given a query. Fluent summaries are also provided to
help understand the content in a more efficient way. Our system has been
awarded as winner for one of the tasks in CORD-19 Kaggle Challenge. We also
launched our CAiRE-COVID website for broader use. The code for our system is
also open-sourced to bootstrap further study.
","[{'version': 'v1', 'created': 'Mon, 4 May 2020 15:07:27 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 02:47:06 GMT'}]",2020-10-19,"[['Su', 'Dan', ''], ['Xu', 'Yan', ''], ['Yu', 'Tiezheng', ''], ['Siddique', 'Farhad Bin', ''], ['Barezi', 'Elham J.', ''], ['Fung', 'Pascale', '']]"
1365930,2010.09600,Dalton Schutte,"Rui Zhang, Dimitar Hristovski, Dalton Schutte, Andrej Kastrin, Marcelo
  Fiszman, Halil Kilicoglu",Drug Repurposing for COVID-19 via Knowledge Graph Completion,"47 pages, 3 figures, submitted to Journal of Biomedical Informatics",,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Objective: To discover candidate drugs to repurpose for COVID-19 using
literature-derived knowledge and knowledge graph completion methods. Methods:
We propose a novel, integrative, and neural network-based literature-based
discovery (LBD) approach to identify drug candidates from both PubMed and
COVID-19-focused research literature. Our approach relies on semantic triples
extracted using SemRep (via SemMedDB). We identified an informative subset of
semantic triples using filtering rules and an accuracy classifier developed on
a BERT variant, and used this subset to construct a knowledge graph. Five SOTA,
neural knowledge graph completion algorithms were used to predict drug
repurposing candidates. The models were trained and assessed using a time
slicing approach and the predicted drugs were compared with a list of drugs
reported in the literature and evaluated in clinical trials. These models were
complemented by a discovery pattern-based approach. Results: Accuracy
classifier based on PubMedBERT achieved the best performance (F1= 0.854) in
classifying semantic predications. Among five knowledge graph completion
models, TransE outperformed others (MR = 0.923, Hits@1=0.417). Some known drugs
linked to COVID-19 in the literature were identified, as well as some candidate
drugs that have not yet been studied. Discovery patterns enabled generation of
plausible hypotheses regarding the relationships between the candidate drugs
and COVID-19. Among them, five highly ranked and novel drugs (paclitaxel, SB
203580, alpha 2-antiplasmin, pyrrolidine dithiocarbamate, and butylated
hydroxytoluene) with their mechanistic explanations were further discussed.
Conclusion: We show that an LBD approach can be feasible for discovering drug
candidates for COVID-19, and for generating mechanistic explanations. Our
approach can be generalized to other diseases as well as to other clinical
questions.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 15:30:51 GMT'}]",2020-10-20,"[['Zhang', 'Rui', ''], ['Hristovski', 'Dimitar', ''], ['Schutte', 'Dalton', ''], ['Kastrin', 'Andrej', ''], ['Fiszman', 'Marcelo', ''], ['Kilicoglu', 'Halil', '']]"
1365928,2010.09598,Jeroen Offerijns,"Jeroen Offerijns, Suzan Verberne, Tessa Verhoef","Better Distractions: Transformer-based Distractor Generation and
  Multiple Choice Question Filtering",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  For the field of education, being able to generate semantically correct and
educationally relevant multiple choice questions (MCQs) could have a large
impact. While question generation itself is an active research topic,
generating distractors (the incorrect multiple choice options) receives much
less attention. A missed opportunity, since there is still a lot of room for
improvement in this area. In this work, we train a GPT-2 language model to
generate three distractors for a given question and text context, using the
RACE dataset. Next, we train a BERT language model to answer MCQs, and use this
model as a filter, to select only questions that can be answered and therefore
presumably make sense. To evaluate our work, we start by using text generation
metrics, which show that our model outperforms earlier work on distractor
generation (DG) and achieves state-of-the-art performance. Also, by calculating
the question answering ability, we show that larger base models lead to better
performance. Moreover, we conducted a human evaluation study, which confirmed
the quality of the generated questions, but showed no statistically significant
effect of the QA filter.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 15:23:24 GMT'}]",2020-10-20,"[['Offerijns', 'Jeroen', ''], ['Verberne', 'Suzan', ''], ['Verhoef', 'Tessa', '']]"
1094114,1903.01728,Chuan Guo,"Xueyao Zhang, Juan Cao, Xirong Li, Qiang Sheng, Lei Zhong, Kai Shu",Mining Dual Emotion for Fake News Detection,"9 pages, 4 figures",,,,cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Emotion plays an important role in detecting fake news online. When
leveraging emotional signals, the existing methods focus on exploiting the
emotions of news contents that conveyed by the publishers (i.e., publisher
emotion). However, fake news is always fabricated to evoke high-arousal or
activating emotions of people to spread like a virus, so the emotions of news
comments that aroused by the crowd (i.e., social emotion) can not be ignored.
Furthermore, it needs to be explored whether there exists a relationship
between publisher emotion and social emotion (i.e., dual emotion), and how the
dual emotion appears in fake news. In the paper, we propose Dual Emotion
Features to mine dual emotion and the relationship between them for fake news
detection. And we design a universal paradigm to plug it into any existing
detectors as an enhancement. Experimental results on three real-world datasets
indicate the effectiveness of the proposed features.
","[{'version': 'v1', 'created': 'Tue, 5 Mar 2019 08:52:33 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Oct 2019 02:47:16 GMT'}, {'version': 'v3', 'created': 'Mon, 19 Oct 2020 17:11:35 GMT'}]",2020-10-20,"[['Zhang', 'Xueyao', ''], ['Cao', 'Juan', ''], ['Li', 'Xirong', ''], ['Sheng', 'Qiang', ''], ['Zhong', 'Lei', ''], ['Shu', 'Kai', '']]"
1365968,2010.09638,Jiawei Sheng,"Jiawei Sheng, Shu Guo, Zhenyu Chen, Juwei Yue, Lihong Wang, Tingwen
  Liu and Hongbo Xu",Adaptive Attentional Network for Few-Shot Knowledge Graph Completion,"11 pages, 3 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Few-shot Knowledge Graph (KG) completion is a focus of current research,
where each task aims at querying unseen facts of a relation given its few-shot
reference entity pairs. Recent attempts solve this problem by learning static
representations of entities and references, ignoring their dynamic properties,
i.e., entities may exhibit diverse roles within task relations, and references
may make different contributions to queries. This work proposes an adaptive
attentional network for few-shot KG completion by learning adaptive entity and
reference representations. Specifically, entities are modeled by an adaptive
neighbor encoder to discern their task-oriented roles, while references are
modeled by an adaptive query-aware aggregator to differentiate their
contributions. Through the attention mechanism, both entities and references
can capture their fine-grained semantic meanings, and thus render more
expressive representations. This will be more predictive for knowledge
acquisition in the few-shot scenario. Evaluation in link prediction on two
public datasets shows that our approach achieves new state-of-the-art results
with different few-shot sizes.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 16:27:48 GMT'}]",2020-10-20,"[['Sheng', 'Jiawei', ''], ['Guo', 'Shu', ''], ['Chen', 'Zhenyu', ''], ['Yue', 'Juwei', ''], ['Wang', 'Lihong', ''], ['Liu', 'Tingwen', ''], ['Xu', 'Hongbo', '']]"
1358669,2010.02339,Ashiqur KhudaBukhsh Ashiqur Rahman KhudaBukhsh,"Ashiqur R. KhudaBukhsh, Rupak Sarkar, Mark S. Kamlet, Tom M. Mitchell","We Don't Speak the Same Language: Interpreting Polarization through
  Machine Translation",,,,,cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Polarization among US political parties, media and elites is a widely studied
topic. Prominent lines of prior research across multiple disciplines have
observed and analyzed growing polarization in social media. In this paper, we
present a new methodology that offers a fresh perspective on interpreting
polarization through the lens of machine translation. With a novel proposition
that two sub-communities are speaking in two different \emph{languages}, we
demonstrate that modern machine translation methods can provide a simple yet
powerful and interpretable framework to understand the differences between two
(or more) large-scale social media discussion data sets at the granularity of
words. Via a substantial corpus of 86.6 million comments by 6.5 million users
on over 200,000 news videos hosted by YouTube channels of four prominent US
news networks, we demonstrate that simple word-level and phrase-level
translation pairs can reveal deep insights into the current political divide --
what is \emph{black lives matter} to one can be \emph{all lives matter} to the
other.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 21:16:30 GMT'}, {'version': 'v2', 'created': 'Sun, 18 Oct 2020 07:34:20 GMT'}]",2020-10-20,"[['KhudaBukhsh', 'Ashiqur R.', ''], ['Sarkar', 'Rupak', ''], ['Kamlet', 'Mark S.', ''], ['Mitchell', 'Tom M.', '']]"
1279213,2004.14443,Johny Moreira,"Johny Moreira, Chaina Oliveira, David Mac\^edo, Cleber Zanchettin,
  Luciano Barbosa","Distantly-Supervised Neural Relation Extraction with Side Information
  using BERT",2020 International Joint Conference on Neural Networks (IJCNN),2020 International Joint Conference on Neural Networks (IJCNN),10.1109/IJCNN48605.2020.9206648,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Relation extraction (RE) consists in categorizing the relationship between
entities in a sentence. A recent paradigm to develop relation extractors is
Distant Supervision (DS), which allows the automatic creation of new datasets
by taking an alignment between a text corpus and a Knowledge Base (KB). KBs can
sometimes also provide additional information to the RE task. One of the
methods that adopt this strategy is the RESIDE model, which proposes a
distantly-supervised neural relation extraction using side information from
KBs. Considering that this method outperformed state-of-the-art baselines, in
this paper, we propose a related approach to RESIDE also using additional side
information, but simplifying the sentence encoding with BERT embeddings.
Through experiments, we show the effectiveness of the proposed method in Google
Distant Supervision and Riedel datasets concerning the BGWA and RESIDE baseline
methods. Although Area Under the Curve is decreased because of unbalanced
datasets, P@N results have shown that the use of BERT as sentence encoding
allows superior performance to baseline methods.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 19:29:10 GMT'}, {'version': 'v2', 'created': 'Sun, 10 May 2020 21:45:15 GMT'}, {'version': 'v3', 'created': 'Thu, 10 Sep 2020 20:30:34 GMT'}]",2020-10-20,"[['Moreira', 'Johny', ''], ['Oliveira', 'Chaina', ''], ['Macêdo', 'David', ''], ['Zanchettin', 'Cleber', ''], ['Barbosa', 'Luciano', '']]"
1365938,2010.09608,David Wan,"David Wan, Chris Kedzie, Faisal Ladhak, Marine Carpuat and Kathleen
  McKeown",Incorporating Terminology Constraints in Automatic Post-Editing,"To appear in WMT, 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Users of machine translation (MT) may want to ensure the use of specific
lexical terminologies. While there exist techniques for incorporating
terminology constraints during inference for MT, current APE approaches cannot
ensure that they will appear in the final translation. In this paper, we
present both autoregressive and non-autoregressive models for lexically
constrained APE, demonstrating that our approach enables preservation of 95% of
the terminologies and also improves translation quality on English-German
benchmarks. Even when applied to lexically constrained MT output, our approach
is able to improve preservation of the terminologies. However, we show that our
models do not learn to copy constraints systematically and suggest a simple
data augmentation technique that leads to improved performance and robustness.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 15:44:03 GMT'}]",2020-10-20,"[['Wan', 'David', ''], ['Kedzie', 'Chris', ''], ['Ladhak', 'Faisal', ''], ['Carpuat', 'Marine', ''], ['McKeown', 'Kathleen', '']]"
1308583,2006.14069,Ramon Ferrer i Cancho,"Ramon Ferrer-i-Cancho, Carlos G\'omez-Rodr\'iguez and Juan Luis
  Esteban",The variation of the sum of edge lengths in linear arrangements of trees,"Clarity and organization have been improved; typos and little
  mathematical errors have been corrected; relevant research by M. Iordanskii's
  is reviewed",,,,cs.DM cs.CL physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A fundamental problem in network science is the normalization of the
topological or physical distance between vertices, that requires understanding
the range of variation of the unnormalized distances. Here we investigate the
limits of the variation of the physical distance in linear arrangements of the
vertices of trees. In particular, we investigate various problems on the sum of
edge lengths in trees of a fixed size: the minimum and the maximum value of the
sum for specific trees, the minimum and the maximum in classes of trees (bistar
trees and caterpillar trees) and finally the minimum and the maximum for any
tree. We establish some foundations for research on optimality scores for
spatial networks in one dimension.
","[{'version': 'v1', 'created': 'Wed, 24 Jun 2020 21:53:39 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 09:32:17 GMT'}]",2020-10-20,"[['Ferrer-i-Cancho', 'Ramon', ''], ['Gómez-Rodríguez', 'Carlos', ''], ['Esteban', 'Juan Luis', '']]"
1365847,2010.09517,Bowen Li,"Bowen Li, Taeuk Kim, Reinald Kim Amplayo, Frank Keller",Heads-up! Unsupervised Constituency Parsing via Self-Attention Heads,AACL-IJCNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer-based pre-trained language models (PLMs) have dramatically
improved the state of the art in NLP across many tasks. This has led to
substantial interest in analyzing the syntactic knowledge PLMs learn. Previous
approaches to this question have been limited, mostly using test suites or
probes. Here, we propose a novel fully unsupervised parsing approach that
extracts constituency trees from PLM attention heads. We rank transformer
attention heads based on their inherent properties, and create an ensemble of
high-ranking heads to produce the final tree. Our method is adaptable to
low-resource languages, as it does not rely on development sets, which can be
expensive to annotate. Our experiments show that the proposed method often
outperform existing approaches if there is no development set present. Our
unsupervised parser can also be used as a tool to analyze the grammars PLMs
learn implicitly. For this, we use the parse trees induced by our method to
train a neural PCFG and compare it to a grammar derived from a human-annotated
treebank.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 13:51:40 GMT'}]",2020-10-20,"[['Li', 'Bowen', ''], ['Kim', 'Taeuk', ''], ['Amplayo', 'Reinald Kim', ''], ['Keller', 'Frank', '']]"
1365812,2010.09482,Shahram Khadivi,"Jingjing Huo, Christian Herold, Yingbo Gao, Leonard Dahlmann, Shahram
  Khadivi, and Hermann Ney",Diving Deep into Context-Aware Neural Machine Translation,Accepted at 5th Conference on Machine Translation (WMT20),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Context-aware neural machine translation (NMT) is a promising direction to
improve the translation quality by making use of the additional context, e.g.,
document-level translation, or having meta-information. Although there exist
various architectures and analyses, the effectiveness of different
context-aware NMT models is not well explored yet. This paper analyzes the
performance of document-level NMT models on four diverse domains with a varied
amount of parallel document-level bilingual data. We conduct a comprehensive
set of experiments to investigate the impact of document-level NMT. We find
that there is no single best approach to document-level NMT, but rather that
different architectures come out on top on different tasks. Looking at
task-specific problems, such as pronoun resolution or headline translation, we
find improvements in the context-aware systems, even in cases where the
corpus-level metrics like BLEU show no significant improvement. We also show
that document-level back-translation significantly helps to compensate for the
lack of document-level bi-texts.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 13:23:12 GMT'}]",2020-10-20,"[['Huo', 'Jingjing', ''], ['Herold', 'Christian', ''], ['Gao', 'Yingbo', ''], ['Dahlmann', 'Leonard', ''], ['Khadivi', 'Shahram', ''], ['Ney', 'Hermann', '']]"
1365789,2010.09459,Leshem Choshen,"Eyal Shnarch, Leshem Choshen, Guy Moshkowich, Noam Slonim, Ranit
  Aharonov","Unsupervised Expressive Rules Provide Explainability and Assist Human
  Experts Grasping New Domains",Accepted to Findings of EMNLP,,,,cs.CL cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Approaching new data can be quite deterrent; you do not know how your
categories of interest are realized in it, commonly, there is no labeled data
at hand, and the performance of domain adaptation methods is unsatisfactory.
  Aiming to assist domain experts in their first steps into a new task over a
new corpus, we present an unsupervised approach to reveal complex rules which
cluster the unexplored corpus by its prominent categories (or facets).
  These rules are human-readable, thus providing an important ingredient which
has become in short supply lately - explainability. Each rule provides an
explanation for the commonality of all the texts it clusters together.
  We present an extensive evaluation of the usefulness of these rules in
identifying target categories, as well as a user study which assesses their
interpretability.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 13:07:15 GMT'}]",2020-10-20,"[['Shnarch', 'Eyal', ''], ['Choshen', 'Leshem', ''], ['Moshkowich', 'Guy', ''], ['Slonim', 'Noam', ''], ['Aharonov', 'Ranit', '']]"
1365733,2010.09403,Du\v{s}an Vari\v{s},Du\v{s}an Vari\v{s} and Ond\v{r}ej Bojar,"Unsupervised Pretraining for Neural Machine Translation Using Elastic
  Weight Consolidation",ACL-SRW 2019 (camera-ready),,10.18653/v1/P19-2017,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This work presents our ongoing research of unsupervised pretraining in neural
machine translation (NMT). In our method, we initialize the weights of the
encoder and decoder with two language models that are trained with monolingual
data and then fine-tune the model on parallel data using Elastic Weight
Consolidation (EWC) to avoid forgetting of the original language modeling
tasks. We compare the regularization by EWC with the previous work that focuses
on regularization by language modeling objectives. The positive result is that
using EWC with the decoder achieves BLEU scores similar to the previous work.
However, the model converges 2-3 times faster and does not require the original
unlabeled training data during the fine-tuning stage. In contrast, the
regularization using EWC is less effective if the original and new tasks are
not closely related. We show that initializing the bidirectional NMT encoder
with a left-to-right language model and forcing the model to remember the
original left-to-right language modeling task limits the learning capacity of
the encoder for the whole bidirectional context.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 11:51:45 GMT'}]",2020-10-20,"[['Variš', 'Dušan', ''], ['Bojar', 'Ondřej', '']]"
1359754,2010.03424,Phuong Le-Hong,"The Viet Bui, Phuong Le-Hong",Cross-lingual Extended Named Entity Classification of Wikipedia Articles,Accepted to NTCIR-15,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The FPT.AI team participated in the SHINRA2020-ML subtask of the NTCIR-15
SHINRA task. This paper describes our method to solving the problem and
discusses the official results. Our method focuses on learning cross-lingual
representations, both on the word level and document level for page
classification. We propose a three-stage approach including multilingual model
pre-training, monolingual model fine-tuning and cross-lingual voting. Our
system is able to achieve the best scores for 25 out of 30 languages; and its
accuracy gaps to the best performing systems of the other five languages are
relatively small.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 14:06:09 GMT'}, {'version': 'v2', 'created': 'Sat, 17 Oct 2020 09:06:42 GMT'}]",2020-10-20,"[['Bui', 'The Viet', ''], ['Le-Hong', 'Phuong', '']]"
1365732,2010.09402,Sungwon Lyu,"Sungwon Lyu, Bokyung Son, Kichang Yang, and Jaekyoung Bae",Revisiting Modularized Multilingual NMT to Meet Industrial Demands,"The 2020 Conference on Empirical Methods in Natural Language
  Processing (EMNLP)",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The complete sharing of parameters for multilingual translation (1-1) has
been the mainstream approach in current research. However, degraded performance
due to the capacity bottleneck and low maintainability hinders its extensive
adoption in industries. In this study, we revisit the multilingual neural
machine translation model that only share modules among the same languages (M2)
as a practical alternative to 1-1 to satisfy industrial requirements. Through
comprehensive experiments, we identify the benefits of multi-way training and
demonstrate that the M2 can enjoy these benefits without suffering from the
capacity bottleneck. Furthermore, the interlingual space of the M2 allows
convenient modification of the model. By leveraging trained modules, we find
that incrementally added modules exhibit better performance than singly trained
models. The zero-shot performance of the added modules is even comparable to
supervised models. Our findings suggest that the M2 can be a competent
candidate for multilingual translation in industries.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 11:51:04 GMT'}]",2020-10-20,"[['Lyu', 'Sungwon', ''], ['Son', 'Bokyung', ''], ['Yang', 'Kichang', ''], ['Bae', 'Jaekyoung', '']]"
1365711,2010.09381,"Abdullatif K\""oksal","Abdullatif K\""oksal, Arzucan \""Ozg\""ur","The RELX Dataset and Matching the Multilingual Blanks for Cross-Lingual
  Relation Classification",Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Relation classification is one of the key topics in information extraction,
which can be used to construct knowledge bases or to provide useful information
for question answering. Current approaches for relation classification are
mainly focused on the English language and require lots of training data with
human annotations. Creating and annotating a large amount of training data for
low-resource languages is impractical and expensive. To overcome this issue, we
propose two cross-lingual relation classification models: a baseline model
based on Multilingual BERT and a new multilingual pretraining setup, which
significantly improves the baseline with distant supervision. For evaluation,
we introduce a new public benchmark dataset for cross-lingual relation
classification in English, French, German, Spanish, and Turkish, called RELX.
We also provide the RELX-Distant dataset, which includes hundreds of thousands
of sentences with relations from Wikipedia and Wikidata collected by distant
supervision for these languages. Our code and data are available at:
https://github.com/boun-tabi/RELX
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 11:08:16 GMT'}]",2020-10-20,"[['Köksal', 'Abdullatif', ''], ['Özgür', 'Arzucan', '']]"
1365987,2010.09657,Nipun Sadvilkar,Nipun Sadvilkar and Mark Neumann,PySBD: Pragmatic Sentence Boundary Disambiguation,"'PySBD: Pragmatic Sentence Boundary Disambiguation' is a short paper
  (5 Pages with references) accepted into 2nd Workshop for Natural Language
  Processing Open Source Software (NLP-OSS) at EMNLP 2020 happening on 19 Nov
  2020",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we present a rule-based sentence boundary disambiguation
Python package that works out-of-the-box for 22 languages. We aim to provide a
realistic segmenter which can provide logical sentences even when the format
and domain of the input text is unknown. In our work, we adapt the Golden Rules
Set (a language-specific set of sentence boundary exemplars) originally
implemented as a ruby gem - pragmatic_segmenter - which we ported to Python
with additional improvements and functionality. PySBD passes 97.92% of the
Golden Rule Set exemplars for English, an improvement of 25% over the next best
open-source Python tool.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 16:56:03 GMT'}]",2020-10-20,"[['Sadvilkar', 'Nipun', ''], ['Neumann', 'Mark', '']]"
1360107,2010.03777,Tianyu Liu,"Tianyu Liu, Xin Zheng, Xiaoan Ding, Baobao Chang and Zhifang Sui","An Empirical Study on Model-agnostic Debiasing Strategies for Robust
  Natural Language Inference",CoNLL 2020,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The prior work on natural language inference (NLI) debiasing mainly targets
at one or few known biases while not necessarily making the models more robust.
In this paper, we focus on the model-agnostic debiasing strategies and explore
how to (or is it possible to) make the NLI models robust to multiple distinct
adversarial attacks while keeping or even strengthening the models'
generalization power. We firstly benchmark prevailing neural NLI models
including pretrained ones on various adversarial datasets. We then try to
combat distinct known biases by modifying a mixture of experts (MoE) ensemble
method and show that it's nontrivial to mitigate multiple NLI biases at the
same time, and that model-level ensemble method outperforms MoE ensemble
method. We also perform data augmentation including text swap, word
substitution and paraphrase and prove its efficiency in combating various
(though not all) adversarial attacks at the same time. Finally, we investigate
several methods to merge heterogeneous training data (1.35M) and perform model
ensembling, which are straightforward but effective to strengthen NLI models.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 05:40:45 GMT'}, {'version': 'v2', 'created': 'Sat, 17 Oct 2020 14:57:09 GMT'}]",2020-10-20,"[['Liu', 'Tianyu', ''], ['Zheng', 'Xin', ''], ['Ding', 'Xiaoan', ''], ['Chang', 'Baobao', ''], ['Sui', 'Zhifang', '']]"
1365696,2010.09366,Xiaoyu Guo,Xiao-Yu Guo and Yuan-Fang Li and Gholamreza Haffari,Understanding Unnatural Questions Improves Reasoning over Text,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Complex question answering (CQA) over raw text is a challenging task. A
prominent approach to this task is based on the programmer-interpreter
framework, where the programmer maps the question into a sequence of reasoning
actions which is then executed on the raw text by the interpreter. Learning an
effective CQA model requires large amounts of human-annotated data,consisting
of the ground-truth sequence of reasoning actions, which is time-consuming and
expensive to collect at scale. In this paper, we address the challenge of
learning a high-quality programmer (parser) by projecting natural
human-generated questions into unnatural machine-generated questions which are
more convenient to parse. We firstly generate synthetic (question,action
sequence) pairs by a data generator, and train a semantic parser that
associates synthetic questions with their corresponding action sequences. To
capture the diversity when applied tonatural questions, we learn a projection
model to map natural questions into their most similar unnatural questions for
which the parser can work well. Without any natural training data, our
projection model provides high-quality action sequences for the CQA task.
Experimental results show that the QA model trained exclusively with synthetic
data generated by our method outperforms its state-of-the-art counterpart
trained on human-labeled data.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 10:22:16 GMT'}]",2020-10-20,"[['Guo', 'Xiao-Yu', ''], ['Li', 'Yuan-Fang', ''], ['Haffari', 'Gholamreza', '']]"
1365852,2010.09522,Devamanyu Hazarika,"Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumdar,
  Soujanya Poria, Roger Zimmermann, and Amir Zadeh",Emerging Trends of Multimodal Research in Vision and Language,,,,,cs.CV cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Deep Learning and its applications have cascaded impactful research and
development with a diverse range of modalities present in the real-world data.
More recently, this has enhanced research interests in the intersection of the
Vision and Language arena with its numerous applications and fast-paced growth.
In this paper, we present a detailed overview of the latest trends in research
pertaining to visual and language modalities. We look at its applications in
their task formulations and how to solve various problems related to semantic
perception and content generation. We also address task-specific trends, along
with their evaluation strategies and upcoming challenges. Moreover, we shed
some light on multi-disciplinary patterns and insights that have emerged in the
recent past, directing this field towards more modular and transparent
intelligent systems. This survey identifies key trends gravitating recent
literature in VisLang research and attempts to unearth directions that the
field is heading towards.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 13:55:10 GMT'}]",2020-10-20,"[['Uppal', 'Shagun', ''], ['Bhagat', 'Sarthak', ''], ['Hazarika', 'Devamanyu', ''], ['Majumdar', 'Navonil', ''], ['Poria', 'Soujanya', ''], ['Zimmermann', 'Roger', ''], ['Zadeh', 'Amir', '']]"
1357941,2010.01611,Pouya Rezazadeh Kalehbasti,"Liubov Nikolenko, Pouya Rezazadeh Kalehbasti","When in Doubt, Ask: Generating Answerable and Unanswerable Questions,
  Unsupervised",,,,,cs.CL cs.FL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Question Answering (QA) is key for making possible a robust communication
between human and machine. Modern language models used for QA have surpassed
the human-performance in several essential tasks; however, these models require
large amounts of human-generated training data which are costly and
time-consuming to create. This paper studies augmenting human-made datasets
with synthetic data as a way of surmounting this problem. A state-of-the-art
model based on deep transformers is used to inspect the impact of using
synthetic answerable and unanswerable questions to complement a well-known
human-made dataset. The results indicate a tangible improvement in the
performance of the language model (measured in terms of F1 and EM scores)
trained on the mixed dataset. Specifically, unanswerable question-answers prove
more effective in boosting the model: the F1 score gain from adding to the
original dataset the answerable, unanswerable, and combined question-answers
were 1.3%, 5.0%, and 6.7%, respectively. [Link to the Github repository:
https://github.com/lnikolenko/EQA]
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 15:56:44 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 02:29:13 GMT'}]",2020-10-20,"[['Nikolenko', 'Liubov', ''], ['Kalehbasti', 'Pouya Rezazadeh', '']]"
1263287,2003.12932,Anuj Gupta,"Ankit Kumar, Piyush Makhija, Anuj Gupta",Noisy Text Data: Achilles' Heel of BERT,"7 pages, 2 tables, 1 plot",,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Owing to the phenomenal success of BERT on various NLP tasks and benchmark
datasets, industry practitioners are actively experimenting with fine-tuning
BERT to build NLP applications for solving industry use cases. For most
datasets that are used by practitioners to build industrial NLP applications,
it is hard to guarantee absence of any noise in the data. While BERT has
performed exceedingly well for transferring the learnings from one use case to
another, it remains unclear how BERT performs when fine-tuned on noisy text. In
this work, we explore the sensitivity of BERT to noise in the data. We work
with most commonly occurring noise (spelling mistakes, typos) and show that
this results in significant degradation in the performance of BERT. We present
experimental results to show that BERT's performance on fundamental NLP tasks
like sentiment analysis and textual similarity drops significantly in the
presence of (simulated) noise on benchmark datasets viz. IMDB Movie Review,
STS-B, SST-2. Further, we identify shortcomings in the existing BERT pipeline
that are responsible for this drop in performance. Our findings suggest that
practitioners need to be vary of presence of noise in their datasets while
fine-tuning BERT to solve industry use cases.
","[{'version': 'v1', 'created': 'Sun, 29 Mar 2020 02:49:11 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Apr 2020 05:50:31 GMT'}, {'version': 'v3', 'created': 'Sun, 18 Oct 2020 09:22:01 GMT'}]",2020-10-20,"[['Kumar', 'Ankit', ''], ['Makhija', 'Piyush', ''], ['Gupta', 'Anuj', '']]"
1366022,2010.09692,Xusen Yin,"Xusen Yin, Jonathan May, Li Zhou, Kevin Small",Question Generation for Supporting Informational Query Intents,9 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Users frequently ask simple factoid questions when encountering question
answering (QA) systems, attenuating the impact of myriad recent works designed
to support more complex questions. Prompting users with automatically generated
suggested questions (SQs) can improve understanding of QA system capabilities
and thus facilitate using this technology more effectively. While question
generation (QG) is a well-established problem, existing methods are not
targeted at producing SQ guidance for human users seeking more in-depth
information about a specific concept. In particular, existing QG works are
insufficient for this task as the generated questions frequently (1) require
access to supporting documents as comprehension context (e.g., How many points
did LeBron score?) and (2) focus on short answer spans, often producing
peripheral factoid questions unlikely to attract interest. In this work, we aim
to generate self-explanatory questions that focus on the main document topics
and are answerable with variable length passages as appropriate. We satisfy
these requirements by using a BERT-based Pointer-Generator Network (BertPGN)
trained on the Natural Questions (NQ) dataset. First, we show that the BertPGN
model produces state-of-the-art QG performance for long and short answers for
in-domain NQ (BLEU-4 for 20.13 and 28.09, respectively). Secondly, we evaluate
this QG model on the out-of-domain NewsQA dataset automatically and with human
evaluation, demonstrating that our method produces better SQs for news
articles, even those from a different domain than the training data.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 17:30:08 GMT'}]",2020-10-20,"[['Yin', 'Xusen', ''], ['May', 'Jonathan', ''], ['Zhou', 'Li', ''], ['Small', 'Kevin', '']]"
1274857,2004.10087,Libo Qin,"Libo Qin, Xiao Xu, Wanxiang Che, Ting Liu","AGIF: An Adaptive Graph-Interactive Framework for Joint Multiple Intent
  Detection and Slot Filling","Accepted at Findings of EMNLP 2020. Data and code are available at
  this [URL] (https://github.com/LooperXX/AGIF)",,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In real-world scenarios, users usually have multiple intents in the same
utterance. Unfortunately, most spoken language understanding (SLU) models
either mainly focused on the single intent scenario, or simply incorporated an
overall intent context vector for all tokens, ignoring the fine-grained
multiple intents information integration for token-level slot prediction. In
this paper, we propose an Adaptive Graph-Interactive Framework (AGIF) for joint
multiple intent detection and slot filling, where we introduce an intent-slot
graph interaction layer to model the strong correlation between the slot and
intents. Such an interaction layer is applied to each token adaptively, which
has the advantage to automatically extract the relevant intents information,
making a fine-grained intent information integration for the token-level slot
prediction. Experimental results on three multi-intent datasets show that our
framework obtains substantial improvement and achieves the state-of-the-art
performance. In addition, our framework achieves new state-of-the-art
performance on two single-intent datasets.
","[{'version': 'v1', 'created': 'Tue, 21 Apr 2020 15:07:34 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Oct 2020 12:44:09 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Oct 2020 02:23:43 GMT'}, {'version': 'v4', 'created': 'Sat, 17 Oct 2020 04:28:29 GMT'}]",2020-10-20,"[['Qin', 'Libo', ''], ['Xu', 'Xiao', ''], ['Che', 'Wanxiang', ''], ['Liu', 'Ting', '']]"
1273937,2004.09167,Akshay Smit,"Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Y. Ng,
  Matthew P. Lungren","CheXbert: Combining Automatic Labelers and Expert Annotations for
  Accurate Radiology Report Labeling Using BERT",Accepted to EMNLP 2020,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The extraction of labels from radiology text reports enables large-scale
training of medical imaging models. Existing approaches to report labeling
typically rely either on sophisticated feature engineering based on medical
domain knowledge or manual annotations by experts. In this work, we introduce a
BERT-based approach to medical image report labeling that exploits both the
scale of available rule-based systems and the quality of expert annotations. We
demonstrate superior performance of a biomedically pretrained BERT model first
trained on annotations of a rule-based labeler and then finetuned on a small
set of expert annotations augmented with automated backtranslation. We find
that our final model, CheXbert, is able to outperform the previous best
rules-based labeler with statistical significance, setting a new SOTA for
report labeling on one of the largest datasets of chest x-rays.
","[{'version': 'v1', 'created': 'Mon, 20 Apr 2020 09:46:40 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Apr 2020 05:32:06 GMT'}, {'version': 'v3', 'created': 'Sun, 18 Oct 2020 20:30:22 GMT'}]",2020-10-20,"[['Smit', 'Akshay', ''], ['Jain', 'Saahil', ''], ['Rajpurkar', 'Pranav', ''], ['Pareek', 'Anuj', ''], ['Ng', 'Andrew Y.', ''], ['Lungren', 'Matthew P.', '']]"
1268231,2004.03461,Mantas Luko\v{s}evi\v{c}ius,Lukas Stankevi\v{c}ius and Mantas Luko\v{s}evi\v{c}ius,Testing pre-trained Transformer models for Lithuanian news clustering,Submission accepted at https://ivus.ktu.edu/,"Proceedings of the Information Society and University Studies
  2020, pp. 46-53, vol. 2698, CEUR, Kaunas, 2020, ISSN: 1613-0073",,,cs.IR cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A recent introduction of Transformer deep learning architecture made
breakthroughs in various natural language processing tasks. However,
non-English languages could not leverage such new opportunities with the
English text pre-trained models. This changed with research focusing on
multilingual models, where less-spoken languages are the main beneficiaries. We
compare pre-trained multilingual BERT, XLM-R, and older learned text
representation methods as encodings for the task of Lithuanian news clustering.
Our results indicate that publicly available pre-trained multilingual
Transformer models can be fine-tuned to surpass word vectors but still score
much lower than specially trained doc2vec embeddings.
","[{'version': 'v1', 'created': 'Fri, 3 Apr 2020 14:41:54 GMT'}]",2020-10-20,"[['Stankevičius', 'Lukas', ''], ['Lukoševičius', 'Mantas', '']]"
1264902,2004.00132,Jo\~ao Ant\^onio Chagas Nunes,"Jo\~ao Ant\^onio Chagas Nunes, David Mac\^edo, Cleber Zanchettin",AM-MobileNet1D: A Portable Model for Speaker Recognition,2020 International Joint Conference on Neural Networks (IJCNN),2020 International Joint Conference on Neural Networks (IJCNN),10.1109/IJCNN48605.2020.9207519,,cs.SD cs.CL cs.LG eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Speaker Recognition and Speaker Identification are challenging tasks with
essential applications such as automation, authentication, and security. Deep
learning approaches like SincNet and AM-SincNet presented great results on
these tasks. The promising performance took these models to real-world
applications that becoming fundamentally end-user driven and mostly mobile. The
mobile computation requires applications with reduced storage size,
non-processing and memory intensive and efficient energy-consuming. The deep
learning approaches, in contrast, usually are energy expensive, demanding
storage, processing power, and memory. To address this demand, we propose a
portable model called Additive Margin MobileNet1D (AM-MobileNet1D) to Speaker
Identification on mobile devices. We evaluated the proposed approach on TIMIT
and MIT datasets obtaining equivalent or better performances concerning the
baseline methods. Additionally, the proposed model takes only 11.6 megabytes on
disk storage against 91.2 from SincNet and AM-SincNet architectures, making the
model seven times faster, with eight times fewer parameters.
","[{'version': 'v1', 'created': 'Tue, 31 Mar 2020 21:42:59 GMT'}]",2020-10-20,"[['Nunes', 'João Antônio Chagas', ''], ['Macêdo', 'David', ''], ['Zanchettin', 'Cleber', '']]"
1365652,2010.09322,Anuj Diwan,"Anuj Diwan, Preethi Jyothi","Reduce and Reconstruct: Improving Low-resource End-to-end ASR Via
  Reconstruction Using Reduced Vocabularies","5 pages, 1 figure",,,,eess.AS cs.AI cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end automatic speech recognition (ASR) systems are increasingly being
favoured due to their direct treatment of the problem of speech to text
conversion. However, these systems are known to be data hungry and hence
underperform in low-resource settings. In this work, we propose a seemingly
simple but effective technique to improve low-resource end-to-end ASR
performance. We compress the output vocabulary of the end-to-end ASR system
using linguistically meaningful reductions and then reconstruct the original
vocabulary using a standalone module. Our objective is two-fold: to lessen the
burden on the low-resource end-to-end ASR system by reducing the output
vocabulary space and to design a powerful reconstruction module that recovers
sequences in the original vocabulary from sequences in the reduced vocabulary.
We present two reconstruction modules, an encoder decoder-based architecture
and a finite state transducer-based model. We demonstrate the efficacy of our
proposed techniques using ASR systems for two Indian languages, Gujarati and
Telugu.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 08:59:58 GMT'}]",2020-10-20,"[['Diwan', 'Anuj', ''], ['Jyothi', 'Preethi', '']]"
1347334,2009.05831,Kai Sun,"Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Claire Cardie","Improving Machine Reading Comprehension with Contextualized Commonsense
  Knowledge",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we aim to extract commonsense knowledge to improve machine
reading comprehension. We propose to represent relations implicitly by
situating structured knowledge in a context instead of relying on a pre-defined
set of relations, and we call it contextualized knowledge. Each piece of
contextualized knowledge consists of a pair of interrelated verbal and
nonverbal messages extracted from a script and the scene in which they occur as
context to implicitly represent the relation between the verbal and nonverbal
messages, which are originally conveyed by different modalities within the
script. We propose a two-stage fine-tuning strategy to use the large-scale
weakly-labeled data based on a single type of contextualized knowledge and
employ a teacher-student paradigm to inject multiple types of contextualized
knowledge into a student machine reader. Experimental results demonstrate that
our method outperforms a state-of-the-art baseline by a 4.3% improvement in
accuracy on the machine reading comprehension dataset C^3, wherein most of the
questions require unstated prior knowledge.
","[{'version': 'v1', 'created': 'Sat, 12 Sep 2020 17:20:01 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 02:22:06 GMT'}]",2020-10-20,"[['Sun', 'Kai', ''], ['Yu', 'Dian', ''], ['Chen', 'Jianshu', ''], ['Yu', 'Dong', ''], ['Cardie', 'Claire', '']]"
1348620,2009.07117,Tianyu Zhao,Tianyu Zhao and Tatsuya Kawahara,Multi-Referenced Training for Dialogue Response Generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In open-domain dialogue response generation, a dialogue context can be
continued with diverse responses, and the dialogue models should capture such
one-to-many relations. In this work, we first analyze the training objective of
dialogue models from the view of Kullback-Leibler divergence (KLD) and show
that the gap between the real world probability distribution and the
single-referenced data's probability distribution prevents the model from
learning the one-to-many relations efficiently. Then we explore approaches to
multi-referenced training in two aspects. Data-wise, we generate diverse pseudo
references from a powerful pretrained model to build multi-referenced data that
provides a better approximation of the real-world distribution. Model-wise, we
propose to equip variational models with an expressive prior, named linear
Gaussian model (LGM). Experimental results of automated evaluation and human
evaluation show that the methods yield significant improvements over baselines.
We will release our code and data in
https://github.com/ZHAOTING/dialog-processing.
","[{'version': 'v1', 'created': 'Tue, 15 Sep 2020 14:17:53 GMT'}, {'version': 'v2', 'created': 'Sun, 18 Oct 2020 08:02:58 GMT'}]",2020-10-20,"[['Zhao', 'Tianyu', ''], ['Kawahara', 'Tatsuya', '']]"
1255516,2003.05161,Laura Ruis,"Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, Brenden M.
  Lake","A Benchmark for Systematic Generalization in Grounded Language
  Understanding",accepted at NeurIPS 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Humans easily interpret expressions that describe unfamiliar situations
composed from familiar parts (""greet the pink brontosaurus by the ferris
wheel""). Modern neural networks, by contrast, struggle to interpret novel
compositions. In this paper, we introduce a new benchmark, gSCAN, for
evaluating compositional generalization in situated language understanding.
Going beyond a related benchmark that focused on syntactic aspects of
generalization, gSCAN defines a language grounded in the states of a grid
world, facilitating novel evaluations of acquiring linguistically motivated
rules. For example, agents must understand how adjectives such as 'small' are
interpreted relative to the current world state or how adverbs such as
'cautiously' combine with new verbs. We test a strong multi-modal baseline
model and a state-of-the-art compositional method finding that, in most cases,
they fail dramatically when generalization requires systematic compositional
rules.
","[{'version': 'v1', 'created': 'Wed, 11 Mar 2020 08:40:15 GMT'}, {'version': 'v2', 'created': 'Sat, 17 Oct 2020 17:02:02 GMT'}]",2020-10-20,"[['Ruis', 'Laura', ''], ['Andreas', 'Jacob', ''], ['Baroni', 'Marco', ''], ['Bouchacourt', 'Diane', ''], ['Lake', 'Brenden M.', '']]"
1288615,2005.08820,Azzam Mourad Dr.,"Azzam Mourad, Ali Srour, Haidar Harmanani, Cathia Jenainatiy, Mohamad
  Arafeh","Critical Impact of Social Networks Infodemic on Defeating Coronavirus
  COVID-19 Pandemic: Twitter-Based Study and Research Directions","11 pages, 10 figures, Journal Article",https://ieeexplore.ieee.org/document/9223699 (2020),10.1109/TNSM.2020.3031034,"In the IEEE Transactions on Network and Service Management (Early
  Access, 2020)",cs.SI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  News creation and consumption has been changing since the advent of social
media. An estimated 2.95 billion people in 2019 used social media worldwide.
The widespread of the Coronavirus COVID-19 resulted with a tsunami of social
media. Most platforms were used to transmit relevant news, guidelines and
precautions to people. According to WHO, uncontrolled conspiracy theories and
propaganda are spreading faster than the COVID-19 pandemic itself, creating an
infodemic and thus causing psychological panic, misleading medical advises, and
economic disruption. Accordingly, discussions have been initiated with the
objective of moderating all COVID-19 communications, except those initiated
from trusted sources such as the WHO and authorized governmental entities. This
paper presents a large-scale study based on data mined from Twitter. Extensive
analysis has been performed on approximately one million COVID-19 related
tweets collected over a period of two months. Furthermore, the profiles of
288,000 users were analyzed including unique users profiles, meta-data and
tweets context. The study noted various interesting conclusions including the
critical impact of the (1) exploitation of the COVID-19 crisis to redirect
readers to irrelevant topics and (2) widespread of unauthentic medical
precautions and information. Further data analysis revealed the importance of
using social networks in a global pandemic crisis by relying on credible users
with variety of occupations, content developers and influencers in specific
fields. In this context, several insights and findings have been provided while
elaborating computing and non-computing implications and research directions
for potential solutions and social networks management strategies during crisis
periods.
","[{'version': 'v1', 'created': 'Mon, 18 May 2020 15:53:13 GMT'}]",2020-10-20,"[['Mourad', 'Azzam', ''], ['Srour', 'Ali', ''], ['Harmanani', 'Haidar', ''], ['Jenainatiy', 'Cathia', ''], ['Arafeh', 'Mohamad', '']]"
1240378,2002.02955,Xavier Garcia,"Xavier Garcia, Pierre Foret, Thibault Sellam, Ankur P. Parikh",A Multilingual View of Unsupervised Machine Translation,Accepted at Findings of EMNLP 2020 [Fixed processing error.],,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a probabilistic framework for multilingual neural machine
translation that encompasses supervised and unsupervised setups, focusing on
unsupervised translation. In addition to studying the vanilla case where there
is only monolingual data available, we propose a novel setup where one language
in the (source, target) pair is not associated with any parallel data, but
there may exist auxiliary parallel data that contains the other. This auxiliary
data can naturally be utilized in our probabilistic framework via a novel
cross-translation loss term. Empirically, we show that our approach results in
higher BLEU scores over state-of-the-art unsupervised models on the WMT'14
English-French, WMT'16 English-German, and WMT'16 English-Romanian datasets in
most directions. In particular, we obtain a +1.65 BLEU advantage over the
best-performing unsupervised model in the Romanian-English direction.
","[{'version': 'v1', 'created': 'Fri, 7 Feb 2020 18:50:21 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Feb 2020 20:39:34 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Oct 2020 20:55:25 GMT'}, {'version': 'v4', 'created': 'Fri, 16 Oct 2020 20:41:25 GMT'}]",2020-10-20,"[['Garcia', 'Xavier', ''], ['Foret', 'Pierre', ''], ['Sellam', 'Thibault', ''], ['Parikh', 'Ankur P.', '']]"
1350528,2009.09025,Craig Stewart,"Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie",COMET: A Neural Framework for MT Evaluation,EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present COMET, a neural framework for training multilingual machine
translation evaluation models which obtains new state-of-the-art levels of
correlation with human judgements. Our framework leverages recent breakthroughs
in cross-lingual pretrained language modeling resulting in highly multilingual
and adaptable MT evaluation models that exploit information from both the
source input and a target-language reference translation in order to more
accurately predict MT quality. To showcase our framework, we train three models
with different types of human judgements: Direct Assessments, Human-mediated
Translation Edit Rate and Multidimensional Quality Metrics. Our models achieve
new state-of-the-art performance on the WMT 2019 Metrics shared task and
demonstrate robustness to high-performing systems.
","[{'version': 'v1', 'created': 'Fri, 18 Sep 2020 18:54:15 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 14:10:10 GMT'}]",2020-10-20,"[['Rei', 'Ricardo', ''], ['Stewart', 'Craig', ''], ['Farinha', 'Ana C', ''], ['Lavie', 'Alon', '']]"
1226138,2001.00725,Chenguang Zhu,"Ziyi Yang, Chenguang Zhu, Robert Gmyr, Michael Zeng, Xuedong Huang,
  Eric Darve","TED: A Pretrained Unsupervised Summarization Model with Theme Modeling
  and Denoising",Accepted at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text summarization aims to extract essential information from a piece of text
and transform the text into a concise version. Existing unsupervised
abstractive summarization models leverage recurrent neural networks framework
while the recently proposed transformer exhibits much more capability.
Moreover, most of previous summarization models ignore abundant unlabeled
corpora resources available for pretraining. In order to address these issues,
we propose TED, a transformer-based unsupervised abstractive summarization
system with pretraining on large-scale data. We first leverage the lead bias in
news articles to pretrain the model on millions of unlabeled corpora. Next, we
finetune TED on target domains through theme modeling and a denoising
autoencoder to enhance the quality of generated summaries. Notably, TED
outperforms all unsupervised abstractive baselines on NYT, CNN/DM and English
Gigaword datasets with various document styles. Further analysis shows that the
summaries generated by TED are highly abstractive, and each component in the
objective function of TED is highly effective.
","[{'version': 'v1', 'created': 'Fri, 3 Jan 2020 05:15:41 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Jan 2020 02:12:26 GMT'}, {'version': 'v3', 'created': 'Sun, 18 Oct 2020 00:26:09 GMT'}]",2020-10-20,"[['Yang', 'Ziyi', ''], ['Zhu', 'Chenguang', ''], ['Gmyr', 'Robert', ''], ['Zeng', 'Michael', ''], ['Huang', 'Xuedong', ''], ['Darve', 'Eric', '']]"
1283340,2005.03545,Devamanyu Hazarika,"Devamanyu Hazarika, Roger Zimmermann, Soujanya Poria","MISA: Modality-Invariant and -Specific Representations for Multimodal
  Sentiment Analysis",Accepted at ACM MM 2020 (Oral Paper),,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Multimodal Sentiment Analysis is an active area of research that leverages
multimodal signals for affective understanding of user-generated videos. The
predominant approach, addressing this task, has been to develop sophisticated
fusion techniques. However, the heterogeneous nature of the signals creates
distributional modality gaps that pose significant challenges. In this paper,
we aim to learn effective modality representations to aid the process of
fusion. We propose a novel framework, MISA, which projects each modality to two
distinct subspaces. The first subspace is modality-invariant, where the
representations across modalities learn their commonalities and reduce the
modality gap. The second subspace is modality-specific, which is private to
each modality and captures their characteristic features. These representations
provide a holistic view of the multimodal data, which is used for fusion that
leads to task predictions. Our experiments on popular sentiment analysis
benchmarks, MOSI and MOSEI, demonstrate significant gains over state-of-the-art
models. We also consider the task of Multimodal Humor Detection and experiment
on the recently proposed UR_FUNNY dataset. Here too, our model fares better
than strong baselines, establishing MISA as a useful multimodal framework.
","[{'version': 'v1', 'created': 'Thu, 7 May 2020 15:13:23 GMT'}, {'version': 'v2', 'created': 'Fri, 8 May 2020 12:14:37 GMT'}, {'version': 'v3', 'created': 'Mon, 19 Oct 2020 13:41:27 GMT'}]",2020-10-20,"[['Hazarika', 'Devamanyu', ''], ['Zimmermann', 'Roger', ''], ['Poria', 'Soujanya', '']]"
1355116,2009.13613,Danish Contractor,"Danish Contractor, Shashank Goel, Mausam, Parag Singla",Joint Spatio-Textual Reasoning for Answering Tourism Questions,Updated version,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Our goal is to answer real-world tourism questions that seek
Points-of-Interest (POI) recommendations. Such questions express various kinds
of spatial and non-spatial constraints, necessitating a combination of textual
and spatial reasoning. In response, we develop the first joint spatio-textual
reasoning model, which combines geo-spatial knowledge with information in
textual corpora to answer questions. We first develop a modular
spatial-reasoning network that uses geo-coordinates of location names mentioned
in a question, and of candidate answer POIs, to reason over only spatial
constraints. We then combine our spatial-reasoner with a textual reasoner in a
joint model and present experiments on a real world POI recommendation task. We
report substantial improvements over existing models with-out joint
spatio-textual reasoning.
","[{'version': 'v1', 'created': 'Mon, 28 Sep 2020 20:35:00 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 07:18:42 GMT'}]",2020-10-20,"[['Contractor', 'Danish', ''], ['Goel', 'Shashank', ''], ['Mausam', '', ''], ['Singla', 'Parag', '']]"
1223498,1912.11602,Chenguang Zhu,"Chenguang Zhu, Ziyi Yang, Robert Gmyr, Michael Zeng, Xuedong Huang",Make Lead Bias in Your Favor: Zero-shot Abstractive News Summarization,"10 pages, 7 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lead bias is a common phenomenon in news summarization, where early parts of
an article often contain the most salient information. While many algorithms
exploit this fact in summary generation, it has a detrimental effect on
teaching the model to discriminate and extract important information in
general. We propose that the lead bias can be leveraged in our favor in a
simple and effective way to pre-train abstractive news summarization models on
large-scale unlabeled news corpora: predicting the leading sentences using the
rest of an article. We collect a massive news corpus and conduct data cleaning
and filtering via statistical analysis. We then apply the proposed
self-supervised pre-training to existing generation models BART and T5 for
domain adaptation. Via extensive experiments on six benchmark datasets, we show
that this approach can dramatically improve the summarization quality and
achieve state-of-the-art results for zero-shot news summarization without any
fine-tuning. For example, in the DUC2003 dataset, the ROUGE-1 score of BART
increases 13.7% after the lead-bias pre-training. We deploy the model in
Microsoft News and provide public APIs as well as a demo website for
multi-lingual news summarization.
","[{'version': 'v1', 'created': 'Wed, 25 Dec 2019 06:05:44 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Jan 2020 04:04:03 GMT'}, {'version': 'v3', 'created': 'Fri, 16 Oct 2020 23:43:44 GMT'}]",2020-10-20,"[['Zhu', 'Chenguang', ''], ['Yang', 'Ziyi', ''], ['Gmyr', 'Robert', ''], ['Zeng', 'Michael', ''], ['Huang', 'Xuedong', '']]"
1366027,2010.09697,William Merrill,"William Merrill and Vivek Ramanujan and Yoav Goldberg and Roy Schwartz
  and Noah Smith",Parameter Norm Growth During Training of Transformers,Preprint. 9 body pages with appendix,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically some variant of gradient
descent (GD). To better understand this bias, we study the tendency of
transformer parameters to grow in magnitude during training. We find, both
theoretically and empirically, that, in certain contexts, GD increases the
parameter $L_2$ norm up to a threshold that itself increases with training-set
accuracy. This means increasing training accuracy over time enables the norm to
increase. Empirically, we show that the norm grows continuously over
pretraining for T5 (Raffel et al., 2019). We show that pretrained T5
approximates a semi-discretized network with saturated activation functions.
Such ""saturated"" networks are known to have a reduced capacity compared to the
original network family that can be described in automata-theoretic terms. This
suggests saturation is a new characterization of an inductive bias implicit in
GD that is of particular interest for NLP. While our experiments focus on
transformers, our theoretical analysis extends to other architectures with
similar formal properties, such as feedforward ReLU networks.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 17:40:38 GMT'}]",2020-10-20,"[['Merrill', 'William', ''], ['Ramanujan', 'Vivek', ''], ['Goldberg', 'Yoav', ''], ['Schwartz', 'Roy', ''], ['Smith', 'Noah', '']]"
1366023,2010.09693,David Wan,"David Wan, Zhengping Jiang, Chris Kedzie, Elsbeth Turcan, Peter Bell
  and Kathleen McKeown","Subtitles to Segmentation: Improving Low-Resource Speech-to-Text
  Translation Pipelines",,CLSST@LREC 2020 68-73,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we focus on improving ASR output segmentation in the context of
low-resource language speech-to-text translation. ASR output segmentation is
crucial, as ASR systems segment the input audio using purely acoustic
information and are not guaranteed to output sentence-like segments. Since most
MT systems expect sentences as input, feeding in longer unsegmented passages
can lead to sub-optimal performance. We explore the feasibility of using
datasets of subtitles from TV shows and movies to train better ASR segmentation
models. We further incorporate part-of-speech (POS) tag and dependency label
information (derived from the unsegmented ASR outputs) into our segmentation
model. We show that this noisy syntactic information can improve model
accuracy. We evaluate our models intrinsically on segmentation quality and
extrinsically on downstream MT performance, as well as downstream tasks
including cross-lingual information retrieval (CLIR) tasks and human relevance
assessments. Our model shows improved performance on downstream tasks for
Lithuanian and Bulgarian.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 17:32:40 GMT'}]",2020-10-20,"[['Wan', 'David', ''], ['Jiang', 'Zhengping', ''], ['Kedzie', 'Chris', ''], ['Turcan', 'Elsbeth', ''], ['Bell', 'Peter', ''], ['McKeown', 'Kathleen', '']]"
1357940,2010.01610,Wenjuan Han,"Wenjuan Han, Liwen Zhang, Yong Jiang, Kewei Tu",Adversarial Attack and Defense of Structured Prediction Models,Accepted to EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building an effective adversarial attacker and elaborating on countermeasures
for adversarial attacks for natural language processing (NLP) have attracted a
lot of research in recent years. However, most of the existing approaches focus
on classification problems. In this paper, we investigate attacks and defenses
for structured prediction tasks in NLP. Besides the difficulty of perturbing
discrete words and the sentence fluency problem faced by attackers in any NLP
tasks, there is a specific challenge to attackers of structured prediction
models: the structured output of structured prediction models is sensitive to
small perturbations in the input. To address these problems, we propose a novel
and unified framework that learns to attack a structured prediction model using
a sequence-to-sequence model with feedbacks from multiple reference models of
the same structured prediction task. Based on the proposed attack, we further
reinforce the victim model with adversarial training, making its prediction
more robust and accurate. We evaluate the proposed framework in dependency
parsing and part-of-speech tagging. Automatic and human evaluations show that
our proposed framework succeeds in both attacking state-of-the-art structured
prediction models and boosting them with adversarial training.
","[{'version': 'v1', 'created': 'Sun, 4 Oct 2020 15:54:03 GMT'}, {'version': 'v2', 'created': 'Sun, 18 Oct 2020 09:39:58 GMT'}]",2020-10-20,"[['Han', 'Wenjuan', ''], ['Zhang', 'Liwen', ''], ['Jiang', 'Yong', ''], ['Tu', 'Kewei', '']]"
1365643,2010.09313,Jonas Wallat,"Jonas Wallat, Jaspreet Singh, Avishek Anand",BERTnesia: Investigating the capture and forgetting of knowledge in BERT,BBNLP 2020,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
paper, we probe BERT specifically to understand and measure the relational
knowledge it captures. We utilize knowledge base completion tasks to probe
every layer of pre-trained as well as fine-tuned BERT (ranking, question
answering, NER). Our findings show that knowledge is not just contained in
BERT's final layers. Intermediate layers contribute a significant amount
(17-60%) to the total knowledge found. Probing intermediate layers also reveals
how different types of knowledge emerge at varying rates. When BERT is
fine-tuned, relational knowledge is forgotten but the extent of forgetting is
impacted by the fine-tuning objective but not the size of the dataset. We found
that ranking models forget the least and retain more knowledge in their final
layer. We release our code on github to repeat the experiments.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 08:46:30 GMT'}]",2020-10-20,"[['Wallat', 'Jonas', ''], ['Singh', 'Jaspreet', ''], ['Anand', 'Avishek', '']]"
1365310,2010.08980,Tom Hosking,"Laurie Burchell, Jie Chi, Tom Hosking, Nina Markl, Bonnie Webber",Querent Intent in Multi-Sentence Questions,"LAW XIV, COLING 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-sentence questions (MSQs) are sequences of questions connected by
relations which, unlike sequences of standalone questions, need to be answered
as a unit. Following Rhetorical Structure Theory (RST), we recognise that
different ""question discourse relations"" between the subparts of MSQs reflect
different speaker intents, and consequently elicit different answering
strategies. Correctly identifying these relations is therefore a crucial step
in automatically answering MSQs. We identify five different types of MSQs in
English, and define five novel relations to describe them. We extract over
162,000 MSQs from Stack Exchange to enable future research. Finally, we
implement a high-precision baseline classifier based on surface features.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 13:17:09 GMT'}]",2020-10-20,"[['Burchell', 'Laurie', ''], ['Chi', 'Jie', ''], ['Hosking', 'Tom', ''], ['Markl', 'Nina', ''], ['Webber', 'Bonnie', '']]"
1360691,2010.04361,Mehdi Rezaee,Mehdi Rezaee and Francis Ferraro,"Event Representation with Sequential, Semi-Supervised Discrete Variables",,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Within the context of event modeling and understanding, we propose a new
method for neural sequence modeling that takes partially-observed sequences of
discrete, external knowledge into account. We construct a sequential, neural
variational autoencoder that uses a carefully defined encoder, and
Gumbel-Softmax reparametrization, to allow for successful backpropagation
during training. We show that our approach outperforms multiple baselines and
the state-of-the-art in narrative script induction on multiple event modeling
tasks. We demonstrate that our approach converges more quickly.
","[{'version': 'v1', 'created': 'Fri, 9 Oct 2020 04:05:49 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 18:54:51 GMT'}]",2020-10-20,"[['Rezaee', 'Mehdi', ''], ['Ferraro', 'Francis', '']]"
1365098,2010.08768,Fatima Haouari,"Fatima Haouari, Maram Hasanain, Reem Suwaileh, Tamer Elsayed","ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation
  Detection",,,,,cs.CL cs.IR cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset
for misinformation detection composed of tweets containing claims from 27th
January till the end of April 2020. We collected 138 verified claims, mostly
from popular fact-checking websites, and identified 9.4K relevant tweets to
those claims. We then manually-annotated the tweets by veracity to support
research on misinformation detection, which is one of the major problems faced
during a pandemic. We aim to support two classes of misinformation detection
problems over Twitter: verifying free-text claims (called claim-level
verification) and verifying claims expressed in tweets (called tweet-level
verification). Our dataset covers, in addition to health, claims related to
other topical categories that were influenced by COVID-19, namely, social,
politics, sports, entertainment, and religious.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 11:21:40 GMT'}]",2020-10-20,"[['Haouari', 'Fatima', ''], ['Hasanain', 'Maram', ''], ['Suwaileh', 'Reem', ''], ['Elsayed', 'Tamer', '']]"
1365086,2010.08756,Sara Renjit,"Sara Renjit, Sumam Mary Idicula","CUSATNLP@HASOC-Dravidian-CodeMix-FIRE2020:Identifying Offensive Language
  from ManglishTweets",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  With the popularity of social media, communications through blogs, Facebook,
Twitter, and other plat-forms have increased. Initially, English was the only
medium of communication. Fortunately, now we can communicate in any language.
It has led to people using English and their own native or mother tongue
language in a mixed form. Sometimes, comments in other languages have English
transliterated format or other cases; people use the intended language scripts.
Identifying sentiments and offensive content from such code mixed tweets is a
necessary task in these times. We present a working model submitted for Task2
of the sub-track HASOC Offensive Language Identification- DravidianCodeMix in
Forum for Information Retrieval Evaluation, 2020. It is a message level
classification task. An embedding model-based classifier identifies offensive
and not offensive comments in our approach. We applied this method in the
Manglish dataset provided along with the sub-track.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 10:11:41 GMT'}]",2020-10-20,"[['Renjit', 'Sara', ''], ['Idicula', 'Sumam Mary', '']]"
1365073,2010.08743,Ismini Lourentzou,"Arkin Dharawat and Ismini Lourentzou and Alex Morales and ChengXiang
  Zhai","Drink bleach or do what now? Covid-HeRA: A dataset for risk-informed
  health decision making in the presence of COVID19 misinformation",,,,,cs.CL cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Given the wide spread of inaccurate medical advice related to the 2019
coronavirus pandemic (COVID-19), such as fake remedies, treatments and
prevention suggestions, misinformation detection has emerged as an open problem
of high importance and interest for the NLP community. To combat potential harm
of COVID19-related misinformation, we release Covid-HeRA, a dataset for health
risk assessment of COVID-19-related social media posts. More specifically, we
study the severity of each misinformation story, i.e., how harmful a message
believed by the audience can be and what type of signals can be used to
discover high malicious fake news and detect refuted claims. We present a
detailed analysis, evaluate several simple and advanced classification models,
and conclude with our experimental analysis that presents open challenges and
future directions.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 08:34:57 GMT'}]",2020-10-20,"[['Dharawat', 'Arkin', ''], ['Lourentzou', 'Ismini', ''], ['Morales', 'Alex', ''], ['Zhai', 'ChengXiang', '']]"
1365068,2010.08738,Jun Quan,"Jun Quan, Shian Zhang, Qian Cao, Zizhong Li and Deyi Xiong","RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich
  Semantic Annotations for Task-Oriented Dialogue Modeling",EMNLP 2020 (long paper),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In order to alleviate the shortage of multi-domain data and to capture
discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a
large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic
Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn
semantically annotated dialogues, with more than 150K utterances spanning over
12 domains, which is larger than all previous annotated H2H conversational
datasets. Both single- and multi-domain dialogues are constructed, accounting
for 65% and 35%, respectively. Each dialogue is labeled with comprehensive
dialogue annotations, including dialogue goal in the form of natural language
description, domain, dialogue states and acts at both the user and system side.
In addition to traditional dialogue annotations, we especially provide
linguistic annotations on discourse phenomena, e.g., ellipsis and coreference,
in dialogues, which are useful for dialogue coreference and ellipsis resolution
tasks. Apart from the fully annotated dataset, we also present a detailed
description of the data collection procedure, statistics and analysis of the
dataset. A series of benchmark models and results are reported, including
natural language understanding (intent detection & slot filling), dialogue
state tracking and dialogue context-to-text generation, as well as coreference
and ellipsis resolution, which facilitate the baseline comparison for future
research on this corpus.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 08:18:59 GMT'}]",2020-10-20,"[['Quan', 'Jun', ''], ['Zhang', 'Shian', ''], ['Cao', 'Qian', ''], ['Li', 'Zizhong', ''], ['Xiong', 'Deyi', '']]"
1365055,2010.08725,Chenhui Chu,"Andrew Merritt, Chenhui Chu, Yuki Arase","A Corpus for English-Japanese Multimodal Neural Machine Translation with
  Comparable Sentences",,,,,cs.CL cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal neural machine translation (NMT) has become an increasingly
important area of research over the years because additional modalities, such
as image data, can provide more context to textual data. Furthermore, the
viability of training multimodal NMT models without a large parallel corpus
continues to be investigated due to low availability of parallel sentences with
images, particularly for English-Japanese data. However, this void can be
filled with comparable sentences that contain bilingual terms and parallel
phrases, which are naturally created through media such as social network posts
and e-commerce product descriptions. In this paper, we propose a new multimodal
English-Japanese corpus with comparable sentences that are compiled from
existing image captioning datasets. In addition, we supplement our comparable
sentences with a smaller parallel corpus for validation and test purposes. To
test the performance of this comparable sentence translation scenario, we train
several baseline NMT models with our comparable corpus and evaluate their
English-Japanese translation performance. Due to low translation scores in our
baseline experiments, we believe that current multimodal NMT models are not
designed to effectively utilize comparable sentence data. Despite this, we hope
for our corpus to be used to further research into multimodal NMT with
comparable sentences.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 06:12:25 GMT'}]",2020-10-20,"[['Merritt', 'Andrew', ''], ['Chu', 'Chenhui', ''], ['Arase', 'Yuki', '']]"
1183826,1909.13788,Junxian He,"Junxian He, Jiatao Gu, Jiajun Shen, Marc'Aurelio Ranzato",Revisiting Self-Training for Neural Sequence Generation,"ICLR 2020. The first two authors contributed equally. Updated to fix
  typos",,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Self-training is one of the earliest and simplest semi-supervised methods.
The key idea is to augment the original labeled dataset with unlabeled data
paired with the model's prediction (i.e. the pseudo-parallel data). While
self-training has been extensively studied on classification problems, in
complex sequence generation tasks (e.g. machine translation) it is still
unclear how self-training works due to the compositionality of the target
space. In this work, we first empirically show that self-training is able to
decently improve the supervised baseline on neural sequence generation tasks.
Through careful examination of the performance gains, we find that the
perturbation on the hidden states (i.e. dropout) is critical for self-training
to benefit from the pseudo-parallel data, which acts as a regularizer and
forces the model to yield close predictions for similar unlabeled inputs. Such
effect helps the model correct some incorrect predictions on unlabeled data. To
further encourage this mechanism, we propose to inject noise to the input
space, resulting in a ""noisy"" version of self-training. Empirical study on
standard machine translation and text summarization benchmarks shows that noisy
self-training is able to effectively utilize unlabeled data and improve the
performance of the supervised baseline by a large margin.
","[{'version': 'v1', 'created': 'Mon, 30 Sep 2019 15:30:00 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Feb 2020 08:35:41 GMT'}, {'version': 'v3', 'created': 'Sun, 18 Oct 2020 22:49:31 GMT'}]",2020-10-20,"[['He', 'Junxian', ''], ['Gu', 'Jiatao', ''], ['Shen', 'Jiajun', ''], ['Ranzato', ""Marc'Aurelio"", '']]"
1365042,2010.08712,Meng Cao,"Meng Cao, Yue Dong, Jiapeng Wu, Jackie Chi Kit Cheung",Factual Error Correction for Abstractive Summarization Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural abstractive summarization systems have achieved promising progress,
thanks to the availability of large-scale datasets and models pre-trained with
self-supervised methods. However, ensuring the factual consistency of the
generated summaries for abstractive summarization systems is a challenge. We
propose a post-editing corrector module to address this issue by identifying
and correcting factual errors in generated summaries. The neural corrector
model is pre-trained on artificial examples that are created by applying a
series of heuristic transformations on reference summaries. These
transformations are inspired by an error analysis of state-of-the-art
summarization model outputs. Experimental results show that our model is able
to correct factual errors in summaries generated by other neural summarization
models and outperforms previous models on factual consistency evaluation on the
CNN/DailyMail dataset. We also find that transferring from artificial error
correction to downstream settings is still very challenging.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 04:24:16 GMT'}]",2020-10-20,"[['Cao', 'Meng', ''], ['Dong', 'Yue', ''], ['Wu', 'Jiapeng', ''], ['Cheung', 'Jackie Chi Kit', '']]"
1365038,2010.08708,Wei Han,"Hantao Huang, Tao Han, Wei Han, Deep Yap, Cheng-Ming Chiang","Answer-checking in Context: A Multi-modal FullyAttention Network for
  Visual Question Answering",Accepted in ICPR2020,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Visual Question Answering (VQA) is challenging due to the complex cross-modal
relations. It has received extensive attention from the research community.
From the human perspective, to answer a visual question, one needs to read the
question and then refer to the image to generate an answer. This answer will
then be checked against the question and image again for the final
confirmation. In this paper, we mimic this process and propose a fully
attention based VQA architecture. Moreover, an answer-checking module is
proposed to perform a unified attention on the jointly answer, question and
image representation to update the answer. This mimics the human answer
checking process to consider the answer in the context. With answer-checking
modules and transferred BERT layers, our model achieves the state-of-the-art
accuracy 71.57\% using fewer parameters on VQA-v2.0 test-standard split.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 03:37:16 GMT'}]",2020-10-20,"[['Huang', 'Hantao', ''], ['Han', 'Tao', ''], ['Han', 'Wei', ''], ['Yap', 'Deep', ''], ['Chiang', 'Cheng-Ming', '']]"
1365014,2010.08684,Mihail Eric,"Shikib Mehri, Mihail Eric, Dilek Hakkani-Tur",Example-Driven Intent Prediction with Observers,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  A key challenge of dialog systems research is to effectively and efficiently
adapt to new domains. A scalable paradigm for adaptation necessitates the
development of generalizable models that perform well in few-shot settings. In
this paper, we focus on the intent classification problem which aims to
identify user intents given utterances addressed to the dialog system. We
propose two approaches for improving the generalizability of utterance
classification models: (1) example-driven training and (2) observers.
Example-driven training learns to classify utterances by comparing to examples,
thereby using the underlying encoder as a sentence similarity model. Prior work
has shown that BERT-like models tend to attribute a significant amount of
attention to the [CLS] token, which we hypothesize results in diluted
representations. Observers are tokens that are not attended to, and are an
alternative to the [CLS] token. The proposed methods attain state-of-the-art
results on three intent prediction datasets (Banking, Clinc}, and HWU) in both
the full data and few-shot (10 examples per intent) settings. Furthermore, we
demonstrate that the proposed approach can transfer to new intents and across
datasets without any additional training.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 01:03:06 GMT'}]",2020-10-20,"[['Mehri', 'Shikib', ''], ['Eric', 'Mihail', ''], ['Hakkani-Tur', 'Dilek', '']]"
1365000,2010.08670,Yanru Qu,"Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Jiawei Han, Weizhu
  Chen","CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for
  Natural Language Understanding",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Data augmentation has been demonstrated as an effective strategy for
improving model generalization and data efficiency. However, due to the
discrete nature of natural language, designing label-preserving transformations
for text data tends to be more challenging. In this paper, we propose a novel
data augmentation framework dubbed CoDA, which synthesizes diverse and
informative augmented examples by integrating multiple transformations
organically. Moreover, a contrastive regularization objective is introduced to
capture the global relationship among all the data samples. A momentum encoder
along with a memory bank is further leveraged to better estimate the
contrastive loss. To verify the effectiveness of the proposed framework, we
apply CoDA to Transformer-based models on a wide range of natural language
understanding tasks. On the GLUE benchmark, CoDA gives rise to an average
improvement of 2.2% while applied to the RoBERTa-large model. More importantly,
it consistently exhibits stronger results relative to several competitive data
augmentation and adversarial training base-lines (including the low-resource
settings). Extensive experiments show that the proposed contrastive objective
can be flexibly combined with various data augmentation approaches to further
boost their performance, highlighting the wide applicability of the CoDA
framework.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 23:57:03 GMT'}]",2020-10-20,"[['Qu', 'Yanru', ''], ['Shen', 'Dinghan', ''], ['Shen', 'Yelong', ''], ['Sajeev', 'Sandra', ''], ['Han', 'Jiawei', ''], ['Chen', 'Weizhu', '']]"
1364982,2010.08652,Jian Ni,Jian Ni and Taesun Moon and Parul Awasthy and Radu Florian,Cross-Lingual Relation Extraction with Transformers,11 pages,,,,cs.CL cs.AI cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Relation extraction (RE) is one of the most important tasks in information
extraction, as it provides essential information for many NLP applications. In
this paper, we propose a cross-lingual RE approach that does not require any
human annotation in a target language or any cross-lingual resources. Building
upon unsupervised cross-lingual representation learning frameworks, we develop
several deep Transformer based RE models with a novel encoding scheme that can
effectively encode both entity location and entity type information. Our RE
models, when trained with English data, outperform several deep neural network
based English RE models. More importantly, our models can be applied to perform
zero-shot cross-lingual RE, achieving the state-of-the-art cross-lingual RE
performance on two datasets (68-89% of the accuracy of the supervised
target-language RE model). The high cross-lingual transfer efficiency without
requiring additional training data or cross-lingual resources shows that our RE
models are especially useful for low-resource languages.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 22:23:37 GMT'}]",2020-10-20,"[['Ni', 'Jian', ''], ['Moon', 'Taesun', ''], ['Awasthy', 'Parul', ''], ['Florian', 'Radu', '']]"
1364972,2010.08642,Tejas Srinivasan,"Tejas Srinivasan, Ramon Sanabria, Florian Metze, Desmond Elliott",Multimodal Speech Recognition with Unstructured Audio Masking,"Accepted to NLP Beyond Text workshop, EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Visual context has been shown to be useful for automatic speech recognition
(ASR) systems when the speech signal is noisy or corrupted. Previous work,
however, has only demonstrated the utility of visual context in an unrealistic
setting, where a fixed set of words are systematically masked in the audio. In
this paper, we simulate a more realistic masking scenario during model
training, called RandWordMask, where the masking can occur for any word
segment. Our experiments on the Flickr 8K Audio Captions Corpus show that
multimodal ASR can generalize to recover different types of masked words in
this unstructured masking setting. Moreover, our analysis shows that our models
are capable of attending to the visual signal when the audio signal is
corrupted. These results show that multimodal ASR systems can leverage the
visual signal in more generalized noisy scenarios.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 21:49:20 GMT'}]",2020-10-20,"[['Srinivasan', 'Tejas', ''], ['Sanabria', 'Ramon', ''], ['Metze', 'Florian', ''], ['Elliott', 'Desmond', '']]"
1364948,2010.08618,Sudha Rao,"Allison Hegel, Sudha Rao, Asli Celikyilmaz and Bill Dolan",Substance over Style: Document-Level Targeted Content Transfer,This paper has been accepted to be published at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing language models excel at writing from scratch, but many real-world
scenarios require rewriting an existing document to fit a set of constraints.
Although sentence-level rewriting has been fairly well-studied, little work has
addressed the challenge of rewriting an entire document coherently. In this
work, we introduce the task of document-level targeted content transfer and
address it in the recipe domain, with a recipe as the document and a dietary
restriction (such as vegan or dairy-free) as the targeted constraint. We
propose a novel model for this task based on the generative pre-trained
language model (GPT-2) and train on a large number of roughly-aligned recipe
pairs (https://github.com/microsoft/document-level-targeted-content-transfer).
Both automatic and human evaluations show that our model out-performs existing
methods by generating coherent and diverse rewrites that obey the constraint
while remaining close to the original document. Finally, we analyze our model's
rewrites to assess progress toward the goal of making language generation more
attuned to constraints that are substantive rather than stylistic.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 20:26:10 GMT'}]",2020-10-20,"[['Hegel', 'Allison', ''], ['Rao', 'Sudha', ''], ['Celikyilmaz', 'Asli', ''], ['Dolan', 'Bill', '']]"
1364936,2010.08606,Yiding Hao,Yiding Hao,Evaluating Attribution Methods using White-Box LSTMs,"To appear in the Proceedings of the 2020 EMNLP Workshop BlackboxNLP:
  Analyzing and Interpreting Neural Networks for NLP",,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Interpretability methods for neural networks are difficult to evaluate
because we do not understand the black-box models typically used to test them.
This paper proposes a framework in which interpretability methods are evaluated
using manually constructed networks, which we call white-box networks, whose
behavior is understood a priori. We evaluate five methods for producing
attribution heatmaps by applying them to white-box LSTM classifiers for tasks
based on formal languages. Although our white-box classifiers solve their tasks
perfectly and transparently, we find that all five attribution methods fail to
produce the expected model explanations.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 19:55:32 GMT'}]",2020-10-20,"[['Hao', 'Yiding', '']]"
1364923,2010.08593,Suraj Kothawade,"Suraj Kothawade, Jiten Girdhar, Chandrashekhar Lavania, Rishabh Iyer",Deep Submodular Networks for Extractive Data Summarization,,,,,cs.LG cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep Models are increasingly becoming prevalent in summarization problems
(e.g. document, video and images) due to their ability to learn complex feature
interactions and representations. However, they do not model characteristics
such as diversity, representation, and coverage, which are also very important
for summarization tasks. On the other hand, submodular functions naturally
model these characteristics because of their diminishing returns property. Most
approaches for modelling and learning submodular functions rely on very simple
models, such as weighted mixtures of submodular functions. Unfortunately, these
models only learn the relative importance of the different submodular functions
(such as diversity, representation or importance), but cannot learn more
complex feature representations, which are often required for state-of-the-art
performance. We propose Deep Submodular Networks (DSN), an end-to-end learning
framework that facilitates the learning of more complex features and richer
functions, crafted for better modelling of all aspects of summarization. The
DSN framework can be used to learn features appropriate for summarization from
scratch. We demonstrate the utility of DSNs on both generic and query focused
image-collection summarization, and show significant improvement over the
state-of-the-art. In particular, we show that DSNs outperform simple mixture
models using off the shelf features. Secondly, we also show that just using
four submodular functions in a DSN with end-to-end learning performs comparably
to the state-of-the-art mixture model with a hand-crafted set of 594 components
and outperforms other methods for image collection summarization.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 19:06:15 GMT'}]",2020-10-20,"[['Kothawade', 'Suraj', ''], ['Girdhar', 'Jiten', ''], ['Lavania', 'Chandrashekhar', ''], ['Iyer', 'Rishabh', '']]"
1364900,2010.08570,Markus Leippold,Rahul Mishra and Dhruv Gupta and Markus Leippold,Generating Fact Checking Summaries for Web Claims,"Accepted paper; The 2020 Conference on Empirical Methods in Natural
  Language Processing EMNLP - WNUT",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present SUMO, a neural attention-based approach that learns to establish
the correctness of textual claims based on evidence in the form of text
documents (e.g., news articles or Web documents). SUMO further generates an
extractive summary by presenting a diversified set of sentences from the
documents that explain its decision on the correctness of the textual claim.
Prior approaches to address the problem of fact checking and evidence
extraction have relied on simple concatenation of claim and document word
embeddings as an input to claim driven attention weight computation. This is
done so as to extract salient words and sentences from the documents that help
establish the correctness of the claim. However, this design of claim-driven
attention does not capture the contextual information in documents properly. We
improve on the prior art by using improved claim and title guided hierarchical
attention to model effective contextual cues. We show the efficacy of our
approach on datasets concerning political, healthcare, and environmental
issues.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 18:10:47 GMT'}]",2020-10-20,"[['Mishra', 'Rahul', ''], ['Gupta', 'Dhruv', ''], ['Leippold', 'Markus', '']]"
1362533,2010.06203,Art\=urs Stafanovi\v{c}s,"Art\=urs Stafanovi\v{c}s, Toms Bergmanis, M\=arcis Pinnis","Mitigating Gender Bias in Machine Translation with Target Gender
  Annotations",EMNLP 2020 Fifth Conference on Machine Translation (WMT20),,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  When translating ""The secretary asked for details."" to a language with
grammatical gender, it might be necessary to determine the gender of the
subject ""secretary"". If the sentence does not contain the necessary
information, it is not always possible to disambiguate. In such cases, machine
translation systems select the most common translation option, which often
corresponds to the stereotypical translations, thus potentially exacerbating
prejudice and marginalisation of certain groups and people. We argue that the
information necessary for an adequate translation can not always be deduced
from the sentence being translated or even might depend on external knowledge.
Therefore, in this work, we propose to decouple the task of acquiring the
necessary information from the task of learning to translate correctly when
such information is available. To that end, we present a method for training
machine translation systems to use word-level annotations containing
information about subject's gender. To prepare training data, we annotate
regular source language words with grammatical gender information of the
corresponding target language words. Using such data to train machine
translation systems reduces their reliance on gender stereotypes when
information about the subject's gender is available. Our experiments on five
language pairs show that this allows improving accuracy on the WinoMT test set
by up to 25.8 percentage points.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 07:07:59 GMT'}, {'version': 'v2', 'created': 'Sun, 18 Oct 2020 16:41:56 GMT'}]",2020-10-20,"[['Stafanovičs', 'Artūrs', ''], ['Bergmanis', 'Toms', ''], ['Pinnis', 'Mārcis', '']]"
1364896,2010.08566,Peter West,"Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena Hwang,
  Yejin Choi",Reflective Decoding: Unsupervised Paraphrasing and Abductive Reasoning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pretrained Language Models (LMs) generate text with remarkable quality,
novelty,and coherence. Yet applying LMs to the problems of paraphrasing and
infilling currently requires direct supervision, since these tasks break the
left-to-right generation setup of pretrained LMs. We present Reflective
Decoding, a novel unsupervised approach to apply the capabilities of pretrained
LMs to non-sequential tasks. Our approach is general and applicable to two
distant tasks - paraphrasing and abductive reasoning. It requires no
supervision or parallel corpora, only two pretrained language models: forward
and backward. Reflective Decoding operates in two intuitive steps. In the
contextualization step, we use LMs to generate many left and right contexts
which collectively capture the meaning of the input sentence. Then, in the
reflection step we decode in the semantic neighborhood of the input,
conditioning on an ensemble of generated contexts with the reverse direction
LM. We reflect through the generated contexts, effectively using them as an
intermediate meaning representation to generate conditional output. Empirical
results demonstrate that Reflective Decoding outperforms strong unsupervised
baselines on both paraphrasing and abductive text infilling, significantly
narrowing the gap between unsupervised and supervised methods.Reflective
Decoding introduces the concept of using generated contexts to represent
meaning, opening up new possibilities for unsupervised conditional text
generation.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 18:02:07 GMT'}]",2020-10-20,"[['West', 'Peter', ''], ['Lu', 'Ximing', ''], ['Holtzman', 'Ari', ''], ['Bhagavatula', 'Chandra', ''], ['Hwang', 'Jena', ''], ['Choi', 'Yejin', '']]"
1364047,2010.07717,Weijie Yu,"Weijie Yu, Chen Xu, Jun Xu, Liang Pang, Xiaopeng Gao, Xiaozhao Wang
  and Ji-Rong Wen","Wasserstein Distance Regularized Sequence Representation for Text
  Matching in Asymmetrical Domains",accepted as long paper by EMNLP 2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One approach to matching texts from asymmetrical domains is projecting the
input sequences into a common semantic space as feature vectors upon which the
matching function can be readily defined and learned. In real-world matching
practices, it is often observed that with the training goes on, the feature
vectors projected from different domains tend to be indistinguishable. The
phenomenon, however, is often overlooked in existing matching models. As a
result, the feature vectors are constructed without any regularization, which
inevitably increases the difficulty of learning the downstream matching
functions. In this paper, we propose a novel match method tailored for text
matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein
distance-based regularizer is defined to regularize the features vectors
projected from different domains. As a result, the method enforces the feature
projection function to generate vectors such that those correspond to different
domains cannot be easily discriminated. The training process of WD-Match
amounts to a game that minimizes the matching loss regularized by the
Wasserstein distance. WD-Match can be used to improve different text matching
methods, by using the method as its underlying matching model. Four popular
text matching methods have been exploited in the paper. Experimental results
based on four publicly available benchmarks showed that WD-Match consistently
outperformed the underlying methods and the baselines.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 12:52:09 GMT'}, {'version': 'v2', 'created': 'Sat, 17 Oct 2020 01:32:08 GMT'}]",2020-10-20,"[['Yu', 'Weijie', ''], ['Xu', 'Chen', ''], ['Xu', 'Jun', ''], ['Pang', 'Liang', ''], ['Gao', 'Xiaopeng', ''], ['Wang', 'Xiaozhao', ''], ['Wen', 'Ji-Rong', '']]"
1323737,2007.12223,Tianlong Chen,"Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang,
  Zhangyang Wang, Michael Carbin",The Lottery Ticket Hypothesis for Pre-trained BERT Networks,NeurIPS 2020,,,,cs.LG cs.CL cs.NE stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In natural language processing (NLP), enormous pre-trained models like BERT
have become the standard starting point for training on a range of downstream
tasks, and similar trends are emerging in other areas of deep learning. In
parallel, work on the lottery ticket hypothesis has shown that models for NLP
and computer vision contain smaller matching subnetworks capable of training in
isolation to full accuracy and transferring to other tasks. In this work, we
combine these observations to assess whether such trainable, transferrable
subnetworks exist in pre-trained BERT models. For a range of downstream tasks,
we indeed find matching subnetworks at 40% to 90% sparsity. We find these
subnetworks at (pre-trained) initialization, a deviation from prior NLP
research where they emerge only after some amount of training. Subnetworks
found on the masked language modeling task (the same task used to pre-train the
model) transfer universally; those found on other tasks transfer in a limited
fashion if at all. As large-scale pre-training becomes an increasingly central
paradigm in deep learning, our results demonstrate that the main lottery ticket
observations remain relevant in this context. Codes available at
https://github.com/VITA-Group/BERT-Tickets.
","[{'version': 'v1', 'created': 'Thu, 23 Jul 2020 19:35:39 GMT'}, {'version': 'v2', 'created': 'Sun, 18 Oct 2020 20:10:29 GMT'}]",2020-10-20,"[['Chen', 'Tianlong', ''], ['Frankle', 'Jonathan', ''], ['Chang', 'Shiyu', ''], ['Liu', 'Sijia', ''], ['Zhang', 'Yang', ''], ['Wang', 'Zhangyang', ''], ['Carbin', 'Michael', '']]"
1363482,2010.07152,Kai Wang,"Kai Wang, Yu Liu, Qian Ma, Quan Z. Sheng",Multi-teacher Knowledge Distillation for Knowledge Graph Completion,"11 pages, 4 figures",,,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Link prediction based on knowledge graph embedding (KGE) aims to predict new
triples to complete knowledge graphs (KGs) automatically. However, recent KGE
models tend to improve performance by excessively increasing vector dimensions,
which would cause enormous training costs and save storage in practical
applications. To address this problem, we first theoretically analyze the
capacity of low-dimensional space for KG embeddings based on the principle of
minimum entropy. Then, we propose a novel knowledge distillation framework for
knowledge graph embedding, utilizing multiple low-dimensional KGE models as
teachers. Under a novel iterative distillation strategy, the MulDE model
produces soft labels according to training epochs and student performance
adaptively. The experimental results show that MulDE can effectively improve
the performance and training speed of low-dimensional KGE models. The distilled
32-dimensional models are very competitive compared to some of state-or-the-art
(SotA) high-dimensional methods on several commonly-used datasets.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 15:09:27 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 09:33:41 GMT'}]",2020-10-20,"[['Wang', 'Kai', ''], ['Liu', 'Yu', ''], ['Ma', 'Qian', ''], ['Sheng', 'Quan Z.', '']]"
1365100,2010.08770,Ismail Shahin,"Mohamed Bader, Ismail Shahin, Abdelfatah Hassan","Studying the Similarity of COVID-19 Sounds based on Correlation Analysis
  of MFCC","5 pages, 4 figures, conference paper",,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently there has been a formidable work which has been put up from the
people who are working in the frontlines such as hospitals, clinics, and labs
alongside researchers and scientists who are also putting tremendous efforts in
the fight against COVID-19 pandemic. Due to the preposterous spread of the
virus, the integration of the artificial intelligence has taken a considerable
part in the health sector, by implementing the fundamentals of Automatic Speech
Recognition (ASR) and deep learning algorithms. In this paper, we illustrate
the importance of speech signal processing in the extraction of the
Mel-Frequency Cepstral Coefficients (MFCCs) of the COVID-19 and non-COVID-19
samples and find their relationship using Pearson correlation coefficients. Our
results show high similarity in MFCCs between different COVID-19 cough and
breathing sounds, while MFCC of voice is more robust between COVID-19 and
non-COVID-19 samples. Moreover, our results are preliminary, and there is a
possibility to exclude the voices of COVID-19 patients from further processing
in diagnosing the disease.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 11:38:05 GMT'}]",2020-10-20,"[['Bader', 'Mohamed', ''], ['Shahin', 'Ismail', ''], ['Hassan', 'Abdelfatah', '']]"
1360622,2010.04292,Douglas Guilbeault R,"Bhargav Srinivasa Desikan, Tasker Hull, Ethan O. Nadler, Douglas
  Guilbeault, Aabir Abubaker Kar, Mark Chu and Donald Ruggiero Lo Sardo",comp-syn: Perceptually Grounded Word Embeddings with Color,"9 pages, 3 figures, all code and data available at
  https://github.com/comp-syn/comp-syn. Forthcoming in the Proceedings of the
  28th International Conference on Computational Linguistics",,,,cs.CL cs.LG cs.SI,http://creativecommons.org/licenses/by/4.0/,"  Popular approaches to natural language processing create word embeddings
based on textual co-occurrence patterns, but often ignore embodied, sensory
aspects of language. Here, we introduce the Python package comp-syn, which
provides grounded word embeddings based on the perceptually uniform color
distributions of Google Image search results. We demonstrate that comp-syn
significantly enriches models of distributional semantics. In particular, we
show that (1) comp-syn predicts human judgments of word concreteness with
greater accuracy and in a more interpretable fashion than word2vec using
low-dimensional word-color embeddings, and (2) comp-syn performs comparably to
word2vec on a metaphorical vs. literal word-pair classification task. comp-syn
is open-source on PyPi and is compatible with mainstream machine-learning
Python packages. Our package release includes word-color embeddings for over
40,000 English words, each associated with crowd-sourced word concreteness
judgments.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 22:50:06 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 05:22:54 GMT'}]",2020-10-20,"[['Desikan', 'Bhargav Srinivasa', ''], ['Hull', 'Tasker', ''], ['Nadler', 'Ethan O.', ''], ['Guilbeault', 'Douglas', ''], ['Kar', 'Aabir Abubaker', ''], ['Chu', 'Mark', ''], ['Sardo', 'Donald Ruggiero Lo', '']]"
1365107,2010.08777,Pengshuai Li,"Pengshuai Li, Xinsong Zhang, Weijia Jia and Wei Zhao","Active Testing: An Unbiased Evaluation Method for Distantly Supervised
  Relation Extraction",accepted to appear at Findings of EMNLP 2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Distant supervision has been a widely used method for neural relation
extraction for its convenience of automatically labeling datasets. However,
existing works on distantly supervised relation extraction suffer from the low
quality of test set, which leads to considerable biased performance evaluation.
These biases not only result in unfair evaluations but also mislead the
optimization of neural relation extraction. To mitigate this problem, we
propose a novel evaluation method named active testing through utilizing both
the noisy test set and a few manual annotations. Experiments on a widely used
benchmark show that our proposed approach can yield approximately unbiased
evaluations for distantly supervised relation extractors.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 12:29:09 GMT'}]",2020-10-20,"[['Li', 'Pengshuai', ''], ['Zhang', 'Xinsong', ''], ['Jia', 'Weijia', ''], ['Zhao', 'Wei', '']]"
1365154,2010.08824,Xueliang Zhao,"Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan",Knowledge-Grounded Dialogue Generation with Pre-trained Language Models,Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study knowledge-grounded dialogue generation with pre-trained language
models. To leverage the redundant external knowledge under capacity constraint,
we propose equipping response generation defined by a pre-trained language
model with a knowledge selection module, and an unsupervised approach to
jointly optimizing knowledge selection and response generation with unlabeled
dialogues. Empirical results on two benchmarks indicate that our model can
significantly outperform state-of-the-art methods in both automatic evaluation
and human judgment.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 16:49:43 GMT'}]",2020-10-20,"[['Zhao', 'Xueliang', ''], ['Wu', 'Wei', ''], ['Xu', 'Can', ''], ['Tao', 'Chongyang', ''], ['Zhao', 'Dongyan', ''], ['Yan', 'Rui', '']]"
1365600,2010.09270,Boliang Zhang,"Boliang Zhang, Spencer Whitehead, Lifu Huang and Heng Ji",Global Attention for Name Tagging,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many name tagging approaches use local contextual information with much
success, but fail when the local context is ambiguous or limited. We present a
new framework to improve name tagging by utilizing local, document-level, and
corpus-level contextual information. We retrieve document-level context from
other sentences within the same document and corpus-level context from
sentences in other topically related documents. We propose a model that learns
to incorporate document-level and corpus-level contextual information alongside
local contextual information via global attentions, which dynamically weight
their respective contextual information, and gating mechanisms, which determine
the influence of this information. Extensive experiments on benchmark datasets
show the effectiveness of our approach, which achieves state-of-the-art results
for Dutch, German, and Spanish on the CoNLL-2002 and CoNLL-2003 datasets.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 07:27:15 GMT'}]",2020-10-20,"[['Zhang', 'Boliang', ''], ['Whitehead', 'Spencer', ''], ['Huang', 'Lifu', ''], ['Ji', 'Heng', '']]"
1365584,2010.09254,Jingang Wang,"Yang Yang, Junmei Hao, Canjia Li, Zili Wang, Jingang Wang, Fuzheng
  Zhang, Rao Fu, Peixu Hou, Gong Zhang, Zhongyuan Wang",Query-aware Tip Generation for Vertical Search,Accepted By CIKM 2020 Applied Research Track,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As a concise form of user reviews, tips have unique advantages to explain the
search results, assist users' decision making, and further improve user
experience in vertical search scenarios. Existing work on tip generation does
not take query into consideration, which limits the impact of tips in search
scenarios. To address this issue, this paper proposes a query-aware tip
generation framework, integrating query information into encoding and
subsequent decoding processes. Two specific adaptations of Transformer and
Recurrent Neural Network (RNN) are proposed. For Transformer, the query impact
is incorporated into the self-attention computation of both the encoder and the
decoder. As for RNN, the query-aware encoder adopts a selective network to
distill query-relevant information from the review, while the query-aware
decoder integrates the query information into the attention computation during
decoding. The framework consistently outperforms the competing methods on both
public and real-world industrial datasets. Last but not least, online
deployment experiments on Dianping demonstrate the advantage of the proposed
framework for tip generation as well as its online business values.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 06:48:40 GMT'}]",2020-10-20,"[['Yang', 'Yang', ''], ['Hao', 'Junmei', ''], ['Li', 'Canjia', ''], ['Wang', 'Zili', ''], ['Wang', 'Jingang', ''], ['Zhang', 'Fuzheng', ''], ['Fu', 'Rao', ''], ['Hou', 'Peixu', ''], ['Zhang', 'Gong', ''], ['Wang', 'Zhongyuan', '']]"
1365582,2010.09252,Tiezheng Yu,Tiezheng Yu and Dan Su and Wenliang Dai and Pascale Fung,"Dimsum @LaySumm 20: BART-based Approach for Scientific Document
  Summarization",4 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lay summarization aims to generate lay summaries of scientific papers
automatically. It is an essential task that can increase the relevance of
science for all of society. In this paper, we build a lay summary generation
system based on the BART model. We leverage sentence labels as extra
supervision signals to improve the performance of lay summarization. In the
CL-LaySumm 2020 shared task, our model achieves 46.00\% Rouge1-F1 score.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 06:36:11 GMT'}]",2020-10-20,"[['Yu', 'Tiezheng', ''], ['Su', 'Dan', ''], ['Dai', 'Wenliang', ''], ['Fung', 'Pascale', '']]"
1365570,2010.09240,Dan Su,"Dan Su, Yan Xu, Wenliang Dai, Ziwei Ji, Tiezheng Yu, Pascale Fung",Multi-hop Question Generation with Graph Convolutional Network,,,,,cs.CL cs.AI,http://creativecommons.org/publicdomain/zero/1.0/,"  Multi-hop Question Generation (QG) aims to generate answer-related questions
by aggregating and reasoning over multiple scattered evidence from different
paragraphs. It is a more challenging yet under-explored task compared to
conventional single-hop QG, where the questions are generated from the sentence
containing the answer or nearby sentences in the same paragraph without complex
reasoning. To address the additional challenges in multi-hop QG, we propose
Multi-Hop Encoding Fusion Network for Question Generation (MulQG), which does
context encoding in multiple hops with Graph Convolutional Network and encoding
fusion via an Encoder Reasoning Gate. To the best of our knowledge, we are the
first to tackle the challenge of multi-hop reasoning over paragraphs without
any sentence-level information. Empirical results on HotpotQA dataset
demonstrate the effectiveness of our method, in comparison with baselines on
automatic evaluation metrics. Moreover, from the human evaluation, our proposed
model is able to generate fluent questions with high completeness and
outperforms the strongest baseline by 20.8% in the multi-hop evaluation. The
code is publicly available at
https://github.com/HLTCHKUST/MulQG}{https://github.com/HLTCHKUST/MulQG .
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 06:15:36 GMT'}]",2020-10-20,"[['Su', 'Dan', ''], ['Xu', 'Yan', ''], ['Dai', 'Wenliang', ''], ['Ji', 'Ziwei', ''], ['Yu', 'Tiezheng', ''], ['Fung', 'Pascale', '']]"
1365520,2010.09190,Jiaxin Ju,"Jiaxin Ju, Ming Liu, Longxiang Gao and Shirui Pan",SciSummPip: An Unsupervised Scientific Paper Summarization Pipeline,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Scholarly Document Processing (SDP) workshop is to encourage more efforts
on natural language understanding of scientific task. It contains three shared
tasks and we participate in the LongSumm shared task. In this paper, we
describe our text summarization system, SciSummPip, inspired by SummPip (Zhao
et al., 2020) that is an unsupervised text summarization system for
multi-document in news domain. Our SciSummPip includes a transformer-based
language model SciBERT (Beltagy et al., 2019) for contextual sentence
representation, content selection with PageRank (Page et al., 1999), sentence
graph construction with both deep and linguistic information, sentence graph
clustering and within-graph summary generation. Our work differs from previous
method in that content selection and a summary length constraint is applied to
adapt to the scientific domain. The experiment results on both training dataset
and blind test dataset show the effectiveness of our method, and we empirically
verify the robustness of modules used in SciSummPip with BERTScore (Zhang et
al., 2019a).
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 03:29:21 GMT'}]",2020-10-20,"[['Ju', 'Jiaxin', ''], ['Liu', 'Ming', ''], ['Gao', 'Longxiang', ''], ['Pan', 'Shirui', '']]"
1365519,2010.09189,Sheng Zhang,"Ye Liu, Sheng Zhang, Rui Song, Suo Feng, Yanghua Xiao","Knowledge-guided Open Attribute Value Extraction with Reinforcement
  Learning",EMNLP 2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open attribute value extraction for emerging entities is an important but
challenging task. A lot of previous works formulate the problem as a
\textit{question-answering} (QA) task. While the collections of articles from
web corpus provide updated information about the emerging entities, the
retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers.
Effectively filtering out noisy articles as well as bad answers is the key to
improving extraction accuracy. Knowledge graph (KG), which contains rich, well
organized information about entities, provides a good resource to address the
challenge. In this work, we propose a knowledge-guided reinforcement learning
(RL) framework for open attribute value extraction. Informed by relevant
knowledge in KG, we trained a deep Q-network to sequentially compare extracted
answers to improve extraction accuracy. The proposed framework is applicable to
different information extraction system. Our experimental results show that our
method outperforms the baselines by 16.5 - 27.8\%.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 03:28:27 GMT'}]",2020-10-20,"[['Liu', 'Ye', ''], ['Zhang', 'Sheng', ''], ['Song', 'Rui', ''], ['Feng', 'Suo', ''], ['Xiao', 'Yanghua', '']]"
1365472,2010.09142,Enamul Hoque,Jason Obeid and Enamul Hoque,"Chart-to-Text: Generating Natural Language Descriptions for Charts by
  Adapting the Transformer Model",10 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Information visualizations such as bar charts and line charts are very
popular for exploring data and communicating insights. Interpreting and making
sense of such visualizations can be challenging for some people, such as those
who are visually impaired or have low visualization literacy. In this work, we
introduce a new dataset and present a neural model for automatically generating
natural language summaries for charts. The generated summaries provide an
interpretation of the chart and convey the key insights found within that
chart. Our neural model is developed by extending the state-of-the-art model
for the data-to-text generation task, which utilizes a transformer-based
encoder-decoder architecture. We found that our approach outperforms the base
model on a content selection metric by a wide margin (55.42% vs. 8.49%) and
generates more informative, concise, and coherent summaries.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 23:57:33 GMT'}]",2020-10-20,"[['Obeid', 'Jason', ''], ['Hoque', 'Enamul', '']]"
1365408,2010.09078,Harish Tayyar Madabushi PhD,Anushka Prakash and Harish Tayyar Madabushi,"Incorporating Count-Based Features into Pre-Trained Models for Improved
  Stance Detection",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The explosive growth and popularity of Social Media has revolutionised the
way we communicate and collaborate. Unfortunately, this same ease of accessing
and sharing information has led to an explosion of misinformation and
propaganda. Given that stance detection can significantly aid in veracity
prediction, this work focuses on boosting automated stance detection, a task on
which pre-trained models have been extremely successful on, as on several other
tasks. This work shows that the task of stance detection can benefit from
feature based information, especially on certain under performing classes,
however, integrating such features into pre-trained models using ensembling is
challenging. We propose a novel architecture for integrating features with
pre-trained models that address these challenges and test our method on the
RumourEval 2019 dataset. This method achieves state-of-the-art results with an
F1-score of 63.94 on the test set.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 19:37:24 GMT'}]",2020-10-20,"[['Prakash', 'Anushka', ''], ['Madabushi', 'Harish Tayyar', '']]"
1365402,2010.09072,Harish Tayyar Madabushi PhD,Eleri Sarsfield and Harish Tayyar Madabushi,"UoB at SemEval-2020 Task 1: Automatic Identification of Novel Word
  Senses",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Much as the social landscape in which languages are spoken shifts, language
too evolves to suit the needs of its users. Lexical semantic change analysis is
a burgeoning field of semantic analysis which aims to trace changes in the
meanings of words over time. This paper presents an approach to lexical
semantic change detection based on Bayesian word sense induction suitable for
novel word sense identification. This approach is used for a submission to
SemEval-2020 Task 1, which shows the approach to be capable of the SemEval
task. The same approach is also applied to a corpus gleaned from 15 years of
Twitter data, the results of which are then used to identify words which may be
instances of slang.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 19:27:06 GMT'}]",2020-10-20,"[['Sarsfield', 'Eleri', ''], ['Madabushi', 'Harish Tayyar', '']]"
1365152,2010.08822,Wei Wang,"Wei Wang, Piji Li, Hai-Tao Zheng",Consistency and Coherency Enhanced Story Generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Story generation is a challenging task, which demands to maintain consistency
of the plots and characters throughout the story. Previous works have shown
that GPT2, a large-scale language model, has achieved good performance on story
generation. However, we observe that several serious issues still exist in the
stories generated by GPT2 which can be categorized into two folds: consistency
and coherency. In terms of consistency, on one hand, GPT2 cannot guarantee the
consistency of the plots explicitly. On the other hand, the generated stories
usually contain coreference errors. In terms of coherency, GPT2 does not take
account of the discourse relations between sentences of stories directly. To
enhance the consistency and coherency of the generated stories, we propose a
two-stage generation framework, where the first stage is to organize the story
outline which depicts the story plots and events, and the second stage is to
expand the outline into a complete story. Therefore the plots consistency can
be controlled and guaranteed explicitly. In addition, coreference supervision
signals are incorporated to reduce coreference errors and improve the
coreference consistency. Moreover, we design an auxiliary task of discourse
relation modeling to improve the coherency of the generated stories.
Experimental results on a story dataset show that our model outperforms the
baseline approaches in terms of both automatic metrics and human evaluation.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 16:40:37 GMT'}]",2020-10-20,"[['Wang', 'Wei', ''], ['Li', 'Piji', ''], ['Zheng', 'Hai-Tao', '']]"
1365376,2010.09046,Cheonbok  Park,"Yunwon Tae, Cheonbok Park, Taehee Kim, Soyoung Yang, Mohammad Azam
  Khan, Eunjeong Park, Tao Qin and Jaegul Choo",Meta-Learning for Low-Resource Unsupervised Neural MachineTranslation,"10 pages, 4 figures",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unsupervised machine translation, which utilizes unpaired monolingual corpora
as training data, has achieved comparable performance against supervised
machine translation. However, it still suffers from data-scarce domains. To
address this issue, this paper presents a meta-learning algorithm for
unsupervised neural machine translation (UNMT) that trains the model to adapt
to another domain by utilizing only a small amount of training data. We assume
that domain-general knowledge is a significant factor in handling data-scarce
domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge
learned from high-resource domains to boost the performance of low-resource
UNMT. Our model surpasses a transfer learning-based approach by up to 2-4 BLEU
scores. Extensive experimental results show that our proposed algorithm is
pertinent for fast adaptation and consistently outperforms other baseline
models.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 17:54:13 GMT'}]",2020-10-20,"[['Tae', 'Yunwon', ''], ['Park', 'Cheonbok', ''], ['Kim', 'Taehee', ''], ['Yang', 'Soyoung', ''], ['Khan', 'Mohammad Azam', ''], ['Park', 'Eunjeong', ''], ['Qin', 'Tao', ''], ['Choo', 'Jaegul', '']]"
1365325,2010.08995,Jinta Weng,"Jinta Weng, Ying Gao, Jing Qiu, Guozhu Ding, Huanqin Zheng","Construction and Application of Teaching System Based on Crowdsourcing
  Knowledge Graph","Number of references:15 Classification code:903.3 Information
  Retrieval and Use Conference code: 235759","4th China Conference on Knowledge Graph and Semantic Computing,
  CCKS 2019",10.1007/978-981-15-1956-7_3,,cs.DB cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Through the combination of crowdsourcing knowledge graph and teaching system,
research methods to generate knowledge graph and its applications. Using two
crowdsourcing approaches, crowdsourcing task distribution and reverse captcha
generation, to construct knowledge graph in the field of teaching system.
Generating a complete hierarchical knowledge graph of the teaching domain by
nodes of school, student, teacher, course, knowledge point and exercise type.
The knowledge graph constructed in a crowdsourcing manner requires many users
to participate collaboratively with fully consideration of teachers' guidance
and users' mobilization issues. Based on the three subgraphs of knowledge
graph, prominent teacher, student learning situation and suitable learning
route could be visualized. Personalized exercises recommendation model is used
to formulate the personalized exercise by algorithm based on the knowledge
graph. Collaborative creation model is developed to realize the crowdsourcing
construction mechanism. Though unfamiliarity with the learning mode of
knowledge graph and learners' less attention to the knowledge structure, system
based on Crowdsourcing Knowledge Graph can still get high acceptance around
students and teachers
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 14:26:10 GMT'}]",2020-10-20,"[['Weng', 'Jinta', ''], ['Gao', 'Ying', ''], ['Qiu', 'Jing', ''], ['Ding', 'Guozhu', ''], ['Zheng', 'Huanqin', '']]"
1365313,2010.08983,Sahana Ramnath,"Sahana Ramnath, Preksha Nema, Deep Sahni, Mitesh M. Khapra",Towards Interpreting BERT for Reading Comprehension Based QA,7 pages including references and appendix. Accepted at EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  BERT and its variants have achieved state-of-the-art performance in various
NLP tasks. Since then, various works have been proposed to analyze the
linguistic information being captured in BERT. However, the current works do
not provide an insight into how BERT is able to achieve near human-level
performance on the task of Reading Comprehension based Question Answering. In
this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have
predefined roles, we define a layer's role or functionality using Integrated
Gradients. Based on the defined roles, we perform a preliminary analysis across
all layers. We observed that the initial layers focus on query-passage
interaction, whereas later layers focus more on contextual understanding and
enhancing the answer prediction. Specifically for quantifier questions (how
much/how many), we notice that BERT focuses on confusing words (i.e., on other
numerical quantities in the passage) in the later layers, but still manages to
predict the answer correctly. The fine-tuning and analysis scripts will be
publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA .
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 13:33:49 GMT'}]",2020-10-20,"[['Ramnath', 'Sahana', ''], ['Nema', 'Preksha', ''], ['Sahni', 'Deep', ''], ['Khapra', 'Mitesh M.', '']]"
1363122,2010.06792,Bowen Tan,"Bowen Tan, Lianhui Qin, Eric P. Xing, Zhiting Hu","Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised
  Approach","EMNLP 2020, code and data available at
  https://github.com/tanyuqian/aspect-based-summarization",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Given a document and a target aspect (e.g., a topic of interest),
aspect-based abstractive summarization attempts to generate a summary with
respect to the aspect. Previous studies usually assume a small pre-defined set
of aspects and fall short of summarizing on other diverse topics. In this work,
we study summarizing on arbitrary aspects relevant to the document, which
significantly expands the application of the task in practice. Due to the lack
of supervision data, we develop a new weak supervision construction method and
an aspect modeling scheme, both of which integrate rich external knowledge
sources such as ConceptNet and Wikipedia. Experiments show our approach
achieves performance boosts on summarizing both real and synthetic documents
given pre-defined or arbitrary aspects.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 03:20:46 GMT'}, {'version': 'v2', 'created': 'Sun, 18 Oct 2020 08:58:23 GMT'}]",2020-10-20,"[['Tan', 'Bowen', ''], ['Qin', 'Lianhui', ''], ['Xing', 'Eric P.', ''], ['Hu', 'Zhiting', '']]"
1365304,2010.08974,Piyush Makhija,"Piyush Makhija, Ankit Kumar, Anuj Gupta","hinglishNorm -- A Corpus of Hindi-English Code Mixed Sentences for Text
  Normalization","Accepted in COLING2020 industry track, 8 pages (including
  references), 4 tables",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We present hinglishNorm -- a human annotated corpus of Hindi-English
code-mixed sentences for text normalization task. Each sentence in the corpus
is aligned to its corresponding human annotated normalized form. To the best of
our knowledge, there is no corpus of Hindi-English code-mixed sentences for
text normalization task that is publicly available. Our work is the first
attempt in this direction. The corpus contains 13494 parallel segments.
Further, we present baseline normalization results on this corpus. We obtain a
Word Error Rate (WER) of 15.55, BiLingual Evaluation Understudy (BLEU) score of
71.2, and Metric for Evaluation of Translation with Explicit ORdering (METEOR)
score of 0.50.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 12:21:37 GMT'}]",2020-10-20,"[['Makhija', 'Piyush', ''], ['Kumar', 'Ankit', ''], ['Gupta', 'Anuj', '']]"
1365291,2010.08961,Zewei Sun,"Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang,
  Jiajun Chen, Lei Li","Capturing Longer Context for Document-level Neural Machine Translation:
  A Multi-resolutional Approach",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Discourse context has been proven useful when translating documents. It is
quite a challenge to incorporate long document context in the prevailing neural
machine translation models such as Transformer. In this paper, we propose
multi-resolutional (MR) Doc2Doc, a method to train a neural
sequence-to-sequence model for document-level translation. Our trained model
can simultaneously translate sentence by sentence as well as a document as a
whole. We evaluate our method and several recent approaches on nine
document-level datasets and two sentence-level datasets across six languages.
Experiments show that MR Doc2Doc outperforms sentence-level models and previous
methods in a comprehensive set of metrics, including BLEU, four lexical
indices, three newly proposed assistant linguistic indicators, and human
evaluation.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 11:18:29 GMT'}]",2020-10-20,"[['Sun', 'Zewei', ''], ['Wang', 'Mingxuan', ''], ['Zhou', 'Hao', ''], ['Zhao', 'Chengqi', ''], ['Huang', 'Shujian', ''], ['Chen', 'Jiajun', ''], ['Li', 'Lei', '']]"
1365253,2010.08923,Chenyu You,"Chenyu You, Nuo Chen, Fenglin Liu, Dongchao Yang, Yuexian Zou","Towards Data Distillation for End-to-end Spoken Conversational Question
  Answering",,,,,cs.CL cs.AI eess.AS eess.SP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In spoken question answering, QA systems are designed to answer questions
from contiguous text spans within the related speech transcripts. However, the
most natural way that human seek or test their knowledge is via human
conversations. Therefore, we propose a new Spoken Conversational Question
Answering task (SCQA), aiming at enabling QA systems to model complex dialogues
flow given the speech utterances and text corpora. In this task, our main
objective is to build a QA system to deal with conversational questions both in
spoken and text forms, and to explore the plausibility of providing more cues
in spoken documents with systems in information gathering. To this end, instead
of adopting automatically generated speech transcripts with highly noisy data,
we propose a novel unified data distillation approach, DDNet, which directly
fuse audio-text features to reduce the misalignment between automatic speech
recognition hypotheses and the reference transcriptions. In addition, to
evaluate the capacity of QA systems in a dialogue-style interaction, we
assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with
more than 120k question-answer pairs. Experiments demonstrate that our proposed
method achieves superior performance in spoken conversational question
answering.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 05:53:39 GMT'}]",2020-10-20,"[['You', 'Chenyu', ''], ['Chen', 'Nuo', ''], ['Liu', 'Fenglin', ''], ['Yang', 'Dongchao', ''], ['Zou', 'Yuexian', '']]"
1365222,2010.08892,Chenguang Zhu,"Ruochen Xu, Chenguang Zhu, Yu Shi, Michael Zeng, Xuedong Huang",Mixed-Lingual Pre-training for Cross-lingual Summarization,"Accepted at Asia-Pacific Chapter of the Association for Computational
  Linguistics (AACL) 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cross-lingual Summarization (CLS) aims at producing a summary in the target
language for an article in the source language. Traditional solutions employ a
two-step approach, i.e. translate then summarize or summarize then translate.
Recently, end-to-end models have achieved better results, but these approaches
are mostly limited by their dependence on large-scale labeled data. We propose
a solution based on mixed-lingual pre-training that leverages both
cross-lingual tasks such as translation and monolingual tasks like masked
language models. Thus, our model can leverage the massive monolingual data to
enhance its modeling of language. Moreover, the architecture has no
task-specific components, which saves memory and increases optimization
efficiency. We show in experiments that this pre-training scheme can
effectively boost the performance of cross-lingual summarization. In Neural
Cross-Lingual Summarization (NCLS) dataset, our model achieves an improvement
of 2.82 (English to Chinese) and 1.15 (Chinese to English) ROUGE-1 scores over
state-of-the-art results.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 00:21:53 GMT'}]",2020-10-20,"[['Xu', 'Ruochen', ''], ['Zhu', 'Chenguang', ''], ['Shi', 'Yu', ''], ['Zeng', 'Michael', ''], ['Huang', 'Xuedong', '']]"
1365213,2010.08883,Sai Sharath Japa,Sai Sharath Japa and Rekabdar Banafsheh,Question Answering over Knowledge Base using Language Model Embeddings,,2020 International Joint Conference on Neural Networks (IJCNN),10.1109/IJCNN48605.2020.9206698,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge Base, represents facts about the world, often in some form of
subsumption ontology, rather than implicitly, embedded in procedural code, the
way a conventional computer program does. While there is a rapid growth in
knowledge bases, it poses a challenge of retrieving information from them.
Knowledge Base Question Answering is one of the promising approaches for
extracting substantial knowledge from Knowledge Bases. Unlike web search,
Question Answering over a knowledge base gives accurate and concise results,
provided that natural language questions can be understood and mapped precisely
to an answer in the knowledge base. However, some of the existing
embedding-based methods for knowledge base question answering systems ignore
the subtle correlation between the question and the Knowledge Base (e.g.,
entity types, relation paths, and context) and suffer from the Out Of
Vocabulary problem. In this paper, we focused on using a pre-trained language
model for the Knowledge Base Question Answering task. Firstly, we used Bert
base uncased for the initial experiments. We further fine-tuned these
embeddings with a two-way attention mechanism from the knowledge base to the
asked question and from the asked question to the knowledge base answer
aspects. Our method is based on a simple Convolutional Neural Network
architecture with a Multi-Head Attention mechanism to represent the asked
question dynamically in multiple aspects. Our experimental results show the
effectiveness and the superiority of the Bert pre-trained language model
embeddings for question answering systems on knowledge bases over other
well-known embedding methods.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 22:59:34 GMT'}]",2020-10-20,"[['Japa', 'Sai Sharath', ''], ['Banafsheh', 'Rekabdar', '']]"
1365195,2010.08865,Thanh Tran,"Thanh Tran, Yifan Hu, Changwei Hu, Kevin Yen, Fei Tan, Kyumin Lee,
  Serim Park",HABERTOR: An Efficient and Effective Deep Hatespeech Detector,,EMNLP 2020,,,cs.CL cs.AI cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present our HABERTOR model for detecting hatespeech in large scale
user-generated content. Inspired by the recent success of the BERT model, we
propose several modifications to BERT to enhance the performance on the
downstream hatespeech classification task. HABERTOR inherits BERT's
architecture, but is different in four aspects: (i) it generates its own
vocabularies and is pre-trained from the scratch using the largest scale
hatespeech dataset; (ii) it consists of Quaternion-based factorized components,
resulting in a much smaller number of parameters, faster training and
inferencing, as well as less memory usage; (iii) it uses our proposed
multi-source ensemble heads with a pooling layer for separate input sources, to
further enhance its effectiveness; and (iv) it uses a regularized adversarial
training with our proposed fine-grained and adaptive noise magnitude to enhance
its robustness. Through experiments on the large-scale real-world hatespeech
dataset with 1.4M annotated comments, we show that HABERTOR works better than
15 state-of-the-art hatespeech detection methods, including fine-tuning
Language Models. In particular, comparing with BERT, our HABERTOR is 4~5 times
faster in the training/inferencing phase, uses less than 1/3 of the memory, and
has better performance, even though we pre-train it by using less than 1% of
the number of words. Our generalizability analysis shows that HABERTOR
transfers well to other unseen hatespeech datasets and is a more efficient and
effective alternative to BERT for the hatespeech classification.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 21:10:08 GMT'}]",2020-10-20,"[['Tran', 'Thanh', ''], ['Hu', 'Yifan', ''], ['Hu', 'Changwei', ''], ['Yen', 'Kevin', ''], ['Tan', 'Fei', ''], ['Lee', 'Kyumin', ''], ['Park', 'Serim', '']]"
1365360,2010.09030,Nazneen Fatema Rajani,"Nazneen Fatema Rajani, Ben Krause, Wengpeng Yin, Tong Niu, Richard
  Socher, Caiming Xiong","Explaining and Improving Model Behavior with k Nearest Neighbor
  Representations",,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Interpretability techniques in NLP have mainly focused on understanding
individual predictions using attention visualization or gradient-based saliency
maps over tokens. We propose using k nearest neighbor (kNN) representations to
identify training examples responsible for a model's predictions and obtain a
corpus-level understanding of the model's behavior. Apart from
interpretability, we show that kNN representations are effective at uncovering
learned spurious associations, identifying mislabeled examples, and improving
the fine-tuned model's performance. We focus on Natural Language Inference
(NLI) as a case study and experiment with multiple datasets. Our method deploys
backoff to kNN for BERT and RoBERTa on examples with low model confidence
without any update to the model parameters. Our results indicate that the kNN
approach makes the finetuned model more robust to adversarial inputs.
","[{'version': 'v1', 'created': 'Sun, 18 Oct 2020 16:55:25 GMT'}]",2020-10-20,"[['Rajani', 'Nazneen Fatema', ''], ['Krause', 'Ben', ''], ['Yin', 'Wengpeng', ''], ['Niu', 'Tong', ''], ['Socher', 'Richard', ''], ['Xiong', 'Caiming', '']]"
1324058,2007.12544,Bertelt Braaksma,"Bertelt Braaksma, Richard Scholtens, Stan van Suijlekom, Remy Wang,
  Ahmet \""Ust\""un",FiSSA at SemEval-2020 Task 9: Fine-tuned For Feelings,"In Proceedings of the 14th International Workshop on Semantic
  Evaluation (SemEval-2020), Barcelona, Spain, December. Association for
  Computational Linguistics",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we present our approach for sentiment classification on
Spanish-English code-mixed social media data in the SemEval-2020 Task 9. We
investigate performance of various pre-trained Transformer models by using
different fine-tuning strategies. We explore both monolingual and multilingual
models with the standard fine-tuning method. Additionally, we propose a custom
model that we fine-tune in two steps: once with a language modeling objective,
and once with a task-specific objective. Although two-step fine-tuning improves
sentiment classification performance over the base model, the large
multilingual XLM-RoBERTa model achieves best weighted F1-score with 0.537 on
development data and 0.739 on test data. With this score, our team jupitter
placed tenth overall in the competition.
","[{'version': 'v1', 'created': 'Fri, 24 Jul 2020 14:48:27 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Jul 2020 15:41:22 GMT'}, {'version': 'v3', 'created': 'Mon, 19 Oct 2020 07:11:18 GMT'}]",2020-10-20,"[['Braaksma', 'Bertelt', ''], ['Scholtens', 'Richard', ''], ['van Suijlekom', 'Stan', ''], ['Wang', 'Remy', ''], ['Üstün', 'Ahmet', '']]"
1279300,2004.14530,Peng Qi,"Peng Qi, Yuhao Zhang, Christopher D. Manning","Stay Hungry, Stay Focused: Generating Informative and Specific Questions
  in Information-Seeking Conversations","Findings of ACL: EMNLP 2020. Code available at:
  https://github.com/qipeng/stay-hungry-stay-focused",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate the problem of generating informative questions in
information-asymmetric conversations. Unlike previous work on question
generation which largely assumes knowledge of what the answer might be, we are
interested in the scenario where the questioner is not given the context from
which answers are drawn, but must reason pragmatically about how to acquire new
information, given the shared conversation history. We identify two core
challenges: (1) formally defining the informativeness of potential questions,
and (2) exploring the prohibitively large space of potential questions to find
the good candidates. To generate pragmatic questions, we use reinforcement
learning to optimize an informativeness metric we propose, combined with a
reward function designed to promote more specific questions. We demonstrate
that the resulting pragmatic questioner substantially improves the
informativeness and specificity of questions generated over a baseline model,
as evaluated by our metrics as well as humans.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 00:49:14 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 16:53:32 GMT'}]",2020-10-21,"[['Qi', 'Peng', ''], ['Zhang', 'Yuhao', ''], ['Manning', 'Christopher D.', '']]"
1279733,2004.14963,Emrah Budur,"Emrah Budur, R{\i}za \""Oz\c{c}elik, Tunga G\""ung\""or, and Christopher
  Potts",Data and Representation for Turkish Natural Language Inference,Accepted to EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large annotated datasets in NLP are overwhelmingly in English. This is an
obstacle to progress in other languages. Unfortunately, obtaining new annotated
resources for each task in each language would be prohibitively expensive. At
the same time, commercial machine translation systems are now robust. Can we
leverage these systems to translate English-language datasets automatically? In
this paper, we offer a positive response for natural language inference (NLI)
in Turkish. We translated two large English NLI datasets into Turkish and had a
team of experts validate their translation quality and fidelity to the original
labels. Using these datasets, we address core issues of representation for
Turkish NLI. We find that in-language embeddings are essential and that
morphological parsing can be avoided where the training set is large. Finally,
we show that models trained on our machine-translated datasets are successful
on human-translated evaluation sets. We share all code, models, and data
publicly.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 17:12:52 GMT'}, {'version': 'v2', 'created': 'Fri, 1 May 2020 23:15:03 GMT'}, {'version': 'v3', 'created': 'Tue, 20 Oct 2020 15:25:07 GMT'}]",2020-10-21,"[['Budur', 'Emrah', ''], ['Özçelik', 'Rıza', ''], ['Güngör', 'Tunga', ''], ['Potts', 'Christopher', '']]"
1366327,2010.09997,Haohan Wang,"Haohan Wang, Peiyan Zhang, Eric P. Xing",Word Shape Matters: Robust Machine Translation with Visual Embedding,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural machine translation has achieved remarkable empirical performance over
standard benchmark datasets, yet recent evidence suggests that the models can
still fail easily dealing with substandard inputs such as misspelled words, To
overcome this issue, we introduce a new encoding heuristic of the input symbols
for character-level NLP models: it encodes the shape of each character through
the images depicting the letters when printed. We name this new strategy visual
embedding and it is expected to improve the robustness of NLP models because
humans also process the corpus visually through printed letters, instead of
machinery one-hot vectors. Empirically, our method improves models' robustness
against substandard inputs, even in the test scenario where the models are
tested with the noises that are beyond what is available during the training
phase.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 04:08:03 GMT'}]",2020-10-21,"[['Wang', 'Haohan', ''], ['Zhang', 'Peiyan', ''], ['Xing', 'Eric P.', '']]"
1294203,2005.14408,Xiao Yang,"Deepak Muralidharan, Joel Ruben Antony Moniz, Sida Gao, Xiao Yang, Lin
  Li, Justine Kao, Stephen Pulman, Atish Kothari, Ray Shen, Yinying Pan, Vivek
  Kaul, Mubarak Seyed Ibrahim, Gang Xiang, Nan Dun, Yidan Zhou, Andy O, Yuan
  Zhang, Pooja Chitkara, Xuan Wang, Alkesh Patel, Kushal Tayal, Roger Zheng,
  Peter Grasch, Jason Williams",Noise-robust Named Entity Understanding for Virtual Assistants,9 pages,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Named Entity Understanding (NEU) plays an essential role in interactions
between users and voice assistants, since successfully identifying entities and
correctly linking them to their standard forms is crucial to understanding the
user's intent. NEU is a challenging task in voice assistants due to the
ambiguous nature of natural language and because noise introduced by speech
transcription and user errors occur frequently in spoken natural language
queries. In this paper, we propose an architecture with novel features that
jointly solves the recognition of named entities (a.k.a. Named Entity
Recognition, or NER) and the resolution to their canonical forms (a.k.a. Entity
Linking, or EL). We show that by combining NER and EL information in a joint
reranking module, our proposed framework improves accuracy in both tasks. This
improved performance and the features that enable it, also lead to better
accuracy in downstream tasks, such as domain classification and semantic
parsing.
","[{'version': 'v1', 'created': 'Fri, 29 May 2020 06:14:53 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 20:32:55 GMT'}]",2020-10-21,"[['Muralidharan', 'Deepak', ''], ['Moniz', 'Joel Ruben Antony', ''], ['Gao', 'Sida', ''], ['Yang', 'Xiao', ''], ['Li', 'Lin', ''], ['Kao', 'Justine', ''], ['Pulman', 'Stephen', ''], ['Kothari', 'Atish', ''], ['Shen', 'Ray', ''], ['Pan', 'Yinying', ''], ['Kaul', 'Vivek', ''], ['Ibrahim', 'Mubarak Seyed', ''], ['Xiang', 'Gang', ''], ['Dun', 'Nan', ''], ['Zhou', 'Yidan', ''], ['O', 'Andy', ''], ['Zhang', 'Yuan', ''], ['Chitkara', 'Pooja', ''], ['Wang', 'Xuan', ''], ['Patel', 'Alkesh', ''], ['Tayal', 'Kushal', ''], ['Zheng', 'Roger', ''], ['Grasch', 'Peter', ''], ['Williams', 'Jason', '']]"
1366407,2010.10077,Aman Madaan,"Aman Madaan, Yiming Yang",Neural Language Modeling for Contextualized Temporal Graph Generation,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This paper presents the first study on using large-scale pre-trained language
models for automated generation of an event-level temporal graph for a
document. Despite the huge success of neural pre-training methods in NLP tasks,
its potential for temporal reasoning over event graphs has not been
sufficiently explored. Part of the reason is the difficulty in obtaining large
training corpora with human-annotated events and temporal links. We address
this challenge by using existing IE/NLP tools to automatically generate a large
quantity (89,000) of system-produced document-graph pairs, and propose a novel
formulation of the contextualized graph generation problem as a
sequence-to-sequence mapping task. These strategies enable us to leverage and
fine-tune pre-trained language models on the system-induced training data for
the graph generation task. Our experiments show that our approach is highly
effective in generating structurally and semantically valid graphs. Further,
evaluation on a challenging hand-labeled, out-domain corpus shows that our
method outperforms the closest existing method by a large margin on several
metrics. Code and pre-trained models are available at
https://github.com/madaan/temporal-graph-gen.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 07:08:00 GMT'}]",2020-10-21,"[['Madaan', 'Aman', ''], ['Yang', 'Yiming', '']]"
1366425,2010.10095,Hung Le,"Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C.H. Hoi","BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded
  Dialogues",,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Video-grounded dialogues are very challenging due to (i) the complexity of
videos which contain both spatial and temporal variations, and (ii) the
complexity of user utterances which query different segments and/or different
objects in videos over multiple dialogue turns. However, existing approaches to
video-grounded dialogues often focus on superficial temporal-level visual cues,
but neglect more fine-grained spatial signals from videos. To address this
drawback, we propose Bi-directional Spatio-Temporal Learning (BiST), a
vision-language neural framework for high-resolution queries in videos based on
textual cues. Specifically, our approach not only exploits both spatial and
temporal-level information, but also learns dynamic information diffusion
between the two feature spaces through spatial-to-temporal and
temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the
evolving semantics of user queries in the dialogue setting. The retrieved
visual cues are used as contextual information to construct relevant responses
to the users. Our empirical results and comprehensive qualitative analysis show
that BiST achieves competitive performance and generates reasonable responses
on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA
setting, and substantially outperform prior approaches on the TGIF-QA
benchmark.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 07:43:00 GMT'}]",2020-10-21,"[['Le', 'Hung', ''], ['Sahoo', 'Doyen', ''], ['Chen', 'Nancy F.', ''], ['Hoi', 'Steven C. H.', '']]"
1366441,2010.10111,Sainik Mahata,"Sainik Kumar Mahata, Dipankar Das, Sivaji Bandyopadhyay","JUNLP@Dravidian-CodeMix-FIRE2020: Sentiment Classification of Code-Mixed
  Tweets using Bi-Directional RNN and Language Tags",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentiment analysis has been an active area of research in the past two
decades and recently, with the advent of social media, there has been an
increasing demand for sentiment analysis on social media texts. Since the
social media texts are not in one language and are largely code-mixed in
nature, the traditional sentiment classification models fail to produce
acceptable results. This paper tries to solve this very research problem and
uses bi-directional LSTMs along with language tagging, to facilitate sentiment
tagging of code-mixed Tamil texts that have been extracted from social media.
The presented algorithm, when evaluated on the test data, garnered precision,
recall, and F1 scores of 0.59, 0.66, and 0.58 respectively.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 08:10:29 GMT'}]",2020-10-21,"[['Mahata', 'Sainik Kumar', ''], ['Das', 'Dipankar', ''], ['Bandyopadhyay', 'Sivaji', '']]"
1366480,2010.10150,Wei Ping,"Sashank Santhanam, Wei Ping, Raul Puri, Mohammad Shoeybi, Mostofa
  Patwary, Bryan Catanzaro",Local Knowledge Powered Conversational Agents,,,,,cs.CL cs.AI cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State-of-the-art conversational agents have advanced significantly in
conjunction with the use of large transformer-based language models. However,
even with these advancements, conversational agents still lack the ability to
produce responses that are informative and coherent with the local context. In
this work, we propose a dialog framework that incorporates both local knowledge
as well as users' past dialogues to generate high quality conversations. We
introduce an approach to build a dataset based on Reddit conversations, where
outbound URL links are widely available in the conversations and the
hyperlinked documents can be naturally included as local external knowledge.
Using our framework and dataset, we demonstrate that incorporating local
knowledge can largely improve informativeness, coherency and realisticness
measures using human evaluations. In particular, our approach consistently
outperforms the state-of-the-art conversational model on the Reddit dataset
across all three measures. We also find that scaling the size of our models
from 117M to 8.3B parameters yields consistent improvement of validation
perplexity as well as human evaluated metrics. Our model with 8.3B parameters
can generate human-like responses as rated by various human evaluations in a
single-turn dialog setting.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 09:34:40 GMT'}]",2020-10-21,"[['Santhanam', 'Sashank', ''], ['Ping', 'Wei', ''], ['Puri', 'Raul', ''], ['Shoeybi', 'Mohammad', ''], ['Patwary', 'Mostofa', ''], ['Catanzaro', 'Bryan', '']]"
1366486,2010.10156,Shivali Agarwal,"Shivali Agarwal, Shubham Atreja, Vikas Agarwal",Extracting Procedural Knowledge from Technical Documents,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Procedures are an important knowledge component of documents that can be
leveraged by cognitive assistants for automation, question-answering or driving
a conversation. It is a challenging problem to parse big dense documents like
product manuals, user guides to automatically understand which parts are
talking about procedures and subsequently extract them. Most of the existing
research has focused on extracting flows in given procedures or understanding
the procedures in order to answer conceptual questions. Identifying and
extracting multiple procedures automatically from documents of diverse formats
remains a relatively less addressed problem. In this work, we cover some of
this ground by -- 1) Providing insights on how structural and linguistic
properties of documents can be grouped to define types of procedures, 2)
Analyzing documents to extract the relevant linguistic and structural
properties, and 3) Formulating procedure identification as a classification
problem that leverages the features of the document derived from the above
analysis. We first implemented and deployed unsupervised techniques which were
used in different use cases. Based on the evaluation in different use cases, we
figured out the weaknesses of the unsupervised approach. We then designed an
improved version which was supervised. We demonstrate that our technique is
effective in identifying procedures from big and complex documents alike by
achieving accuracy of 89%.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 09:47:52 GMT'}]",2020-10-21,"[['Agarwal', 'Shivali', ''], ['Atreja', 'Shubham', ''], ['Agarwal', 'Vikas', '']]"
1366506,2010.10176,Markus J. Hofmann,"Markus J. Hofmann, Lara M\""uller, Andre R\""olke, Ralph Radach and
  Chris Biemann",Individual corpora predict fast memory retrieval during reading,"Proceedings of the 6th workshop on Cognitive Aspects of the Lexicon
  (CogALex-VI), Barcelona, Spain, December 12, 2020; accepted manuscript; 11
  pages, 2 figures, 4 Tables",,,,cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  The corpus, from which a predictive language model is trained, can be
considered the experience of a semantic system. We recorded everyday reading of
two participants for two months on a tablet, generating individual corpus
samples of 300/500K tokens. Then we trained word2vec models from individual
corpora and a 70 million-sentence newspaper corpus to obtain individual and
norm-based long-term memory structure. To test whether individual corpora can
make better predictions for a cognitive task of long-term memory retrieval, we
generated stimulus materials consisting of 134 sentences with uncorrelated
individual and norm-based word probabilities. For the subsequent eye tracking
study 1-2 months later, our regression analyses revealed that individual, but
not norm-corpus-based word probabilities can account for first-fixation
duration and first-pass gaze duration. Word length additionally affected gaze
duration and total viewing duration. The results suggest that corpora
representative for an individual's longterm memory structure can better explain
reading performance than a norm corpus, and that recently acquired information
is lexically accessed rapidly.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 10:18:20 GMT'}]",2020-10-21,"[['Hofmann', 'Markus J.', ''], ['Müller', 'Lara', ''], ['Rölke', 'Andre', ''], ['Radach', 'Ralph', ''], ['Biemann', 'Chris', '']]"
1366533,2010.10203,Ondrej Skopek,"Daria Soboleva, Ondrej Skopek, M\'arius \v{S}ajgal\'ik, Victor
  C\u{a}rbune, Felix Weissenberger, Julia Proskurnia, Bogdan Prisacari, Daniel
  Valcarce, Justin Lu, Rohit Prabhavalkar, Balint Miklos","Replacing Human Audio with Synthetic Audio for On-device Unspoken
  Punctuation Prediction",,,,,cs.LG cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a novel multi-modal unspoken punctuation prediction system for the
English language which combines acoustic and text features. We demonstrate for
the first time, that by relying exclusively on synthetic data generated using a
prosody-aware text-to-speech system, we can outperform a model trained with
expensive human audio recordings on the unspoken punctuation prediction
problem. Our model architecture is well suited for on-device use. This is
achieved by leveraging hash-based embeddings of automatic speech recognition
text output in conjunction with acoustic features as input to a quasi-recurrent
neural network, keeping the model size small and latency low.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 11:30:26 GMT'}]",2020-10-21,"[['Soboleva', 'Daria', ''], ['Skopek', 'Ondrej', ''], ['Šajgalík', 'Márius', ''], ['Cărbune', 'Victor', ''], ['Weissenberger', 'Felix', ''], ['Proskurnia', 'Julia', ''], ['Prisacari', 'Bogdan', ''], ['Valcarce', 'Daniel', ''], ['Lu', 'Justin', ''], ['Prabhavalkar', 'Rohit', ''], ['Miklos', 'Balint', '']]"
1366546,2010.10216,Danish Contractor,"Biswesh Mohapatra, Gaurav Pandey, Danish Contractor, Sachindra Joshi","Simulated Chats for Task-oriented Dialog: Learning to Generate
  Conversations from Instructions",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Popular task-oriented dialog data sets such as MultiWOZ (Budzianowski et al.
2018) are created by providing crowd-sourced workers a goal instruction,
expressed in natural language, that describes the task to be accomplished.
Crowd-sourced workers play the role of a user and an agent to generate dialogs
to accomplish tasks involving booking restaurant tables, making train
reservations, calling a taxi etc. However, creating large crowd-sourced
datasets can be time consuming and expensive. To reduce the cost associated
with generating such dialog datasets, recent work has explored methods to
automatically create larger datasets from small samples.In this paper, we
present a data creation strategy that uses the pre-trained language model, GPT2
(Radford et al. 2018), to simulate the interaction between crowd-sourced
workers by creating a user bot and an agent bot. We train the simulators using
a smaller percentage of actual crowd-generated conversations and their
corresponding goal instructions. We demonstrate that by using the simulated
data, we achieve significant improvements in both low-resource setting as well
as in over-all task performance. To the best of our knowledge we are the first
to present a model for generating entire conversations by simulating the
crowd-sourced data collection process
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 12:04:19 GMT'}]",2020-10-21,"[['Mohapatra', 'Biswesh', ''], ['Pandey', 'Gaurav', ''], ['Contractor', 'Danish', ''], ['Joshi', 'Sachindra', '']]"
1366568,2010.10238,Thomas Ruprecht,"Richard M\""orbitz and Thomas Ruprecht",Supertagging-based Parsing with Linear Context-free Rewriting Systems,"10 pages, 4 figures, 4 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present the first supertagging-based parser for LCFRS. It utilizes neural
classifiers and tremendously outperforms previous LCFRS-based parsers in both
accuracy and parsing speed. Moreover, our results keep up with the best
(general) discontinuous parsers, particularly the scores for discontinuous
constitutents are excellent. The heart of our approach is an efficient
lexicalization procedure which induces a lexical LCFRS from any discontinuous
treebank. It is an adaptation of previous work by M\""orbitz and Ruprecht
(2020). We also describe a modification to usual chart-based LCFRS parsing that
accounts for supertagging and introduce a procedure for the transformation of
lexical LCFRS derivations into equivalent parse trees of the original treebank.
Our approach is implemented and evaluated on the English Discontinuous Penn
Treebank and the German corpora NeGra and Tiger.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 13:02:42 GMT'}]",2020-10-21,"[['Mörbitz', 'Richard', ''], ['Ruprecht', 'Thomas', '']]"
1366569,2010.10239,Markus Freitag,"Markus Freitag, Orhan Firat",Complete Multilingual Neural Machine Translation,Accepted at WMT 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual Neural Machine Translation (MNMT) models are commonly trained on
a joint set of bilingual corpora which is acutely English-centric (i.e. English
either as the source or target language). While direct data between two
languages that are non-English is explicitly available at times, its use is not
common. In this paper, we first take a step back and look at the commonly used
bilingual corpora (WMT), and resurface the existence and importance of implicit
structure that existed in it: multi-way alignment across examples (the same
sentence in more than two languages). We set out to study the use of multi-way
aligned examples to enrich the original English-centric parallel corpora. We
reintroduce this direct parallel data from multi-way aligned corpora between
all source and target languages. By doing so, the English-centric graph expands
into a complete graph, every language pair being connected. We call MNMT with
such connectivity pattern complete Multilingual Neural Machine Translation
(cMNMT) and demonstrate its utility and efficacy with a series of experiments
and analysis. In combination with a novel training data sampling strategy that
is conditioned on the target language only, cMNMT yields competitive
translation quality for all language pairs. We further study the size effect of
multi-way aligned data, its transfer learning capabilities and how it eases
adding a new language in MNMT. Finally, we stress test cMNMT at scale and
demonstrate that we can train a cMNMT model with up to 111*112=12,432 language
pairs that provides competitive translation quality for all language pairs.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 13:03:48 GMT'}]",2020-10-21,"[['Freitag', 'Markus', ''], ['Firat', 'Orhan', '']]"
1366575,2010.10245,Markus Freitag,"Markus Freitag, George Foster, David Grangier, Colin Cherry",Human-Paraphrased References Improve Neural Machine Translation,Accepted at WMT 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic evaluation comparing candidate translations to human-generated
paraphrases of reference translations has recently been proposed by Freitag et
al. When used in place of original references, the paraphrased versions produce
metric scores that correlate better with human judgment. This effect holds for
a variety of different automatic metrics, and tends to favor natural
formulations over more literal (translationese) ones. In this paper we compare
the results of performing end-to-end system development using standard and
paraphrased references. With state-of-the-art English-German NMT components, we
show that tuning to paraphrased references produces a system that is
significantly better according to human judgment, but 5 BLEU points worse when
tested on standard references. Our work confirms the finding that paraphrased
references yield metric scores that correlate better with human judgment, and
demonstrates for the first time that using these scores for system development
can lead to significant improvements.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 13:14:57 GMT'}]",2020-10-21,"[['Freitag', 'Markus', ''], ['Foster', 'George', ''], ['Grangier', 'David', ''], ['Cherry', 'Colin', '']]"
1366582,2010.10252,Marco Wrzalik,Marco Wrzalik and Dirk Krechel,CoRT: Complementary Rankings from Transformers,Pre-print,,,,cs.IR cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent approaches towards passage retrieval have successfully employed
representations from pretrained Language Models(LMs) with large effectiveness
gains. However, due to high computational cost those approaches are usually
limited to re-ranking scenarios. The candidates in such a scenario are
typically retrieved by scalable bag-of-words retrieval models such as BM25.
Although BM25 has proven decent performance as a first-stage ranker, it tends
to miss relevant passages. In this context we propose CoRT, a framework and
neural first-stage ranking model that leverages contextual representations from
transformer-based language models to complement candidates from term-based
ranking functions while causing no significant delay. Using the MS MARCO
dataset, we show that CoRT significantly increases first-stage ranking quality
and recall by complementing BM25 with missing candidates. Consequently, we
found subsequent re-rankers achieve superior results while requiring less
candidates to saturate ranking quality. Finally, we demonstrate that with CoRT
a representation-focused retrieval at web-scale can be realized with latencies
as low as BM25.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 13:28:27 GMT'}]",2020-10-21,"[['Wrzalik', 'Marco', ''], ['Krechel', 'Dirk', '']]"
1366597,2010.10267,Kakia Chatsiou,Kakia Chatsiou,"Text Classification of COVID-19 Press Briefings using BERT and
  Convolutional Neural Networks","12 pages, 1 figure, 4 tables",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  We build a sentence-level political discourse classifier using existing human
expert annotated corpora of political manifestos from the Manifestos Project
(Volkens et al.,2020a) and applying them to a corpus ofCOVID-19Press Briefings
(Chatsiou,2020). We use manually annotated political manifestos as training
data to train a local topic ConvolutionalNeural Network (CNN) classifier; then
apply it to the COVID-19PressBriefings Corpus to automatically classify
sentences in the test corpus.We report on a series of experiments with CNN
trained on top of pre-trained embeddings for sentence-level classification
tasks. We show thatCNN combined with transformers like BERT outperforms CNN
combined with other embeddings (Word2Vec, Glove, ELMo) and that it is possible
to use a pre-trained classifier to conduct automatic classification on
different political texts without additional training.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 13:39:58 GMT'}]",2020-10-21,"[['Chatsiou', 'Kakia', '']]"
1366616,2010.10286,Wei Peng,"Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Jing Yu, Yajing Sun,
  Xiangpeng Wei","Bi-directional Cognitive Thinking Network for Machine Reading
  Comprehension",Accepted to COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a novel Bi-directional Cognitive Knowledge Framework (BCKF) for
reading comprehension from the perspective of complementary learning systems
theory. It aims to simulate two ways of thinking in the brain to answer
questions, including reverse thinking and inertial thinking. To validate the
effectiveness of our framework, we design a corresponding Bi-directional
Cognitive Thinking Network (BCTN) to encode the passage and generate a question
(answer) given an answer (question) and decouple the bi-directional knowledge.
The model has the ability to reverse reasoning questions which can assist
inertial thinking to generate more accurate answers. Competitive improvement is
observed in DuReader dataset, confirming our hypothesis that bi-directional
knowledge helps the QA task. The novel framework shows an interesting
perspective on machine reading comprehension and cognitive science.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 13:56:30 GMT'}]",2020-10-21,"[['Peng', 'Wei', ''], ['Hu', 'Yue', ''], ['Xing', 'Luxi', ''], ['Xie', 'Yuqiang', ''], ['Yu', 'Jing', ''], ['Sun', 'Yajing', ''], ['Wei', 'Xiangpeng', '']]"
1366653,2010.10323,Chujie Zheng,"Chujie Zheng, Kunpeng Zhang, Harry Jiannan Wang, Ling Fan",Topic-Aware Abstractive Text Summarization,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic text summarization aims at condensing a document to a shorter
version while preserving the key information. Different from extractive
summarization which simply selects text fragments from the document,
abstractive summarization generates the summary in a word-by-word manner. Most
current state-of-the-art (SOTA) abstractive summarization methods are based on
the Transformer-based encoder-decoder architecture and focus on novel
self-supervised objectives in pre-training. While these models well capture the
contextual information among words in documents, little attention has been paid
to incorporating global semantics to better fine-tune for the downstream
abstractive summarization task.
  In this study, we propose a topic-aware abstractive summarization (TAAS)
framework by leveraging the underlying semantic structure of documents
represented by their latent topics. Specifically, TAAS seamlessly incorporates
a neural topic modeling into an encoder-decoder based sequence generation
procedure via attention for summarization. This design is able to learn and
preserve global semantics of documents and thus makes summarization effective,
which has been proved by our experiments on real-world datasets. As compared to
several cutting-edge baseline methods, we show that TAAS outperforms BART, a
well-recognized SOTA model, by 2%, 8%, and 12% regarding the F measure of
ROUGE-1, ROUGE-2, and ROUGE-L, respectively. TAAS also achieves comparable
performance to PEGASUS and ProphetNet, which is difficult to accomplish given
that training PEGASUS and ProphetNet requires enormous computing capacity
beyond what we used in this study.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 14:45:25 GMT'}]",2020-10-21,"[['Zheng', 'Chujie', ''], ['Zhang', 'Kunpeng', ''], ['Wang', 'Harry Jiannan', ''], ['Fan', 'Ling', '']]"
1366663,2010.10333,Wenchang Ma,"Wenchang Ma, Ryuichi Takanobu, Minghao Tu, Minlie Huang","Bridging the Gap between Conversational Reasoning and Interactive
  Recommendation",,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There have been growing interests in building a conversational recommender
system, where the system simultaneously interacts with the user and explores
the user's preference throughout conversational interactions. Recommendation
and conversation were usually treated as two separate modules with limited
information exchange in existing works, which hinders the capability of both
systems: (1) dialog merely incorporated recommendation entities without being
guided by an explicit recommendation-oriented policy; (2) recommendation
utilized dialog only as a form of interaction instead of improving
recommendation effectively. To address the above issues, we propose a novel
recommender dialog model: CR-Walker. In order to view the two separate systems
within a unified framework, we seek high-level mapping between hierarchical
dialog acts and multi-hop knowledge graph reasoning. The model walks on a
large-scale knowledge graph to form a reasoning tree at each turn, then mapped
to dialog acts to guide response generation. With such a mapping mechanism as a
bridge between recommendation and conversation, our framework maximizes the
mutual benefit between two systems: dialog as an enhancement to recommendation
quality and explainability, recommendation as a goal and enrichment to dialog
semantics. Quantitative evaluation shows that our model excels in conversation
informativeness and recommendation effectiveness, at the same time explainable
on the policy level.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 14:53:22 GMT'}]",2020-10-21,"[['Ma', 'Wenchang', ''], ['Takanobu', 'Ryuichi', ''], ['Tu', 'Minghao', ''], ['Huang', 'Minlie', '']]"
1366716,2010.10386,Spyretta Leivaditi,"Spyretta Leivaditi, Julien Rossi, Evangelos Kanoulas",A Benchmark for Lease Contract Review,,,,,cs.IR cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Extracting entities and other useful information from legal contracts is an
important task whose automation can help legal professionals perform contract
reviews more efficiently and reduce relevant risks. In this paper, we tackle
the problem of detecting two different types of elements that play an important
role in a contract review, namely entities and red flags. The latter are terms
or sentences that indicate that there is some danger or other potentially
problematic situation for one or more of the signing parties. We focus on
supporting the review of lease agreements, a contract type that has received
little attention in the legal information extraction literature, and we define
the types of entities and red flags needed for that task. We release a new
benchmark dataset of 179 lease agreement documents that we have manually
annotated with the entities and red flags they contain, and which can be used
to train and test relevant extraction algorithms. Finally, we release a new
language model, called ALeaseBERT, pre-trained on this dataset and fine-tuned
for the detection of the aforementioned elements, providing a baseline for
further research
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 15:50:50 GMT'}]",2020-10-21,"[['Leivaditi', 'Spyretta', ''], ['Rossi', 'Julien', ''], ['Kanoulas', 'Evangelos', '']]"
1366721,2010.10391,Georgios Michalopoulos,"George Michalopoulos, Yuanxin Wang, Hussam Kaka, Helen Chen and Alex
  Wong","UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual
  Embeddings Using the Unified Medical Language System Metathesaurus","8 pages, 4 figures",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have
achieved state-of-the-art results in biomedical natural language processing
tasks by focusing their pre-training process on domain-specific corpora.
However, such models do not take into consideration expert domain knowledge.
  In this work, we introduced UmlsBERT, a contextual embedding model that
integrates domain knowledge during the pre-training process via a novel
knowledge augmentation strategy. More specifically, the augmentation on
UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was
performed in two ways: i) connecting words that have the same underlying
`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to
create clinically meaningful input embeddings. By applying these two
strategies, UmlsBERT can encode clinical domain knowledge into word embeddings
and outperform existing domain-specific models on common named-entity
recognition (NER) and clinical natural language inference clinical NLP tasks.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 15:56:31 GMT'}]",2020-10-21,"[['Michalopoulos', 'George', ''], ['Wang', 'Yuanxin', ''], ['Kaka', 'Hussam', ''], ['Chen', 'Helen', ''], ['Wong', 'Alex', '']]"
1366769,2010.10439,Wenhu Chen,"Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Wang, William W.
  Cohen",Open Question Answering over Tables and Text,Technical Report,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In open question answering (QA), the answer to a question is produced by
retrieving and then analyzing documents that might contain answers to the
question. Most open QA systems have considered only retrieving information from
unstructured text. Here we consider for the first time open QA over both
tabular and textual data and present a new large-scale dataset Open Table-Text
Question Answering (OTT-QA) to evaluate performance on this task. Most
questions in OTT-QA require multi-hop inference across tabular data and
unstructured text, and the evidence required to answer a question can be
distributed in different ways over these two types of input, making evidence
retrieval challenging---our baseline model using an iterative retriever and
BERT-based reader achieves an exact match score less than 10%. We then propose
two novel techniques to address the challenge of retrieving and aggregating
evidence for OTT-QA. The first technique is to use ""early fusion"" to group
multiple highly relevant tabular and textual units into a fused block, which
provides more context for the retriever to search for. The second technique is
to use a cross-block reader to model the cross-dependency between multiple
retrieved evidences with global-local sparse attention. Combining these two
techniques improves the score significantly, to above 27%.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 16:48:14 GMT'}]",2020-10-21,"[['Chen', 'Wenhu', ''], ['Chang', 'Ming-Wei', ''], ['Schlinger', 'Eva', ''], ['Wang', 'William', ''], ['Cohen', 'William W.', '']]"
1366783,2010.10453,Maria Leonor Pacheco,Maria Leonor Pacheco and Dan Goldwasser,Modeling Content and Context with Deep Relational Learning,TACL pre-MIT Press version,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building models for realistic natural language tasks requires dealing with
long texts and accounting for complicated structural dependencies.
Neural-symbolic representations have emerged as a way to combine the reasoning
capabilities of symbolic methods, with the expressiveness of neural networks.
However, most of the existing frameworks for combining neural and symbolic
representations have been designed for classic relational learning tasks that
work over a universe of symbolic entities and relations. In this paper, we
present DRaiL, an open-source declarative framework for specifying deep
relational models, designed to support a variety of NLP scenarios. Our
framework supports easy integration with expressive language encoders, and
provides an interface to study the interactions between representation,
inference and learning.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 17:09:35 GMT'}]",2020-10-21,"[['Pacheco', 'Maria Leonor', ''], ['Goldwasser', 'Dan', '']]"
1366802,2010.10472,Yiyuan Li,"Yiyuan Li, Antonios Anastasopoulos, Alan W Black","Comparison of Interactive Knowledge Base Spelling Correction Models for
  Low-Resource Languages",9 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Spelling normalization for low resource languages is a challenging task
because the patterns are hard to predict and large corpora are usually required
to collect enough examples. This work shows a comparison of a neural model and
character language models with varying amounts on target language data. Our
usage scenario is interactive correction with nearly zero amounts of training
examples, improving models as more data is collected, for example within a chat
app. Such models are designed to be incrementally improved as feedback is given
from users. In this work, we design a knowledge-base and prediction model
embedded system for spelling correction in low-resource languages. Experimental
results on multiple languages show that the model could become effective with a
small amount of data. We perform experiments on both natural and synthetic
data, as well as on data from two endangered languages (Ainu and Griko). Last,
we built a prototype system that was used for a small case study on Hinglish,
which further demonstrated the suitability of our approach in real world
scenarios.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 17:31:07 GMT'}]",2020-10-21,"[['Li', 'Yiyuan', ''], ['Anastasopoulos', 'Antonios', ''], ['Black', 'Alan W', '']]"
1366374,2010.10044,Xiachong Feng,"Xiachong Feng, Xiaocheng Feng, Bing Qin, Ting Liu","Incorporating Commonsense Knowledge into Abstractive Dialogue
  Summarization via Heterogeneous Graph Networks",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Abstractive dialogue summarization is the task of capturing the highlights of
a dialogue and rewriting them into a concise version. In this paper, we present
a novel multi-speaker dialogue summarizer to demonstrate how large-scale
commonsense knowledge can facilitate dialogue understanding and summary
generation. In detail, we consider utterance and commonsense knowledge as two
different types of data and design a Dialogue Heterogeneous Graph Network
(D-HGN) for modeling both information. Meanwhile, we also add speakers as
heterogeneous nodes to facilitate information flow. Experimental results on the
SAMSum dataset show that our model can outperform various methods. We also
conduct zero-shot setting experiments on the Argumentative Dialogue Summary
Corpus, the results show that our model can better generalized to the new
domain.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 05:44:55 GMT'}]",2020-10-21,"[['Feng', 'Xiachong', ''], ['Feng', 'Xiaocheng', ''], ['Qin', 'Bing', ''], ['Liu', 'Ting', '']]"
1366829,2010.10499,Adrian de Wynter,Adrian de Wynter and Daniel J. Perry,Optimal Subarchitecture Extraction For BERT,Preprint. Under review,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We extract an optimal subset of architectural parameters for the BERT
architecture from Devlin et al. (2018) by applying recent breakthroughs in
algorithms for neural architecture search. This optimal subset, which we refer
to as ""Bort"", is demonstrably smaller, having an effective (that is, not
counting the embedding layer) size of $5.5\%$ the original BERT-large
architecture, and $16\%$ of the net size. Bort is also able to be pretrained in
$288$ GPU hours, which is $1.2\%$ of the time required to pretrain the
highest-performing BERT parametric architectural variant, RoBERTa-large (Liu et
al., 2019), and about $33\%$ of that of the world-record, in GPU hours,
required to train BERT-large on the same hardware. It is also $7.9$x faster on
a CPU, as well as being better performing than other compressed variants of the
architecture, and some of the non-compressed variants: it obtains performance
improvements of between $0.3\%$ and $31\%$, absolute, with respect to
BERT-large, on multiple public natural language understanding (NLU) benchmarks.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 17:53:01 GMT'}]",2020-10-21,"[['de Wynter', 'Adrian', ''], ['Perry', 'Daniel J.', '']]"
1366372,2010.10042,Yasuhide Miura,"Yasuhide Miura, Yuhao Zhang, Curtis P. Langlotz, Dan Jurafsky","Improving Factual Completeness and Consistency of Image-to-Text
  Radiology Report Generation","13 pages, 3 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural image-to-text radiology report generation systems offer the potential
to accelerate clinical processes by saving radiologists from the repetitive
labor of drafting radiology reports and preventing medical errors. However,
existing report generation systems, despite achieving high performances on
natural language generation metrics such as CIDEr or BLEU, still suffer from
incomplete and inconsistent generations, rendering these systems unusable in
practice. In this work, we aim to overcome this problem by proposing two new
metrics that encourage the factual completeness and consistency of generated
radiology reports. The first metric, the Exact Entity Match score, evaluates a
generation by its coverage of radiology domain entities against the references.
The second metric, the Entailing Entity Match score, augments the first metric
by introducing a natural language inference model into the entity match process
to encourage consistent generations that can be entailed from the references.
To achieve this, we also developed an in-domain NLI model via weak supervision
to improve its performance on radiology text. We further propose a report
generation system that optimizes these two new metrics via reinforcement
learning. On two open radiology report datasets, our system not only achieves
the best performance on these two metrics compared to baselines, but also leads
to as much as +2.0 improvement on the F1 score of a clinical finding metric. We
show via analysis and examples that our system leads to generations that are
more complete and consistent compared to the baselines.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 05:42:47 GMT'}]",2020-10-21,"[['Miura', 'Yasuhide', ''], ['Zhang', 'Yuhao', ''], ['Langlotz', 'Curtis P.', ''], ['Jurafsky', 'Dan', '']]"
1366365,2010.10035,Neha Srikanth,"Neha Srikanth, Junyi Jessy Li","Elaborative Simplification: Content Addition and Explanation Generation
  in Text Simplification",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Much of modern day text simplification research focuses on sentence-level
simplification, transforming original, more complex sentences to simplified
versions. However, adding content can often be useful when difficult concepts
and reasoning need to be explained. In this work, we present the first
data-driven study of content addition in document simplification, which we call
elaborative simplification. We introduce a new annotated dataset of 1.3K
instances of elaborative simplification and analyze how entities, ideas, and
concepts are elaborated through the lens of contextual specificity. We
establish baselines for elaboration generation using large scale pre-trained
language models, and illustrate that considering contextual specificity during
generation can improve performance. Our results illustrate the complexities of
elaborative simplification, suggesting many interesting directions for future
work.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 05:06:23 GMT'}]",2020-10-21,"[['Srikanth', 'Neha', ''], ['Li', 'Junyi Jessy', '']]"
1342529,2009.01026,Hammond Pearce,"Hammond Pearce, Benjamin Tan, Ramesh Karri",DAVE: Deriving Automatically Verilog from English,"6 pages, 2 figures",,10.1145/3380446.3430634,,cs.SE cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While specifications for digital systems are provided in natural language,
engineers undertake significant efforts to translate them into the programming
languages understood by compilers for digital systems. Automating this process
allows designers to work with the language in which they are most comfortable
--the original natural language -- and focus instead on other downstream design
challenges. We explore the use of state-of-the-art machine learning (ML) to
automatically derive Verilog snippets from English via fine-tuning GPT-2, a
natural language ML system. We describe our approach for producing a suitable
dataset of novice-level digital design tasks and provide a detailed exploration
of GPT-2, finding encouraging translation performance across our task sets
(94.8% correct), with the ability to handle both simple and abstract design
tasks.
","[{'version': 'v1', 'created': 'Thu, 27 Aug 2020 15:25:03 GMT'}]",2020-10-21,"[['Pearce', 'Hammond', ''], ['Tan', 'Benjamin', ''], ['Karri', 'Ramesh', '']]"
1358424,2010.02094,Gaurav Arora,Gaurav Arora,"Gauravarora@HASOC-Dravidian-CodeMix-FIRE2020: Pre-training ULMFiT on
  Synthetically Generated Code-Mixed Data for Hate Speech Detection","System description paper for 2nd ranked system in Sub-task B accepted
  at Dravidian-Codemix-HASOC2020@FIRE2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes the system submitted to Dravidian-Codemix-HASOC2020:
Hate Speech and Offensive Content Identification in Dravidian languages
(Tamil-English and Malayalam-English). The task aims to identify offensive
language in code-mixed dataset of comments/posts in Dravidian languages
collected from social media. We participated in both Sub-task A, which aims to
identify offensive content in mixed-script (mixture of Native and Roman script)
and Sub-task B, which aims to identify offensive content in Roman script, for
Dravidian languages. In order to address these tasks, we proposed pre-training
ULMFiT on synthetically generated code-mixed data, generated by modelling
code-mixed data generation as a Markov process using Markov chains. Our model
achieved 0.88 weighted F1-score for code-mixed Tamil-English language in
Sub-task B and got 2nd rank on the leader-board. Additionally, our model
achieved 0.91 weighted F1-score (4th Rank) for mixed-script Malayalam-English
in Sub-task A and 0.74 weighted F1-score (5th Rank) for code-mixed
Malayalam-English language in Sub-task B.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 15:25:47 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 18:11:41 GMT'}]",2020-10-21,"[['Arora', 'Gaurav', '']]"
1358682,2010.02352,Julia Kreutzer,"Julia Kreutzer, George Foster, Colin Cherry",Inference Strategies for Machine Translation with Conditional Masking,"EMNLP 2020, updated Fig 3",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conditional masked language model (CMLM) training has proven successful for
non-autoregressive and semi-autoregressive sequence generation tasks, such as
machine translation. Given a trained CMLM, however, it is not clear what the
best inference strategy is. We formulate masked inference as a factorization of
conditional probabilities of partial sequences, show that this does not harm
performance, and investigate a number of simple heuristics motivated by this
perspective. We identify a thresholding strategy that has advantages over the
standard ""mask-predict"" algorithm, and provide analyses of its behavior on
machine translation tasks.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 21:50:09 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 15:14:40 GMT'}]",2020-10-21,"[['Kreutzer', 'Julia', ''], ['Foster', 'George', ''], ['Cherry', 'Colin', '']]"
1359331,2010.03001,Giannis Bekoulis,"Giannis Bekoulis, Christina Papagiannopoulou, Nikos Deligiannis",A Review on Fact Extraction and VERification: The FEVER case,author preprint version,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fact Extraction and VERification (FEVER) is a recently introduced task which
aims to identify the veracity of a given claim based on Wikipedia documents. A
lot of methods have been proposed to address this problem which consists of the
subtasks of (i) retrieving the relevant documents (and sentences) from
Wikipedia and (ii) validating whether the information in the documents supports
or refutes a given claim. This task is essential since it can be the building
block of applications that require a deep understanding of the language such as
fake news detection and medical claim verification. In this paper, we aim to
get a better understanding of the challenges in the task by presenting the
literature in a structured and comprehensive way. In addition, we describe the
proposed methods by analyzing the technical perspectives of the different
approaches and discussing the performance results on the FEVER dataset.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 20:05:43 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 20:35:52 GMT'}]",2020-10-21,"[['Bekoulis', 'Giannis', ''], ['Papagiannopoulou', 'Christina', ''], ['Deligiannis', 'Nikos', '']]"
1360062,2010.03732,Inigo Jauregi Unanue,"Inigo Jauregi Unanue, Nazanin Esmaili, Gholamreza Haffari, Massimo
  Piccardi","Leveraging Discourse Rewards for Document-Level Neural Machine
  Translation",Accepted at COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Document-level machine translation focuses on the translation of entire
documents from a source to a target language. It is widely regarded as a
challenging task since the translation of the individual sentences in the
document needs to retain aspects of the discourse at document level. However,
document-level translation models are usually not trained to explicitly ensure
discourse quality. Therefore, in this paper we propose a training approach that
explicitly optimizes two established discourse metrics, lexical cohesion (LC)
and coherence (COH), by using a reinforcement learning objective. Experiments
over four different language pairs and three translation domains have shown
that our training approach has been able to achieve more cohesive and coherent
document translations than other competitive approaches, yet without
compromising the faithfulness to the reference translation. In the case of the
Zh-En language pair, our method has achieved an improvement of 2.46 percentage
points (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time
improving 0.63 pp in BLEU score and 0.47 pp in F_BERT.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 02:26:22 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 23:33:06 GMT'}]",2020-10-21,"[['Unanue', 'Inigo Jauregi', ''], ['Esmaili', 'Nazanin', ''], ['Haffari', 'Gholamreza', ''], ['Piccardi', 'Massimo', '']]"
1360627,2010.04297,Thibault Sellam,"Thibault Sellam, Amy Pu, Hyung Won Chung, Sebastian Gehrmann, Qijun
  Tan, Markus Freitag, Dipanjan Das, Ankur P. Parikh","Learning to Evaluate Translation Beyond English: BLEURT Submissions to
  the WMT Metrics 2020 Shared Task",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The quality of machine translation systems has dramatically improved over the
last decade, and as a result, evaluation has become an increasingly challenging
problem. This paper describes our contribution to the WMT 2020 Metrics Shared
Task, the main benchmark for automatic evaluation of translation. We make
several submissions based on BLEURT, a previously published metric based on
transfer learning. We extend the metric beyond English and evaluate it on 14
language pairs for which fine-tuning data is available, as well as 4
""zero-shot"" language pairs, for which we have no labelled examples.
Additionally, we focus on English to German and demonstrate how to combine
BLEURT's predictions with those of YiSi and use alternative reference
translations to enhance the performance. Empirical results show that the models
achieve competitive results on the WMT Metrics 2019 Shared Task, indicating
their promise for the 2020 edition.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 23:16:26 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Oct 2020 21:45:11 GMT'}, {'version': 'v3', 'created': 'Mon, 19 Oct 2020 22:40:08 GMT'}]",2020-10-21,"[['Sellam', 'Thibault', ''], ['Pu', 'Amy', ''], ['Chung', 'Hyung Won', ''], ['Gehrmann', 'Sebastian', ''], ['Tan', 'Qijun', ''], ['Freitag', 'Markus', ''], ['Das', 'Dipanjan', ''], ['Parikh', 'Ankur P.', '']]"
1362236,2010.05906,Lianhui Qin,"Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena
  Hwang, Ronan Le Bras, Antoine Bosselut, Yejin Choi","Back to the Future: Unsupervised Backprop-based Decoding for
  Counterfactual and Abductive Commonsense Reasoning",EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Abductive and counterfactual reasoning, core abilities of everyday human
cognition, require reasoning about what might have happened at time t, while
conditioning on multiple contexts from the relative past and future. However,
simultaneous incorporation of past and future contexts using generative
language models (LMs) can be challenging, as they are trained either to
condition only on the past context or to perform narrowly scoped
text-infilling. In this paper, we propose DeLorean, a new unsupervised decoding
algorithm that can flexibly incorporate both the past and future contexts using
only off-the-shelf, left-to-right language models and no supervision. The key
intuition of our algorithm is incorporating the future through
back-propagation, during which, we only update the internal representation of
the output while fixing the model parameters. By alternating between forward
and backward propagation, DeLorean can decode the output representation that
reflects both the left and right contexts. We demonstrate that our approach is
general and applicable to two nonmonotonic reasoning tasks: abductive text
generation and counterfactual story revision, where DeLorean outperforms a
range of unsupervised and some supervised methods, based on automatic and human
evaluation.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 17:58:43 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 05:39:07 GMT'}]",2020-10-21,"[['Qin', 'Lianhui', ''], ['Shwartz', 'Vered', ''], ['West', 'Peter', ''], ['Bhagavatula', 'Chandra', ''], ['Hwang', 'Jena', ''], ['Bras', 'Ronan Le', ''], ['Bosselut', 'Antoine', ''], ['Choi', 'Yejin', '']]"
1364165,2010.07835,Yue Yu,"Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao and Chao Zhang","Fine-Tuning Pre-trained Language Model with Weak Supervision: A
  Contrastive-Regularized Self-Training Approach",,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Fine-tuned pre-trained language models (LMs) achieve enormous success in many
natural language processing (NLP) tasks, but they still require excessive
labeled data in the fine-tuning stage. We study the problem of fine-tuning
pre-trained LMs using only weak supervision, without any labeled data. This
problem is challenging because the high capacity of LMs makes them prone to
overfitting the noisy labels generated by weak supervision. To address this
problem, we develop a contrastive self-training framework, COSINE, to enable
fine-tuning LMs with weak supervision. Underpinned by contrastive
regularization and confidence-based reweighting, this contrastive self-training
framework can gradually improve model fitting while effectively suppressing
error propagation. Experiments on sequence, token, and sentence pair
classification tasks show that our model outperforms the strongest baseline by
large margins on 7 benchmarks in 6 tasks, and achieves competitive performance
with fully-supervised fine-tuning methods.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 15:55:08 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 17:05:28 GMT'}]",2020-10-21,"[['Yu', 'Yue', ''], ['Zuo', 'Simiao', ''], ['Jiang', 'Haoming', ''], ['Ren', 'Wendi', ''], ['Zhao', 'Tuo', ''], ['Zhang', 'Chao', '']]"
1364848,2010.08518,Biao Zhang,"Biao Zhang, Ivan Titov, Barry Haddow, Rico Sennrich",Adaptive Feature Selection for End-to-End Speech Translation,"EMNLP2020 Findings; source code is at
  https://github.com/bzhangGo/zero",,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Information in speech signals is not evenly distributed, making it an
additional challenge for end-to-end (E2E) speech translation (ST) to learn to
focus on informative features. In this paper, we propose adaptive feature
selection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR
encoder and apply AFS to dynamically estimate the importance of each encoded
speech feature to SR. A ST encoder, stacked on top of the ASR encoder, then
receives the filtered features from the (frozen) ASR encoder. We take L0DROP
(Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech
features with respect to both temporal and feature dimensions. Results on
LibriSpeech En-Fr and MuST-C benchmarks show that AFS facilitates learning of
ST by pruning out ~84% temporal features, yielding an average translation gain
of ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces
the performance gap compared to the cascade baseline, and outperforms it on
LibriSpeech En-Fr with a BLEU score of 18.56 (without data augmentation)
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 17:21:00 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 13:53:39 GMT'}]",2020-10-21,"[['Zhang', 'Biao', ''], ['Titov', 'Ivan', ''], ['Haddow', 'Barry', ''], ['Sennrich', 'Rico', '']]"
1365743,2010.09413,Du\v{s}an Vari\v{s},"Du\v{s}an Vari\v{s}, Katsuhito Sudoh, and Satoshi Nakamura","Image Captioning with Visual Object Representations Grounded in the
  Textual Modality",,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present our work in progress exploring the possibilities of a shared
embedding space between textual and visual modality. Leveraging the textual
nature of object detection labels and the hypothetical expressiveness of
extracted visual object representations, we propose an approach opposite to the
current trend, grounding of the representations in the word embedding space of
the captioning system instead of grounding words or sentences in their
associated images. Based on the previous work, we apply additional grounding
losses to the image captioning training objective aiming to force visual object
representations to create more heterogeneous clusters based on their class
label and copy a semantic structure of the word embedding space. In addition,
we provide an analysis of the learned object vector space projection and its
impact on the IC system performance. With only slight change in performance,
grounded models reach the stopping criterion during training faster than the
unconstrained model, needing about two to three times less training updates.
Additionally, an improvement in structural correlation between the word
embeddings and both original and projected object vectors suggests that the
grounding is actually mutual.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 12:21:38 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 12:24:39 GMT'}]",2020-10-21,"[['Variš', 'Dušan', ''], ['Sudoh', 'Katsuhito', ''], ['Nakamura', 'Satoshi', '']]"
1365932,2010.09602,Yusuke Yasuda,"Yusuke Yasuda, Xin Wang, Junichi Yamagishi",End-to-End Text-to-Speech using Latent Duration based on VQ-VAE,,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Explicit duration modeling is a key to achieving robust and efficient
alignment in text-to-speech synthesis (TTS). We propose a new TTS framework
using explicit duration modeling that incorporates duration as a discrete
latent variable to TTS and enables joint optimization of whole modules from
scratch. We formulate our method based on conditional VQ-VAE to handle discrete
duration in a variational autoencoder and provide a theoretical explanation to
justify our method. In our framework, a connectionist temporal classification
(CTC) -based force aligner acts as the approximate posterior, and
text-to-duration works as the prior in the variational autoencoder. We
evaluated our proposed method with a listening test and compared it with other
TTS methods based on soft-attention or explicit duration modeling. The results
showed that our systems rated between soft-attention-based methods
(Transformer-TTS, Tacotron2) and explicit duration modeling-based methods
(Fastspeech).
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 15:34:49 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 13:46:35 GMT'}]",2020-10-21,"[['Yasuda', 'Yusuke', ''], ['Wang', 'Xin', ''], ['Yamagishi', 'Junichi', '']]"
1365953,2010.09623,Vi Tran Tuan,"Tuan-Vi Tran, Xuan-Thien Pham, Duc-Vu Nguyen, Kiet Van Nguyen, Ngan
  Luu-Thuy Nguyen",An Empirical Study for Vietnamese Constituency Parsing with Pre-training,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we use a span-based approach for Vietnamese constituency
parsing. Our method follows the self-attention encoder architecture and a chart
decoder using a CKY-style inference algorithm. We present analyses of the
experiment results of the comparison of our empirical method using pre-training
models XLM-Roberta and PhoBERT on both Vietnamese datasets VietTreebank and
NIIVTB1. The results show that our model with XLM-Roberta archived the
significantly F1-score better than other pre-training models, VietTreebank at
81.19% and NIIVTB1 at 85.70%.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 16:02:00 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 02:44:02 GMT'}]",2020-10-21,"[['Tran', 'Tuan-Vi', ''], ['Pham', 'Xuan-Thien', ''], ['Nguyen', 'Duc-Vu', ''], ['Van Nguyen', 'Kiet', ''], ['Nguyen', 'Ngan Luu-Thuy', '']]"
1366110,2010.09780,Wenhao Yu,"Wenhao Yu, Lingfei Wu, Yu Deng, Qingkai Zeng, Ruchi Mahindru, Sinem
  Guven, Meng Jiang",Technical Question Answering across Tasks and Domains,"10 pages, 6 figures",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Building automatic technical support system is an important yet challenge
task. Conceptually, to answer a user question on a technical forum, a human
expert has to first retrieve relevant documents, and then read them carefully
to identify the answer snippet. Despite huge success the researchers have
achieved in coping with general domain question answering (QA), much less
attentions have been paid for investigating technical QA. Specifically,
existing methods suffer from several unique challenges (i) the question and
answer rarely overlaps substantially and (ii) very limited data size. In this
paper, we propose a novel framework of deep transfer learning to effectively
address technical QA across tasks and domains. To this end, we present an
adjustable joint learning approach for document retrieval and reading
comprehension tasks. Our experiments on the TechQA demonstrates superior
performance compared with state-of-the-art methods.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 18:39:30 GMT'}]",2020-10-21,"[['Yu', 'Wenhao', ''], ['Wu', 'Lingfei', ''], ['Deng', 'Yu', ''], ['Zeng', 'Qingkai', ''], ['Mahindru', 'Ruchi', ''], ['Guven', 'Sinem', ''], ['Jiang', 'Meng', '']]"
1366118,2010.09788,Yufei Feng,"Mo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan,
  Murray Campbell",Deriving Commonsense Inference Tasks from Interactive Fictions,,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Commonsense reasoning simulates the human ability to make presumptions about
our physical world, and it is an indispensable cornerstone in building general
AI systems. We propose a new commonsense reasoning dataset based on human's
interactive fiction game playings as human players demonstrate plentiful and
diverse commonsense reasoning. The new dataset mitigates several limitations of
the prior art. Experiments show that our task is solvable to human experts with
sufficient commonsense knowledge but poses challenges to existing machine
reading models, with a big performance gap of more than 30%.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 19:02:34 GMT'}]",2020-10-21,"[['Yu', 'Mo', ''], ['Guo', 'Xiaoxiao', ''], ['Feng', 'Yufei', ''], ['Zhu', 'Xiaodan', ''], ['Greenspan', 'Michael', ''], ['Campbell', 'Murray', '']]"
1366133,2010.09803,Jie Zhao,"Jie Zhao, Huan Sun","Adversarial Training for Code Retrieval with Question-Description
  Relevance Regularization",,,,,cs.CL cs.AI cs.IR cs.PL,http://creativecommons.org/licenses/by/4.0/,"  Code retrieval is a key task aiming to match natural and programming
languages. In this work, we propose adversarial learning for code retrieval,
that is regularized by question-description relevance. First, we adapt a simple
adversarial learning technique to generate difficult code snippets given the
input question, which can help the learning of code retrieval that faces
bi-modal and data-scarce challenges. Second, we propose to leverage
question-description relevance to regularize adversarial learning, such that a
generated code snippet should contribute more to the code retrieval training
loss, only if its paired natural language description is predicted to be less
relevant to the user given question. Experiments on large-scale code retrieval
datasets of two programming languages show that our adversarial learning method
is able to improve the performance of state-of-the-art models. Moreover, using
an additional duplicate question prediction model to regularize adversarial
learning further improves the performance, and this is more effective than
using the duplicated questions in strong multi-task learning baselines
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 19:32:03 GMT'}]",2020-10-21,"[['Zhao', 'Jie', ''], ['Sun', 'Huan', '']]"
1366158,2010.09828,Elliot Schumacher,"Elliot Schumacher, James Mayfield, Mark Dredze",Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cross-language entity linking grounds mentions in multiple languages to a
single-language knowledge base. We propose a neural ranking architecture for
this task that uses multilingual BERT representations of the mention and the
context in a neural network. We find that the multilingual ability of BERT
leads to robust performance in monolingual and multilingual settings.
Furthermore, we explore zero-shot language transfer and find surprisingly
robust performance. We investigate the zero-shot degradation and find that it
can be partially mitigated by a proposed auxiliary training objective, but that
the remaining error can best be attributed to domain shift rather than language
transfer.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 20:08:26 GMT'}]",2020-10-21,"[['Schumacher', 'Elliot', ''], ['Mayfield', 'James', ''], ['Dredze', 'Mark', '']]"
1366235,2010.09905,Ilya Valmianski,"Ilya Valmianski, Ian M. Finn, Nave Frost, Yang Wang, Baodong Liu,
  James J. Zhu, Sunil Karumuri and Daniel S. Zisook","SmartTriage: A system for personalized patient data capture,
  documentation generation, and decision support",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Symptom checkers have emerged as an important tool for collecting symptoms
and diagnosing patients, minimizing the involvement of clinical personnel. We
developed a machine-learning-backed system, SmartTriage, which goes beyond
conventional symptom checking through a tight bi-directional integration with
the electronic medical record (EMR). Conditioned on EMR-derived patient
history, our system identifies the patient's chief complaint from a free-text
entry and then asks a series of discrete questions to obtain relevant
symptomatology. The patient-specific data are used to predict detailed
ICD-10-CM codes as well as medication, laboratory, and imaging orders. Patient
responses and clinical decision support (CDS) predictions are then inserted
back into the EMR. To train the machine learning components of SmartTriage, we
employed novel data sets of over 25 million primary care encounters and 1
million patient free-text reason-for-visit entries. These data sets were used
to construct: (1) a long short-term memory (LSTM) based patient history
representation, (2) a fine-tuned transformer model for chief complaint
extraction, (3) a random forest model for question sequencing, and (4) a
feed-forward network for CDS predictions. We also present the full production
architecture for the pilot deployment of SmartTriage that covers 337 patient
chief complaints.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 22:45:27 GMT'}]",2020-10-21,"[['Valmianski', 'Ilya', ''], ['Finn', 'Ian M.', ''], ['Frost', 'Nave', ''], ['Wang', 'Yang', ''], ['Liu', 'Baodong', ''], ['Zhu', 'James J.', ''], ['Karumuri', 'Sunil', ''], ['Zisook', 'Daniel S.', '']]"
1366256,2010.09926,Neema Kotonya,Neema Kotonya and Francesca Toni,Explainable Automated Fact-Checking for Public Health Claims,"Accepted to EMNLP 2020. 15 pages, 7 figures, 9 tables. The dataset is
  available at https://github.com/neemakot/Health-Fact-Checking",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fact-checking is the task of verifying the veracity of claims by assessing
their assertions against credible evidence. The vast majority of fact-checking
studies focus exclusively on political claims. Very little research explores
fact-checking for other topics, specifically subject matters for which
expertise is required. We present the first study of explainable fact-checking
for claims which require specific expertise. For our case study we choose the
setting of public health. To support this case study we construct a new dataset
PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard
explanations (i.e., judgments) to support the fact-check labels for claims. We
explore two tasks: veracity prediction and explanation generation. We also
define and evaluate, with humans and computationally, three coherence
properties of explanation quality. Our results indicate that, by training on
in-domain data, gains can be made in explainable, automated fact-checking for
claims which require specific expertise.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 23:51:33 GMT'}]",2020-10-21,"[['Kotonya', 'Neema', ''], ['Toni', 'Francesca', '']]"
1366257,2010.09927,Arvind Srikantan,"Karthik Radhakrishnan, Arvind Srikantan, Xi Victoria Lin",ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries,"IntEx-SemPar Workshop at EMNLP 2020, 12 pages, 3 figures",,,,cs.CL cs.AI cs.DB cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Translating natural language utterances to executable queries is a helpful
technique in making the vast amount of data stored in relational databases
accessible to a wider range of non-tech-savvy end users. Prior work in this
area has largely focused on textual input that is linguistically correct and
semantically unambiguous. However, real-world user queries are often succinct,
colloquial, and noisy, resembling the input of a search engine. In this work,
we introduce data augmentation techniques and a sampling-based content-aware
BERT model (ColloQL) to achieve robust text-to-SQL modeling over natural
language search (NLS) questions. Due to the lack of evaluation data, we curate
a new dataset of NLS questions and demonstrate the efficacy of our approach.
ColloQL's superior performance extends to well-formed text, achieving 84.9%
(logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to
the best of our knowledge, the highest performing model that does not use
execution guided decoding.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 23:53:17 GMT'}]",2020-10-21,"[['Radhakrishnan', 'Karthik', ''], ['Srikantan', 'Arvind', ''], ['Lin', 'Xi Victoria', '']]"
1366265,2010.09935,Faeze Brahman,"Faeze Brahman, Alexandru Petrusca, and Snigdha Chaturvedi",Cue Me In: Content-Inducing Approaches to Interactive Story Generation,AACL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatically generating stories is a challenging problem that requires
producing causally related and logical sequences of events about a topic.
Previous approaches in this domain have focused largely on one-shot generation,
where a language model outputs a complete story based on limited initial input
from a user. Here, we instead focus on the task of interactive story
generation, where the user provides the model mid-level sentence abstractions
in the form of cue phrases during the generation process. This provides an
interface for human users to guide the story generation. We present two
content-inducing approaches to effectively incorporate this additional
information. Experimental results from both automatic and human evaluations
show that these methods produce more topically coherent and personalized
stories compared to baseline methods.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 00:36:15 GMT'}]",2020-10-21,"[['Brahman', 'Faeze', ''], ['Petrusca', 'Alexandru', ''], ['Chaturvedi', 'Snigdha', '']]"
1366284,2010.09954,Runzhe Yang,"Runzhe Yang, Jingxiao Chen, Karthik Narasimhan",Generating Strategic Dialogue for Negotiation with Theory of Mind,"12 pages, 3 figures",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a framework to integrate the concept of Theory of Mind (ToM) into
generating utterances for task-oriented dialogue. Our approach explores the
ability to model and infer personality types of opponents, predicts their
responses, and uses this information to adapt the agent's high-level strategy
in negotiation tasks. We introduce a probabilistic formulation for the
first-order theory of mind and test our approach on the CraigslistBargain
dataset. Experiments show that our method using ToM inference achieves a 40\%
higher dialogue agreement rate compared to baselines on a mixed population of
opponents. We also show that our model displays diverse negotiation behavior
with different types of opponents.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 01:46:03 GMT'}]",2020-10-21,"[['Yang', 'Runzhe', ''], ['Chen', 'Jingxiao', ''], ['Narasimhan', 'Karthik', '']]"
1366368,2010.10038,Sameer Dharur,"Sameer Dharur, Purva Tendulkar, Dhruv Batra, Devi Parikh, Ramprasaath
  R. Selvaraju","SOrT-ing VQA Models : Contrastive Gradient Learning for Improved
  Consistency",,,,,cs.CV cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent research in Visual Question Answering (VQA) has revealed
state-of-the-art models to be inconsistent in their understanding of the world
-- they answer seemingly difficult questions requiring reasoning correctly but
get simpler associated sub-questions wrong. These sub-questions pertain to
lower level visual concepts in the image that models ideally should understand
to be able to answer the higher level question correctly. To address this, we
first present a gradient-based interpretability approach to determine the
questions most strongly correlated with the reasoning question on an image, and
use this to evaluate VQA models on their ability to identify the relevant
sub-questions needed to answer a reasoning question. Next, we propose a
contrastive gradient learning based approach called Sub-question Oriented
Tuning (SOrT) which encourages models to rank relevant sub-questions higher
than irrelevant questions for an <$image, reasoning-question$> pair. We show
that SOrT improves model consistency by upto 6.5% points over existing
baselines, while also improving visual grounding.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 05:15:48 GMT'}]",2020-10-21,"[['Dharur', 'Sameer', ''], ['Tendulkar', 'Purva', ''], ['Batra', 'Dhruv', ''], ['Parikh', 'Devi', ''], ['Selvaraju', 'Ramprasaath R.', '']]"
1366831,2010.10501,William Gantt,"William Gantt, Benjamin Kane, Aaron Steven White",Natural Language Inference with Mixed Effects,,"The Ninth Joint Conference on Lexical and Computational Semantics
  (*SEM2020)",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is growing evidence that the prevalence of disagreement in the raw
annotations used to construct natural language inference datasets makes the
common practice of aggregating those annotations to a single label problematic.
We propose a generic method that allows one to skip the aggregation step and
train on the raw annotations directly without subjecting the model to unwanted
noise that can arise from annotator response biases. We demonstrate that this
method, which generalizes the notion of a \textit{mixed effects model} by
incorporating \textit{annotator random effects} into any existing neural model,
improves performance over models that do not incorporate such effects.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 17:54:16 GMT'}]",2020-10-21,"[['Gantt', 'William', ''], ['Kane', 'Benjamin', ''], ['White', 'Aaron Steven', '']]"
1278888,2004.14118,Mario Giulianelli,"Mario Giulianelli, Marco Del Tredici, Raquel Fern\'andez","Analysing Lexical Semantic Change with Contextualised Word
  Representations","To appear in Proceedings of the 58th Annual Meeting of the
  Association for Computational Linguistics (ACL-2020)",,10.18653/v1/2020.acl-main.365,,cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents the first unsupervised approach to lexical semantic
change that makes use of contextualised word representations. We propose a
novel method that exploits the BERT neural language model to obtain
representations of word usages, clusters these representations into usage
types, and measures change along time with three proposed metrics. We create a
new evaluation dataset and show that the model representations and the detected
semantic shifts are positively correlated with human judgements. Our extensive
qualitative analysis demonstrates that our method captures a variety of
synchronic and diachronic linguistic phenomena. We expect our work to inspire
further research in this direction.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 12:18:14 GMT'}]",2020-10-21,"[['Giulianelli', 'Mario', ''], ['Del Tredici', 'Marco', ''], ['Fernández', 'Raquel', '']]"
1336576,2008.08854,Liesbeth Allein,Liesbeth Allein and Marie-Francine Moens,"Checkworthiness in Automatic Claim Detection Models: Definitions and
  Analysis of Datasets",,,10.1007/978-3-030-61841-4_1,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Public, professional and academic interest in automated fact-checking has
drastically increased over the past decade, with many aiming to automate one of
the first steps in a fact-check procedure: the selection of so-called
checkworthy claims. However, there is little agreement on the definition and
characteristics of checkworthiness among fact-checkers, which is consequently
reflected in the datasets used for training and testing checkworthy claim
detection models. After elaborate analysis of checkworthy claim selection
procedures in fact-check organisations and analysis of state-of-the-art claim
detection datasets, checkworthiness is defined as the concept of having a
spatiotemporal and context-dependent worth and need to have the correctness of
the objectivity it conveys verified. This is irrespective of the claim's
perceived veracity judgement by an individual based on prior knowledge and
beliefs. Concerning the characteristics of current datasets, it is argued that
the data is not only highly imbalanced and noisy, but also too limited in scope
and language. Furthermore, we believe that the subjective concept of
checkworthiness might not be a suitable filter for claim detection.
","[{'version': 'v1', 'created': 'Thu, 20 Aug 2020 09:30:05 GMT'}]",2020-10-21,"[['Allein', 'Liesbeth', ''], ['Moens', 'Marie-Francine', '']]"
1268840,2004.04070,Ivan Vuli\'c,"Ivan Vuli\'c, Sebastian Ruder, and Anders S{\o}gaard",Are All Good Word Vector Spaces Isomorphic?,EMNLP 2020: Long paper. Equal contribution from all three authors,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Existing algorithms for aligning cross-lingual word vector spaces assume that
vector spaces are approximately isomorphic. As a result, they perform poorly or
fail completely on non-isomorphic spaces. Such non-isomorphism has been
hypothesised to result from typological differences between languages. In this
work, we ask whether non-isomorphism is also crucially a sign of degenerate
word vector spaces. We present a series of experiments across diverse languages
which show that variance in performance across language pairs is not only due
to typological differences, but can mostly be attributed to the size of the
monolingual resources available, and to the properties and duration of
monolingual training (e.g. ""under-training"").
","[{'version': 'v1', 'created': 'Wed, 8 Apr 2020 15:49:19 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 17:22:02 GMT'}]",2020-10-21,"[['Vulić', 'Ivan', ''], ['Ruder', 'Sebastian', ''], ['Søgaard', 'Anders', '']]"
1270833,2004.06063,Markus Freitag,"Markus Freitag, David Grangier, Isaac Caswell",BLEU might be Guilty but References are not Innocent,Accepted at EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The quality of automatic metrics for machine translation has been
increasingly called into question, especially for high-quality systems. This
paper demonstrates that, while choice of metric is important, the nature of the
references is also critical. We study different methods to collect references
and compare their value in automated evaluation by reporting correlation with
human evaluation for a variety of systems and metrics. Motivated by the finding
that typical references exhibit poor diversity, concentrating around
translationese language, we develop a paraphrasing task for linguists to
perform on existing reference translations, which counteracts this bias. Our
method yields higher correlation with human judgment not only for the
submissions of WMT 2019 English to German, but also for Back-translation and
APE augmented MT output, which have been shown to have low correlation with
automatic metrics using standard references. We demonstrate that our
methodology improves correlation with all modern evaluation metrics we look at,
including embedding-based methods. To complete this picture, we reveal that
multi-reference BLEU does not improve the correlation for high quality output,
and present an alternative multi-reference formulation that is more effective.
","[{'version': 'v1', 'created': 'Mon, 13 Apr 2020 16:49:09 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 13:02:12 GMT'}]",2020-10-21,"[['Freitag', 'Markus', ''], ['Grangier', 'David', ''], ['Caswell', 'Isaac', '']]"
1035942,1810.04755,Maria Leonor Pacheco,"Samuel Jero, Maria Leonor Pacheco, Dan Goldwasser and Cristina
  Nita-Rotaru","Leveraging Textual Specifications for Grammar-based Fuzzing of Network
  Protocols",,,10.1609/aaai.v33i01.33019478,,cs.CR cs.CL cs.NI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Grammar-based fuzzing is a technique used to find software vulnerabilities by
injecting well-formed inputs generated following rules that encode application
semantics. Most grammar-based fuzzers for network protocols rely on human
experts to manually specify these rules. In this work we study automated
learning of protocol rules from textual specifications (i.e. RFCs). We evaluate
the automatically extracted protocol rules by applying them to a
state-of-the-art fuzzer for transport protocols and show that it leads to a
smaller number of test cases while finding the same attacks as the system that
uses manually specified rules.
","[{'version': 'v1', 'created': 'Wed, 10 Oct 2018 21:42:29 GMT'}]",2020-10-21,"[['Jero', 'Samuel', ''], ['Pacheco', 'Maria Leonor', ''], ['Goldwasser', 'Dan', ''], ['Nita-Rotaru', 'Cristina', '']]"
1329099,2008.01377,Stefan Heid,"Stefan Heid, Marcel Wever, Eyke H\""ullermeier","Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued
  Prediction","14 pages, 8 figures",,,,cs.CL cs.IR cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a
key requirement for both linguistic research and subsequent automated natural
language processing (NLP) tasks. This problem is commonly tackled using machine
learning methods, i.e., by training a POS tagger on a sufficiently large corpus
of labeled data. While the problem of POS tagging can essentially be considered
as solved for modern languages, historical corpora turn out to be much more
difficult, especially due to the lack of native speakers and sparsity of
training data. Moreover, most texts have no sentences as we know them today,
nor a common orthography. These irregularities render the task of automated POS
tagging more difficult and error-prone. Under these circumstances, instead of
forcing the POS tagger to predict and commit to a single tag, it should be
enabled to express its uncertainty. In this paper, we consider POS tagging
within the framework of set-valued prediction, which allows the POS tagger to
express its uncertainty via predicting a set of candidate POS tags instead of
guessing a single one. The goal is to guarantee a high confidence that the
correct POS tag is included while keeping the number of candidates small. In
our experimental study, we find that extending state-of-the-art POS taggers to
set-valued prediction yields more precise and robust taggings, especially for
unknown words, i.e., words not occurring in the training data.
","[{'version': 'v1', 'created': 'Tue, 4 Aug 2020 07:21:36 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 12:59:57 GMT'}]",2020-10-21,"[['Heid', 'Stefan', ''], ['Wever', 'Marcel', ''], ['Hüllermeier', 'Eyke', '']]"
1232676,2001.07263,"Zolt\'an T\""uske","Zolt\'an T\""uske, George Saon, Kartik Audhkhasi, Brian Kingsbury","Single headed attention based sequence-to-sequence model for
  state-of-the-art results on Switchboard","5 pages, 2 figures",,,,eess.AS cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is generally believed that direct sequence-to-sequence (seq2seq) speech
recognition models are competitive with hybrid models only when a large amount
of data, at least a thousand hours, is available for training. In this paper,
we show that state-of-the-art recognition performance can be achieved on the
Switchboard-300 database using a single headed attention, LSTM based model.
Using a cross-utterance language model, our single-pass speaker independent
system reaches 6.4% and 12.5% word error rate (WER) on the Switchboard and
CallHome subsets of Hub5'00, without a pronunciation lexicon. While careful
regularization and data augmentation are crucial in achieving this level of
performance, experiments on Switchboard-2000 show that nothing is more useful
than more data. Overall, the combination of various regularizations and a
simple but fairly large model results in a new state of the art, 4.7% and 7.8%
WER on the Switchboard and CallHome sets, using SWB-2000 without any external
data resources.
","[{'version': 'v1', 'created': 'Mon, 20 Jan 2020 22:03:42 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Oct 2020 01:57:28 GMT'}, {'version': 'v3', 'created': 'Tue, 20 Oct 2020 03:33:19 GMT'}]",2020-10-21,"[['Tüske', 'Zoltán', ''], ['Saon', 'George', ''], ['Audhkhasi', 'Kartik', ''], ['Kingsbury', 'Brian', '']]"
1302901,2006.08387,William Havard,"William N. Havard, Jean-Pierre Chevrot, Laurent Besacier","Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually
  Grounded Speech",Accepted at CoNLL20,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The language acquisition literature shows that children do not build their
lexicon by segmenting the spoken input into phonemes and then building up words
from them, but rather adopt a top-down approach and start by segmenting
word-like units and then break them down into smaller units. This suggests that
the ideal way of learning a language is by starting from full semantic units.
In this paper, we investigate if this is also the case for a neural model of
Visually Grounded Speech trained on a speech-image retrieval task. We evaluated
how well such a network is able to learn a reliable speech-to-image mapping
when provided with phone, syllable, or word boundary information. We present a
simple way to introduce such information into an RNN-based model and
investigate which type of boundary is the most efficient. We also explore at
which level of the network's architecture such information should be introduced
so as to maximise its performances. Finally, we show that using multiple
boundary types at once in a hierarchical structure, by which low-level segments
are used to recompose high-level segments, is beneficial and yields better
results than using low-level or high-level segments in isolation.
","[{'version': 'v1', 'created': 'Mon, 15 Jun 2020 13:20:13 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 13:15:59 GMT'}]",2020-10-21,"[['Havard', 'William N.', ''], ['Chevrot', 'Jean-Pierre', ''], ['Besacier', 'Laurent', '']]"
1367147,2010.10817,Chengzhi Zhang,"Yuzhuo Wang, Chengzhi Zhang","Using the Full-text Content of Academic Articles to Identify and
  Evaluate Algorithm Entities in the Domain of Natural Language Processing",,"Journal of Informetrics,2020",10.1016/j.joi.2020.101091,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the era of big data, the advancement, improvement, and application of
algorithms in academic research have played an important role in promoting the
development of different disciplines. Academic papers in various disciplines,
especially computer science, contain a large number of algorithms. Identifying
the algorithms from the full-text content of papers can determine popular or
classical algorithms in a specific field and help scholars gain a comprehensive
understanding of the algorithms and even the field. To this end, this article
takes the field of natural language processing (NLP) as an example and
identifies algorithms from academic papers in the field. A dictionary of
algorithms is constructed by manually annotating the contents of papers, and
sentences containing algorithms in the dictionary are extracted through
dictionary-based matching. The number of articles mentioning an algorithm is
used as an indicator to analyze the influence of that algorithm. Our results
reveal the algorithm with the highest influence in NLP papers and show that
classification algorithms represent the largest proportion among the
high-impact algorithms. In addition, the evolution of the influence of
algorithms reflects the changes in research tasks and topics in the field, and
the changes in the influence of different algorithms show different trends. As
a preliminary exploration, this paper conducts an analysis of the impact of
algorithms mentioned in the academic text, and the results can be used as
training data for the automatic extraction of large-scale algorithms in the
future. The methodology in this paper is domain-independent and can be applied
to other domains.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 08:24:18 GMT'}]",2020-10-22,"[['Wang', 'Yuzhuo', ''], ['Zhang', 'Chengzhi', '']]"
1367150,2010.10820,Chan Young Park,"Chan Young Park, Xinru Yan, Anjalie Field, Yulia Tsvetkov","Multilingual Contextual Affective Analysis of LGBT People Portrayals in
  Wikipedia",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Specific lexical choices in how people are portrayed both reflect the
writer's attitudes towards people in the narrative and influence the audience's
reactions. Prior work has examined descriptions of people in English using
contextual affective analysis, a natural language processing (NLP) technique
that seeks to analyze how people are portrayed along dimensions of power,
agency, and sentiment. Our work presents an extension of this methodology to
multilingual settings, which is enabled by a new corpus that we collect and a
new multilingual model. We additionally show how word connotations differ
across languages and cultures, which makes existing English datasets and
methods difficult to generalize. We then demonstrate the usefulness of our
method by analyzing Wikipedia biography pages of members of the LGBT community
across three languages: English, Russian, and Spanish. Our results show
systematic differences in how the LGBT community is portrayed across languages,
surfacing cultural differences in narratives and signs of social biases.
Practically, this model can be used to surface Wikipedia articles for further
manual analysis---articles that might contain content gaps or an imbalanced
representation of particular social groups.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 08:27:36 GMT'}]",2020-10-22,"[['Park', 'Chan Young', ''], ['Yan', 'Xinru', ''], ['Field', 'Anjalie', ''], ['Tsvetkov', 'Yulia', '']]"
1367204,2010.10874,Erik Ekstedt,Erik Ekstedt and Gabriel Skantze,"TurnGPT: a Transformer-based Language Model for Predicting Turn-taking
  in Spoken Dialog",Accepted to Findings of ACL: EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Syntactic and pragmatic completeness is known to be important for turn-taking
prediction, but so far machine learning models of turn-taking have used such
linguistic information in a limited way. In this paper, we introduce TurnGPT, a
transformer-based language model for predicting turn-shifts in spoken dialog.
The model has been trained and evaluated on a variety of written and spoken
dialog datasets. We show that the model outperforms two baselines used in prior
work. We also report on an ablation study, as well as attention and gradient
analyses, which show that the model is able to utilize the dialog context and
pragmatic completeness for turn-taking prediction. Finally, we explore the
model's potential in not only detecting, but also projecting, turn-completions.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 09:58:39 GMT'}]",2020-10-22,"[['Ekstedt', 'Erik', ''], ['Skantze', 'Gabriel', '']]"
1367163,2010.10833,Xinyu Zuo,"Xinyu Zuo, Yubo Chen, Kang Liu, Jun Zhao","KnowDis: Knowledge Enhanced Data Augmentation for Event Causality
  Detection via Distant Supervision",Accepted to COLING2020,COLING2020,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern models of event causality detection (ECD) are mainly based on
supervised learning from small hand-labeled corpora. However, hand-labeled
training data is expensive to produce, low coverage of causal expressions and
limited in size, which makes supervised methods hard to detect causal relations
between events. To solve this data lacking problem, we investigate a data
augmentation framework for ECD, dubbed as Knowledge Enhanced Distant Data
Augmentation (KnowDis). Experimental results on two benchmark datasets
EventStoryLine corpus and Causal-TimeBank show that 1) KnowDis can augment
available training data assisted with the lexical and causal commonsense
knowledge for ECD via distant supervision, and 2) our method outperforms
previous methods by a large margin assisted with automatically labeled training
data.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 08:44:54 GMT'}]",2020-10-22,"[['Zuo', 'Xinyu', ''], ['Chen', 'Yubo', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]"
1367141,2010.10811,Puhai Yang,"Puhai Yang, Heyan Huang, Xianling Mao","STN4DST: A Scalable Dialogue State Tracking based on Slot Tagging
  Navigation",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Scalability for handling unknown slot values is a important problem in
dialogue state tracking (DST). As far as we know, previous scalable DST
approaches generally rely on either the candidate generation from slot tagging
output or the span extraction in dialogue context. However, the candidate
generation based DST often suffers from error propagation due to its pipelined
two-stage process; meanwhile span extraction based DST has the risk of
generating invalid spans in the lack of semantic constraints between start and
end position pointers. To tackle the above drawbacks, in this paper, we propose
a novel scalable dialogue state tracking method based on slot tagging
navigation, which implements an end-to-end single-step pointer to locate and
extract slot value quickly and accurately by the joint learning of slot tagging
and slot value position prediction in the dialogue context, especially for
unknown slot values. Extensive experiments over several benchmark datasets show
that the proposed model performs better than state-of-the-art baselines
greatly.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 08:09:20 GMT'}]",2020-10-22,"[['Yang', 'Puhai', ''], ['Huang', 'Heyan', ''], ['Mao', 'Xianling', '']]"
1367119,2010.10789,Weizhen Qi,"Weizhen Qi, Yeyun Gong, Yu Yan, Jian Jiao, Bo Shao, Ruofei Zhang,
  Houqiang Li, Nan Duan, Ming Zhou","ProphetNet-Ads: A Looking Ahead Strategy for Generative Retrieval Models
  in Sponsored Search Engine",Accepted to NLPCC 2020,,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In a sponsored search engine, generative retrieval models are recently
proposed to mine relevant advertisement keywords for users' input queries.
Generative retrieval models generate outputs token by token on a path of the
target library prefix tree (Trie), which guarantees all of the generated
outputs are legal and covered by the target library. In actual use, we found
several typical problems caused by Trie-constrained searching length. In this
paper, we analyze these problems and propose a looking ahead strategy for
generative retrieval models named ProphetNet-Ads. ProphetNet-Ads improves the
retrieval ability by directly optimizing the Trie-constrained searching space.
We build a dataset from a real-word sponsored search engine and carry out
experiments to analyze different generative retrieval models. Compared with
Trie-based LSTM generative retrieval model proposed recently, our single model
result and integrated result improve the recall by 15.58\% and 18.8\%
respectively with beam size 5. Case studies further demonstrate how these
problems are alleviated by ProphetNet-Ads clearly.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 07:03:20 GMT'}]",2020-10-22,"[['Qi', 'Weizhen', ''], ['Gong', 'Yeyun', ''], ['Yan', 'Yu', ''], ['Jiao', 'Jian', ''], ['Shao', 'Bo', ''], ['Zhang', 'Ruofei', ''], ['Li', 'Houqiang', ''], ['Duan', 'Nan', ''], ['Zhou', 'Ming', '']]"
1367166,2010.10836,Deepak P,"Soumya Suvra Ghosal, Deepak P, Anna Jurek-Loughrey",ReSCo-CC: Unsupervised Identification of Key Disinformation Sentences,"The 22nd International Conference on Information Integration and
  Web-based Applications & Services (iiWAS '20), Chiang Mai, Thailand",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Disinformation is often presented in long textual articles, especially when
it relates to domains such as health, often seen in relation to COVID-19. These
articles are typically observed to have a number of trustworthy sentences among
which core disinformation sentences are scattered. In this paper, we propose a
novel unsupervised task of identifying sentences containing key disinformation
within a document that is known to be untrustworthy. We design a three-phase
statistical NLP solution for the task which starts with embedding sentences
within a bespoke feature space designed for the task. Sentences represented
using those features are then clustered, following which the key sentences are
identified through proximity scoring. We also curate a new dataset with
sentence level disinformation scorings to aid evaluation for this task; the
dataset is being made publicly available to facilitate further research. Based
on a comprehensive empirical evaluation against techniques from related tasks
such as claim detection and summarization, as well as against simplified
variants of our proposed approach, we illustrate that our method is able to
identify core disinformation effectively.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 08:53:36 GMT'}]",2020-10-22,"[['Ghosal', 'Soumya Suvra', ''], ['P', 'Deepak', ''], ['Jurek-Loughrey', 'Anna', '']]"
1367087,2010.10757,Srinivasan Iyer,"Srinivasan Iyer, Sewon Min, Yashar Mehdad, Wen-tau Yih","RECONSIDER: Re-Ranking using Span-Focused Cross-Attention for Open
  Domain Question Answering",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain
Question Answering (QA) are typically trained for span selection using
distantly supervised positive examples and heuristically retrieved negative
examples. This training scheme possibly explains empirical observations that
these models achieve a high recall amongst their top few predictions, but a low
overall accuracy, motivating the need for answer re-ranking. We develop a
simple and effective re-ranking approach (RECONSIDER) for span-extraction
tasks, that improves upon the performance of large pre-trained MRC models.
RECONSIDER is trained on positive and negative examples extracted from high
confidence predictions of MRC models, and uses in-passage span annotations to
perform span-focused re-ranking over a smaller candidate set. As a result,
RECONSIDER learns to eliminate close false positive passages, and achieves a
new state of the art on four QA tasks, including 45.5% Exact Match accuracy on
Natural Questions with real user questions, and 61.7% on TriviaQA.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 04:28:42 GMT'}]",2020-10-22,"[['Iyer', 'Srinivasan', ''], ['Min', 'Sewon', ''], ['Mehdad', 'Yashar', ''], ['Yih', 'Wen-tau', '']]"
1367169,2010.10839,Wubo Li,"Wubo Li, Dongwei Jiang, Wei Zou, Xiangang Li","TMT: A Transformer-based Modal Translator for Improving Multimodal
  Sequence Representations in Audio Visual Scene-aware Dialog",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Audio Visual Scene-aware Dialog (AVSD) is a task to generate responses when
discussing about a given video. The previous state-of-the-art model shows
superior performance for this task using Transformer-based architecture.
However, there remain some limitations in learning better representation of
modalities. Inspired by Neural Machine Translation (NMT), we propose the
Transformer-based Modal Translator (TMT) to learn the representations of the
source modal sequence by translating the source modal sequence to the related
target modal sequence in a supervised manner. Based on Multimodal Transformer
Networks (MTN), we apply TMT to video and dialog, proposing MTN-TMT for the
video-grounded dialog system. On the AVSD track of the Dialog System Technology
Challenge 7, MTN-TMT outperforms the MTN and other submission models in both
Video and Text task and Text Only task. Compared with MTN, MTN-TMT improves all
metrics, especially, achieving relative improvement up to 14.1% on CIDEr. Index
Terms: multimodal learning, audio-visual scene-aware dialog, neural machine
translation, multi-task learning
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 09:02:30 GMT'}]",2020-10-22,"[['Li', 'Wubo', ''], ['Jiang', 'Dongwei', ''], ['Zou', 'Wei', ''], ['Li', 'Xiangang', '']]"
1367203,2010.10873,Milad Moradi,"Milad Moradi, Matthias Samwald","Explaining black-box text classifiers for disease-treatment information
  extraction",,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks and other intricate Artificial Intelligence (AI) models
have reached high levels of accuracy on many biomedical natural language
processing tasks. However, their applicability in real-world use cases may be
limited due to their vague inner working and decision logic. A post-hoc
explanation method can approximate the behavior of a black-box AI model by
extracting relationships between feature values and outcomes. In this paper, we
introduce a post-hoc explanation method that utilizes confident itemsets to
approximate the behavior of black-box classifiers for medical information
extraction. Incorporating medical concepts and semantics into the explanation
process, our explanator finds semantic relations between inputs and outputs in
different parts of the decision space of a black-box classifier. The
experimental results show that our explanation method can outperform
perturbation and decision set based explanators in terms of fidelity and
interpretability of explanations produced for predictions on a
disease-treatment information extraction task.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 09:58:00 GMT'}]",2020-10-22,"[['Moradi', 'Milad', ''], ['Samwald', 'Matthias', '']]"
1367143,2010.10813,Jinman Zhao,"Zhao Jinman, Shawn Zhong, Xiaomin Zhang, Yingyu Liang",PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding,"16 pages including 4 pages of appendix. Accepted to Findings of EMNLP
  2020 and SustaiNLP 2020. Code can be found at
  [https://github.com/jmzhao/pbos]",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We look into the task of \emph{generalizing} word embeddings: given a set of
pre-trained word vectors over a finite vocabulary, the goal is to predict
embedding vectors for out-of-vocabulary words, \emph{without} extra contextual
information. We rely solely on the spellings of words and propose a model,
along with an efficient algorithm, that simultaneously models subword
segmentation and computes subword-based compositional word embedding. We call
the model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords
for all possible segmentations based on their likelihood. Inspections and affix
prediction experiment show that PBoS is able to produce meaningful subword
segmentations and subword rankings without any source of explicit morphological
knowledge. Word similarity and POS tagging experiments show clear advantages of
PBoS over previous subword-level models in the quality of generated word
embeddings across languages.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 08:11:08 GMT'}]",2020-10-22,"[['Jinman', 'Zhao', ''], ['Zhong', 'Shawn', ''], ['Zhang', 'Xiaomin', ''], ['Liang', 'Yingyu', '']]"
1367131,2010.10801,Arthur Jacobs M,Arthur M. Jacobs and Annette Kinder,"Quasi Error-free Text Classification and Authorship Recognition in a
  large Corpus of English Literature based on a Novel Feature Set","18 pages, 3 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Gutenberg Literary English Corpus (GLEC) provides a rich source of
textual data for research in digital humanities, computational linguistics or
neurocognitive poetics. However, so far only a small subcorpus, the Gutenberg
English Poetry Corpus, has been submitted to quantitative text analyses
providing predictions for scientific studies of literature. Here we show that
in the entire GLEC quasi error-free text classification and authorship
recognition is possible with a method using the same set of five style and five
content features, computed via style and sentiment analysis, in both tasks. Our
results identify two standard and two novel features (i.e., type-token ratio,
frequency, sonority score, surprise) as most diagnostic in these tasks. By
providing a simple tool applicable to both short poems and long novels
generating quantitative predictions about features that co-determe the
cognitive and affective processing of specific text categories or authors, our
data pave the way for many future computational and empirical studies of
literature or experiments in reading psychology.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 07:39:55 GMT'}]",2020-10-22,"[['Jacobs', 'Arthur M.', ''], ['Kinder', 'Annette', '']]"
1367237,2010.10907,Elena Voita,"Elena Voita, Rico Sennrich, Ivan Titov","Analyzing the Source and Target Contributions to Predictions in Neural
  Machine Translation",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In Neural Machine Translation (and, more generally, conditional language
modeling), the generation of a target token is influenced by two types of
context: the source and the prefix of the target sequence. While many attempts
to understand the internal workings of NMT models have been made, none of them
explicitly evaluates relative source and target contributions to a generation
decision. We argue that this relative contribution can be evaluated by adopting
a variant of Layerwise Relevance Propagation (LRP). Its underlying
'conservation principle' makes relevance propagation unique: differently from
other methods, it evaluates not an abstract quantity reflecting token
importance, but the proportion of each token's influence. We extend LRP to the
Transformer and conduct an analysis of NMT models which explicitly evaluates
the source and target relative contributions to the generation process. We
analyze changes in these contributions when conditioning on different types of
prefixes, when varying the training objective or the amount of training data,
and during the training process. We find that models trained with more data
tend to rely on source information more and to have more sharp token
contributions; the training process is non-monotonic with several stages of
different nature.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 11:37:27 GMT'}]",2020-10-22,"[['Voita', 'Elena', ''], ['Sennrich', 'Rico', ''], ['Titov', 'Ivan', '']]"
1367230,2010.10900,Tommaso Soru,Anand Panchbhai and Tommaso Soru and Edgard Marx,Exploring Sequence-to-Sequence Models for SPARQL Pattern Composition,"Proceedings of the First Indo-American Knowledge Graph and Semantic
  Web Conference (KGSWC-India 2020)",,,,cs.CL cs.AI cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A booming amount of information is continuously added to the Internet as
structured and unstructured data, feeding knowledge bases such as DBpedia and
Wikidata with billions of statements describing millions of entities. The aim
of Question Answering systems is to allow lay users to access such data using
natural language without needing to write formal queries. However, users often
submit questions that are complex and require a certain level of abstraction
and reasoning to decompose them into basic graph patterns. In this short paper,
we explore the use of architectures based on Neural Machine Translation called
Neural SPARQL Machines to learn pattern compositions. We show that
sequence-to-sequence models are a viable and promising option to transform long
utterances into complex SPARQL queries.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 11:12:01 GMT'}]",2020-10-22,"[['Panchbhai', 'Anand', ''], ['Soru', 'Tommaso', ''], ['Marx', 'Edgard', '']]"
1367085,2010.10755,Bill Yuchen Lin,"Bill Yuchen Lin, Ying Sheng, Nguyen Vo, Sandeep Tata","FreeDOM: A Transferable Neural Architecture for Structured Information
  Extraction on Web Documents",in Proc. of KDD 2020 (Research Track). Figure 5 updated,,10.1145/3394486.3403153,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Extracting structured data from HTML documents is a long-studied problem with
a broad range of applications like augmenting knowledge bases, supporting
faceted search, and providing domain-specific experiences for key verticals
like shopping and movies. Previous approaches have either required a small
number of examples for each target site or relied on carefully handcrafted
heuristics built over visual renderings of websites. In this paper, we present
a novel two-stage neural approach, named FreeDOM, which overcomes both these
limitations. The first stage learns a representation for each DOM node in the
page by combining both the text and markup information. The second stage
captures longer range distance and semantic relatedness using a relational
neural network. By combining these stages, FreeDOM is able to generalize to
unseen sites after training on a small number of seed sites from that vertical
without requiring expensive hand-crafted features over visual renderings of the
page. Through experiments on a public dataset with 8 different verticals, we
show that FreeDOM beats the previous state of the art by nearly 3.7 F1 points
on average without requiring features over rendered pages or expensive
hand-crafted features.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 04:20:13 GMT'}]",2020-10-22,"[['Lin', 'Bill Yuchen', ''], ['Sheng', 'Ying', ''], ['Vo', 'Nguyen', ''], ['Tata', 'Sandeep', '']]"
1367240,2010.10910,Mali Jin,Mali Jin and Nikolaos Aletras,Complaint Identification in Social Media with Transformer Networks,Accepted at COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Complaining is a speech act extensively used by humans to communicate a
negative inconsistency between reality and expectations. Previous work on
automatically identifying complaints in social media has focused on using
feature-based and task-specific neural network models. Adapting
state-of-the-art pre-trained neural language models and their combinations with
other linguistic information from topics or sentiment for complaint prediction
has yet to be explored. In this paper, we evaluate a battery of neural models
underpinned by transformer networks which we subsequently combine with
linguistic information. Experiments on a publicly available data set of
complaints demonstrate that our models outperform previous state-of-the-art
methods by a large margin achieving a macro F1 up to 87.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 11:44:04 GMT'}]",2020-10-22,"[['Jin', 'Mali', ''], ['Aletras', 'Nikolaos', '']]"
1367251,2010.10921,Aibek Makazhanov,"Aibek Makazhanov, Sharon Goldwater, Adam Lopez","LemMED: Fast and Effective Neural Morphological Analysis with Short
  Context Windows",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present LemMED, a character-level encoder-decoder for contextual
morphological analysis (combined lemmatization and tagging). LemMED extends and
is named after two other attention-based models, namely Lematus, a contextual
lemmatizer, and MED, a morphological (re)inflection model. Our approach does
not require training separate lemmatization and tagging models, nor does it
need additional resources and tools, such as morphological dictionaries or
transducers. Moreover, LemMED relies solely on character-level representations
and on local context. Although the model can, in principle, account for global
context on sentence level, our experiments show that using just a single word
of context around each target word is not only more computationally feasible,
but yields better results as well. We evaluate LemMED in the framework of the
SIMGMORPHON-2019 shared task on combined lemmatization and tagging. In terms of
average performance LemMED ranks 5th among 13 systems and is bested only by the
submissions that use contextualized embeddings.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 12:08:02 GMT'}]",2020-10-22,"[['Makazhanov', 'Aibek', ''], ['Goldwater', 'Sharon', ''], ['Lopez', 'Adam', '']]"
1367262,2010.10932,Sungchul Choi,"Jaewoong Choi, Sion Jang, Jaeyoung Kim, Jiho Lee, Janghyeok Yoona,
  Sungchul Choi",Deep learning-based citation recommendation system for patents,,,,,cs.IR cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In this study, we address the challenges in developing a deep learning-based
automatic patent citation recommendation system. Although deep learning-based
recommendation systems have exhibited outstanding performance in various
domains (such as movies, products, and paper citations), their validity in
patent citations has not been investigated, owing to the lack of a freely
available high-quality dataset and relevant benchmark model. To solve these
problems, we present a novel dataset called PatentNet that includes textual
information and metadata for approximately 110,000 patents from the Google Big
Query service. Further, we propose strong benchmark models considering the
similarity of textual information and metadata (such as cooperative patent
classification code). Compared with existing recommendation methods, the
proposed benchmark method achieved a mean reciprocal rank of 0.2377 on the test
set, whereas the existing state-of-the-art recommendation method achieved
0.2073.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 12:18:21 GMT'}]",2020-10-22,"[['Choi', 'Jaewoong', ''], ['Jang', 'Sion', ''], ['Kim', 'Jaeyoung', ''], ['Lee', 'Jiho', ''], ['Yoona', 'Janghyeok', ''], ['Choi', 'Sungchul', '']]"
1367268,2010.10938,Chi-Liang Liu,Chi-Liang Liu and Tsung-Yuan Hsu and Yung-Sung Chuang and Hung-yi Lee,What makes multilingual BERT multilingual?,arXiv admin note: substantial text overlap with arXiv:2004.09205,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, multilingual BERT works remarkably well on cross-lingual transfer
tasks, superior to static non-contextualized word embeddings. In this work, we
provide an in-depth experimental study to supplement the existing literature of
cross-lingual ability. We compare the cross-lingual ability of
non-contextualized and contextualized representation model with the same data.
We found that datasize and context window size are crucial factors to the
transferability.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 05:41:56 GMT'}]",2020-10-22,"[['Liu', 'Chi-Liang', ''], ['Hsu', 'Tsung-Yuan', ''], ['Chuang', 'Yung-Sung', ''], ['Lee', 'Hung-yi', '']]"
1367329,2010.10999,Sohee Yang,"Sohee Yang, Minjoon Seo",Is Retriever Merely an Approximator of Reader?,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The state of the art in open-domain question answering (QA) relies on an
efficient retriever that drastically reduces the search space for the expensive
reader. A rather overlooked question in the community is the relationship
between the retriever and the reader, and in particular, if the whole purpose
of the retriever is just a fast approximation for the reader. Our empirical
evidence indicates that the answer is no, and that the reader and the retriever
are complementary to each other even in terms of accuracy only. We make a
careful conjecture that the architectural constraint of the retriever, which
has been originally intended for enabling approximate search, seems to also
make the model more robust in large-scale search. We then propose to distill
the reader into the retriever so that the retriever absorbs the strength of the
reader while keeping its own benefit. Experimental results show that our method
can enhance the document recall rate as well as the end-to-end QA accuracy of
off-the-shelf retrievers in open-domain QA tasks.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 13:40:15 GMT'}]",2020-10-22,"[['Yang', 'Sohee', ''], ['Seo', 'Minjoon', '']]"
1367333,2010.11003,Chi-Liang Liu,Chi-Liang Liu and Hung-yi Lee,"Unsupervised Deep Learning based Multiple Choices Question Answering:
  Start Learning from Basic Knowledge",preprint,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we study the possibility of almost unsupervised Multiple
Choices Question Answering (MCQA). Starting from very basic knowledge, MCQA
model knows that some choices have higher probabilities of being correct than
the others. The information, though very noisy, guides the training of an MCQA
model. The proposed method is shown to outperform the baseline approaches on
RACE and even comparable with some supervised learning approaches on MC500.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 13:44:35 GMT'}]",2020-10-22,"[['Liu', 'Chi-Liang', ''], ['Lee', 'Hung-yi', '']]"
1367334,2010.11004,Mounica Maddela,"Mounica Maddela, Fernando Alva-Manchego, Wei Xu",Controllable Text Simplification with Explicit Paraphrasing,,,,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  Text Simplification improves the readability of sentences through several
rewriting transformations, such as lexical paraphrasing, deletion, and
splitting. Current simplification systems are predominantly
sequence-to-sequence models that are trained end-to-end to perform all these
operations simultaneously. However, such systems limit themselves to mostly
deleting words and cannot easily adapt to the requirements of different target
audiences. In this paper, we propose a novel hybrid approach that leverages
linguistically-motivated rules for splitting and deletion, and couples them
with a neural paraphrasing model to produce varied rewriting styles. We
introduce a new data augmentation method to improve the paraphrasing capability
of our model. Through automatic and manual evaluations, we show that our
proposed model establishes a new state-of-the-art for the task, paraphrasing
more often than the existing systems, and can control the degree of each
simplification operation applied to the input texts.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 13:44:40 GMT'}]",2020-10-22,"[['Maddela', 'Mounica', ''], ['Alva-Manchego', 'Fernando', ''], ['Xu', 'Wei', '']]"
1367348,2010.11018,Huaao Zhang,"Huaao Zhang, Shigui Qiu, Xiangyu Duan, Min Zhang",Token Drop mechanism for Neural Machine Translation,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Neural machine translation with millions of parameters is vulnerable to
unfamiliar inputs. We propose Token Drop to improve generalization and avoid
overfitting for the NMT model. Similar to word dropout, whereas we replace
dropped token with a special token instead of setting zero to words. We further
introduce two self-supervised objectives: Replaced Token Detection and Dropped
Token Prediction. Our method aims to force model generating target translation
with less information, in this way the model can learn textual representation
better. Experiments on Chinese-English and English-Romanian benchmark
demonstrate the effectiveness of our approach and our model achieves
significant improvements over a strong Transformer baseline.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 14:02:27 GMT'}]",2020-10-22,"[['Zhang', 'Huaao', ''], ['Qiu', 'Shigui', ''], ['Duan', 'Xiangyu', ''], ['Zhang', 'Min', '']]"
1367224,2010.10894,Junwei Bao Ph.D.,"Yingyao Wang, Junwei Bao, Guangyi Liu, Youzheng Wu, Xiaodong He, Bowen
  Zhou and Tiejun Zhao","Learning to Decouple Relations: Few-Shot Relation Classification with
  Entity-Guided Attention and Confusion-Aware Training","11 pages, 5 figures, accepted by COLING2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper aims to enhance the few-shot relation classification especially
for sentences that jointly describe multiple relations. Due to the fact that
some relations usually keep high co-occurrence in the same context, previous
few-shot relation classifiers struggle to distinguish them with few annotated
instances. To alleviate the above relation confusion problem, we propose CTEG,
a model equipped with two mechanisms to learn to decouple these easily-confused
relations. On the one hand, an Entity-Guided Attention (EGA) mechanism, which
leverages the syntactic relations and relative positions between each word and
the specified entity pair, is introduced to guide the attention to filter out
information causing confusion. On the other hand, a Confusion-Aware Training
(CAT) method is proposed to explicitly learn to distinguish relations by
playing a pushing-away game between classifying a sentence into a true relation
and its confusing relation. Extensive experiments are conducted on the FewRel
dataset, and the results show that our proposed model achieves comparable and
even much better results to strong baselines in terms of accuracy. Furthermore,
the ablation test and case study verify the effectiveness of our proposed EGA
and CAT, especially in addressing the relation confusion problem.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 11:07:53 GMT'}]",2020-10-22,"[['Wang', 'Yingyao', ''], ['Bao', 'Junwei', ''], ['Liu', 'Guangyi', ''], ['Wu', 'Youzheng', ''], ['He', 'Xiaodong', ''], ['Zhou', 'Bowen', ''], ['Zhao', 'Tiejun', '']]"
1367029,2010.10699,Zhao Xinyan,"Xinyan Zhao, Liangwei Chen, Huanhuan Chen","A Graph Based and Patient Demographics Aware Dialogue System for Disease
  Diagnosis",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A dialogue system for disease diagnosis aims at making a diagnosis by
conversing with patients. Existing disease diagnosis dialogue systems highly
rely on data-driven methods and statistical features, lacking profound
comprehension of medical knowledge, such as symptom-disease relations. In
addition, previous work pays less attention to demographic attributes of a
patient, which are important factors in clinical diagnoses. To tackle these
issues, this work presents a graph based and demographic attributes aware
dialogue system for disease diagnosis. Specifically, we first build a weighted
bidirectional graph based on clinical dialogues to depict the relationship
between symptoms and diseases and then present a bidirectional graph based deep
Q-network (BG-DQN) for dialogue management. By extending Graph Convolutional
Network (GCN) to learn the embeddings of diseases and symptoms from both the
structural and attribute information in the graph, BG-DQN could capture the
relations between diseases and symptoms better. Moreover, BG-DQN also encodes
the demographic attributes of a patient to assist the disease diagnosis
process. Experimental results show that the proposed dialogue system
outperforms several competitive methods in terms of diagnostic accuracy. More
importantly, our method can complete the task with less dialogue turns and
possesses better distinguishing capability on diseases with similar symptoms.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 01:22:37 GMT'}]",2020-10-22,"[['Zhao', 'Xinyan', ''], ['Chen', 'Liangwei', ''], ['Chen', 'Huanhuan', '']]"
1366982,2010.10652,Wei-Fan Chen,"Wei-Fan Chen, Khalid Al-Khatib, Henning Wachsmuth and Benno Stein","Analyzing Political Bias and Unfairness in News Articles at Different
  Levels of Granularity",,NLP+CSS 2020,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Media organizations bear great reponsibility because of their considerable
influence on shaping beliefs and positions of our society. Any form of media
can contain overly biased content, e.g., by reporting on political events in a
selective or incomplete manner. A relevant question hence is whether and how
such form of imbalanced news coverage can be exposed. The research presented in
this paper addresses not only the automatic detection of bias but goes one step
further in that it explores how political bias and unfairness are manifested
linguistically. In this regard we utilize a new corpus of 6964 news articles
with labels derived from adfontesmedia.com and develop a neural model for bias
assessment. By analyzing this model on article excerpts, we find insightful
bias patterns at different levels of text granularity, from single words to the
whole article discourse.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 22:25:00 GMT'}]",2020-10-22,"[['Chen', 'Wei-Fan', ''], ['Al-Khatib', 'Khalid', ''], ['Wachsmuth', 'Henning', ''], ['Stein', 'Benno', '']]"
1367003,2010.10673,Ram\'on Fernandez Astudillo,"Young-Suk Lee, Ramon Fernandez Astudillo, Tahira Naseem, Revanth Gangi
  Reddy, Radu Florian, Salim Roukos",Pushing the Limits of AMR Parsing with Self-Learning,"Accepted to Findings of EMNLP2020, open review
  https://openreview.net/forum?id=4q5-oJgLiO, code
  https://github.com/IBM/transition-amr-parser",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Abstract Meaning Representation (AMR) parsing has experienced a notable
growth in performance in the last two years, due both to the impact of transfer
learning and the development of novel architectures specific to AMR. At the
same time, self-learning techniques have helped push the performance boundaries
of other natural language processing applications, such as machine translation
or question answering. In this paper, we explore different ways in which
trained models can be applied to improve AMR parsing performance, including
generation of synthetic text and AMR annotations as well as refinement of
actions oracle. We show that, without any additional human annotations, these
techniques improve an already performant parser and achieve state-of-the-art
results on AMR 1.0 and AMR 2.0.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 23:45:04 GMT'}]",2020-10-22,"[['Lee', 'Young-Suk', ''], ['Astudillo', 'Ramon Fernandez', ''], ['Naseem', 'Tahira', ''], ['Reddy', 'Revanth Gangi', ''], ['Florian', 'Radu', ''], ['Roukos', 'Salim', '']]"
1340301,2008.12579,Andrea Madotto Mr,"Andrea Madotto, Zhaojiang Lin, Yejin Bang, Pascale Fung",The Adapter-Bot: All-In-One Controllable Conversational Model,"Andrea Madotto and Zhaojiang Lin contributed equally to this work.
  Video demo: https://www.youtube.com/watch?v=Jz8KWE_gKH0&feature=youtu.be",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Considerable progress has been made towards conversational models that
generate coherent and fluent responses by training large language models on
large dialogue datasets. These models have little or no control of the
generated responses and miss two important features: continuous dialogue skills
integration and seamlessly leveraging diverse knowledge sources. In this paper,
we propose the Adapter-Bot, a dialogue model that uses a fixed backbone
conversational model such as DialGPT (Zhang et al., 2019) and triggers
on-demand dialogue skills (e.g., emphatic response, weather information, movie
recommendation) via different adapters (Houlsby et al., 2019). Each adapter can
be trained independently, thus allowing a continual integration of skills
without retraining the entire model. Depending on the skills, the model is able
to process multiple knowledge types, such as text, tables, and graphs, in a
seamless manner. The dialogue skills can be triggered automatically via a
dialogue manager, or manually, thus allowing high-level control of the
generated responses. At the current stage, we have implemented 12 response
styles (e.g., positive, negative etc.), 8 goal-oriented skills (e.g. weather
information, movie recommendation, etc.), and personalized and emphatic
responses. We evaluate our model using automatic evaluation by comparing it
with existing state-of-the-art conversational models, and we have released an
interactive system at adapter.bot.ust.hk.
","[{'version': 'v1', 'created': 'Fri, 28 Aug 2020 10:59:31 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 02:44:30 GMT'}]",2020-10-22,"[['Madotto', 'Andrea', ''], ['Lin', 'Zhaojiang', ''], ['Bang', 'Yejin', ''], ['Fung', 'Pascale', '']]"
1349201,2009.07698,Reuben Tan,"Reuben Tan, Bryan A. Plummer, Kate Saenko",Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News,Accepted at EMNLP 2020,,,,cs.AI cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale dissemination of disinformation online intended to mislead or
deceive the general population is a major societal problem. Rapid progression
in image, video, and natural language generative models has only exacerbated
this situation and intensified our need for an effective defense mechanism.
While existing approaches have been proposed to defend against neural fake
news, they are generally constrained to the very limited setting where articles
only have text and metadata such as the title and authors. In this paper, we
introduce the more realistic and challenging task of defending against
machine-generated news that also includes images and captions. To identify the
possible weaknesses that adversaries can exploit, we create a NeuralNews
dataset composed of 4 different types of generated articles as well as conduct
a series of human user study experiments based on this dataset. In addition to
the valuable insights gleaned from our user study experiments, we provide a
relatively effective approach based on detecting visual-semantic
inconsistencies, which will serve as an effective first line of defense and a
useful reference for future work in defending against machine-generated
disinformation.
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 14:13:15 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Sep 2020 01:17:19 GMT'}, {'version': 'v3', 'created': 'Tue, 22 Sep 2020 21:37:02 GMT'}, {'version': 'v4', 'created': 'Thu, 24 Sep 2020 21:10:15 GMT'}, {'version': 'v5', 'created': 'Wed, 21 Oct 2020 15:16:20 GMT'}]",2020-10-22,"[['Tan', 'Reuben', ''], ['Plummer', 'Bryan A.', ''], ['Saenko', 'Kate', '']]"
1356289,2009.14786,Nicolas Gontier,Nicolas Gontier and Koustuv Sinha and Siva Reddy and Christopher Pal,"Measuring Systematic Generalization in Neural Proof Generation with
  Transformers",NeurIPS 2020; 17 pages; 9 figures; 6 tables,,,,cs.LG cs.AI cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We are interested in understanding how well Transformer language models
(TLMs) can perform reasoning tasks when trained on knowledge encoded in the
form of natural language. We investigate their systematic generalization
abilities on a logical reasoning task in natural language, which involves
reasoning over relationships between entities grounded in first-order logical
proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to
generate natural language proofs. We test the generated proofs for logical
consistency, along with the accuracy of the final inference. We observe
length-generalization issues when evaluated on longer-than-trained sequences.
However, we observe TLMs improve their generalization performance after being
exposed to longer, exhaustive proofs. In addition, we discover that TLMs are
able to generalize better using backward-chaining proofs compared to their
forward-chaining counterparts, while they find it easier to generate forward
chaining proofs. We observe that models that are not trained to generate proofs
are better at generalizing to problems based on longer proofs. This suggests
that Transformers have efficient internal reasoning strategies that are harder
to interpret. These results highlight the systematic generalization behavior of
TLMs in the context of logical reasoning, and we believe this work motivates
deeper inspection of their underlying reasoning strategies.
","[{'version': 'v1', 'created': 'Wed, 30 Sep 2020 16:54:37 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 20:31:11 GMT'}]",2020-10-22,"[['Gontier', 'Nicolas', ''], ['Sinha', 'Koustuv', ''], ['Reddy', 'Siva', ''], ['Pal', 'Christopher', '']]"
1306394,2006.11880,Jianjun Hu,"Changchang Zeng, Shaobo Li, Qin Li, Jie Hu, and Jianjun Hu","A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and
  Benchmark Datasets",68 pages,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine Reading Comprehension (MRC) is a challenging Natural Language
Processing(NLP) research field with wide real-world applications. The great
progress of this field in recent years is mainly due to the emergence of
large-scale datasets and deep learning. At present, a lot of MRC models have
already surpassed human performance on various benchmark datasets despite the
obvious giant gap between existing MRC models and genuine human-level reading
comprehension. This shows the need for improving existing datasets, evaluation
metrics, and models to move current MRC models toward ""real"" understanding. To
address the current lack of comprehensive survey of existing MRC tasks,
evaluation metrics, and datasets, herein, (1) we analyze 57 MRC tasks and
datasets and propose a more precise classification method of MRC tasks with 4
different attributes; (2) we summarized 9 evaluation metrics of MRC tasks, 7
attributes and 10 characteristics of MRC datasets; (3) We also discuss key open
issues in MRC research and highlighted future research directions. In addition,
we have collected, organized, and published our data on the companion
website(https://mrc-datasets.github.io/) where MRC researchers could directly
access each MRC dataset, papers, baseline projects, and the leaderboard.
","[{'version': 'v1', 'created': 'Sun, 21 Jun 2020 19:18:54 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 04:19:14 GMT'}]",2020-10-22,"[['Zeng', 'Changchang', ''], ['Li', 'Shaobo', ''], ['Li', 'Qin', ''], ['Hu', 'Jie', ''], ['Hu', 'Jianjun', '']]"
1312322,2007.00808,Li Xiong,"Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul
  Bennett, Junaid Ahmed, Arnold Overwijk","Approximate Nearest Neighbor Negative Contrastive Learning for Dense
  Text Retrieval",,,,,cs.IR cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conducting text retrieval in a dense learned representation space has many
intriguing advantages over sparse retrieval. Yet the effectiveness of dense
retrieval (DR) often requires combination with sparse retrieval. In this paper,
we identify that the main bottleneck is in the training mechanisms, where the
negative instances used in training are not representative of the irrelevant
documents in testing. This paper presents Approximate nearest neighbor Negative
Contrastive Estimation (ANCE), a training mechanism that constructs negatives
from an Approximate Nearest Neighbor (ANN) index of the corpus, which is
parallelly updated with the learning process to select more realistic negative
training instances. This fundamentally resolves the discrepancy between the
data distribution used in the training and testing of DR. In our experiments,
ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and
sparse retrieval baselines. It nearly matches the accuracy of
sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned
representation space and provides almost 100x speed-up.
","[{'version': 'v1', 'created': 'Wed, 1 Jul 2020 23:15:56 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 22:17:19 GMT'}]",2020-10-22,"[['Xiong', 'Lee', ''], ['Xiong', 'Chenyan', ''], ['Li', 'Ye', ''], ['Tang', 'Kwok-Fung', ''], ['Liu', 'Jialin', ''], ['Bennett', 'Paul', ''], ['Ahmed', 'Junaid', ''], ['Overwijk', 'Arnold', '']]"
1363152,2010.06822,Faeze Brahman,"Faeze Brahman, Snigdha Chaturvedi",Modeling Protagonist Emotions for Emotion-Aware Storytelling,"EMNLP 2020, update: Conference version of Weber et al. (2020) is
  cited",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Emotions and their evolution play a central role in creating a captivating
story. In this paper, we present the first study on modeling the emotional
trajectory of the protagonist in neural storytelling. We design methods that
generate stories that adhere to given story titles and desired emotion arcs for
the protagonist. Our models include Emotion Supervision (EmoSup) and two
Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards
designed to regularize the story generation process through reinforcement
learning. Our automatic and manual evaluations demonstrate that these models
are significantly better at generating stories that follow the desired emotion
arcs compared to baseline methods, without sacrificing story quality.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 06:24:25 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 19:23:52 GMT'}]",2020-10-22,"[['Brahman', 'Faeze', ''], ['Chaturvedi', 'Snigdha', '']]"
1276175,2004.11405,Ori Terner,"Ori Terner, Kfir Bar, Nachum Dershowitz","Transliteration of Judeo-Arabic Texts into Arabic Script Using Recurrent
  Neural Networks",accepted for WANLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We trained a model to automatically transliterate Judeo-Arabic texts into
Arabic script, enabling Arabic readers to access those writings. We employ a
recurrent neural network (RNN), combined with the connectionist temporal
classification (CTC) loss to deal with unequal input/output lengths. This
obligates adjustments in the training data to avoid input sequences that are
shorter than their corresponding outputs. We also utilize a pretraining stage
with a different loss function to improve network converge. Since only a single
source of parallel text was available for training, we take advantage of the
possibility of generating data synthetically. We train a model that has the
capability to memorize words in the output language, and that also utilizes
context for distinguishing ambiguities in the transliteration. We obtain an
improvement over the baseline 9.5% character error, achieving 2% error with our
best configuration. To measure the contribution of context to learning, we also
tested word-shuffled data, for which the error rises to 2.5%.
","[{'version': 'v1', 'created': 'Thu, 23 Apr 2020 18:03:41 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 09:08:53 GMT'}]",2020-10-22,"[['Terner', 'Ori', ''], ['Bar', 'Kfir', ''], ['Dershowitz', 'Nachum', '']]"
1268894,2004.04124,Yihuan Mao,"Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming
  Yang, Quanlu Zhang, Yunhai Tong, Jing Bai","LadaBERT: Lightweight Adaptation of BERT through Hybrid Model
  Compression",COLING2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  BERT is a cutting-edge language representation model pre-trained by a large
corpus, which achieves superior performances on various natural language
understanding tasks. However, a major blocking issue of applying BERT to online
services is that it is memory-intensive and leads to unsatisfactory latency of
user requests, raising the necessity of model compression. Existing solutions
leverage the knowledge distillation framework to learn a smaller model that
imitates the behaviors of BERT. However, the training procedure of knowledge
distillation is expensive itself as it requires sufficient training data to
imitate the teacher model. In this paper, we address this issue by proposing a
hybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid
model compression), which combines the advantages of different model
compression methods, including weight pruning, matrix factorization and
knowledge distillation. LadaBERT achieves state-of-the-art accuracy on various
public datasets while the training overheads can be reduced by an order of
magnitude.
","[{'version': 'v1', 'created': 'Wed, 8 Apr 2020 17:18:56 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 15:15:11 GMT'}]",2020-10-22,"[['Mao', 'Yihuan', ''], ['Wang', 'Yujing', ''], ['Wu', 'Chufan', ''], ['Zhang', 'Chen', ''], ['Wang', 'Yang', ''], ['Yang', 'Yaming', ''], ['Zhang', 'Quanlu', ''], ['Tong', 'Yunhai', ''], ['Bai', 'Jing', '']]"
1255346,2003.04991,Jitin Krishnan,"Jitin Krishnan, Hemant Purohit and Huzefa Rangwala","Unsupervised and Interpretable Domain Adaptation to Rapidly Filter
  Tweets for Emergency Services","8 pages, 4 Figures, 6 Tables, Source Code Available",,,,cs.CL cs.LG cs.SI stat.ML,http://creativecommons.org/licenses/by/4.0/,"  During the onset of a disaster event, filtering relevant information from the
social web data is challenging due to its sparse availability and practical
limitations in labeling datasets of an ongoing crisis. In this paper, we
hypothesize that unsupervised domain adaptation through multi-task learning can
be a useful framework to leverage data from past crisis events for training
efficient information filtering models during the sudden onset of a new crisis.
We present a novel method to classify relevant tweets during an ongoing crisis
without seeing any new examples, using the publicly available dataset of TREC
incident streams. Specifically, we construct a customized multi-task
architecture with a multi-domain discriminator for crisis analytics: multi-task
domain adversarial attention network. This model consists of dedicated
attention layers for each task to provide model interpretability; critical for
real-word applications. As deep networks struggle with sparse datasets, we show
that this can be improved by sharing a base layer for multi-task learning and
domain adversarial training. Evaluation of domain adaptation for crisis events
is performed by choosing a target event as the test set and training on the
rest. Our results show that the multi-task model outperformed its single task
counterpart. For the qualitative evaluation of interpretability, we show that
the attention layer can be used as a guide to explain the model predictions and
empower emergency services for exploring accountability of the model, by
showcasing the words in a tweet that are deemed important in the classification
process. Finally, we show a practical implication of our work by providing a
use-case for the COVID-19 pandemic.
","[{'version': 'v1', 'created': 'Wed, 4 Mar 2020 06:40:14 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 18:01:19 GMT'}]",2020-10-22,"[['Krishnan', 'Jitin', ''], ['Purohit', 'Hemant', ''], ['Rangwala', 'Huzefa', '']]"
1248360,2002.10937,Jitin Krishnan,"Jitin Krishnan, Hemant Purohit, and Huzefa Rangwala","Diversity-Based Generalization for Unsupervised Text Classification
  under Domain Shift","16 pages, 3 figures, 5 Tables, Source Code Available",,,,cs.LG cs.CL stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Domain adaptation approaches seek to learn from a source domain and
generalize it to an unseen target domain. At present, the state-of-the-art
unsupervised domain adaptation approaches for subjective text classification
problems leverage unlabeled target data along with labeled source data. In this
paper, we propose a novel method for domain adaptation of single-task text
classification problems based on a simple but effective idea of diversity-based
generalization that does not require unlabeled target data but still matches
the state-of-the-art in performance. Diversity plays the role of promoting the
model to better generalize and be indiscriminate towards domain shift by
forcing the model not to rely on same features for prediction. We apply this
concept on the most explainable component of neural networks, the attention
layer. To generate sufficient diversity, we create a multi-head attention model
and infuse a diversity constraint between the attention heads such that each
head will learn differently. We further expand upon our model by tri-training
and designing a procedure with an additional diversity constraint between the
attention heads of the tri-trained classifiers. Extensive evaluation using the
standard benchmark dataset of Amazon reviews and a newly constructed dataset of
Crisis events shows that our fully unsupervised method matches with the
competing baselines that uses unlabeled target data. Our results demonstrate
that machine learning architectures that ensure sufficient diversity can
generalize better; encouraging future research to design ubiquitously usable
learning models without using unlabeled target data.
","[{'version': 'v1', 'created': 'Tue, 25 Feb 2020 15:11:02 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 18:06:10 GMT'}]",2020-10-22,"[['Krishnan', 'Jitin', ''], ['Purohit', 'Hemant', ''], ['Rangwala', 'Huzefa', '']]"
1246676,2002.09253,C\'edric Colas,"C\'edric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux,
  Cl\'ement Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer","Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven
  Exploration",Contains main article and supplementaries,NeurIPS 2020,,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Developmental machine learning studies how artificial agents can model the
way children learn open-ended repertoires of skills. Such agents need to create
and represent goals, select which ones to pursue and learn to achieve them.
Recent approaches have considered goal spaces that were either fixed and
hand-defined or learned using generative models of states. This limited agents
to sample goals within the distribution of known effects. We argue that the
ability to imagine out-of-distribution goals is key to enable creative
discoveries and open-ended learning. Children do so by leveraging the
compositionality of language as a tool to imagine descriptions of outcomes they
never experienced before, targeting them as goals during play. We introduce
IMAGINE, an intrinsically motivated deep reinforcement learning architecture
that models this ability. Such imaginative agents, like children, benefit from
the guidance of a social peer who provides language descriptions. To take
advantage of goal imagination, agents must be able to leverage these
descriptions to interpret their imagined out-of-distribution goals. This
generalization is made possible by modularity: a decomposition between learned
goal-achievement reward function and policy relying on deep sets, gated
attention and object-centered representations. We introduce the Playground
environment and study how this form of goal imagination improves generalization
and exploration over agents lacking this capacity. In addition, we identify the
properties of goal imagination that enable these results and study the impacts
of modularity and social interactions.
","[{'version': 'v1', 'created': 'Fri, 21 Feb 2020 12:59:57 GMT'}, {'version': 'v2', 'created': 'Mon, 20 Apr 2020 11:38:50 GMT'}, {'version': 'v3', 'created': 'Fri, 12 Jun 2020 09:23:40 GMT'}, {'version': 'v4', 'created': 'Wed, 21 Oct 2020 16:48:51 GMT'}]",2020-10-22,"[['Colas', 'Cédric', ''], ['Karch', 'Tristan', ''], ['Lair', 'Nicolas', ''], ['Dussoux', 'Jean-Michel', ''], ['Moulin-Frier', 'Clément', ''], ['Dominey', 'Peter Ford', ''], ['Oudeyer', 'Pierre-Yves', '']]"
1229476,2001.04063,Weizhen Qi,"Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen,
  Ruofei Zhang, Ming Zhou","ProphetNet: Predicting Future N-gram for Sequence-to-Sequence
  Pre-training","Accepted to EMNLP 2020 Findings. Project page:
  https://github.com/microsoft/ProphetNet",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a new sequence-to-sequence pre-training model called
ProphetNet, which introduces a novel self-supervised objective named future
n-gram prediction and the proposed n-stream self-attention mechanism. Instead
of optimizing one-step-ahead prediction in the traditional sequence-to-sequence
model, the ProphetNet is optimized by n-step ahead prediction that predicts the
next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for
the future tokens and prevent overfitting on strong local correlations. We
pre-train ProphetNet using a base scale dataset (16GB) and a large-scale
dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail,
Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question
generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the
same scale pre-training corpus.
","[{'version': 'v1', 'created': 'Mon, 13 Jan 2020 05:12:38 GMT'}, {'version': 'v2', 'created': 'Sat, 22 Feb 2020 09:29:12 GMT'}, {'version': 'v3', 'created': 'Wed, 21 Oct 2020 05:45:35 GMT'}]",2020-10-22,"[['Qi', 'Weizhen', ''], ['Yan', 'Yu', ''], ['Gong', 'Yeyun', ''], ['Liu', 'Dayiheng', ''], ['Duan', 'Nan', ''], ['Chen', 'Jiusheng', ''], ['Zhang', 'Ruofei', ''], ['Zhou', 'Ming', '']]"
1287876,2005.08081,Fenglin Liu,"Fenglin Liu, Xuancheng Ren, Guangxiang Zhao, Xu Sun, Liangyou Li",Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning,"Achieve state-of-the-art BLEU scores on WMT14 EN-DE, EN-FR, and IWSLT
  DE-EN datasets",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In sequence-to-sequence learning, the decoder relies on the attention
mechanism to efficiently extract information from the encoder. While it is
common practice to draw information from only the last encoder layer, recent
work has proposed to use representations from different encoder layers for
diversified levels of information. Nonetheless, the decoder still obtains only
a single view of the source sequences, which might lead to insufficient
training of the encoder layer stack due to the hierarchy bypassing problem. In
this work, we propose layer-wise cross-view decoding, where for each decoder
layer, together with the representations from the last encoder layer, which
serve as a global view, those from other encoder layers are supplemented for a
stereoscopic view of the source sequences. Systematic experiments show that we
successfully address the hierarchy bypassing problem and substantially improve
the performance of sequence-to-sequence learning with deep representations on
diverse tasks.
","[{'version': 'v1', 'created': 'Sat, 16 May 2020 20:00:39 GMT'}, {'version': 'v2', 'created': 'Wed, 3 Jun 2020 10:21:33 GMT'}, {'version': 'v3', 'created': 'Wed, 21 Oct 2020 05:58:38 GMT'}]",2020-10-22,"[['Liu', 'Fenglin', ''], ['Ren', 'Xuancheng', ''], ['Zhao', 'Guangxiang', ''], ['Sun', 'Xu', ''], ['Li', 'Liangyou', '']]"
1366722,2010.10392,Hicham El Boukkouri,"Hicham El Boukkouri, Olivier Ferret, Thomas Lavergne, Hiroshi Noji,
  Pierre Zweigenbaum, Junichi Tsujii","CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary
  Representations From Characters","13 pages, 8 figures and 3 tables. Accepted at COLING 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Due to the compelling improvements brought by BERT, many recent
representation models adopted the Transformer architecture as their main
building block, consequently inheriting the wordpiece tokenization system
despite it not being intrinsically linked to the notion of Transformers. While
this system is thought to achieve a good balance between the flexibility of
characters and the efficiency of full words, using predefined wordpiece
vocabularies from the general domain is not always suitable, especially when
building models for specialized domains (e.g., the medical domain). Moreover,
adopting a wordpiece tokenization shifts the focus from the word level to the
subword level, making the models conceptually more complex and arguably less
convenient in practice. For these reasons, we propose CharacterBERT, a new
variant of BERT that drops the wordpiece system altogether and uses a
Character-CNN module instead to represent entire words by consulting their
characters. We show that this new model improves the performance of BERT on a
variety of medical domain tasks while at the same time producing robust,
word-level and open-vocabulary representations.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 15:58:53 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 14:57:52 GMT'}]",2020-10-22,"[['Boukkouri', 'Hicham El', ''], ['Ferret', 'Olivier', ''], ['Lavergne', 'Thomas', ''], ['Noji', 'Hiroshi', ''], ['Zweigenbaum', 'Pierre', ''], ['Tsujii', 'Junichi', '']]"
1366886,2010.10556,Peidong Wang,"Peidong Wang, Zhuo Chen, DeLiang Wang, Jinyu Li, Yifan Gong",Speaker Separation Using Speaker Inventories and Estimated Speech,,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose speaker separation using speaker inventories and estimated speech
(SSUSIES), a framework leveraging speaker profiles and estimated speech for
speaker separation. SSUSIES contains two methods, speaker separation using
speaker inventories (SSUSI) and speaker separation using estimated speech
(SSUES). SSUSI performs speaker separation with the help of speaker inventory.
By combining the advantages of permutation invariant training (PIT) and speech
extraction, SSUSI significantly outperforms conventional approaches. SSUES is a
widely applicable technique that can substantially improve speaker separation
performance using the output of first-pass separation. We evaluate the models
on both speaker separation and speech recognition metrics.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 18:15:45 GMT'}]",2020-10-22,"[['Wang', 'Peidong', ''], ['Chen', 'Zhuo', ''], ['Wang', 'DeLiang', ''], ['Li', 'Jinyu', ''], ['Gong', 'Yifan', '']]"
1366893,2010.10563,Pablo Messina,"Pablo Messina, Pablo Pino, Denis Parra, Alvaro Soto, Cecilia Besa,
  Sergio Uribe, Marcelo and\'ia, Cristian Tejos, Claudia Prieto and Daniel
  Capurro","A Survey on Deep Learning and Explainability for Automatic Image-based
  Medical Report Generation",,,,,cs.CV cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Every year physicians face an increasing demand of image-based diagnosis from
patients, a problem that can be addressed with recent artificial intelligence
methods. In this context, we survey works in the area of automatic report
generation from medical images, with emphasis on methods using deep neural
networks, with respect to: (1) Datasets, (2) Architecture Design, (3)
Explainability and (4) Evaluation Metrics. Our survey identifies interesting
developments, but also remaining challenges. Among them, the current evaluation
of generated reports is especially weak, since it mostly relies on traditional
Natural Language Processing (NLP) metrics, which do not accurately capture
medical correctness.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 18:48:37 GMT'}]",2020-10-22,"[['Messina', 'Pablo', ''], ['Pino', 'Pablo', ''], ['Parra', 'Denis', ''], ['Soto', 'Alvaro', ''], ['Besa', 'Cecilia', ''], ['Uribe', 'Sergio', ''], ['andía', 'Marcelo', ''], ['Tejos', 'Cristian', ''], ['Prieto', 'Claudia', ''], ['Capurro', 'Daniel', '']]"
1366896,2010.10566,Fei Liu,"Sangwoo Cho and Kaiqiang Song and Chen Li and Dong Yu and Hassan
  Foroosh and Fei Liu",Better Highlighting: Creating Sub-Sentence Summary Highlights,EMNLP 2020 (Long Paper),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Amongst the best means to summarize is highlighting. In this paper, we aim to
generate summary highlights to be overlaid on the original documents to make it
easier for readers to sift through a large amount of text. The method allows
summaries to be understood in context to prevent a summarizer from distorting
the original meaning, of which abstractive summarizers usually fall short. In
particular, we present a new method to produce self-contained highlights that
are understandable on their own to avoid confusion. Our method combines
determinantal point processes and deep contextualized representations to
identify an optimal set of sub-sentence segments that are both important and
non-redundant to form summary highlights. To demonstrate the flexibility and
modeling power of our method, we conduct extensive experiments on summarization
datasets. Our analysis provides evidence that highlighting is a promising
avenue of research towards future summarization.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 18:57:42 GMT'}]",2020-10-22,"[['Cho', 'Sangwoo', ''], ['Song', 'Kaiqiang', ''], ['Li', 'Chen', ''], ['Yu', 'Dong', ''], ['Foroosh', 'Hassan', ''], ['Liu', 'Fei', '']]"
1366903,2010.10573,Hoang Nguyen Hung Van,"Hoang Van, David Kauchak, Gondy Leroy",AutoMeTS: The Autocomplete for Medical Text Simplification,"9 pages, 3 figures, and 8 tables, Accpeted to COLING 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The goal of text simplification (TS) is to transform difficult text into a
version that is easier to understand and more broadly accessible to a wide
variety of readers. In some domains, such as healthcare, fully automated
approaches cannot be used since information must be accurately preserved.
Instead, semi-automated approaches can be used that assist a human writer in
simplifying text faster and at a higher quality. In this paper, we examine the
application of autocomplete to text simplification in the medical domain. We
introduce a new parallel medical data set consisting of aligned English
Wikipedia with Simple English Wikipedia sentences and examine the application
of pretrained neural language models (PNLMs) on this dataset. We compare four
PNLMs(BERT, RoBERTa, XLNet, and GPT-2), and show how the additional context of
the sentence to be simplified can be incorporated to achieve better results
(6.17% absolute improvement over the best individual model). We also introduce
an ensemble model that combines the four PNLMs and outperforms the best
individual model by 2.1%, resulting in an overall word prediction accuracy of
64.52%.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 19:20:29 GMT'}]",2020-10-22,"[['Van', 'Hoang', ''], ['Kauchak', 'David', ''], ['Leroy', 'Gondy', '']]"
1366927,2010.10597,Aditya Kalyanpur,"Clifton McFate, Aditya Kalyanpur, Dave Ferrucci, Andrea Bradshaw,
  Ariel Diertani, David Melville, Lori Moon",SKATE: A Natural Language Interface for Encoding Structured Knowledge,In submission,,,,cs.CL cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In Natural Language (NL) applications, there is often a mismatch between what
the NL interface is capable of interpreting and what a lay user knows how to
express. This work describes a novel natural language interface that reduces
this mismatch by refining natural language input through successive,
automatically generated semi-structured templates. In this paper we describe
how our approach, called SKATE, uses a neural semantic parser to parse NL input
and suggest semi-structured templates, which are recursively filled to produce
fully structured interpretations. We also show how SKATE integrates with a
neural rule-generation model to interactively suggest and acquire commonsense
knowledge. We provide a preliminary coverage analysis of SKATE for the task of
story understanding, and then describe a current business use-case of the tool
in a specific domain: COVID-19 policy design.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 20:13:09 GMT'}]",2020-10-22,"[['McFate', 'Clifton', ''], ['Kalyanpur', 'Aditya', ''], ['Ferrucci', 'Dave', ''], ['Bradshaw', 'Andrea', ''], ['Diertani', 'Ariel', ''], ['Melville', 'David', ''], ['Moon', 'Lori', '']]"
1366978,2010.10648,Elman Mansimov,"Elman Mansimov, Mitchell Stern, Mia Chen, Orhan Firat, Jakob
  Uszkoreit, Puneet Jain",Towards End-to-End In-Image Neural Machine Translation,"Accepted as an oral presentation at EMNLP, NLP Beyond Text workshop,
  2020",,,,cs.CL cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we offer a preliminary investigation into the task of in-image
machine translation: transforming an image containing text in one language into
an image containing the same text in another language. We propose an end-to-end
neural model for this task inspired by recent approaches to neural machine
translation, and demonstrate promising initial results based purely on
pixel-level supervision. We then offer a quantitative and qualitative
evaluation of our system outputs and discuss some common failure modes.
Finally, we conclude with directions for future work.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 22:20:04 GMT'}]",2020-10-22,"[['Mansimov', 'Elman', ''], ['Stern', 'Mitchell', ''], ['Chen', 'Mia', ''], ['Firat', 'Orhan', ''], ['Uszkoreit', 'Jakob', ''], ['Jain', 'Puneet', '']]"
1366979,2010.10649,Wei-Fan Chen,"Wei-Fan Chen, Khalid Al-Khatib, Benno Stein and Henning Wachsmuth",Detecting Media Bias in News Articles using Gaussian Bias Distributions,,EMNLP 2020 Findings,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Media plays an important role in shaping public opinion. Biased media can
influence people in undesirable directions and hence should be unmasked as
such. We observe that featurebased and neural text classification approaches
which rely only on the distribution of low-level lexical information fail to
detect media bias. This weakness becomes most noticeable for articles on new
events, where words appear in new contexts and hence their ""bias
predictiveness"" is unclear. In this paper, we therefore study how second-order
information about biased statements in an article helps to improve detection
effectiveness. In particular, we utilize the probability distributions of the
frequency, positions, and sequential order of lexical and informational
sentence-level bias in a Gaussian Mixture Model. On an existing media bias
dataset, we find that the frequency and positions of biased statements strongly
impact article-level bias, whereas their exact sequential order is secondary.
Using a standard model for sentence-level bias detection, we provide empirical
evidence that article-level bias detectors that use second-order information
clearly outperform those without.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 22:20:49 GMT'}]",2020-10-22,"[['Chen', 'Wei-Fan', ''], ['Al-Khatib', 'Khalid', ''], ['Stein', 'Benno', ''], ['Wachsmuth', 'Henning', '']]"
1367349,2010.11019,Pranaydeep Singh,Pranaydeep Singh and Els Lefever,"LT3 at SemEval-2020 Task 9: Cross-lingual Embeddings for Sentiment
  Analysis of Hinglish Social Media Text",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes our contribution to the SemEval-2020 Task 9 on Sentiment
Analysis for Code-mixed Social Media Text. We investigated two approaches to
solve the task of Hinglish sentiment analysis. The first approach uses
cross-lingual embeddings resulting from projecting Hinglish and pre-trained
English FastText word embeddings in the same space. The second approach
incorporates pre-trained English embeddings that are incrementally retrained
with a set of Hinglish tweets. The results show that the second approach
performs best, with an F1-score of 70.52% on the held-out test data.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 14:03:16 GMT'}]",2020-10-22,"[['Singh', 'Pranaydeep', ''], ['Lefever', 'Els', '']]"
1366999,2010.10669,Ram\'on Fernandez Astudillo,"Ramon Fernandez Astudillo, Miguel Ballesteros, Tahira Naseem, Austin
  Blodgett, Radu Florian",Transition-based Parsing with Stack-Transformers,"Accepted to Findings of EMNLP2020, open review
  https://openreview.net/forum?id=b36spsuUAde, code
  https://github.com/IBM/transition-amr-parser",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modeling the parser state is key to good performance in transition-based
parsing. Recurrent Neural Networks considerably improved the performance of
transition-based systems by modelling the global state, e.g. stack-LSTM
parsers, or local state modeling of contextualized features, e.g. Bi-LSTM
parsers. Given the success of Transformer architectures in recent parsing
systems, this work explores modifications of the sequence-to-sequence
Transformer architecture to model either global or local parser states in
transition-based parsing. We show that modifications of the cross attention
mechanism of the Transformer considerably strengthen performance both on
dependency and Abstract Meaning Representation (AMR) parsing tasks,
particularly for smaller models or limited training data.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 23:20:31 GMT'}]",2020-10-22,"[['Astudillo', 'Ramon Fernandez', ''], ['Ballesteros', 'Miguel', ''], ['Naseem', 'Tahira', ''], ['Blodgett', 'Austin', ''], ['Florian', 'Radu', '']]"
1367024,2010.10694,Erica Cooper,"Antoine Perquin, Erica Cooper, Junichi Yamagishi",Grapheme or phoneme? An Analysis of Tacotron's Embedded Representations,Submitted to ICASSP 2021,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end models, particularly Tacotron-based ones, are currently a popular
solution for text-to-speech synthesis. They allow the production of
high-quality synthesized speech with little to no text preprocessing. Phoneme
inputs are usually preferred over graphemes in order to limit the amount of
pronunciation errors. In this work we show that, in the case of a well-curated
French dataset, graphemes can be used as input without increasing the amount of
pronunciation errors. Furthermore, we perform an analysis of the representation
learned by the Tacotron model and show that the contextual grapheme embeddings
encode phoneme information, and that they can be used for grapheme-to-phoneme
conversion and phoneme control of synthetic speech.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 00:58:29 GMT'}]",2020-10-22,"[['Perquin', 'Antoine', ''], ['Cooper', 'Erica', ''], ['Yamagishi', 'Junichi', '']]"
1367396,2010.11066,Chenyu You,"Chenyu You, Nuo Chen, Yuexian Zou","Contextualized Attention-based Knowledge Transfer for Spoken
  Conversational Question Answering",,,,,cs.CL cs.AI cs.IR cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 15:17:18 GMT'}]",2020-10-22,"[['You', 'Chenyu', ''], ['Chen', 'Nuo', ''], ['Zou', 'Yuexian', '']]"
1367384,2010.11054,Jiaming Luo,"Jiaming Luo, Frederik Hartmann, Enrico Santus, Yuan Cao, Regina
  Barzilay",Deciphering Undersegmented Ancient Scripts Using Phonetic Prior,"TACL 2020, pre-MIT Press publication version",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most undeciphered lost languages exhibit two characteristics that pose
significant decipherment challenges: (1) the scripts are not fully segmented
into words; (2) the closest known language is not determined. We propose a
decipherment model that handles both of these challenges by building on rich
linguistic constraints reflecting consistent patterns in historical sound
change. We capture the natural phonological geometry by learning character
embeddings based on the International Phonetic Alphabet (IPA). The resulting
generative framework jointly models word segmentation and cognate alignment,
informed by phonological constraints. We evaluate the model on both deciphered
languages (Gothic, Ugaritic) and an undeciphered one (Iberian). The experiments
show that incorporating phonetic geometry leads to clear and consistent gains.
Additionally, we propose a measure for language closeness which correctly
identifies related languages for Gothic and Ugaritic. For Iberian, the method
does not show strong evidence supporting Basque as a related language,
concurring with the favored position by the current scholarship.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 15:03:52 GMT'}]",2020-10-22,"[['Luo', 'Jiaming', ''], ['Hartmann', 'Frederik', ''], ['Santus', 'Enrico', ''], ['Cao', 'Yuan', ''], ['Barzilay', 'Regina', '']]"
1367405,2010.11075,Nils Barlaug,"Nils Barlaug, Jon Atle Gulla",Neural Networks for Entity Matching,Under review in TKDD,,,,cs.DB cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Entity matching is the problem of identifying which records refer to the same
real-world entity. It has been actively researched for decades, and a variety
of different approaches have been developed. Even today, it remains a
challenging problem, and there is still generous room for improvement. In
recent years we have seen new methods based upon deep learning techniques for
natural language processing emerge.
  In this survey, we present how neural networks have been used for entity
matching. Specifically, we identify which steps of the entity matching process
existing work have targeted using neural networks, and provide an overview of
the different techniques used at each step. We also discuss contributions from
deep learning in entity matching compared to traditional methods, and propose a
taxonomy of deep neural networks for entity matching.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 15:36:03 GMT'}]",2020-10-22,"[['Barlaug', 'Nils', ''], ['Gulla', 'Jon Atle', '']]"
1226995,2001.01582,Hossein Amirkhani,"Razieh Baradaran, Razieh Ghiasi, and Hossein Amirkhani",A Survey on Machine Reading Comprehension Systems,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine reading comprehension is a challenging task and hot topic in natural
language processing. Its goal is to develop systems to answer the questions
regarding a given context. In this paper, we present a comprehensive survey on
different aspects of machine reading comprehension systems, including their
approaches, structures, input/outputs, and research novelties. We illustrate
the recent trends in this field based on 241 reviewed papers from 2016 to 2020.
Our investigations demonstrate that the focus of research has changed in recent
years from answer extraction to answer generation, from single to
multi-document reading comprehension, and from learning from scratch to using
pre-trained embeddings. We also discuss the popular datasets and the evaluation
metrics in this field. The paper ends with investigating the most cited papers
and their contributions.
","[{'version': 'v1', 'created': 'Mon, 6 Jan 2020 13:54:06 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 20:50:51 GMT'}]",2020-10-22,"[['Baradaran', 'Razieh', ''], ['Ghiasi', 'Razieh', ''], ['Amirkhani', 'Hossein', '']]"
1367397,2010.11067,Chenyu You,"Chenyu You, Nuo Chen, Yuexian Zou","Knowledge Distillation for Improved Accuracy in Spoken Question
  Answering",,,,,cs.CL cs.AI cs.IR cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Spoken question answering (SQA) is a challenging task that requires the
machine to fully understand the complex spoken documents. Automatic speech
recognition (ASR) plays a significant role in the development of QA systems.
However, the recent work shows that ASR systems generate highly noisy
transcripts, which critically limit the capability of machine comprehension on
the SQA task. To address the issue, we present a novel distillation framework.
Specifically, we devise a training strategy to perform knowledge distillation
(KD) from spoken documents and written counterparts. Our work makes a step
towards distilling knowledge from the language model as a supervision signal to
lead to better student accuracy by reducing the misalignment between automatic
and manual transcriptions. Experiments demonstrate that our approach
outperforms several state-of-the-art language models on the Spoken-SQuAD
dataset.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 15:18:01 GMT'}]",2020-10-22,"[['You', 'Chenyu', ''], ['Chen', 'Nuo', ''], ['Zou', 'Yuexian', '']]"
1367500,2010.11170,Ozan \.Irsoy,"Tianze Shi, Igor Malioutov, Ozan \.Irsoy",Semantic Role Labeling as Syntactic Dependency Parsing,Appeared in EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We reduce the task of (span-based) PropBank-style semantic role labeling
(SRL) to syntactic dependency parsing. Our approach is motivated by our
empirical analysis that shows three common syntactic patterns account for over
98% of the SRL annotations for both English and Chinese data. Based on this
observation, we present a conversion scheme that packs SRL annotations into
dependency tree representations through joint labels that permit highly
accurate recovery back to the original format. This representation allows us to
train statistical dependency parsers to tackle SRL and achieve competitive
performance with the current state of the art. Our findings show the promise of
syntactic dependency trees in encoding semantic role relations within their
syntactic domain of locality, and point to potential further integration of
syntactic methods into semantic role labeling in the future.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 17:46:11 GMT'}]",2020-10-22,"[['Shi', 'Tianze', ''], ['Malioutov', 'Igor', ''], ['İrsoy', 'Ozan', '']]"
1367478,2010.11148,Jiahui Yu,"Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo-yiin Chang, Tara N. Sainath,
  Yanzhang He, Arun Narayanan, Wei Han, Anmol Gulati, Yonghui Wu, Ruoming Pang","FastEmit: Low-latency Streaming ASR with Sequence-level Emission
  Regularization",tech report,,,,eess.AS cs.AI cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Streaming automatic speech recognition (ASR) aims to emit each hypothesized
word as quickly and accurately as possible. However, emitting fast without
degrading quality, as measured by word error rate (WER), is highly challenging.
Existing approaches including Early and Late Penalties and Constrained
Alignments penalize emission delay by manipulating per-token or per-frame
probability prediction in sequence transducer models. While being successful in
reducing delay, these approaches suffer from significant accuracy regression
and also require additional word alignment information from an existing model.
In this work, we propose a sequence-level emission regularization method, named
FastEmit, that applies latency regularization directly on per-sequence
probability in training transducer models, and does not require any alignment.
We demonstrate that FastEmit is more suitable to the sequence-level
optimization of transducer models for streaming ASR by applying it on various
end-to-end streaming ASR networks including RNN-Transducer,
Transformer-Transducer, ConvNet-Transducer and Conformer-Transducer. We achieve
150-300 ms latency reduction with significantly better accuracy over previous
techniques on a Voice Search test set. FastEmit also improves streaming ASR
accuracy from 4.4%/8.9% to 3.1%/7.5% WER, meanwhile reduces 90th percentile
latency from 210 ms to only 30 ms on LibriSpeech.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 17:05:01 GMT'}]",2020-10-22,"[['Yu', 'Jiahui', ''], ['Chiu', 'Chung-Cheng', ''], ['Li', 'Bo', ''], ['Chang', 'Shuo-yiin', ''], ['Sainath', 'Tara N.', ''], ['He', 'Yanzhang', ''], ['Narayanan', 'Arun', ''], ['Han', 'Wei', ''], ['Gulati', 'Anmol', ''], ['Wu', 'Yonghui', ''], ['Pang', 'Ruoming', '']]"
1367470,2010.11140,Yan Zeng,Yan Zeng and Jian-Yun Nie,"Generalized Conditioned Dialogue Generation Based on Pre-trained
  Language Model","9 pages, 2 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate the general problem of conditioned dialogue, in which a
condition label is used as input to designate the type of the target response
such as a persona. A major challenge for conditioned dialogue generation is the
lack of substantial dialogue data labeled with conditions. Thus, we propose to
complement the labeled dialogue data with labeled non-dialogue text data, and
fine-tune BERT based on them. Our fine-tuning approach utilizes BERT for both
encoder and decoder via different input representations and self-attention
masks in order to distinguish the source and target side. On the target
(generation) side, we use a new attention routing mechanism to choose between
generating a generic word or condition-related word at each position. Our model
is instantiated to persona- and topic-related dialogue. Experimental results in
both cases show that our approach can produce significantly better responses
than the state-of-the-art baselines.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 16:56:49 GMT'}]",2020-10-22,"[['Zeng', 'Yan', ''], ['Nie', 'Jian-Yun', '']]"
1367467,2010.11137,Yan Zeng,Yan Zeng and Jian-Yun Nie,Multi-Domain Dialogue State Tracking based on State Graph,"9 pages, 3 figures",,,,cs.CL cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate the problem of multi-domain Dialogue State Tracking (DST) with
open vocabulary, which aims to extract the state from the dialogue. Existing
approaches usually concatenate previous dialogue state with dialogue history as
the input to a bi-directional Transformer encoder. They rely on the
self-attention mechanism of Transformer to connect tokens in them. However,
attention may be paid to spurious connections, leading to wrong inference. In
this paper, we propose to construct a dialogue state graph in which domains,
slots and values from the previous dialogue state are connected properly.
Through training, the graph node and edge embeddings can encode co-occurrence
relations between domain-domain, slot-slot and domain-slot, reflecting the
strong transition paths in general dialogue. The state graph, encoded with
relational-GCN, is fused into the Transformer encoder. Experimental results
show that our approach achieves a new state of the art on the task while
remaining efficient. It outperforms existing open-vocabulary DST approaches.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 16:55:18 GMT'}]",2020-10-22,"[['Zeng', 'Yan', ''], ['Nie', 'Jian-Yun', '']]"
1367462,2010.11132,Te I,"Daniel Li, Te I, Naveen Arivazhagan, Colin Cherry, Dirk Padfield",Sentence Boundary Augmentation For Neural Machine Translation Robustness,"5 pages, 4 figures",,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural Machine Translation (NMT) models have demonstrated strong state of the
art performance on translation tasks where well-formed training and evaluation
data are provided, but they remain sensitive to inputs that include errors of
various types. Specifically, in the context of long-form speech translation
systems, where the input transcripts come from Automatic Speech Recognition
(ASR), the NMT models have to handle errors including phoneme substitutions,
grammatical structure, and sentence boundaries, all of which pose challenges to
NMT robustness. Through in-depth error analysis, we show that sentence boundary
segmentation has the largest impact on quality, and we develop a simple data
augmentation strategy to improve segmentation robustness.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 16:44:48 GMT'}]",2020-10-22,"[['Li', 'Daniel', ''], ['I', 'Te', ''], ['Arivazhagan', 'Naveen', ''], ['Cherry', 'Colin', ''], ['Padfield', 'Dirk', '']]"
1367455,2010.11125,Angela Fan,"Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,
  Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav
  Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov,
  Edouard Grave, Michael Auli, Armand Joulin",Beyond English-Centric Multilingual Machine Translation,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing work in translation demonstrated the potential of massively
multilingual machine translation by training a single model able to translate
between any pair of languages. However, much of this work is English-Centric by
training only on data which was translated from or to English. While this is
supported by large sources of training data, it does not reflect translation
needs worldwide. In this work, we create a true Many-to-Many multilingual
translation model that can translate directly between any pair of 100
languages. We build and open source a training dataset that covers thousands of
language directions with supervised data, created through large-scale mining.
Then, we explore how to effectively increase model capacity through a
combination of dense scaling and language-specific sparse parameters to create
high quality models. Our focus on non-English-Centric models brings gains of
more than 10 BLEU when directly translating between non-English directions
while performing competitively to the best single systems of WMT. We
open-source our scripts so that others may reproduce the data, evaluation, and
final M2M-100 model.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 17:01:23 GMT'}]",2020-10-22,"[['Fan', 'Angela', ''], ['Bhosale', 'Shruti', ''], ['Schwenk', 'Holger', ''], ['Ma', 'Zhiyi', ''], ['El-Kishky', 'Ahmed', ''], ['Goyal', 'Siddharth', ''], ['Baines', 'Mandeep', ''], ['Celebi', 'Onur', ''], ['Wenzek', 'Guillaume', ''], ['Chaudhary', 'Vishrav', ''], ['Goyal', 'Naman', ''], ['Birch', 'Tom', ''], ['Liptchinsky', 'Vitaliy', ''], ['Edunov', 'Sergey', ''], ['Grave', 'Edouard', ''], ['Auli', 'Michael', ''], ['Joulin', 'Armand', '']]"
1367483,2010.11153,Tsz Kin Lam,"Tsz Kin Lam, Shigehiko Schamoni, Stefan Riezler",Cascaded Models With Cyclic Feedback For Direct Speech Translation,"5 pages, 1 figure",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Direct speech translation describes a scenario where only speech inputs and
corresponding translations are available. Such data are notoriously limited. We
present a technique that allows cascades of automatic speech recognition (ASR)
and machine translation (MT) to exploit in-domain direct speech translation
data in addition to out-of-domain MT and ASR data. After pre-training MT and
ASR, we use a feedback cycle where the downstream performance of the MT system
is used as a signal to improve the ASR system by self-training, and the MT
component is fine-tuned on multiple ASR outputs, making it more tolerant
towards spelling variations. A comparison to end-to-end speech translation
using components of identical architecture and the same data shows gains of up
to 3.8 BLEU points on LibriVoxDeEn and up to 5.1 BLEU points on CoVoST for
German-to-English speech translation.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 17:18:51 GMT'}]",2020-10-22,"[['Lam', 'Tsz Kin', ''], ['Schamoni', 'Shigehiko', ''], ['Riezler', 'Stefan', '']]"
1367453,2010.11123,Daniel Ajisafe,"Daniel Ajisafe, Oluwabukola Adegboro, Esther Oduntan, Tayo Arulogun","Towards End-to-End Training of Automatic Speech Recognition for Nigerian
  Pidgin",To appear in ICASSP 2021,,,,eess.AS cs.AI cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Nigerian Pidgin remains one of the most popular languages in West Africa.
With at least 75 million speakers along the West African coast, the language
has spread to diasporic communities through Nigerian immigrants in England,
Canada, and America, amongst others. In contrast, the language remains an
under-resourced one in the field of natural language processing, particularly
on speech recognition and translation tasks. In this work, we present the first
parallel (speech-to-text) data on Nigerian pidgin. We also trained the first
end-to-end speech recognition system (QuartzNet and Jasper model) on this
language which were both optimized using Connectionist Temporal Classification
(CTC) loss. With baseline results, we were able to achieve a low word error
rate (WER) of 0.77% using a greedy decoder on our dataset. Finally, we
open-source the data and code along with this publication in order to encourage
future research in this direction.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 16:32:58 GMT'}]",2020-10-22,"[['Ajisafe', 'Daniel', ''], ['Adegboro', 'Oluwabukola', ''], ['Oduntan', 'Esther', ''], ['Arulogun', 'Tayo', '']]"
1367449,2010.11119,Torsten Scholak,"Torsten Scholak, Raymond Li, Dzmitry Bahdanau, Harm de Vries, Chris
  Pal",DuoRAT: Towards Simpler Text-to-SQL Models,Code is available at https://github.com/ElementAI/duorat,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent research has shown that neural text-to-SQL models can effectively
translate natural language questions into corresponding SQL queries on unseen
databases. Working mostly on the Spider dataset, researchers have been
proposing increasingly sophisticated modelling approaches to the problem.
Contrary to this trend, in this paper we identify the aspects in which
text-to-SQL models can be simplified. We begin by building DuoRAT, a
re-implementation of the state-of-the-art RAT-SQL model that unlike RAT-SQL is
using only relation-aware or vanilla transformers as the building blocks. We
perform several ablation experiments using DuoRAT as the baseline model. Our
experiments confirm the usefulness of some of the techniques and point out the
redundancy of others, including structural SQL features and features that link
the question with the schema.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 16:27:49 GMT'}]",2020-10-22,"[['Scholak', 'Torsten', ''], ['Li', 'Raymond', ''], ['Bahdanau', 'Dzmitry', ''], ['de Vries', 'Harm', ''], ['Pal', 'Chris', '']]"
1367422,2010.11092,Rian Adam Rajagede,Rian Adam Rajagede and Rochana Prih Hastuti,Stacking Neural Network Models for Automatic Short Answer Scoring,"submitted to The 5th International Conference on Information
  Technology and Digital Applications 2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic short answer scoring is one of the text classification problems to
assess students' answers during exams automatically. Several challenges can
arise in making an automatic short answer scoring system, one of which is the
quantity and quality of the data. The data labeling process is not easy because
it requires a human annotator who is an expert in their field. Further, the
data imbalance process is also a challenge because the number of labels for
correct answers is always much less than the wrong answers. In this paper, we
propose the use of a stacking model based on neural network and XGBoost for
classification process with sentence embedding feature. We also propose to use
data upsampling method to handle imbalance classes and hyperparameters
optimization algorithm to find a robust model automatically. We use Ukara 1.0
Challenge dataset and our best model obtained an F1-score of 0.821 exceeding
the previous work at the same dataset.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 16:00:09 GMT'}]",2020-10-22,"[['Rajagede', 'Rian Adam', ''], ['Hastuti', 'Rochana Prih', '']]"
1367421,2010.11091,Mohiuddin Md Abdul Qudar,"Mohiuddin Md Abdul Qudar, Vijay Mago","TweetBERT: A Pretrained Language Representation Model for Twitter Text
  Analysis",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Twitter is a well-known microblogging social site where users express their
views and opinions in real-time. As a result, tweets tend to contain valuable
information. With the advancements of deep learning in the domain of natural
language processing, extracting meaningful information from tweets has become a
growing interest among natural language researchers. Applying existing language
representation models to extract information from Twitter does not often
produce good results. Moreover, there is no existing language representation
models for text analysis specific to the social media domain. Hence, in this
article, we introduce two TweetBERT models, which are domain specific language
presentation models, pre-trained on millions of tweets. We show that the
TweetBERT models significantly outperform the traditional BERT models in
Twitter text mining tasks by more than 7% on each Twitter dataset. We also
provide an extensive analysis by evaluating seven BERT models on 31 different
datasets. Our results validate our hypothesis that continuously training
language models on twitter corpus help performance with Twitter.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 00:45:02 GMT'}]",2020-10-22,"[['Qudar', 'Mohiuddin Md Abdul', ''], ['Mago', 'Vijay', '']]"
1367419,2010.11089,U\u{g}ur Merto\u{g}lu,U\u{g}ur Merto\u{g}lu Burkay Gen\c{c},Lexicon generation for detecting fake news,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  With the digitization of media, an immense amount of news data has been
generated by online sources, including mainstream media outlets as well as
social networks. However, the ease of production and distribution resulted in
circulation of fake news as well as credible, authentic news. The pervasive
dissemination of fake news has extreme negative impacts on individuals and
society. Therefore, fake news detection has recently become an emerging topic
as an interdisciplinary research field that is attracting significant attention
from many research disciplines, including social sciences and linguistics. In
this study, we propose a method primarily based on lexicons including a scoring
system to facilitate the detection of the fake news in Turkish. We contribute
to the literature by collecting a novel, large scale, and credible dataset of
Turkish news, and by constructing the first fake news detection lexicon for
Turkish.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 20:39:57 GMT'}]",2020-10-22,"[['Genç', 'Uğur Mertoğlu Burkay', '']]"
1367415,2010.11085,Sai Muralidhar Jayanthi,"Sai Muralidhar Jayanthi, Danish Pruthi, Graham Neubig",NeuSpell: A Neural Spelling Correction Toolkit,Accepted at EMNLP 2020 (system demonstrations),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce NeuSpell, an open-source toolkit for spelling correction in
English. Our toolkit comprises ten different models, and benchmarks them on
naturally occurring misspellings from multiple sources. We find that many
systems do not adequately leverage the context around the misspelt token. To
remedy this, (i) we train neural models using spelling errors in context,
synthetically constructed by reverse engineering isolated misspellings; and
(ii) use contextual representations. By training on our synthetic examples,
correction rates improve by 9% (absolute) compared to the case when models are
trained on randomly sampled character perturbations. Using richer contextual
representations boosts the correction rate by another 3%. Our toolkit enables
practitioners to use our proposed and existing spelling correction systems,
both via a unified command line, as well as a web interface. Among many
potential applications, we demonstrate the utility of our spell-checkers in
combating adversarial misspellings. The toolkit can be accessed at
neuspell.github.io. Code and pretrained models are available at
http://github.com/neuspell/neuspell.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 15:53:29 GMT'}]",2020-10-22,"[['Jayanthi', 'Sai Muralidhar', ''], ['Pruthi', 'Danish', ''], ['Neubig', 'Graham', '']]"
1367410,2010.11080,Tao Yu,"Tao Yu, Shafiq Joty",Online Conversation Disentanglement with Pointer Networks,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Huge amounts of textual conversations occur online every day, where multiple
conversations take place concurrently. Interleaved conversations lead to
difficulties in not only following the ongoing discussions but also extracting
relevant information from simultaneous messages. Conversation disentanglement
aims to separate intermingled messages into detached conversations. However,
existing disentanglement methods rely mostly on handcrafted features that are
dataset specific, which hinders generalization and adaptability. In this work,
we propose an end-to-end online framework for conversation disentanglement that
avoids time-consuming domain-specific feature engineering. We design a novel
way to embed the whole utterance that comprises timestamp, speaker, and message
text, and proposes a custom attention mechanism that models disentanglement as
a pointing problem while effectively capturing inter-utterance interactions in
an end-to-end fashion. We also introduce a joint-learning objective to better
capture contextual information. Our experiments on the Ubuntu IRC dataset show
that our method achieves state-of-the-art performance in both link and
conversation prediction tasks.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 15:43:07 GMT'}]",2020-10-22,"[['Yu', 'Tao', ''], ['Joty', 'Shafiq', '']]"
1319317,2007.07803,Anant Khandelwal,Anant Khandelwal,Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity,"10 pages, 2 figures, 6 tables; Accepted at ACM CoDS-COMAD 2021",,10.1145/3430984.3431007,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Increased usage of social media caused the popularity of news and events
which are not even verified, resulting in spread of rumors allover the web. Due
to widely available social media platforms and increased usage caused the data
to be available in huge amounts.The manual methods to process such large data
is costly and time-taking, so there has been an increased attention to process
and verify such content automatically for the presence of rumors. A lot of
research studies reveal that to identify the stances of posts in the discussion
thread of such events and news is an important preceding step before identify
the rumor veracity. In this paper,we propose a multi-task learning framework
for jointly predicting rumor stance and veracity on the dataset released at
SemEval 2019 RumorEval: Determining rumor veracity and support for
rumors(SemEval 2019 Task 7), which includes social media rumors stem from a
variety of breaking news stories from Reddit as well as Twit-ter. Our framework
consists of two parts: a) The bottom part of our framework classifies the
stance for each post in the conversation thread discussing a rumor via
modelling the multi-turn conversation and make each post aware of its
neighboring posts. b) The upper part predicts the rumor veracity of the
conversation thread with stance evolution obtained from the bottom part.
Experimental results on SemEval 2019 Task 7 dataset show that our method
outperforms previous methods on both rumor stance classification and veracity
prediction
","[{'version': 'v1', 'created': 'Wed, 15 Jul 2020 17:09:17 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 11:25:20 GMT'}]",2020-10-23,"[['Khandelwal', 'Anant', '']]"
1365058,2010.08728,Jin Xu,"Jin Xu, Yinuo Guo, Junfeng Hu","Incorporate Semantic Structures into Machine Translation Evaluation via
  UCCA",WMT2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Copying mechanism has been commonly used in neural paraphrasing networks and
other text generation tasks, in which some important words in the input
sequence are preserved in the output sequence. Similarly, in machine
translation, we notice that there are certain words or phrases appearing in all
good translations of one source text, and these words tend to convey important
semantic information. Therefore, in this work, we define words carrying
important semantic meanings in sentences as semantic core words. Moreover, we
propose an MT evaluation approach named Semantically Weighted Sentence
Similarity (SWSS). It leverages the power of UCCA to identify semantic core
words, and then calculates sentence similarity scores on the overlap of
semantic core words. Experimental results show that SWSS can consistently
improve the performance of popular MT evaluation metrics which are based on
lexical similarity.
","[{'version': 'v1', 'created': 'Sat, 17 Oct 2020 06:47:58 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 03:38:19 GMT'}]",2020-10-23,"[['Xu', 'Jin', ''], ['Guo', 'Yinuo', ''], ['Hu', 'Junfeng', '']]"
1142383,1906.10197,Kanishk Gandhi,Kanishk Gandhi and Brenden M. Lake,Mutual exclusivity as a challenge for deep neural networks,"Published in Advances in Neural Information Processing Systems
  (NeurIPS) 33",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Strong inductive biases allow children to learn in fast and adaptable ways.
Children use the mutual exclusivity (ME) bias to help disambiguate how words
map to referents, assuming that if an object has one label then it does not
need another. In this paper, we investigate whether or not standard neural
architectures have an ME bias, demonstrating that they lack this learning
assumption. Moreover, we show that their inductive biases are poorly matched to
lifelong learning formulations of classification and translation. We
demonstrate that there is a compelling case for designing neural networks that
reason by mutual exclusivity, which remains an open challenge.
","[{'version': 'v1', 'created': 'Mon, 24 Jun 2019 19:47:05 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Dec 2019 18:51:28 GMT'}, {'version': 'v3', 'created': 'Wed, 21 Oct 2020 21:50:33 GMT'}]",2020-10-23,"[['Gandhi', 'Kanishk', ''], ['Lake', 'Brenden M.', '']]"
1367934,2010.11604,Changzhen Ji,"Changzhen Ji, Conghui Zhu and Tiejun Zhao",AI-lead Court Debate Case Investigation,"4 pages, 2 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The multi-role judicial debate composed of the plaintiff, defendant, and
judge is an important part of the judicial trial. Different from other types of
dialogue, questions are raised by the judge, The plaintiff, plaintiff's agent
defendant, and defendant's agent would be to debating so that the trial can
proceed in an orderly manner. Question generation is an important task in
Natural Language Generation. In the judicial trial, it can help the judge raise
efficient questions so that the judge has a clearer understanding of the case.
In this work, we propose an innovative end-to-end question generation
model-Trial Brain Model (TBM) to build a Trial Brain, it can generate the
questions the judge wants to ask through the historical dialogue between the
plaintiff and the defendant. Unlike prior efforts in natural language
generation, our model can learn the judge's questioning intention through
predefined knowledge. We do experiments on real-world datasets, the
experimental results show that our model can provide a more accurate question
in the multi-role court debate scene.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 11:05:14 GMT'}]",2020-10-23,"[['Ji', 'Changzhen', ''], ['Zhu', 'Conghui', ''], ['Zhao', 'Tiejun', '']]"
1333495,2008.05773,Yu Wu,"Sanyuan Chen, Yu Wu, Zhuo Chen, Jian Wu, Jinyu Li, Takuya Yoshioka,
  Chengyi Wang, Shujie Liu, Ming Zhou",Continuous Speech Separation with Conformer,,,,,eess.AS cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Continuous speech separation plays a vital role in complicated speech related
tasks such as conversation transcription. The separation model extracts a
single speaker signal from a mixed speech. In this paper, we use transformer
and conformer in lieu of recurrent neural networks in the separation system, as
we believe capturing global information with the self-attention based method is
crucial for the speech separation. Evaluating on the LibriCSS dataset, the
conformer separation model achieves state of the art results, with a relative
23.5% word error rate (WER) reduction from bi-directional LSTM (BLSTM) in the
utterance-wise evaluation and a 15.4% WER reduction in the continuous
evaluation.
","[{'version': 'v1', 'created': 'Thu, 13 Aug 2020 09:36:05 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 12:38:51 GMT'}]",2020-10-23,"[['Chen', 'Sanyuan', ''], ['Wu', 'Yu', ''], ['Chen', 'Zhuo', ''], ['Wu', 'Jian', ''], ['Li', 'Jinyu', ''], ['Yoshioka', 'Takuya', ''], ['Wang', 'Chengyi', ''], ['Liu', 'Shujie', ''], ['Zhou', 'Ming', '']]"
1204524,1911.05930,Ruobing Xie,"Ruobing Xie, Yanan Lu, Fen Lin, Leyu Lin",FAQ-based Question Answering via Knowledge Anchors,"12 pages, accepted by NLPCC-2020",NLPCC-2020,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Question answering (QA) aims to understand questions and find appropriate
answers. In real-world QA systems, Frequently Asked Question (FAQ) based QA is
usually a practical and effective solution, especially for some complicated
questions (e.g., How and Why). Recent years have witnessed the great successes
of knowledge graphs (KGs) in KBQA systems, while there are still few works
focusing on making full use of KGs in FAQ-based QA. In this paper, we propose a
novel Knowledge Anchor based Question Answering (KAQA) framework for FAQ-based
QA to better understand questions and retrieve more appropriate answers. More
specifically, KAQA mainly consists of three modules: knowledge graph
construction, query anchoring and query-document matching. We consider entities
and triples of KGs in texts as knowledge anchors to precisely capture the core
semantics, which brings in higher precision and better interpretability. The
multi-channel matching strategy also enables most sentence matching models to
be flexibly plugged in our KAQA framework to fit different real-world
computation limitations. In experiments, we evaluate our models on both offline
and online query-document matching tasks on a real-world FAQ-based QA system in
WeChat Search, with detailed analysis, ablation tests and case studies. The
significant improvements confirm the effectiveness and robustness of the KAQA
framework in real-world FAQ-based QA.
","[{'version': 'v1', 'created': 'Thu, 14 Nov 2019 04:18:55 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 07:50:31 GMT'}]",2020-10-23,"[['Xie', 'Ruobing', ''], ['Lu', 'Yanan', ''], ['Lin', 'Fen', ''], ['Lin', 'Leyu', '']]"
1367923,2010.11593,Hari Krishna Vydana Mr,"Hari Krishna Vydana, Lukas Burget, Jan Cernocky",A Technical Report: BUT Speech Translation Systems,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The paper describes the BUT's speech translation systems. The systems are
English$\longrightarrow$German offline speech translation systems. The systems
are based on our previous works \cite{Jointly_trained_transformers}. Though
End-to-End and cascade~(ASR-MT) spoken language translation~(SLT) systems are
reaching comparable performances, a large degradation is observed when
translating ASR hypothesis compared to the oracle input text. To reduce this
performance degradation, we have jointly-trained ASR and MT modules with ASR
objective as an auxiliary loss. Both the networks are connected through the
neural hidden representations. This model has an End-to-End differentiable path
with respect to the final objective function and also utilizes the ASR
objective for better optimization. During the inference both the modules(i.e.,
ASR and MT) are connected through the hidden representations corresponding to
the n-best hypotheses. Ensembling with independently trained ASR and MT models
have further improved the performance of the system.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 10:52:31 GMT'}]",2020-10-23,"[['Vydana', 'Hari Krishna', ''], ['Burget', 'Lukas', ''], ['Cernocky', 'Jan', '']]"
1367908,2010.11578,Navita Goyal,"Navita Goyal, Balaji Vasan Srinivasan, Anandhavelu N, Abhilasha
  Sancheti","Multi-dimensional Style Transfer for Partially Annotated Data using
  Language Models as Discriminators",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Style transfer has been widely explored in natural language generation with
non-parallel corpus by directly or indirectly extracting a notion of style from
source and target domain corpus. A common aspect among the existing approaches
is the prerequisite of joint annotations across all the stylistic dimensions
under consideration. Availability of such dataset across a combination of
styles is a limiting factor in extending state-of-the art style transfer setups
to multiple style dimensions. While cascading single-dimensional models across
multiple styles is a possibility, it suffers from content loss, especially when
the style dimensions are not completely independent of each other. In our work,
we attempt to relax this restriction on requirement of jointly annotated data
across multiple styles being inspected and make use of independently acquired
data across different style dimensions without any additional annotations. We
initialize an encoder-decoder setup with large transformer-based language
models pre-trained on a generic corpus and enhance its re-writing capability to
multiple styles by employing multiple language models as discriminators.
Through quantitative and qualitative evaluation, we show the ability of our
model to control for styles across multiple style-dimensions while preserving
content of the input text and compare it against baselines which involve
cascaded state-of-the-art uni-dimensional style transfer models.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 10:16:29 GMT'}]",2020-10-23,"[['Goyal', 'Navita', ''], ['Srinivasan', 'Balaji Vasan', ''], ['N', 'Anandhavelu', ''], ['Sancheti', 'Abhilasha', '']]"
1367892,2010.11562,Amit Gajbhiye,"Amit Gajbhiye, Thomas Winterbottom, Noura Al Moubayed, and Steven
  Bradley",Bilinear Fusion of Commonsense Knowledge with Attention-Based NLI Models,"Published in Lecture Notes in Computer Science, Springer
  International Publishing",,10.1007/978-3-030-61609-0_50,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the task of incorporating real-world commonsense knowledge into
deep Natural Language Inference (NLI) models. Existing external knowledge
incorporation methods are limited to lexical level knowledge and lack
generalization across NLI models, datasets, and commonsense knowledge sources.
To address these issues, we propose a novel NLI model-independent neural
framework, BiCAM. BiCAM incorporates real-world commonsense knowledge into NLI
models. Combined with convolutional feature detectors and bilinear feature
fusion, BiCAM provides a conceptually simple mechanism that generalizes well.
Quantitative evaluations with two state-of-the-art NLI baselines on SNLI and
SciTail datasets in conjunction with ConceptNet and Aristo Tuple KGs show that
BiCAM considerably improves the accuracy the incorporated NLI baselines. For
example, our BiECAM model, an instance of BiCAM, on the challenging SciTail
dataset, improves the accuracy of incorporated baselines by 7.0% with
ConceptNet, and 8.0% with Aristo Tuple KG.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 09:38:08 GMT'}]",2020-10-23,"[['Gajbhiye', 'Amit', ''], ['Winterbottom', 'Thomas', ''], ['Moubayed', 'Noura Al', ''], ['Bradley', 'Steven', '']]"
1274117,2004.09347,Abhishek Niranjan,"Abhishek Niranjan, Mukesh Sharma, Sai Bharath Chandra Gutha, M Ali
  Basha Shaik","WHALETRANS: E2E WHisper to nAturaL spEech conversion using modified
  TRANSformer network",,,,,eess.AS cs.CL cs.LG cs.SD stat.ML,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We investigate whisper-to-natural-speech conversion using
sequence-to-sequence approach by proposing modified transformer architecture.
We investigate different features like mel frequency cepstral coefficients and
smoothed spectral features. The proposed networks are trained end-to-end using
supervised approach for feature-to-feature transformation. Further, We also
investigate the effectiveness of embedded auxiliary decoder used after N
encoder sub-layers, and is trained with the frame-level objective function for
identifying source phoneme labels. We show results on wTIMIT and CHAINS
datasets by measuring word error rate using end-to-end ASR and also BLEU scores
for the generated speech. In addition, we measure spectral shape of it by
measuring formant distributions w.r.t. the reference speech, as formant
divergence metric. We have found whisper-to-natural converted speech formants
probability distribution is similar to the ground-truth distribution. To the
authors' best knowledge, this is the first time modified transformer has been
applied for whisper-to-natural-speech conversion and vice versa.
","[{'version': 'v1', 'created': 'Mon, 20 Apr 2020 14:47:46 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 07:08:37 GMT'}]",2020-10-23,"[['Niranjan', 'Abhishek', ''], ['Sharma', 'Mukesh', ''], ['Gutha', 'Sai Bharath Chandra', ''], ['Shaik', 'M Ali Basha', '']]"
1367854,2010.11524,Tatiana Likhomanenko,"Tatiana Likhomanenko, Qiantong Xu, Jacob Kahn, Gabriel Synnaeve, Ronan
  Collobert",slimIPL: Language-Model-Free Iterative Pseudo-Labeling,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent results in end-to-end ASR have demonstrated the efficacy of simple
pseudo-labeling for semi-supervised models trained both with Connectionist
Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq) losses.
Iterative Pseudo-Labeling (IPL), which continuously trains a single model using
pseudo-labels iteratively re-generated as the model learns, has been shown to
further increase performance in ASR. We improve upon the IPL algorithm: as the
model learns, we propose to iteratively re-generate transcriptions with hard
labels (the most probable tokens) assignments, that is without a language
model. We call this approach Language-Model-Free IPL (slimIPL) and we give a
resultant training setup for CTC and seq2seq models. At inference, our
experiments show that decoding with a strong language model is more beneficial
with slimIPL than IPL, asIPL exhibits some language model over-fitting issues.
Compared to prior work on semi-supervised and unsupervised approaches, slimIPL
not only simplifies the training process, but also achieves competitive and
state-of-the-art results on LibriSpeech test sets in both standard and
low-resource settings.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 08:36:33 GMT'}]",2020-10-23,"[['Likhomanenko', 'Tatiana', ''], ['Xu', 'Qiantong', ''], ['Kahn', 'Jacob', ''], ['Synnaeve', 'Gabriel', ''], ['Collobert', 'Ronan', '']]"
1278659,2004.13889,Tasnim Mohiuddin,"Tasnim Mohiuddin, M Saiful Bari, and Shafiq Joty","LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon
  Induction Through Non-Linear Mapping in Latent Space",EMNLP 2020 accepted paper,,,,cs.CL cs.LG,http://creativecommons.org/publicdomain/zero/1.0/,"  Most of the successful and predominant methods for bilingual lexicon
induction (BLI) are mapping-based, where a linear mapping function is learned
with the assumption that the word embedding spaces of different languages
exhibit similar geometric structures (i.e., approximately isomorphic). However,
several recent studies have criticized this simplified assumption showing that
it does not hold in general even for closely related languages. In this work,
we propose a novel semi-supervised method to learn cross-lingual word
embeddings for BLI. Our model is independent of the isomorphic assumption and
uses nonlinear mapping in the latent space of two independently trained
auto-encoders. Through extensive experiments on fifteen (15) different language
pairs (in both directions) comprising resource-rich and low-resource languages
from two different datasets, we demonstrate that our method outperforms
existing models by a good margin. Ablation studies show the importance of
different model components and the necessity of non-linear mapping.
","[{'version': 'v1', 'created': 'Tue, 28 Apr 2020 23:28:26 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 00:42:16 GMT'}]",2020-10-23,"[['Mohiuddin', 'Tasnim', ''], ['Bari', 'M Saiful', ''], ['Joty', 'Shafiq', '']]"
1367883,2010.11553,Hrituraj Singh,"Hrituraj Singh, Gaurav Verma, Balaji Vasan Srinivasan","Incorporating Stylistic Lexical Preferences in Generative Language
  Models",To Appear in Findings of EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While recent advances in language modeling have resulted in powerful
generation models, their generation style remains implicitly dependent on the
training data and can not emulate a specific target style. Leveraging the
generative capabilities of a transformer-based language models, we present an
approach to induce certain target-author attributes by incorporating continuous
multi-dimensional lexical preferences of an author into generative language
models. We introduce rewarding strategies in a reinforcement learning framework
that encourages the use of words across multiple categorical dimensions, to
varying extents. Our experiments demonstrate that the proposed approach can
generate text that distinctively aligns with a given target author's lexical
style. We conduct quantitative and qualitative comparisons with competitive and
relevant baselines to illustrate the benefits of the proposed approach.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 09:24:05 GMT'}]",2020-10-23,"[['Singh', 'Hrituraj', ''], ['Verma', 'Gaurav', ''], ['Srinivasan', 'Balaji Vasan', '']]"
1367878,2010.11548,Artem Kramov,"S.D. Pogorilyy, A.A. Kramov",Method of noun phrase detection in Ukrainian texts,"25 pages, in Ukrainian, 5 figures, 2 tables",Control Systems and Computers. 2019. Issue 5. P. 48-59,10.15407/csc.2019.05.048,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Introduction. The area of natural language processing considers AI-complete
tasks that cannot be solved using traditional algorithmic actions. Such tasks
are commonly implemented with the usage of machine learning methodology and
means of computer linguistics. One of the preprocessing tasks of a text is the
search of noun phrases. The accuracy of this task has implications for the
effectiveness of many other tasks in the area of natural language processing.
In spite of the active development of research in the area of natural language
processing, the investigation of the search for noun phrases within Ukrainian
texts are still at an early stage. Results. The different methods of noun
phrases detection have been analyzed. The expediency of the representation of
sentences as a tree structure has been justified. The key disadvantage of many
methods of noun phrase detection is the severe dependence of the effectiveness
of their detection from the features of a certain language. Taking into account
the unified format of sentence processing and the availability of the trained
model for the building of sentence trees for Ukrainian texts, the Universal
Dependency model has been chosen. The complex method of noun phrases detection
in Ukrainian texts utilizing Universal Dependencies means and named-entity
recognition model has been suggested. Experimental verification of the
effectiveness of the suggested method on the corpus of Ukrainian news has been
performed. Different metrics of method accuracy have been calculated.
Conclusions. The results obtained can indicate that the suggested method can be
used to find noun phrases in Ukrainian texts. An accuracy increase of the
method can be made with the usage of appropriate named-entity recognition
models according to a subject area.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 09:20:24 GMT'}]",2020-10-23,"[['Pogorilyy', 'S. D.', ''], ['Kramov', 'A. A.', '']]"
1206964,1911.08370,Vladimir Vargas-Calder\'on,"Vladimir Vargas-Calder\'on and Nicol\'as Parra-A. and Jorge E. Camargo
  and Herbert Vinck-Posada","Event detection in Colombian security Twitter news using fine-grained
  latent topic analysis","pre-print exposed at CATAI (Bogot\'a, Colombia)",,,,cs.SI cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Cultural and social dynamics are important concepts that must be understood
in order to grasp what a community cares about. To that end, an excellent
source of information on what occurs in a community is the news, especially in
recent years, when mass media giants use social networks to communicate and
interact with their audience. In this work, we use a method to discover latent
topics in tweets from Colombian Twitter news accounts in order to identify the
most prominent events in the country. We pay particular attention to security,
violence and crime-related tweets because of the violent environment that
surrounds Colombian society. The latent topic discovery method that we use
builds vector representations of the tweets by using FastText and finds
clusters of tweets through the K-means clustering algorithm. The number of
clusters is found by measuring the $C_V$ coherence for a range of number of
topics of the Latent Dirichlet Allocation (LDA) model. We finally use Uniform
Manifold Approximation and Projection (UMAP) for dimensionality reduction to
visualise the tweets vectors. Once the clusters related to security, violence
and crime are identified, we proceed to apply the same method within each
cluster to perform a fine-grained analysis in which specific events mentioned
in the news are grouped together. Our method is able to discover event-specific
sets of news, which is the baseline to perform an extensive analysis of how
people engage in Twitter threads on the different types of news, with an
emphasis on security, violence and crime-related tweets.
","[{'version': 'v1', 'created': 'Tue, 19 Nov 2019 15:58:14 GMT'}]",2020-10-23,"[['Vargas-Calderón', 'Vladimir', ''], ['Parra-A.', 'Nicolás', ''], ['Camargo', 'Jorge E.', ''], ['Vinck-Posada', 'Herbert', '']]"
1366378,2010.10048,Renjie Zheng,"Renjie Zheng, Mingbo Ma, Baigong Zheng, Kaibo Liu, Jiahong Yuan,
  Kenneth Church, Liang Huang","Fluent and Low-latency Simultaneous Speech-to-Speech Translation with
  Self-adaptive Training","10 pages, accepted by Findings of EMNLP 2020",Findings of EMNLP 2020,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Simultaneous speech-to-speech translation is widely useful but extremely
challenging, since it needs to generate target-language speech concurrently
with the source-language speech, with only a few seconds delay. In addition, it
needs to continuously translate a stream of sentences, but all recent solutions
merely focus on the single-sentence scenario. As a result, current approaches
accumulate latencies progressively when the speaker talks faster, and introduce
unnatural pauses when the speaker talks slower. To overcome these issues, we
propose Self-Adaptive Translation (SAT) which flexibly adjusts the length of
translations to accommodate different source speech rates. At similar levels of
translation quality (as measured by BLEU), our method generates more fluent
target speech (as measured by the naturalness metric MOS) with substantially
lower latency than the baseline, in both Zh <-> En directions.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 06:02:15 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 19:12:17 GMT'}]",2020-10-23,"[['Zheng', 'Renjie', ''], ['Ma', 'Mingbo', ''], ['Zheng', 'Baigong', ''], ['Liu', 'Kaibo', ''], ['Yuan', 'Jiahong', ''], ['Church', 'Kenneth', ''], ['Huang', 'Liang', '']]"
1367869,2010.11539,Changzhen Ji,"Changzhen Ji, Xin Zhou, Yating Zhang, Xiaozhong Liu, Changlong Sun,
  Conghui Zhu and Tiejun Zhao",Cross Copy Network for Dialogue Generation,"11 pages, 4 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the past few years, audiences from different fields witness the
achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer
Generator Networks, and Transformer) to enhance dialogue content generation.
While content fluency and accuracy often serve as the major indicators for
model training, dialogue logics, carrying critical information for some
particular domains, are often ignored. Take customer service and court debate
dialogue as examples, compatible logics can be observed across different
dialogue instances, and this information can provide vital evidence for
utterance generation. In this paper, we propose a novel network architecture -
Cross Copy Networks(CCN) to explore the current dialog context and similar
dialogue instances' logical structure simultaneously. Experiments with two
tasks, court debate and customer service content generation, proved that the
proposed algorithm is superior to existing state-of-art content generation
models.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 09:03:23 GMT'}]",2020-10-23,"[['Ji', 'Changzhen', ''], ['Zhou', 'Xin', ''], ['Zhang', 'Yating', ''], ['Liu', 'Xiaozhong', ''], ['Sun', 'Changlong', ''], ['Zhu', 'Conghui', ''], ['Zhao', 'Tiejun', '']]"
1367904,2010.11574,Jan Christian Blaise Cruz,"Jan Christian Blaise Cruz, Jose Kristian Resabal, James Lin, Dan John
  Velasco and Charibeth Cheng","Investigating the True Performance of Transformers in Low-Resource
  Languages: A Case Study in Automatic Corpus Creation","Code and data available at
  https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Transformers represent the state-of-the-art in Natural Language Processing
(NLP) in recent years, proving effective even in tasks done in low-resource
languages. While pretrained transformers for these languages can be made, it is
challenging to measure their true performance and capacity due to the lack of
hard benchmark datasets, as well as the difficulty and cost of producing them.
In this paper, we present three contributions: First, we propose a methodology
for automatically producing Natural Language Inference (NLI) benchmark datasets
for low-resource languages using published news articles. Through this, we
create and release NewsPH-NLI, the first sentence entailment benchmark dataset
in the low-resource Filipino language. Second, we produce new pretrained
transformers based on the ELECTRA technique to further alleviate the resource
scarcity in Filipino, benchmarking them on our dataset against other
commonly-used transfer learning techniques. Lastly, we perform analyses on
transfer learning techniques to shed light on their true performance when
operating in low-data domains through the use of degradation tests.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 10:09:10 GMT'}]",2020-10-23,"[['Cruz', 'Jan Christian Blaise', ''], ['Resabal', 'Jose Kristian', ''], ['Lin', 'James', ''], ['Velasco', 'Dan John', ''], ['Cheng', 'Charibeth', '']]"
1363409,2010.07079,Emily Dinan,"Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, Emily Dinan",Recipes for Safety in Open-domain Chatbots,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Models trained on large unlabeled corpora of human interactions will learn
patterns and mimic behaviors therein, which include offensive or otherwise
toxic behavior and unwanted biases. We investigate a variety of methods to
mitigate these issues in the context of open-domain generative dialogue models.
We introduce a new human-and-model-in-the-loop framework for both training
safer models and for evaluating them, as well as a novel method to distill
safety considerations inside generative models without the use of an external
classifier at deployment time. We conduct experiments comparing these methods
and find our new techniques are (i) safer than existing models as measured by
automatic and human evaluations while (ii) maintaining usability metrics such
as engagingness relative to the state of the art. We then discuss the
limitations of this work by analyzing failure cases of our models.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 13:26:39 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 16:56:50 GMT'}]",2020-10-23,"[['Xu', 'Jing', ''], ['Ju', 'Da', ''], ['Li', 'Margaret', ''], ['Boureau', 'Y-Lan', ''], ['Weston', 'Jason', ''], ['Dinan', 'Emily', '']]"
1305991,2006.11477,Michael Auli,"Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli","wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
  Representations",,,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We show for the first time that learning powerful representations from speech
audio alone followed by fine-tuning on transcribed speech can outperform the
best semi-supervised methods while being conceptually simpler. wav2vec 2.0
masks the speech input in the latent space and solves a contrastive task
defined over a quantization of the latent representations which are jointly
learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER
on the clean/other test sets. When lowering the amount of labeled data to one
hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour
subset while using 100 times less labeled data. Using just ten minutes of
labeled data and pre-training on 53k hours of unlabeled data still achieves
4.8/8.2 WER. This demonstrates the feasibility of speech recognition with
limited amounts of labeled data.
","[{'version': 'v1', 'created': 'Sat, 20 Jun 2020 02:35:02 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Sep 2020 04:26:03 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Oct 2020 06:09:10 GMT'}]",2020-10-23,"[['Baevski', 'Alexei', ''], ['Zhou', 'Henry', ''], ['Mohamed', 'Abdelrahman', ''], ['Auli', 'Michael', '']]"
1362320,2010.05990,Jordan J. Bird,"Jordan J. Bird, Anik\'o Ek\'art, Diego R. Faria","Chatbot Interaction with Artificial Intelligence: Human Data
  Augmentation with T5 and Language Transformer Ensemble for Text
  Classification","18 pages, 10 figures, 8 tables",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we present the Chatbot Interaction with Artificial Intelligence
(CI-AI) framework as an approach to the training of deep learning chatbots for
task classification. The intelligent system augments human-sourced data via
artificial paraphrasing in order to generate a large set of training data for
further classical, attention, and language transformation-based learning
approaches for Natural Language Processing. Human beings are asked to
paraphrase commands and questions for task identification for further execution
of a machine. The commands and questions are split into training and validation
sets. A total of 483 responses were recorded. Secondly, the training set is
paraphrased by the T5 model in order to augment it with further data. Seven
state-of-the-art transformer-based text classification algorithms (BERT,
DistilBERT, RoBERTa, DistilRoBERTa, XLM, XLM-RoBERTa, and XLNet) are
benchmarked for both sets after fine-tuning on the training data for two
epochs. We find that all models are improved when training data is augmented by
the T5 model, with an average increase of classification accuracy by 4.01%. The
best result was the RoBERTa model trained on T5 augmented data which achieved
98.96% classification accuracy. Finally, we found that an ensemble of the five
best-performing transformer models via Logistic Regression of output label
predictions led to an accuracy of 99.59% on the dataset of human responses. A
highly-performing model allows the intelligent system to interpret human
commands at the social-interaction level through a chatbot-like interface (e.g.
""Robot, can we have a conversation?"") and allows for better accessibility to AI
by non-technical users.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 19:37:18 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 14:33:08 GMT'}]",2020-10-23,"[['Bird', 'Jordan J.', ''], ['Ekárt', 'Anikó', ''], ['Faria', 'Diego R.', '']]"
1338905,2008.11183,Vladimir Vargas-Calder\'on,"Vladimir Vargas-Calder\'on and Juan S. Fl\'orez and Leonel F. Ardila
  and Nicolas Parra-A. and Jorge E. Camargo and Nelson Vargas",Learning from students' perception on professors through opinion mining,,,,,cs.CL cs.CY cs.NA math.NA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Students' perception of classes measured through their opinions on teaching
surveys allows to identify deficiencies and problems, both in the environment
and in the learning methodologies. The purpose of this paper is to study,
through sentiment analysis using natural language processing (NLP) and machine
learning (ML) techniques, those opinions in order to identify topics that are
relevant for students, as well as predicting the associated sentiment via
polarity analysis. As a result, it is implemented, trained and tested two
algorithms to predict the associated sentiment as well as the relevant topics
of such opinions. The combination of both approaches then becomes useful to
identify specific properties of the students' opinions associated with each
sentiment label (positive, negative or neutral opinions) and topic.
Furthermore, we explore the possibility that students' perception surveys are
carried out without closed questions, relying on the information that students
can provide through open questions where they express their opinions about
their classes.
","[{'version': 'v1', 'created': 'Tue, 25 Aug 2020 17:36:45 GMT'}]",2020-10-23,"[['Vargas-Calderón', 'Vladimir', ''], ['Flórez', 'Juan S.', ''], ['Ardila', 'Leonel F.', ''], ['Parra-A.', 'Nicolas', ''], ['Camargo', 'Jorge E.', ''], ['Vargas', 'Nelson', '']]"
1368077,2010.11747,Ivana Kvapilikova,"Ivana Kvapil\'ikov\'a, Tom Kocmi, Ond\v{r}ej Bojar","CUNI Systems for the Unsupervised and Very Low Resource Translation Task
  in WMT20",WMT20,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a description of CUNI systems submitted to the WMT20 task
on unsupervised and very low-resource supervised machine translation between
German and Upper Sorbian. We experimented with training on synthetic data and
pre-training on a related language pair. In the fully unsupervised scenario, we
achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian,
respectively. Our low-resource systems relied on transfer learning from
German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an
improvement of 10 BLEU points over the baseline trained only on the available
small German-Upper Sorbian parallel corpus.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 14:04:01 GMT'}]",2020-10-23,"[['Kvapilíková', 'Ivana', ''], ['Kocmi', 'Tom', ''], ['Bojar', 'Ondřej', '']]"
1298666,2006.04152,Canwen Xu,"Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu
  and Furu Wei",BERT Loses Patience: Fast and Robust Inference with Early Exit,NeurIPS 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose Patience-based Early Exit, a straightforward yet
effective inference method that can be used as a plug-and-play technique to
simultaneously improve the efficiency and robustness of a pretrained language
model (PLM). To achieve this, our approach couples an internal-classifier with
each layer of a PLM and dynamically stops inference when the intermediate
predictions of the internal classifiers remain unchanged for a pre-defined
number of steps. Our approach improves inference efficiency as it allows the
model to make a prediction with fewer layers. Meanwhile, experimental results
with an ALBERT model show that our method can improve the accuracy and
robustness of the model by preventing it from overthinking and exploiting
multiple classifiers for prediction, yielding a better accuracy-speed trade-off
compared to existing early exit methods.
","[{'version': 'v1', 'created': 'Sun, 7 Jun 2020 13:38:32 GMT'}, {'version': 'v2', 'created': 'Mon, 29 Jun 2020 04:46:19 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Oct 2020 06:37:36 GMT'}]",2020-10-23,"[['Zhou', 'Wangchunshu', ''], ['Xu', 'Canwen', ''], ['Ge', 'Tao', ''], ['McAuley', 'Julian', ''], ['Xu', 'Ke', ''], ['Wei', 'Furu', '']]"
1342550,2009.01047,Vahid Behzadan,Bibek Upadhayay and Vahid Behzadan,"Sentimental LIAR: Extended Corpus and Deep Learning Models for Fake
  Claim Classification",Accepted for publication in the proceedings of IEEE ISI '20,,,,cs.CL cs.LG cs.SI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rampant integration of social media in our every day lives and culture
has given rise to fast and easier access to the flow of information than ever
in human history. However, the inherently unsupervised nature of social media
platforms has also made it easier to spread false information and fake news.
Furthermore, the high volume and velocity of information flow in such platforms
make manual supervision and control of information propagation infeasible. This
paper aims to address this issue by proposing a novel deep learning approach
for automated detection of false short-text claims on social media. We first
introduce Sentimental LIAR, which extends the LIAR dataset of short claims by
adding features based on sentiment and emotion analysis of claims. Furthermore,
we propose a novel deep learning architecture based on the BERT-Base language
model for classification of claims as genuine or fake. Our results demonstrate
that the proposed architecture trained on Sentimental LIAR can achieve an
accuracy of 70%, which is an improvement of ~30% over previously reported
results for the LIAR benchmark.
","[{'version': 'v1', 'created': 'Tue, 1 Sep 2020 02:48:11 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 04:57:21 GMT'}]",2020-10-23,"[['Upadhayay', 'Bibek', ''], ['Behzadan', 'Vahid', '']]"
1347870,2009.06367,Benjamin Krause,"Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish
  Keskar, Shafiq Joty, Richard Socher, Nazneen Fatema Rajani",GeDi: Generative Discriminator Guided Sequence Generation,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While large-scale language models (LMs) are able to imitate the distribution
of natural language well enough to generate realistic text, it is difficult to
control which regions of the distribution they generate. This is especially
problematic because datasets used for training large LMs usually contain
significant toxicity, hate, bias, and negativity. We propose GeDi as an
efficient method for using smaller LMs as generative discriminators to guide
generation from large LMs to make them safer and more controllable. GeDi guides
generation at each step by computing classification probabilities for all
possible next tokens via Bayes rule by normalizing over two class-conditional
distributions; one conditioned on the desired attribute, or control code, and
another conditioned on the undesired attribute, or anti control code. We find
that GeDi gives stronger controllability than the state of the art method while
also achieving generation speeds more than 30 times faster. Additionally,
training GeDi on only four topics allows us to controllably generate new topics
zero-shot from just a keyword, unlocking a new capability that previous
controllable generation methods do not have. Lastly, we show that GeDi can make
GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic
quality, making it by far the most practical existing method for detoxifying
large language models while maintaining a fast generation speed.
","[{'version': 'v1', 'created': 'Mon, 14 Sep 2020 17:45:36 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 14:14:09 GMT'}]",2020-10-23,"[['Krause', 'Ben', ''], ['Gotmare', 'Akhilesh Deepak', ''], ['McCann', 'Bryan', ''], ['Keskar', 'Nitish Shirish', ''], ['Joty', 'Shafiq', ''], ['Socher', 'Richard', ''], ['Rajani', 'Nazneen Fatema', '']]"
1348326,2009.06823,Zhenglun Kong,"Wei Niu, Zhenglun Kong, Geng Yuan, Weiwen Jiang, Jiexiong Guan, Caiwen
  Ding, Pu Zhao, Sijia Liu, Bin Ren, Yanzhi Wang",Real-Time Execution of Large-scale Language Models on Mobile,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained large-scale language models have increasingly demonstrated high
accuracy on many natural language processing (NLP) tasks. However, the limited
weight storage and computational speed on hardware platforms have impeded the
popularity of pre-trained models, especially in the era of edge computing. In
this paper, we seek to find the best model structure of BERT for a given
computation size to match specific devices. We propose the first compiler-aware
neural architecture optimization framework. Our framework can guarantee the
identified model to meet both resource and real-time specifications of mobile
devices, thus achieving real-time execution of large transformer-based models
like BERT variants. We evaluate our model on several NLP tasks, achieving
competitive results on well-known benchmarks with lower latency on mobile
devices. Specifically, our model is 5.2x faster on CPU and 4.1x faster on GPU
with 0.5-2% accuracy loss compared with BERT-base. Our overall framework
achieves up to 7.8x speedup compared with TensorFlow-Lite with only minor
accuracy loss.
","[{'version': 'v1', 'created': 'Tue, 15 Sep 2020 01:59:17 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 17:53:07 GMT'}]",2020-10-23,"[['Niu', 'Wei', ''], ['Kong', 'Zhenglun', ''], ['Yuan', 'Geng', ''], ['Jiang', 'Weiwen', ''], ['Guan', 'Jiexiong', ''], ['Ding', 'Caiwen', ''], ['Zhao', 'Pu', ''], ['Liu', 'Sijia', ''], ['Ren', 'Bin', ''], ['Wang', 'Yanzhi', '']]"
1349119,2009.07616,Junfan Chen,"Junfan Chen, Richong Zhang, Yongyi Mao, Jie Xu",Parallel Interactive Networks for Multi-Domain Dialogue State Generation,Accepted by EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The dependencies between system and user utterances in the same turn and
across different turns are not fully considered in existing multidomain
dialogue state tracking (MDST) models. In this study, we argue that the
incorporation of these dependencies is crucial for the design of MDST and
propose Parallel Interactive Networks (PIN) to model these dependencies.
Specifically, we integrate an interactive encoder to jointly model the in-turn
dependencies and cross-turn dependencies. The slot-level context is introduced
to extract more expressive features for different slots. And a distributed copy
mechanism is utilized to selectively copy words from historical system
utterances or historical user utterances. Empirical studies demonstrated the
superiority of the proposed PIN model.
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 11:54:15 GMT'}, {'version': 'v2', 'created': 'Sat, 3 Oct 2020 07:32:21 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Oct 2020 14:19:12 GMT'}]",2020-10-23,"[['Chen', 'Junfan', ''], ['Zhang', 'Richong', ''], ['Mao', 'Yongyi', ''], ['Xu', 'Jie', '']]"
1368075,2010.11745,Gabriel Synnaeve,"Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello,
  Jacob Kahn, Gilad Avidov, Ronan Collobert, Gabriel Synnaeve",Rethinking Evaluation in ASR: Are Our Models Robust Enough?,,,,,cs.LG cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Is pushing numbers on a single benchmark valuable in automatic speech
recognition? Research results in acoustic modeling are typically evaluated
based on performance on a single dataset. While the research community has
coalesced around various benchmarks, we set out to understand generalization
performance in acoustic modeling across datasets -- in particular, if models
trained on a single dataset transfer to other (possibly out-of-domain)
datasets. Further, we demonstrate that when a large enough set of benchmarks is
used, average word error rate (WER) performance over them provides a good proxy
for performance on real-world data. Finally, we show that training a single
acoustic model on the most widely-used datasets -- combined -- reaches
competitive performance on both research and real-world benchmarks.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 14:01:32 GMT'}]",2020-10-23,"[['Likhomanenko', 'Tatiana', ''], ['Xu', 'Qiantong', ''], ['Pratap', 'Vineel', ''], ['Tomasello', 'Paden', ''], ['Kahn', 'Jacob', ''], ['Avidov', 'Gilad', ''], ['Collobert', 'Ronan', ''], ['Synnaeve', 'Gabriel', '']]"
1367969,2010.11639,Li-Hsin Chang,"Li-Hsin Chang, Sampo Pyysalo, Jenna Kanerva, Filip Ginter",Towards Fully Bilingual Deep Language Modeling,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language models based on deep neural networks have facilitated great advances
in natural language processing and understanding tasks in recent years. While
models covering a large number of languages have been introduced, their
multilinguality has come at a cost in terms of monolingual performance, and the
best-performing models at most tasks not involving cross-lingual transfer
remain monolingual. In this paper, we consider the question of whether it is
possible to pre-train a bilingual model for two remotely related languages
without compromising performance at either language. We collect pre-training
data, create a Finnish-English bilingual BERT model and evaluate its
performance on datasets used to evaluate the corresponding monolingual models.
Our bilingual model performs on par with Google's original English BERT on GLUE
and nearly matches the performance of monolingual Finnish BERT on a range of
Finnish NLP tasks, clearly outperforming multilingual BERT. We find that when
the model vocabulary size is increased, the BERT-Base architecture has
sufficient capacity to learn two remotely related languages to a level where it
achieves comparable performance with monolingual models, demonstrating the
feasibility of training fully bilingual deep language models. The model and all
tools involved in its creation are freely available at
https://github.com/TurkuNLP/biBERT
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 12:22:50 GMT'}]",2020-10-23,"[['Chang', 'Li-Hsin', ''], ['Pyysalo', 'Sampo', ''], ['Kanerva', 'Jenna', ''], ['Ginter', 'Filip', '']]"
1355405,2009.13902,Soujanya Poria,"Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Soujanya Poria",Utterance-level Dialogue Understanding: An Empirical Study,,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  The recent abundance of conversational data on the Web and elsewhere calls
for effective NLP systems for dialog understanding. Complete utterance-level
understanding often requires context understanding, defined by nearby
utterances. In recent years, a number of approaches have been proposed for
various utterance-level dialogue understanding tasks. Most of these approaches
account for the context for effective understanding. In this paper, we explore
and quantify the role of context for different aspects of a dialogue, namely
emotion, intent, and dialogue act identification, using state-of-the-art dialog
understanding methods as baselines. Specifically, we employ various
perturbations to distort the context of a given utterance and study its impact
on the different tasks and baselines. This provides us with insights into the
fundamental contextual controlling factors of different aspects of a dialogue.
Such insights can inspire more effective dialogue understanding models, and
provide support for future text generation approaches. The implementation
pertaining to this work is available at
https://github.com/declare-lab/dialogue-understanding.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 09:50:21 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Oct 2020 16:12:59 GMT'}, {'version': 'v3', 'created': 'Sun, 11 Oct 2020 14:38:07 GMT'}, {'version': 'v4', 'created': 'Mon, 19 Oct 2020 03:03:05 GMT'}, {'version': 'v5', 'created': 'Thu, 22 Oct 2020 11:16:56 GMT'}]",2020-10-23,"[['Ghosal', 'Deepanway', ''], ['Majumder', 'Navonil', ''], ['Mihalcea', 'Rada', ''], ['Poria', 'Soujanya', '']]"
1357391,2010.01061,Nils Rethmeier,Nils Rethmeier and Isabelle Augenstein,"Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and
  for Small Data",added citations to current work,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  For natural language processing (NLP) tasks such as sentiment or topic
classification, currently prevailing approaches heavily rely on pretraining
large self-supervised models on massive external data resources. However, this
methodology is being critiqued for: exceptional compute and pretraining data
requirements; diminishing returns on both large and small datasets; and
importantly, favourable evaluation settings that overestimate performance
differences. The core belief behind current methodology, coined `the bitter
lesson' by R. Sutton, is that `compute scale-up beats data and
compute-efficient algorithms', neglecting that progress in compute hardware
scale-up is based almost entirely on the miniaturisation of resource
consumption. We thus approach pretraining from a miniaturisation perspective,
such as not to require massive external data sources and models, or learned
translations from continuous input embeddings to discrete labels. To minimise
overly favourable evaluation, we examine learning on a long-tailed,
low-resource, multi-label text classification dataset with noisy, highly sparse
labels and many rare concepts. To this end, we propose a novel
`dataset-internal' contrastive autoencoding approach to self-supervised
pretraining and demonstrate marked improvements in zero-shot, few-shot and
solely supervised learning performance; even under an unfavorable low-resource
scenario, and without defaulting to large-scale external datasets for
self-supervision. We also find empirical evidence that zero and few-shot
learning markedly benefit from adding more `dataset-internal', self-supervised
training signals, which is of practical importance when retrieving or computing
on large external sources of such signals is infeasible.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 15:41:57 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 18:24:13 GMT'}]",2020-10-23,"[['Rethmeier', 'Nils', ''], ['Augenstein', 'Isabelle', '']]"
1368031,2010.11701,Philipp Sadler,Philipp Sadler,Spatial Attention as an Interface for Image Captioning Models,"A thesis submitted in fulfillment of the requirements for the degree
  Master of Science in Cognitive Systems",,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The internal workings of modern deep learning models stay often unclear to an
external observer, although spatial attention mechanisms are involved. The idea
of this work is to translate these spatial attentions into natural language to
provide a simpler access to the model's function. Thus, I took a neural image
captioning model and measured the reactions to external modification in its
spatial attention for three different interface methods: a fixation over the
whole generation process, a fixation for the first time-steps and an addition
to the generator's attention. The experimental results for bounding box based
spatial attention vectors have shown that the captioning model reacts to method
dependent changes in up to 52.65% and includes in 9.00% of the cases object
categories, which were otherwise unmentioned. Afterwards, I established such a
link to a hierarchical co-attention network for visual question answering by
extraction of its word, phrase and question level spatial attentions. Here,
generated captions for the word level included details of the question-answer
pairs in up to 55.20% of the cases. This work indicates that spatial attention
seen as an external interface for image caption generators is an useful method
to access visual functions in natural language.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 16:04:08 GMT'}]",2020-10-23,"[['Sadler', 'Philipp', '']]"
1185948,1910.02029,Arun Balajee Vasudevan,"Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool","Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention
  and Spatial Memory","Accepted to IJCV 2020, 20 pages, 10 Figures, Demo Video:
  https://people.ee.ethz.ch/~arunv/resources/talk2nav.mp4",,10.1007/s11263-020-01374-3,,cs.CV cs.CL cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The role of robots in society keeps expanding, bringing with it the necessity
of interacting and communicating with humans. In order to keep such interaction
intuitive, we provide automatic wayfinding based on verbal navigational
instructions. Our first contribution is the creation of a large-scale dataset
with verbal navigation instructions. To this end, we have developed an
interactive visual navigation environment based on Google Street View; we
further design an annotation method to highlight mined anchor landmarks and
local directions between them in order to help annotators formulate typical,
human references to those. The annotation task was crowdsourced on the AMT
platform, to construct a new Talk2Nav dataset with $10,714$ routes. Our second
contribution is a new learning method. Inspired by spatial cognition research
on the mental conceptualization of navigational instructions, we introduce a
soft dual attention mechanism defined over the segmented language instructions
to jointly extract two partial instructions -- one for matching the next
upcoming visual landmark and the other for matching the local directions to the
next landmark. On the similar lines, we also introduce spatial memory scheme to
encode the local directional transitions. Our work takes advantage of the
advance in two lines of research: mental formalization of verbal navigational
instructions and training neural network agents for automatic way finding.
Extensive experiments show that our method significantly outperforms previous
navigation methods. For demo video, dataset and code, please refer to our
project page: https://www.trace.ethz.ch/publications/2019/talk2nav/index.html
","[{'version': 'v1', 'created': 'Fri, 4 Oct 2019 16:44:59 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Feb 2020 11:25:09 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Oct 2020 12:03:18 GMT'}]",2020-10-23,"[['Vasudevan', 'Arun Balajee', ''], ['Dai', 'Dengxin', ''], ['Van Gool', 'Luc', '']]"
1368013,2010.11683,Xiang Dai,Xiang Dai and Heike Adel,An Analysis of Simple Data Augmentation for Named Entity Recognition,COLING 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Simple yet effective data augmentation techniques have been proposed for
sentence-level and sentence-pair natural language processing tasks. Inspired by
these efforts, we design and compare data augmentation for named entity
recognition, which is usually modeled as a token-level sequence labeling
problem. Through experiments on two data sets from the biomedical and materials
science domains (i2b2-2010 and MaSciP), we show that simple augmentation can
boost performance for both recurrent and transformer-based models, especially
for small training sets.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 13:21:03 GMT'}]",2020-10-23,"[['Dai', 'Xiang', ''], ['Adel', 'Heike', '']]"
1361469,2010.05139,Yiran Chen,"Yiran Chen, Pengfei Liu, Ming Zhong, Zi-Yi Dou, Danqing Wang, Xipeng
  Qiu and Xuanjing Huang","CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural
  Summarization Systems","13 pages, Findings of EMNLP2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural network-based models augmented with unsupervised pre-trained knowledge
have achieved impressive performance on text summarization. However, most
existing evaluation methods are limited to an in-domain setting, where
summarizers are trained and evaluated on the same dataset. We argue that this
approach can narrow our understanding of the generalization ability for
different summarization systems. In this paper, we perform an in-depth analysis
of characteristics of different datasets and investigate the performance of
different summarization models under a cross-dataset setting, in which a
summarizer trained on one corpus will be evaluated on a range of out-of-domain
corpora. A comprehensive study of 11 representative summarization systems on 5
datasets from different domains reveals the effect of model architectures and
generation ways (i.e. abstractive and extractive) on model generalization
ability. Further, experimental results shed light on the limitations of
existing summarizers. Brief introduction and supplementary code can be found in
https://github.com/zide05/CDEvalSumm.
","[{'version': 'v1', 'created': 'Sun, 11 Oct 2020 02:19:15 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 12:11:46 GMT'}]",2020-10-23,"[['Chen', 'Yiran', ''], ['Liu', 'Pengfei', ''], ['Zhong', 'Ming', ''], ['Dou', 'Zi-Yi', ''], ['Wang', 'Danqing', ''], ['Qiu', 'Xipeng', ''], ['Huang', 'Xuanjing', '']]"
1361808,2010.05478,Tanya Goyal,"Tanya Goyal, Greg Durrett",Evaluating Factuality in Generation with Dependency-level Entailment,Findings of Emnlp 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite significant progress in text generation models, a serious limitation
is their tendency to produce text that is factually inconsistent with
information in the input. Recent work has studied whether textual entailment
systems can be used to identify factual errors; however, these sentence-level
entailment models are trained to solve a different problem than generation
filtering and they do not localize which part of a generation is non-factual.
In this paper, we propose a new formulation of entailment that decomposes it at
the level of dependency arcs. Rather than focusing on aggregate decisions, we
instead ask whether the semantic relationship manifested by individual
dependency arcs in the generated output is supported by the input. Human
judgments on this task are difficult to obtain; we therefore propose a method
to automatically create data based on existing entailment or paraphrase
corpora. Experiments show that our dependency arc entailment model trained on
this data can identify factual inconsistencies in paraphrasing and
summarization better than sentence-level methods or those based on question
generation, while additionally localizing the erroneous parts of the
generation.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 06:43:10 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 06:35:58 GMT'}]",2020-10-23,"[['Goyal', 'Tanya', ''], ['Durrett', 'Greg', '']]"
1362070,2010.05740,Yanjie Gou,"Yanjie Gou, Yinjie Lei, Lingqiao Liu","Contextualize Knowledge Bases with Transformer for End-to-end
  Task-Oriented Dialogue Systems",Third version of this work; Correct some typos,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies try to build task-oriented dialogue systems in an end-to-end
manner and the existing works make great progress on this task. However, there
is still an issue need to be further considered, i.e., how to effectively
represent the knowledge bases and incorporate that into dialogue systems. To
solve this issue, we design a novel Transformer-based Context-aware Memory
Generator to model the entities in knowledge bases, which can produce entity
representations with perceiving all the relevant entities and dialogue history.
Furthermore, we propose Context-aware Memory Enhanced Transformer (CMET), which
can effectively aggregate information from the dialogue history and knowledge
bases to generate more accurate responses. Through extensive experiments, our
method can achieve superior performance over the state-of-the-art methods.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 14:34:07 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 09:37:22 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Oct 2020 13:37:40 GMT'}]",2020-10-23,"[['Gou', 'Yanjie', ''], ['Lei', 'Yinjie', ''], ['Liu', 'Lingqiao', '']]"
1367996,2010.11666,Pavel Kalaidin,"Nadezhda Zueva, Madina Kabirova, Pavel Kalaidin",Reducing Unintended Identity Bias in Russian Hate Speech Detection,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Toxicity has become a grave problem for many online communities and has been
growing across many languages, including Russian. Hate speech creates an
environment of intimidation, discrimination, and may even incite some
real-world violence. Both researchers and social platforms have been focused on
developing models to detect toxicity in online communication for a while now. A
common problem of these models is the presence of bias towards some words (e.g.
woman, black, jew) that are not toxic, but serve as triggers for the classifier
due to model caveats. In this paper, we describe our efforts towards
classifying hate speech in Russian, and propose simple techniques of reducing
unintended bias, such as generating training data with language models using
terms and words related to protected identities as context and applying word
dropout to such words.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 12:54:14 GMT'}]",2020-10-23,"[['Zueva', 'Nadezhda', ''], ['Kabirova', 'Madina', ''], ['Kalaidin', 'Pavel', '']]"
1368061,2010.11731,Akbar Karimi,"Akbar Karimi, Leonardo Rossi, Andrea Prati",Improving BERT Performance for Aspect-Based Sentiment Analysis,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect-Based Sentiment Analysis (ABSA) studies the consumer opinion on the
market products. It involves examining the type of sentiments as well as
sentiment targets expressed in product reviews. Analyzing the language used in
a review is a difficult task that requires a deep understanding of the
language. In recent years, deep language models, such as BERT
\cite{devlin2019bert}, have shown great progress in this regard. In this work,
we propose two simple modules called Parallel Aggregation and Hierarchical
Aggregation to be utilized on top of BERT for two main ABSA tasks namely Aspect
Extraction (AE) and Aspect Sentiment Classification (ASC) in order to improve
the model's performance. We show that applying the proposed models eliminates
the need for further training of the BERT model. The source code is available
on the Web for further research and reproduction of the results.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 13:52:18 GMT'}]",2020-10-23,"[['Karimi', 'Akbar', ''], ['Rossi', 'Leonardo', ''], ['Prati', 'Andrea', '']]"
1367852,2010.11522,Jiaoyan Chen,"Ziheng Zhang and Jiaoyan Chen and Xi Chen and Hualuo Liu and Yuejia
  Xiang and Bo Liu and Yefeng Zheng",An Industry Evaluation of Embedding-based Entity Alignment,,Coling'2020,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Embedding-based entity alignment has been widely investigated in recent
years, but most proposed methods still rely on an ideal supervised learning
setting with a large number of unbiased seed mappings for training and
validation, which significantly limits their usage. In this study, we evaluate
those state-of-the-art methods in an industrial context, where the impact of
seed mappings with different sizes and different biases is explored. Besides
the popular benchmarks from DBpedia and Wikidata, we contribute and evaluate a
new industrial benchmark that is extracted from two heterogeneous knowledge
graphs (KGs) under deployment for medical applications. The experimental
results enable the analysis of the advantages and disadvantages of these
alignment methods and the further discussion of suitable strategies for their
industrial deployment.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 08:33:58 GMT'}]",2020-10-23,"[['Zhang', 'Ziheng', ''], ['Chen', 'Jiaoyan', ''], ['Chen', 'Xi', ''], ['Liu', 'Hualuo', ''], ['Xiang', 'Yuejia', ''], ['Liu', 'Bo', ''], ['Zheng', 'Yefeng', '']]"
1368094,2010.11764,Aman Madaan,"Aman Madaan, Dheeraj Rajagopal, Yiming Yang, Abhilasha Ravichander,
  Eduard Hovy, Shrimai Prabhumoye",EIGEN: Event Influence GENeration using Pre-trained Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Reasoning about events and tracking their influences is fundamental to
understanding processes. In this paper, we present EIGEN - a method to leverage
pre-trained language models to generate event influences conditioned on a
context, nature of their influence, and the distance in a reasoning chain. We
also derive a new dataset for research and evaluation of methods for event
influence generation. EIGEN outperforms strong baselines both in terms of
automated evaluation metrics (by 10 ROUGE points) and human judgments on
closeness to reference and relevance of generations. Furthermore, we show that
the event influences generated by EIGEN improve the performance on a ""what-if""
Question Answering (WIQA) benchmark (over 3% F1), especially for questions that
require background knowledge and multi-hop reasoning.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 14:36:04 GMT'}]",2020-10-23,"[['Madaan', 'Aman', ''], ['Rajagopal', 'Dheeraj', ''], ['Yang', 'Yiming', ''], ['Ravichander', 'Abhilasha', ''], ['Hovy', 'Eduard', ''], ['Prabhumoye', 'Shrimai', '']]"
1227793,2001.02380,Yang Liu,Amir Zeldes and Yang Liu,A Neural Approach to Discourse Relation Signal Detection,"33 pages, 7 figures. Submitted to Dialogue & Discourse (D&D);
  Addressed reviewers' comments: strengthened arguments, added references,
  corrected typos etc",,10.5087/dad,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous data-driven work investigating the types and distributions of
discourse relation signals, including discourse markers such as 'however' or
phrases such as 'as a result' has focused on the relative frequencies of signal
words within and outside text from each discourse relation. Such approaches do
not allow us to quantify the signaling strength of individual instances of a
signal on a scale (e.g. more or less discourse-relevant instances of 'and'), to
assess the distribution of ambiguity for signals, or to identify words that
hinder discourse relation identification in context ('anti-signals' or
'distractors'). In this paper we present a data-driven approach to signal
detection using a distantly supervised neural network and develop a metric,
Delta s (or 'delta-softmax'), to quantify signaling strength. Ranging between
-1 and 1 and relying on recent advances in contextualized words embeddings, the
metric represents each word's positive or negative contribution to the
identifiability of a relation in specific instances in context. Based on an
English corpus annotated for discourse relations using Rhetorical Structure
Theory and signal type annotations anchored to specific tokens, our analysis
examines the reliability of the metric, the places where it overlaps with and
differs from human judgments, and the implications for identifying features
that neural models may need in order to perform better on automatic discourse
relation classification.
","[{'version': 'v1', 'created': 'Wed, 8 Jan 2020 05:14:49 GMT'}, {'version': 'v2', 'created': 'Wed, 11 Mar 2020 19:56:42 GMT'}]",2020-10-23,"[['Zeldes', 'Amir', ''], ['Liu', 'Yang', '']]"
1367713,2010.11383,Li Wanli,Wanli Li and Tieyun Qian,"Exploit Multiple Reference Graphs for Semi-supervised Relation
  Extraction",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Manual annotation of the labeled data for relation extraction is
time-consuming and labor-intensive. Semi-supervised methods can offer helping
hands for this problem and have aroused great research interests. Existing work
focuses on mapping the unlabeled samples to the classes to augment the labeled
dataset. However, it is hard to find an overall good mapping function,
especially for the samples with complicated syntactic components in one
sentence.
  To tackle this limitation, we propose to build the connection between the
unlabeled data and the labeled ones rather than directly mapping the unlabeled
samples to the classes. Specifically, we first use three kinds of information
to construct reference graphs, including entity reference, verb reference, and
semantics reference. The goal is to semantically or lexically connect the
unlabeled sample(s) to the labeled one(s). Then, we develop a Multiple
Reference Graph (MRefG) model to exploit the reference information for better
recognizing high-quality unlabeled samples. The effectiveness of our method is
demonstrated by extensive comparison experiments with the state-of-the-art
baselines on two public datasets.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 02:14:27 GMT'}]",2020-10-23,"[['Li', 'Wanli', ''], ['Qian', 'Tieyun', '']]"
1367704,2010.11374,Devendra Singh Sachan,"Devendra Singh Sachan and Lingfei Wu and Mrinmaya Sachan and William
  Hamilton",Stronger Transformers for Neural Multi-Hop Question Generation,Code will be made available,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prior work on automated question generation has almost exclusively focused on
generating simple questions whose answers can be extracted from a single
document. However, there is an increasing interest in developing systems that
are capable of more complex multi-hop question generation, where answering the
questions requires reasoning over multiple documents. In this work, we
introduce a series of strong transformer models for multi-hop question
generation, including a graph-augmented transformer that leverages relations
between entities in the text. While prior work has emphasized the importance of
graph-based models, we show that we can substantially outperform the
state-of-the-art by 5 BLEU points using a standard transformer architecture. We
further demonstrate that graph-based augmentations can provide complimentary
improvements on top of this foundation. Interestingly, we find that several
important factors--such as the inclusion of an auxiliary contrastive objective
and data filtering could have larger impacts on performance. We hope that our
stronger baselines and analysis provide a constructive foundation for future
work in this area.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 01:51:09 GMT'}]",2020-10-23,"[['Sachan', 'Devendra Singh', ''], ['Wu', 'Lingfei', ''], ['Sachan', 'Mrinmaya', ''], ['Hamilton', 'William', '']]"
1367692,2010.11362,Rithesh Kumar,"Rithesh Kumar, Kundan Kumar, Vicki Anand, Yoshua Bengio, Aaron
  Courville",NU-GAN: High resolution neural upsampling with GAN,,,,,cs.SD cs.AI cs.CL cs.LG eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose NU-GAN, a new method for resampling audio from
lower to higher sampling rates (upsampling). Audio upsampling is an important
problem since productionizing generative speech technology requires operating
at high sampling rates. Such applications use audio at a resolution of 44.1 kHz
or 48 kHz, whereas current speech synthesis methods are equipped to handle a
maximum of 24 kHz resolution. NU-GAN takes a leap towards solving audio
upsampling as a separate component in the text-to-speech (TTS) pipeline by
leveraging techniques for audio generation using GANs. ABX preference tests
indicate that our NU-GAN resampler is capable of resampling 22 kHz to 44.1 kHz
audio that is distinguishable from original audio only 7.4% higher than random
chance for single speaker dataset, and 10.8% higher than chance for
multi-speaker dataset.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 01:00:23 GMT'}]",2020-10-23,"[['Kumar', 'Rithesh', ''], ['Kumar', 'Kundan', ''], ['Anand', 'Vicki', ''], ['Bengio', 'Yoshua', ''], ['Courville', 'Aaron', '']]"
1367688,2010.11358,Aaron Baier-Reinio,Aaron Baier-Reinio and Hans De Sterck,"N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using
  Neural Ordinary Differential Equations",,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We use neural ordinary differential equations to formulate a variant of the
Transformer that is depth-adaptive in the sense that an input-dependent number
of time steps is taken by the ordinary differential equation solver. Our goal
in proposing the N-ODE Transformer is to investigate whether its
depth-adaptivity may aid in overcoming some specific known theoretical
limitations of the Transformer in handling nonlocal effects. Specifically, we
consider the simple problem of determining the parity of a binary sequence, for
which the standard Transformer has known limitations that can only be overcome
by using a sufficiently large number of layers or attention heads. We find,
however, that the depth-adaptivity of the N-ODE Transformer does not provide a
remedy for the inherently nonlocal nature of the parity problem, and provide
explanations for why this is so. Next, we pursue regularization of the N-ODE
Transformer by penalizing the arclength of the ODE trajectories, but find that
this fails to improve the accuracy or efficiency of the N-ODE Transformer on
the challenging parity problem. We suggest future avenues of research for
modifications and extensions of the N-ODE Transformer that may lead to improved
accuracy and efficiency for sequence modelling tasks such as neural machine
translation.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 00:48:24 GMT'}]",2020-10-23,"[['Baier-Reinio', 'Aaron', ''], ['De Sterck', 'Hans', '']]"
1367681,2010.11351,Minghan Li,"M. Li, H. Bai, L. Tan, K. Xiong, M. Li, J. Lin","Latte-Mix: Measuring Sentence Semantic Similarity with Latent
  Categorical Mixtures",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Measuring sentence semantic similarity using pre-trained language models such
as BERT generally yields unsatisfactory zero-shot performance, and one main
reason is ineffective token aggregation methods such as mean pooling. In this
paper, we demonstrate under a Bayesian framework that distance between
primitive statistics such as the mean of word embeddings are fundamentally
flawed for capturing sentence-level semantic similarity. To remedy this issue,
we propose to learn a categorical variational autoencoder (VAE) based on
off-the-shelf pre-trained language models. We theoretically prove that
measuring the distance between the latent categorical mixtures, namely
Latte-Mix, can better reflect the true sentence semantic similarity. In
addition, our Bayesian framework provides explanations for why models finetuned
on labelled sentence pairs have better zero-shot performance. We also
empirically demonstrate that these finetuned models could be further improved
by Latte-Mix. Our method not only yields the state-of-the-art zero-shot
performance on semantic similarity datasets such as STS, but also enjoy the
benefits of fast training and having small memory footprints.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 23:45:18 GMT'}]",2020-10-23,"[['Li', 'M.', ''], ['Bai', 'H.', ''], ['Tan', 'L.', ''], ['Xiong', 'K.', ''], ['Li', 'M.', ''], ['Lin', 'J.', '']]"
1367679,2010.11349,Xie Chen,"Xie Chen, Sarangarajan Parthasarathy, William Gale, Shuangyu Chang,
  Michael Zeng","LSTM-LM with Long-Term History for First-Pass Decoding in Conversational
  Speech Recognition",5 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  LSTM language models (LSTM-LMs) have been proven to be powerful and yielded
significant performance improvements over count based n-gram LMs in modern
speech recognition systems. Due to its infinite history states and
computational load, most previous studies focus on applying LSTM-LMs in the
second-pass for rescoring purpose. Recent work shows that it is feasible and
computationally affordable to adopt the LSTM-LMs in the first-pass decoding
within a dynamic (or tree based) decoder framework. In this work, the LSTM-LM
is composed with a WFST decoder on-the-fly for the first-pass decoding.
Furthermore, motivated by the long-term history nature of LSTM-LMs, the use of
context beyond the current utterance is explored for the first-pass decoding in
conversational speech recognition. The context information is captured by the
hidden states of LSTM-LMs across utterance and can be used to guide the
first-pass search effectively. The experimental results in our internal meeting
transcription system show that significant performance improvements can be
obtained by incorporating the contextual information with LSTM-LMs in the
first-pass decoding, compared to applying the contextual information in the
second-pass rescoring.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 23:40:26 GMT'}]",2020-10-23,"[['Chen', 'Xie', ''], ['Parthasarathy', 'Sarangarajan', ''], ['Gale', 'William', ''], ['Chang', 'Shuangyu', ''], ['Zeng', 'Michael', '']]"
1367668,2010.11338,Yun Tang,"Yun Tang, Juan Pino, Changhan Wang, Xutai Ma, Dmitriy Genzel","A General Multi-Task Learning Framework to Leverage Text Data for Speech
  to Text Tasks",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Attention-based sequence-to-sequence modeling provides a powerful and elegant
solution for applications that need to map one sequence to a different
sequence. Its success heavily relies on the availability of large amounts of
training data. This presents a challenge for speech applications where labelled
speech data is very expensive to obtain, such as automatic speech recognition
(ASR) and speech translation (ST). In this study, we propose a general
multi-task learning framework to leverage text data for ASR and ST tasks. Two
auxiliary tasks, a denoising autoencoder task and machine translation task, are
proposed to be co-trained with ASR and ST tasks respectively. We demonstrate
that representing text input as phoneme sequences can reduce the difference
between speech and text inputs, and enhance the knowledge transfer from text
corpora to the speech to text tasks. Our experiments show that the proposed
method achieves a relative 10~15% word error rate reduction on the English
Librispeech task, and improves the speech translation quality on the MuST-C
tasks by 4.2~11.1 BLEU.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 22:40:43 GMT'}]",2020-10-23,"[['Tang', 'Yun', ''], ['Pino', 'Juan', ''], ['Wang', 'Changhan', ''], ['Ma', 'Xutai', ''], ['Genzel', 'Dmitriy', '']]"
1367664,2010.11334,Chiyu Zhang,"Muhammad Abdul-Mageed, Chiyu Zhang, Houda Bouamor and Nizar Habash",NADI 2020: The First Nuanced Arabic Dialect Identification Shared Task,Accepted in WANLP 2020,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We present the results and findings of the First Nuanced Arabic Dialect
Identification Shared Task (NADI). The shared task includes two subtasks:
country level dialect identification (Subtask 1) and province level sub-dialect
identification (Subtask 2). The data for the shared task covers a total of 100
provinces from 21 Arab countries, and are collected from the Twitter domain. As
such, NADI is the first shared task to target naturally-occurring fine-grained
dialectal text at the sub-country level. A total of 61 teams from 25 countries
registered to participate in the tasks, thus reflecting the interest of the
community in this area. We received 47 submissions for Subtask 1 from 18 teams
and 9 submissions to Subtask 2 from 9 teams.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 22:14:28 GMT'}]",2020-10-23,"[['Abdul-Mageed', 'Muhammad', ''], ['Zhang', 'Chiyu', ''], ['Bouamor', 'Houda', ''], ['Habash', 'Nizar', '']]"
1367663,2010.11333,Yogarshi Vyas,"Yogarshi Vyas, Miguel Ballesteros",Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In entity linking, mentions of named entities in raw text are disambiguated
against a knowledge base (KB). This work focuses on linking to unseen KBs that
do not have training data and whose schema is unknown during training. Our
approach relies on methods to flexibly convert entities from arbitrary KBs with
several attribute-value pairs into flat strings, which we use in conjunction
with state-of-the-art models for zero-shot linking. To improve the
generalization of our model, we use two regularization schemes based on
shuffling of entity attributes and handling of unseen attributes. Experiments
on English datasets where models are trained on the CoNLL dataset, and tested
on the TAC-KBP 2010 dataset show that our models outperform baseline models by
over 12 points of accuracy. Unlike prior work, our approach also allows for
seamlessly combining multiple training datasets. We test this ability by adding
both a completely different dataset (Wikia), as well as increasing amount of
training data from the TAC-KBP 2010 training set. Our models perform favorably
across the board.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 22:07:31 GMT'}]",2020-10-23,"[['Vyas', 'Yogarshi', ''], ['Ballesteros', 'Miguel', '']]"
1367655,2010.11325,Rui Feng,"Rui Feng, Jie Yuan, Chao Zhang","Probing and Fine-tuning Reading Comprehension Models for Few-shot Event
  Extraction",,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We study the problem of event extraction from text data, which requires both
detecting target event types and their arguments. Typically, both the event
detection and argument detection subtasks are formulated as supervised sequence
labeling problems. We argue that the event extraction models so trained are
inherently label-hungry, and can generalize poorly across domains and text
genres.We propose a reading comprehension framework for event
extraction.Specifically, we formulate event detection as a textual entailment
prediction problem, and argument detection as a question answer-ing problem. By
constructing proper query templates, our approach can effectively distill rich
knowledge about tasks and label semantics from pretrained reading comprehension
models. Moreover, our model can be fine-tuned with a small amount of data to
boost its performance. Our experiment results show that our method performs
strongly for zero-shot and few-shot event extraction, and it achieves
state-of-the-art performance on the ACE 2005 benchmark when trained with full
supervision.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 21:48:39 GMT'}]",2020-10-23,"[['Feng', 'Rui', ''], ['Yuan', 'Jie', ''], ['Zhang', 'Chao', '']]"
1367714,2010.11384,Gabriele Pergola,"Gabriele Pergola, Lin Gui, Yulan He","A Disentangled Adversarial Neural Topic Model for Separating Opinions
  from Plots in User Reviews","12 pages, 4 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The flexibility of the inference process in Variational Autoencoders (VAEs)
has recently led to revising traditional probabilistic topic models giving rise
to Neural Topic Models (NTM). Although these approaches have achieved
significant results, surprisingly very little work has been done on how to
disentangle the latent topics. Existing topic models when applied to reviews
may extract topics associated with writers' subjective opinions mixed with
those related to factual descriptions such as plot summaries in movie and book
reviews. It is thus desirable to automatically separate opinion topics from
plot/neutral ones enabling a better interpretability. In this paper, we propose
a neural topic model combined with adversarial training to disentangle opinion
topics from plot and neutral ones. We conduct an extensive experimental
assessment introducing a new collection of movie and book reviews paired with
their plots, namely MOBO dataset, showing an improved coherence and variety of
topics, a consistent disentanglement rate, and sentiment classification
performance superior to other supervised topic models.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 02:15:13 GMT'}]",2020-10-23,"[['Pergola', 'Gabriele', ''], ['Gui', 'Lin', ''], ['He', 'Yulan', '']]"
1368114,2010.11784,Fangyu Liu,"Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, Nigel
  Collier",Self-alignment Pre-training for Biomedical Entity Representations,8 pages. work in progress,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the widespread success of self-supervised learning via masked
language models, learning representations directly from text to accurately
capture complex and fine-grained semantic relationships in the biomedical
domain remains as a challenge. Addressing this is of paramount importance for
tasks such as entity linking where complex relational knowledge is pivotal. We
propose SapBERT, a pre-training scheme based on BERT. It self-aligns the
representation space of biomedical entities with a metric learning objective
function leveraging UMLS, a collection of biomedical ontologies with >4M
concepts. Our experimental results on six medical entity linking benchmarking
datasets demonstrate that SapBERT outperforms many domain-specific BERT-based
variants such as BioBERT, BlueBERT and PubMedBERT, achieving the
state-of-the-art (SOTA) performances.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 14:59:57 GMT'}]",2020-10-23,"[['Liu', 'Fangyu', ''], ['Shareghi', 'Ehsan', ''], ['Meng', 'Zaiqiao', ''], ['Basaldella', 'Marco', ''], ['Collier', 'Nigel', '']]"
1367583,2010.11253,Rico Angell,"Rico Angell, Nicholas Monath, Sunil Mohan, Nishant Yadav and Andrew
  McCallum",Clustering-based Inference for Zero-Shot Biomedical Entity Linking,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Due to large number of entities in biomedical knowledge bases, only a small
fraction of entities have corresponding labelled training data. This
necessitates a zero-shot entity linking model which is able to link mentions of
unseen entities using learned representations of entities. Existing zero-shot
entity linking models however link each mention independently, ignoring the
inter/intra-document relationships between the entity mentions. These relations
can be very useful for linking mentions in biomedical text where linking
decisions are often difficult due mentions having a generic or a highly
specialized form. In this paper, we introduce a model in which linking
decisions can be made not merely by linking to a KB entity but also by grouping
multiple mentions together via clustering and jointly making linking
predictions. In experiments on the largest publicly available biomedical
dataset, we improve the best independent prediction for zero-shot entity
linking by 2.5 points of accuracy, and our joint inference model further
improves entity linking by 1.8 points.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 19:16:27 GMT'}]",2020-10-23,"[['Angell', 'Rico', ''], ['Monath', 'Nicholas', ''], ['Mohan', 'Sunil', ''], ['Yadav', 'Nishant', ''], ['McCallum', 'Andrew', '']]"
1367577,2010.11247,Renjie Zheng,"Junkun Chen, Renjie Zheng, Atsuhito Kita, Mingbo Ma, Liang Huang",Improving Simultaneous Translation with Pseudo References,6 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Simultaneous translation is vastly different from full-sentence translation,
in the sense that it starts translation before the source sentence ends, with
only a few words delay. However, due to the lack of large scale and publicly
available simultaneous translation datasets, most simultaneous translation
systems still train with ordinary full-sentence parallel corpora which are not
suitable for the simultaneous scenario due to the existence of unnecessary
long-distance reorderings. Instead of expensive, time-consuming annotation, we
propose a novel method that rewrites the target side of existing full-sentence
corpus into simultaneous-style translation. Experiments on Chinese-to-English
translation demonstrate about +2.7 BLEU improvements with the addition of newly
generated pseudo references.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 19:03:06 GMT'}]",2020-10-23,"[['Chen', 'Junkun', ''], ['Zheng', 'Renjie', ''], ['Kita', 'Atsuhito', ''], ['Ma', 'Mingbo', ''], ['Huang', 'Liang', '']]"
1367576,2010.11246,Tianze Shi,"Tianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daum\'e III and Lillian
  Lee","On the Potential of Lexico-logical Alignments for Semantic Parsing to
  SQL Queries",Findings of ACL: EMNLP 2020,Findings of ACL: EMNLP 2020,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale semantic parsing datasets annotated with logical forms have
enabled major advances in supervised approaches. But can richer supervision
help even more? To explore the utility of fine-grained, lexical-level
supervision, we introduce Squall, a dataset that enriches 11,276
WikiTableQuestions English-language questions with manually created SQL
equivalents plus alignments between SQL and question fragments. Our annotation
enables new training possibilities for encoder-decoder models, including
approaches from machine translation previously precluded by the absence of
alignments. We propose and test two methods: (1) supervised attention; (2)
adopting an auxiliary objective of disambiguating references in the input
queries to table columns. In 5-fold cross validation, these strategies improve
over strong baselines by 4.4% execution accuracy. Oracle experiments suggest
that annotated alignments can support further accuracy gains of up to 23.9%.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 19:01:00 GMT'}]",2020-10-23,"[['Shi', 'Tianze', ''], ['Zhao', 'Chen', ''], ['Boyd-Graber', 'Jordan', ''], ['Daumé', 'Hal', 'III'], ['Lee', 'Lillian', '']]"
1367568,2010.11238,Sirigireddy Dhana Laxmi,"Sirigireddy Dhanalaxmi, Rohit Agarwal, Aman Sinha",Detection of COVID-19 informative tweets using RoBERTa,,,,,cs.CL cs.SI,http://creativecommons.org/publicdomain/zero/1.0/,"  Social media such as Twitter is a hotspot of user-generated information. In
this ongoing Covid-19 pandemic, there has been an abundance of data on social
media which can be classified as informative and uninformative content. In this
paper, we present our work to detect informative Covid-19 English tweets using
RoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our
model on a public dataset with an F1-score of 0.89 on the validation dataset
and 0.87 on the leaderboard.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 18:43:13 GMT'}]",2020-10-23,"[['Dhanalaxmi', 'Sirigireddy', ''], ['Agarwal', 'Rohit', ''], ['Sinha', 'Aman', '']]"
1367560,2010.11230,Mohammad Kachuee Mr.,"Mohammad Kachuee, Hao Yuan, Young-Bum Kim, Sungjin Lee","Self-Supervised Contrastive Learning for Efficient User Satisfaction
  Prediction in Conversational Agents",,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Turn-level user satisfaction is one of the most important performance metrics
for conversational agents. It can be used to monitor the agent's performance
and provide insights about defective user experiences. Moreover, a powerful
satisfaction model can be used as an objective function that a conversational
agent continuously optimizes for. While end-to-end deep learning has shown
promising results, having access to a large number of reliable annotated
samples required by these methods remains challenging. In a large-scale
conversational system, there is a growing number of newly developed skills,
making the traditional data collection, annotation, and modeling process
impractical due to the required annotation costs as well as the turnaround
times. In this paper, we suggest a self-supervised contrastive learning
approach that leverages the pool of unlabeled data to learn user-agent
interactions. We show that the pre-trained models using the self-supervised
objective are transferable to the user satisfaction prediction. In addition, we
propose a novel few-shot transfer learning approach that ensures better
transferability for very small sample sizes. The suggested few-shot method does
not require any inner loop optimization process and is scalable to very large
datasets and complex models. Based on our experiments using real-world data
from a large-scale commercial system, the suggested approach is able to
significantly reduce the required number of annotations, while improving the
generalization on unseen out-of-domain skills.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 18:10:58 GMT'}]",2020-10-23,"[['Kachuee', 'Mohammad', ''], ['Yuan', 'Hao', ''], ['Kim', 'Young-Bum', ''], ['Lee', 'Sungjin', '']]"
1367196,2010.10866,Cl\'ement Rebuffel,"Cl\'ement Rebuffel, Laure Soulier, Geoffrey Scoutheeten, Patrick
  Gallinari","PARENTing via Model-Agnostic Reinforcement Learning to Correct
  Pathological Behaviors in Data-to-Text Generation","Accepted at the 13th International Conference on Natural Language
  Generation (INLG 2020)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In language generation models conditioned by structured data, the classical
training via maximum likelihood almost always leads models to pick up on
dataset divergence (i.e., hallucinations or omissions), and to incorporate them
erroneously in their own generations at inference. In this work, we build ontop
of previous Reinforcement Learning based approaches and show that a
model-agnostic framework relying on the recently introduced PARENT metric is
efficient at reducing both hallucinations and omissions. Evaluations on the
widely used WikiBIO and WebNLG benchmarks demonstrate the effectiveness of this
framework compared to state-of-the-art models.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 09:49:47 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 13:00:20 GMT'}]",2020-10-23,"[['Rebuffel', 'Clément', ''], ['Soulier', 'Laure', ''], ['Scoutheeten', 'Geoffrey', ''], ['Gallinari', 'Patrick', '']]"
1367222,2010.10892,Yang Jiao,Yang Jiao,"BERT for Joint Multichannel Speech Dereverberation with Spatial-aware
  Tasks",,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a method for joint multichannel speech dereverberation with two
spatial-aware tasks: direction-of-arrival (DOA) estimation and speech
separation. The proposed method addresses involved tasks as a sequence to
sequence mapping problem, which is general enough for a variety of front-end
speech enhancement tasks. The proposed method is inspired by the excellent
sequence modeling capability of bidirectional encoder representation from
transformers (BERT). Instead of utilizing explicit representations from
pretraining in a self-supervised manner, we utilizes transformer encoded hidden
representations in a supervised manner. Both multichannel spectral magnitude
and spectral phase information of varying length utterances are encoded.
Experimental result demonstrates the effectiveness of the proposed method.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 11:05:17 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 02:41:39 GMT'}]",2020-10-23,"[['Jiao', 'Yang', '']]"
1367236,2010.10906,Branden Chan,"Branden Chan, Stefan Schweter, Timo M\""oller",German's Next Language Model,Accepted by COLING2020 Industry Track,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work we present the experiments which lead to the creation of our
BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying
the input training data, model size, and the presence of Whole Word Masking
(WWM) we were able to attain SoTA performance across a set of document
classification and named entity recognition (NER) tasks for both models of base
and large size. We adopt an evaluation driven approach in training these models
and our results indicate that both adding more data and utilizing WWM improve
model performance. By benchmarking against existing German models, we show that
these models are the best German models to date. Our trained models will be
made publicly available to the research community.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 11:28:23 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 08:39:30 GMT'}]",2020-10-23,"[['Chan', 'Branden', ''], ['Schweter', 'Stefan', ''], ['Möller', 'Timo', '']]"
772506,1609.07028,Ruobing Xie,"Ruobing Xie, Zhiyuan Liu, Huanbo Luan, Maosong Sun",Image-embodied Knowledge Representation Learning,7 pages; Accepted by IJCAI-2017,IJCAI-2017,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Entity images could provide significant visual information for knowledge
representation learning. Most conventional methods learn knowledge
representations merely from structured triples, ignoring rich visual
information extracted from entity images. In this paper, we propose a novel
Image-embodied Knowledge Representation Learning model (IKRL), where knowledge
representations are learned with both triple facts and images. More
specifically, we first construct representations for all images of an entity
with a neural image encoder. These image representations are then integrated
into an aggregated image-based representation via an attention-based method. We
evaluate our IKRL models on knowledge graph completion and triple
classification. Experimental results demonstrate that our models outperform all
baselines on both tasks, which indicates the significance of visual information
for knowledge representations and the capability of our models in learning
knowledge representations with images.
","[{'version': 'v1', 'created': 'Thu, 22 Sep 2016 15:37:45 GMT'}, {'version': 'v2', 'created': 'Mon, 22 May 2017 08:14:27 GMT'}]",2020-10-23,"[['Xie', 'Ruobing', ''], ['Liu', 'Zhiyuan', ''], ['Luan', 'Huanbo', ''], ['Sun', 'Maosong', '']]"
1292326,2005.12531,Dongyang Dai,"Dongyang Dai, Li Chen, Yuping Wang, Mu Wang, Rui Xia, Xuchen Song,
  Zhiyong Wu, Yuxuan Wang","Noise Robust TTS for Low Resource Speakers using Pre-trained Model and
  Speech Enhancement",,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the popularity of deep neural network, speech synthesis task has
achieved significant improvements based on the end-to-end encoder-decoder
framework in the recent days. More and more applications relying on speech
synthesis technology have been widely used in our daily life. Robust speech
synthesis model depends on high quality and customized data which needs lots of
collecting efforts. It is worth investigating how to take advantage of
low-quality and low resource voice data which can be easily obtained from the
Internet for usage of synthesizing personalized voice. In this paper, the
proposed end-to-end speech synthesis model uses both speaker embedding and
noise representation as conditional inputs to model speaker and noise
information respectively. Firstly, the speech synthesis model is pre-trained
with both multi-speaker clean data and noisy augmented data; then the
pre-trained model is adapted on noisy low-resource new speaker data; finally,
by setting the clean speech condition, the model can synthesize the new
speaker's clean voice. Experimental results show that the speech generated by
the proposed approach has better subjective evaluation results than the method
directly fine-tuning pre-trained multi-speaker speech synthesis model with
denoised new speaker data.
","[{'version': 'v1', 'created': 'Tue, 26 May 2020 06:14:06 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 11:36:56 GMT'}]",2020-10-23,"[['Dai', 'Dongyang', ''], ['Chen', 'Li', ''], ['Wang', 'Yuping', ''], ['Wang', 'Mu', ''], ['Xia', 'Rui', ''], ['Song', 'Xuchen', ''], ['Wu', 'Zhiyong', ''], ['Wang', 'Yuxuan', '']]"
1367652,2010.11322,Jonathan Pilault,"Jaehong Park, Jonathan Pilault and Christopher Pal",Learning to Summarize Long Texts with Memory Compression and Transfer,,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  We introduce Mem2Mem, a memory-to-memory mechanism for hierarchical recurrent
neural network based encoder decoder architectures and we explore its use for
abstractive document summarization. Mem2Mem transfers ""memories"" via
readable/writable external memory modules that augment both the encoder and
decoder. Our memory regularization compresses an encoded input article into a
more compact set of sentence representations. Most importantly, the memory
compression step performs implicit extraction without labels, sidestepping
issues with suboptimal ground-truth data and exposure bias of hybrid
extractive-abstractive summarization techniques. By allowing the decoder to
read/write over the encoded input memory, the model learns to read salient
information about the input article while keeping track of what has been
generated. Our Mem2Mem approach yields results that are competitive with state
of the art transformer based summarization methods, but with 16 times fewer
parameters
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 21:45:44 GMT'}]",2020-10-23,"[['Park', 'Jaehong', ''], ['Pilault', 'Jonathan', ''], ['Pal', 'Christopher', '']]"
1367836,2010.11506,Lingkai Kong,"Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, Chao
  Zhang","Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution
  Data",EMNLP2020 long paper,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fine-tuned pre-trained language models can suffer from severe miscalibration
for both in-distribution and out-of-distribution (OOD) data due to
over-parameterization. To mitigate this issue, we propose a regularized
fine-tuning method. Our method introduces two types of regularization for
better calibration: (1) On-manifold regularization, which generates pseudo
on-manifold samples through interpolation within the data manifold. Augmented
training with these pseudo samples imposes a smoothness regularization to
improve in-distribution calibration. (2) Off-manifold regularization, which
encourages the model to output uniform distributions for pseudo off-manifold
samples to address the over-confidence issue for OOD data. Our experiments
demonstrate that the proposed method outperforms existing calibration methods
for text classification in terms of expectation calibration error,
misclassification detection, and OOD detection on six datasets. Our code can be
found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 07:48:38 GMT'}]",2020-10-23,"[['Kong', 'Lingkai', ''], ['Jiang', 'Haoming', ''], ['Zhuang', 'Yuchen', ''], ['Lyu', 'Jie', ''], ['Zhao', 'Tuo', ''], ['Zhang', 'Chao', '']]"
1367716,2010.11386,Jheng-Hong Yang,"Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin","Distilling Dense Representations for Ranking using Tightly-Coupled
  Teachers",,,,,cs.IR cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present an approach to ranking with dense representations that applies
knowledge distillation to improve the recently proposed late-interaction
ColBERT model. Specifically, we distill the knowledge from ColBERT's expressive
MaxSim operator for computing relevance scores into a simple dot product, thus
enabling single-step ANN search. Our key insight is that during distillation,
tight coupling between the teacher model and the student model enables more
flexible distillation strategies and yields better learned representations. We
empirically show that our approach improves query latency and greatly reduces
the onerous storage requirements of ColBERT, while only making modest
sacrifices in terms of effectiveness. By combining our dense representations
with sparse representations derived from document expansion, we are able to
approach the effectiveness of a standard cross-encoder reranker using BERT that
is orders of magnitude slower.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 02:26:01 GMT'}]",2020-10-23,"[['Lin', 'Sheng-Chieh', ''], ['Yang', 'Jheng-Hong', ''], ['Lin', 'Jimmy', '']]"
1367775,2010.11445,Mingbo Ma,"Junkun Chen, Mingbo Ma, Renjie Zheng, Liang Huang",MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation,10 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end Speech-to-text Translation (E2E- ST), which directly translates
source language speech to target language text, is widely useful in practice,
but traditional cascaded approaches (ASR+MT) often suffer from error
propagation in the pipeline. On the other hand, existing end-to-end solutions
heavily depend on the source language transcriptions for pre-training or
multi-task training with Automatic Speech Recognition (ASR). We instead propose
a simple technique to learn a robust speech encoder in a self-supervised
fashion only on the speech side, which can utilize speech data without
transcription. This technique, termed Masked Acoustic Modeling (MAM), can also
perform pre-training, for the first time, on any acoustic signals (including
non-speech ones) without annotation. Compared with current state-of-the-art
models on ST, our technique achieves +1.4 BLEU improvement without using
transcriptions, and +1.2 BLEU using transcriptions. The pre-training of MAM
with arbitrary acoustic signals also boosts the downstream speech-related
tasks.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 05:02:06 GMT'}]",2020-10-23,"[['Chen', 'Junkun', ''], ['Ma', 'Mingbo', ''], ['Zheng', 'Renjie', ''], ['Huang', 'Liang', '']]"
1367820,2010.11490,Christophe Cerisara,"Christophe Cerisara (SYNALP), Pavel Kral, Ladislav Lenc","On the Effects of Using word2vec Representations in Neural Networks for
  Dialogue Act Recognition",,"Computer Speech and Language, Elsevier, 2018, 47, pp.175 - 193",10.1016/j.csl.2017.07.009,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialogue act recognition is an important component of a large number of
natural language processing pipelines. Many research works have been carried
out in this area, but relatively few investigate deep neural networks and word
embeddings. This is surprising, given that both of these techniques have proven
exceptionally good in most other language-related domains. We propose in this
work a new deep neural network that explores recurrent models to capture word
sequences within sentences, and further study the impact of pretrained word
embeddings. We validate this model on three languages: English, French and
Czech. The performance of the proposed approach is consistent across these
languages and it is comparable to the state-of-the-art results in English. More
importantly, we confirm that deep neural networks indeed outperform a Maximum
Entropy classifier, which was expected. However , and this is more surprising,
we also found that standard word2vec em-beddings do not seem to bring valuable
information for this task and the proposed model, whatever the size of the
training corpus is. We thus further analyse the resulting embeddings and
conclude that a possible explanation may be related to the mismatch between the
type of lexical-semantic information captured by the word2vec embeddings, and
the kind of relations between words that is the most useful for the dialogue
act recognition task.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 07:21:17 GMT'}]",2020-10-23,"[['Cerisara', 'Christophe', '', 'SYNALP'], ['Kral', 'Pavel', ''], ['Lenc', 'Ladislav', '']]"
1368266,2010.11936,Tomasz Stanis{\l}awek,Rafal Powalski and Tomasz Stanislawek,UniCase -- Rethinking Casing in Language Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we introduce a new approach to dealing with the problem of
case-sensitiveness in Language Modelling (LM). We propose simple architecture
modification to the RoBERTa language model, accompanied by a new tokenization
strategy, which we named Unified Case LM (UniCase). We tested our solution on
the GLUE benchmark, which led to increased performance by 0.42 points.
Moreover, we prove that the UniCase model works much better when we have to
deal with text data, where all tokens are uppercased (+5.88 point).
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 17:58:44 GMT'}]",2020-10-23,"[['Powalski', 'Rafal', ''], ['Stanislawek', 'Tomasz', '']]"
1368260,2010.11930,Rodrigo Nogueira,"Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, Jimmy Lin",Scientific Claim Verification with VERT5ERINI,,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This work describes the adaptation of a pretrained sequence-to-sequence model
to the task of scientific claim verification in the biomedical domain. We
propose VERT5ERINI that exploits T5 for abstract retrieval, sentence selection
and label prediction, which are three critical sub-tasks of claim verification.
We evaluate our pipeline on SCIFACT, a newly curated dataset that requires
models to not just predict the veracity of claims but also provide relevant
sentences from a corpus of scientific literature that support this decision.
Empirically, our pipeline outperforms a strong baseline in each of the three
steps. Finally, we show VERT5ERINI's ability to generalize to two new datasets
of COVID-19 claims using evidence from the ever-expanding CORD-19 corpus.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 17:56:33 GMT'}]",2020-10-23,"[['Pradeep', 'Ronak', ''], ['Ma', 'Xueguang', ''], ['Nogueira', 'Rodrigo', ''], ['Lin', 'Jimmy', '']]"
1368248,2010.11918,"Andreas R\""uckl\'e","Andreas R\""uckl\'e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas
  Pfeiffer, Nils Reimers, Iryna Gurevych",AdapterDrop: On the Efficiency of Adapters in Transformers,,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Massively pre-trained transformer models are computationally expensive to
fine-tune, slow for inference, and have large storage requirements. Recent
approaches tackle these shortcomings by training smaller models, dynamically
reducing the model size, and by training light-weight adapters. In this paper,
we propose AdapterDrop, removing adapters from lower transformer layers during
training and inference, which incorporates concepts from all three directions.
We show that AdapterDrop can dynamically reduce the computational overhead when
performing inference over multiple tasks simultaneously, with minimal decrease
in task performances. We further prune adapters from AdapterFusion, which
improves the inference efficiency while maintaining the task performances
entirely.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 17:49:42 GMT'}]",2020-10-23,"[['Rücklé', 'Andreas', ''], ['Geigle', 'Gregor', ''], ['Glockner', 'Max', ''], ['Beck', 'Tilman', ''], ['Pfeiffer', 'Jonas', ''], ['Reimers', 'Nils', ''], ['Gurevych', 'Iryna', '']]"
1367717,2010.11387,George Boateng,George Boateng,Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Introductory hands-on courses such as our smartphone-based coding courses,
SuaCode require a lot of support for students to accomplish learning goals.
Online environments make it even more difficult to get assistance especially
more recently because of COVID-19. Given the multilingual context of our
students (learners across 38 African countries), in this work, we developed an
AI Teaching Assistant (Kwame) that provides answers to students' coding
questions from our SuaCode courses in English and French. Kwame is a
Sentence-BERT(SBERT)-based question-answering (QA) system that we trained and
evaluated using question-answer pairs created from our course's quizzes and
students' questions in past cohorts. It finds the paragraph most semantically
similar to the question via cosine similarity. We compared the system with
TF-IDF and Universal Sentence Encoder. Our results showed that SBERT performed
the worst for the duration of 6 secs per question but the best for accuracy and
fine-tuning on our course data improved the result.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 02:26:12 GMT'}]",2020-10-23,"[['Boateng', 'George', '']]"
1368199,2010.11869,Lei Xu,"Lei Xu, Ivan Ramirez, Kalyan Veeramachaneni","Rewriting Meaningful Sentences via Conditional BERT Sampling and an
  application on fooling text classifiers",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most adversarial attack methods that are designed to deceive a text
classifier change the text classifier's prediction by modifying a few words or
characters. Few try to attack classifiers by rewriting a whole sentence, due to
the difficulties inherent in sentence-level rephrasing as well as the problem
of setting the criteria for legitimate rewriting.
  In this paper, we explore the problem of creating adversarial examples with
sentence-level rewriting. We design a new sampling method, named
ParaphraseSampler, to efficiently rewrite the original sentence in multiple
ways. Then we propose a new criteria for modification, called a sentence-level
threaten model. This criteria allows for both word- and sentence-level changes,
and can be adjusted independently in two dimensions: semantic similarity and
grammatical quality. Experimental results show that many of these rewritten
sentences are misclassified by the classifier. On all 6 datasets, our
ParaphraseSampler achieves a better attack success rate than our baseline.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 17:03:13 GMT'}]",2020-10-23,"[['Xu', 'Lei', ''], ['Ramirez', 'Ivan', ''], ['Veeramachaneni', 'Kalyan', '']]"
1368189,2010.11859,Nikolay Bogoychev Dr,Nikolay Bogoychev,Not all parameters are born equal: Attention is mostly what you need,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformers are widely used in state-of-the-art machine translation, but the
key to their success is still unknown. To gain insight into this, we consider
three groups of parameters: embeddings, attention, and feed forward neural
network (FFN) layers. We examine the relative importance of each by performing
an ablation study where we initialise them at random and freeze them, so that
their weights do not change over the course of the training. Through this, we
show that the attention and FFN are equally important and fulfil the same
functionality in a model. We show that the decision about whether a component
is frozen or allowed to train is at least as important for the final model
performance as its number of parameters. At the same time, the number of
parameters alone is not indicative of a component's importance. Finally, while
the embedding layer is the least essential for machine translation tasks, it is
the most important component for language modelling tasks.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 16:49:18 GMT'}]",2020-10-23,"[['Bogoychev', 'Nikolay', '']]"
1367811,2010.11481,Yu-An Chung,Yu-An Chung and Yonatan Belinkov and James Glass,Similarity Analysis of Self-Supervised Speech Representations,,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Self-supervised speech representation learning has recently been a prosperous
research topic. Many algorithms have been proposed for learning useful
representations from large-scale unlabeled data, and their applications to a
wide range of speech tasks have also been investigated. However, there has been
little research focusing on understanding the properties of existing
approaches. In this work, we aim to provide a comparative study of some of the
most representative self-supervised algorithms. Specifically, we quantify the
similarities between different self-supervised representations using existing
similarity measures. We also design probing tasks to study the correlation
between the models' pre-training loss and the amount of specific speech
information contained in their learned representations. In addition to showing
how various self-supervised models behave differently given the same input, our
study also finds that the training objective has a higher impact on
representation similarity than architectural choices such as building blocks
(RNN/Transformer/CNN) and directionality (uni/bidirectional). Our results also
suggest that there exists a strong correlation between pre-training loss and
downstream performance for some self-supervised algorithms.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 07:02:21 GMT'}]",2020-10-23,"[['Chung', 'Yu-An', ''], ['Belinkov', 'Yonatan', ''], ['Glass', 'James', '']]"
1368245,2010.11915,Akari Asai,Akari Asai and Eunsol Choi,"Challenges in Information Seeking QA:Unanswerable Questions and
  Paragraph Retrieval",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent progress in pretrained language model ""solved"" many reading
comprehension benchmark datasets. Yet information-seeking Question Answering
(QA) datasets, where questions are written without the evidence document,
remain unsolved. We analyze two such datasets (Natural Questions and TyDi QA)
to identify remaining headrooms: paragraph selection and answerability
classification, i.e. determining whether the paired evidence document contains
the answer to the query or not. In other words, given a gold paragraph and
knowing whether it contains an answer or not, models easily outperform a single
annotator in both datasets. After identifying unanswerability as a bottleneck,
we further inspect what makes questions unanswerable. Our study points to
avenues for future research, both for dataset creation and model development.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 17:48:17 GMT'}]",2020-10-23,"[['Asai', 'Akari', ''], ['Choi', 'Eunsol', '']]"
1368185,2010.11855,Michael Wick,"Michael L. Wick, Kate Silverstein, Jean-Baptiste Tristan, Adam Pocock,
  Mark Johnson","Detecting and Exorcising Statistical Demons from Language Models with
  Anti-Models of Negative Data",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It's been said that ""Language Models are Unsupervised Multitask Learners.""
Indeed, self-supervised language models trained on ""positive"" examples of
English text generalize in desirable ways to many natural language tasks. But
if such models can stray so far from an initial self-supervision objective, a
wayward model might generalize in undesirable ways too, say to nonsensical
""negative"" examples of unnatural language. A key question in this work is: do
language models trained on (positive) training data also generalize to
(negative) test data? We use this question as a contrivance to assess the
extent to which language models learn undesirable properties of text, such as
n-grams, that might interfere with the learning of more desirable properties of
text, such as syntax. We find that within a model family, as the number of
parameters, training epochs, and data set size increase, so does a model's
ability to generalize to negative n-gram data, indicating standard
self-supervision generalizes too far. We propose a form of inductive bias that
attenuates such undesirable signals with negative data distributions
automatically learned from positive data. We apply the method to remove n-gram
signals from LSTMs and find that doing so causes them to favor syntactic
signals, as demonstrated by large error reductions (up to 46% on the hardest
cases) on a syntactic subject-verb agreement task.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 16:45:32 GMT'}]",2020-10-23,"[['Wick', 'Michael L.', ''], ['Silverstein', 'Kate', ''], ['Tristan', 'Jean-Baptiste', ''], ['Pocock', 'Adam', ''], ['Johnson', 'Mark', '']]"
1368121,2010.11791,Ivan Vuli\'c,Matthew Henderson and Ivan Vuli\'c,ConVEx: Data-Efficient and Few-Shot Slot Labeling,,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  We propose ConVEx (Conversational Value Extractor), an efficient pretraining
and fine-tuning neural approach for slot-labeling dialog tasks. Instead of
relying on more general pretraining objectives from prior work (e.g., language
modeling, response selection), ConVEx's pretraining objective, a novel pairwise
cloze task using Reddit data, is well aligned with its intended usage on
sequence labeling tasks. This enables learning domain-specific slot labelers by
simply fine-tuning decoding layers of the pretrained general-purpose sequence
labeling model, while the majority of the pretrained model's parameters are
kept frozen. We report state-of-the-art performance of ConVEx across a range of
diverse domains and data sets for dialog slot-labeling, with the largest gains
in the most challenging, few-shot setups. We believe that ConVEx's reduced
pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its
efficient fine-tuning and strong performance, promise wider portability and
scalability for data-efficient sequence-labeling tasks in general.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 15:13:35 GMT'}]",2020-10-23,"[['Henderson', 'Matthew', ''], ['Vulić', 'Ivan', '']]"
1368183,2010.11853,Johannes E. M. Mosig,"Johannes E. M. Mosig, Shikib Mehri, Thomas Kober",STAR: A Schema-Guided Dialog Dataset for Transfer Learning,"Equal contribution: Johannes E. M. Mosig, Shikib Mehri",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present STAR, a schema-guided task-oriented dialog dataset consisting of
127,833 utterances and knowledge base queries across 5,820 task-oriented
dialogs in 13 domains that is especially designed to facilitate task and domain
transfer learning in task-oriented dialog. Furthermore, we propose a scalable
crowd-sourcing paradigm to collect arbitrarily large datasets of the same
quality as STAR. Moreover, we introduce novel schema-guided dialog models that
use an explicit description of the task(s) to generalize from known to unknown
tasks. We demonstrate the effectiveness of these models, particularly for
zero-shot generalization across tasks and domains.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 16:45:00 GMT'}]",2020-10-23,"[['Mosig', 'Johannes E. M.', ''], ['Mehri', 'Shikib', ''], ['Kober', 'Thomas', '']]"
1368148,2010.11818,Hao Zheng,Hao Zheng and Mirella Lapata,Compositional Generalization via Semantic Tagging,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although neural sequence-to-sequence models have been successfully applied to
semantic parsing, they struggle to perform well on query-based data splits that
require \emph{composition generalization}, an ability of systematically
generalizing to unseen composition of seen components. Motivated by the
explicitly built-in compositionality in traditional statistical semantic
parsing, we propose a new decoding framework that preserves the expressivity
and generality of sequence-to-sequence models while featuring explicit
lexicon-style alignments and disentangled information processing. Specifically,
we decompose decoding into two phases where an input utterance is first tagged
with semantic symbols representing the meanings of its individual words, and
then a sequence-to-sequence model is used to predict the final meaning
representation conditioning on the utterance and the predicted tag sequence.
Experimental results on three semantic parsing datasets with query-based splits
show that the proposed approach consistently improves compositional
generalization of sequence-to-sequence models across different model
architectures, domains and semantic formalisms.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 15:55:15 GMT'}]",2020-10-23,"[['Zheng', 'Hao', ''], ['Lapata', 'Mirella', '']]"
1366748,2010.10418,Swarnadeep Saha,"Swarnadeep Saha, Yixin Nie, Mohit Bansal",ConjNLI: Natural Language Inference Over Conjunctive Sentences,EMNLP 2020 (14 pages),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reasoning about conjuncts in conjunctive sentences is important for a deeper
understanding of conjunctions in English and also how their usages and
semantics differ from conjunctive and disjunctive boolean logic. Existing NLI
stress tests do not consider non-boolean usages of conjunctions and use
templates for testing such model knowledge. Hence, we introduce ConjNLI, a
challenge stress-test for natural language inference over conjunctive
sentences, where the premise differs from the hypothesis by conjuncts removed,
added, or replaced. These sentences contain single and multiple instances of
coordinating conjunctions (""and"", ""or"", ""but"", ""nor"") with quantifiers,
negations, and requiring diverse boolean and non-boolean inferences over
conjuncts. We find that large-scale pre-trained language models like RoBERTa do
not understand conjunctive semantics well and resort to shallow heuristics to
make inferences over such sentences. As some initial solutions, we first
present an iterative adversarial fine-tuning method that uses synthetically
created training data based on boolean and non-boolean heuristics. We also
propose a direct model advancement by making RoBERTa aware of predicate
semantic roles. While we observe some performance gains, ConjNLI is still
challenging for current methods, thus encouraging interesting future work for
better understanding of conjunctions. Our data and code are publicly available
at: https://github.com/swarnaHub/ConjNLI
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 16:29:13 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 21:49:00 GMT'}]",2020-10-23,"[['Saha', 'Swarnadeep', ''], ['Nie', 'Yixin', ''], ['Bansal', 'Mohit', '']]"
1367873,2010.11543,Jee-Weon Jung,"Jee-weon Jung, Hee-Soo Heo, Ha-Jin Yu, Joon Son Chung",Graph Attention Networks for Speaker Verification,"5 pages, 1 figure, 2 tables, submitted to ICASSP 2021 as a conference
  paper",,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This work presents a novel back-end framework for speaker verification using
graph attention networks. Segment-wise speaker embeddings extracted from
multiple crops within an utterance are interpreted as node representations of a
graph. The proposed framework inputs segment-wise speaker embeddings from an
enrollment and a test utterance and directly outputs a similarity score. We
first construct a graph using segment-wise speaker embeddings and then input
these to graph attention networks. After a few graph attention layers with
residual connections, each node is projected into a one-dimensional space using
affine transform, followed by a readout operation resulting in a scalar
similarity score. To enable successful adaptation for speaker verification, we
propose techniques such as separating trainable weights for attention map
calculations between segment-wise speaker embeddings from different utterances.
The effectiveness of the proposed framework is validated using three different
speaker embedding extractors trained with different architectures and objective
functions. Experimental results demonstrate consistent improvement over various
baseline back-end classifiers, with an average equal error rate improvement of
20% over the cosine similarity back-end without test time augmentation.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 09:08:02 GMT'}]",2020-10-24,"[['Jung', 'Jee-weon', ''], ['Heo', 'Hee-Soo', ''], ['Yu', 'Ha-Jin', ''], ['Chung', 'Joon Son', '']]"
1368133,2010.11803,Zeqian Li,"Zeqian Li, Jacob Whitehill","Compositional embedding models for speaker identification and
  diarization with simultaneous speech from 2+ speakers",,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a new method for speaker diarization that can handle overlapping
speech with 2+ people. Our method is based on compositional embeddings [1]:
Like standard speaker embedding methods such as x-vector [2], compositional
embedding models contain a function f that separates speech from different
speakers. In addition, they include a composition function g to compute
set-union operations in the embedding space so as to infer the set of speakers
within the input audio. In an experiment on multi-person speaker identification
using synthesized LibriSpeech data, the proposed method outperforms traditional
embedding methods that are only trained to separate single speakers (not
speaker sets). In a speaker diarization experiment on the AMI Headset Mix
corpus, we achieve state-of-the-art accuracy (DER=22.93%), slightly higher than
the previous best result (23.82% from [3]).
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 15:33:36 GMT'}]",2020-10-24,"[['Li', 'Zeqian', ''], ['Whitehill', 'Jacob', '']]"
1368269,2010.11939,Chu-Cheng Lin,"Chu-Cheng Lin and Aaron Jaech and Xin Li and Matt Gormley and Jason
  Eisner",Autoregressive Modeling is Misspecified for Some Sequence Distributions,,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Should sequences be modeled autoregressively---one symbol at a time? How much
computation is needed to predict the next symbol? While local normalization is
cheap, this also limits its power. We point out that some probability
distributions over discrete sequences cannot be well-approximated by any
autoregressive model whose runtime and parameter size grow polynomially in the
sequence length---even though their unnormalized sequence probabilities are
efficient to compute exactly. Intuitively, the probability of the next symbol
can be expensive to compute or approximate (even via randomized algorithms)
when it marginalizes over exponentially many possible futures, which is in
general $\mathrm{NP}$-hard. Our result is conditional on the widely believed
hypothesis that $\mathrm{NP} \nsubseteq \mathrm{P/poly}$ (without which the
polynomial hierarchy would collapse at the second level). This theoretical
observation serves as a caution to the viewpoint that pumping up parameter size
is a straightforward way to improve autoregressive models (e.g., in language
modeling). It also suggests that globally normalized (energy-based) models may
sometimes outperform locally normalized (autoregressive) models, as we
demonstrate experimentally for language modeling.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 17:59:09 GMT'}]",2020-10-24,"[['Lin', 'Chu-Cheng', ''], ['Jaech', 'Aaron', ''], ['Li', 'Xin', ''], ['Gormley', 'Matt', ''], ['Eisner', 'Jason', '']]"
1260776,2003.10421,"Eric M\""uller-Budack","Eric M\""uller-Budack, Jonas Theiner, Sebastian Diering, Maximilian
  Idahl, Ralph Ewerth","Multimodal Analytics for Real-world News using Measures of Cross-modal
  Entity Consistency","Accepted for publication in: International Conference on Multimedia
  Retrieval (ICMR), Dublin, 2020",,,,cs.CL cs.IR cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The World Wide Web has become a popular source for gathering information and
news. Multimodal information, e.g., enriching text with photos, is typically
used to convey the news more effectively or to attract attention. Photo content
can range from decorative, depict additional important information, or can even
contain misleading information. Therefore, automatic approaches to quantify
cross-modal consistency of entity representation can support human assessors to
evaluate the overall multimodal message, for instance, with regard to bias or
sentiment. In some cases such measures could give hints to detect fake news,
which is an increasingly important topic in today's society. In this paper, we
introduce a novel task of cross-modal consistency verification in real-world
news and present a multimodal approach to quantify the entity coherence between
image and text. Named entity linking is applied to extract persons, locations,
and events from news texts. Several measures are suggested to calculate
cross-modal similarity for these entities using state of the art approaches. In
contrast to previous work, our system automatically gathers example data from
the Web and is applicable to real-world news. Results on two novel datasets
that cover different languages, topics, and domains demonstrate the feasibility
of our approach. Datasets and code are publicly available to foster research
towards this new direction.
","[{'version': 'v1', 'created': 'Mon, 23 Mar 2020 17:49:06 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 09:22:53 GMT'}]",2020-10-26,"[['Müller-Budack', 'Eric', ''], ['Theiner', 'Jonas', ''], ['Diering', 'Sebastian', ''], ['Idahl', 'Maximilian', ''], ['Ewerth', 'Ralph', '']]"
1268577,2004.03807,Abhinav Ramesh Kashyap,"Abhinav Ramesh Kashyap, Min-Yen Kan",SciWING -- A Software Toolkit for Scientific Document Processing,"6 pages, 3 figures, First Workshop on Scholarly Document Processing -
  SDP@EMNLP 2020",,,,cs.DL cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce SciWING, an open-source software toolkit which provides access
to pre-trained models for scientific document processing tasks, inclusive of
citation string parsing and logical structure recovery. SciWING enables
researchers to rapidly experiment with different models by swapping and
stacking different modules. It also enables them declare and run models from a
configuration file. It enables researchers to perform production-ready transfer
learning from general, pre-trained transformers (i.e., BERT, SciBERT etc), and
aids development of end-user applications. It includes ready-to-use web and
terminal-based applications and demonstrations (Available from
http://sciwing.io).
","[{'version': 'v1', 'created': 'Wed, 8 Apr 2020 04:43:37 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 07:27:01 GMT'}]",2020-10-26,"[['Kashyap', 'Abhinav Ramesh', ''], ['Kan', 'Min-Yen', '']]"
1278000,2004.13230,Marjan Albooyeh,"Marjan Albooyeh, Rishab Goel, Seyed Mehran Kazemi",Out-of-Sample Representation Learning for Multi-Relational Graphs,,,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many important problems can be formulated as reasoning in knowledge graphs.
Representation learning has proved extremely effective for transductive
reasoning, in which one needs to make new predictions for already observed
entities. This is true for both attributed graphs(where each entity has an
initial feature vector) and non-attributed graphs (where the only initial
information derives from known relations with other entities). For
out-of-sample reasoning, where one needs to make predictions for entities that
were unseen at training time, much prior work considers attributed graph.
However, this problem is surprisingly under-explored for non-attributed graphs.
In this paper, we study the out-of-sample representation learning problem for
non-attributed knowledge graphs, create benchmark datasets for this task,
develop several models and baselines, and provide empirical analyses and
comparisons of the proposed models and baselines.
","[{'version': 'v1', 'created': 'Tue, 28 Apr 2020 00:53:01 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 16:22:50 GMT'}]",2020-10-26,"[['Albooyeh', 'Marjan', ''], ['Goel', 'Rishab', ''], ['Kazemi', 'Seyed Mehran', '']]"
1213482,1912.01586,Tongfei Chen,"Yunmo Chen, Tongfei Chen, Seth Ebner, Aaron Steven White, Benjamin Van
  Durme",Reading the Manual: Event Extraction as Definition Comprehension,Accepted at the EMNLP 2020 Workshop on Structured Prediction for NLP,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We ask whether text understanding has progressed to where we may extract
event information through incremental refinement of bleached statements derived
from annotation manuals. Such a capability would allow for the trivial
construction and extension of an extraction framework by intended end-users
through declarations such as, ""Some person was born in some location at some
time."" We introduce an example of a model that employs such statements, with
experiments illustrating we can extract events under closed ontologies and
generalize to unseen event types simply by reading new definitions.
","[{'version': 'v1', 'created': 'Tue, 3 Dec 2019 18:31:42 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 02:27:24 GMT'}]",2020-10-26,"[['Chen', 'Yunmo', ''], ['Chen', 'Tongfei', ''], ['Ebner', 'Seth', ''], ['White', 'Aaron Steven', ''], ['Van Durme', 'Benjamin', '']]"
1276843,2004.12073,Sho Takase,Sho Takase and Sosuke Kobayashi,All Word Embeddings from One Embedding,NeurIPS 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In neural network-based models for natural language processing (NLP), the
largest part of the parameters often consists of word embeddings. Conventional
models prepare a large embedding matrix whose size depends on the vocabulary
size. Therefore, storing these models in memory and disk storage is costly. In
this study, to reduce the total number of parameters, the embeddings for all
words are represented by transforming a shared embedding. The proposed method,
ALONE (all word embeddings from one), constructs the embedding of a word by
modifying the shared embedding with a filter vector, which is word-specific but
non-trainable. Then, we input the constructed embedding into a feed-forward
neural network to increase its expressiveness. Naively, the filter vectors
occupy the same memory size as the conventional embedding matrix, which depends
on the vocabulary size. To solve this issue, we also introduce a
memory-efficient filter construction approach. We indicate our ALONE can be
used as word representation sufficiently through an experiment on the
reconstruction of pre-trained word embeddings. In addition, we also conduct
experiments on NLP application tasks: machine translation and summarization. We
combined ALONE with the current state-of-the-art encoder-decoder model, the
Transformer, and achieved comparable scores on WMT 2014 English-to-German
translation and DUC 2004 very short summarization with less parameters.
","[{'version': 'v1', 'created': 'Sat, 25 Apr 2020 07:38:08 GMT'}, {'version': 'v2', 'created': 'Mon, 25 May 2020 03:36:32 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Oct 2020 03:12:12 GMT'}]",2020-10-26,"[['Takase', 'Sho', ''], ['Kobayashi', 'Sosuke', '']]"
968940,1804.07247,Dominic Seyler,"Dominic Seyler, Lunan Li, ChengXiang Zhai","Semantic Text Analysis for Detection of Compromised Accounts on Social
  Networks",,,,,cs.SI cs.CL cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Compromised accounts on social networks are regular user accounts that have
been taken over by an entity with malicious intent. Since the adversary
exploits the already established trust of a compromised account, it is crucial
to detect these accounts to limit the damage they can cause. We propose a novel
general framework for semantic analysis of text messages coming out from an
account to detect compromised accounts. Our framework is built on the
observation that normal users will use language that is measurably different
from the language that an adversary would use when the account is compromised.
We propose to use the difference of language models of users and adversaries to
define novel interpretable semantic features for measuring semantic incoherence
in a message stream. We study the effectiveness of the proposed semantic
features using a Twitter data set. Evaluation results show that the proposed
framework is effective for discovering compromised accounts on social networks
and a KL-divergence-based language model feature works best.
","[{'version': 'v1', 'created': 'Thu, 19 Apr 2018 16:06:29 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Feb 2020 18:06:25 GMT'}, {'version': 'v3', 'created': 'Wed, 6 May 2020 21:26:02 GMT'}, {'version': 'v4', 'created': 'Fri, 23 Oct 2020 15:56:27 GMT'}]",2020-10-26,"[['Seyler', 'Dominic', ''], ['Li', 'Lunan', ''], ['Zhai', 'ChengXiang', '']]"
1246550,2002.09127,Eric Yuan,"Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre C\^ot\'e, Mikul\'a\v{s}
  Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang,
  Adam Trischler, William L. Hamilton",Learning Dynamic Belief Graphs to Generalize on Text-Based Games,NeurIPS 2020 cameraready version,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Playing text-based games requires skills in processing natural language and
sequential decision making. Achieving human-level performance on text-based
games remains an open challenge, and prior research has largely relied on
hand-crafted structured representations and heuristics. In this work, we
investigate how an agent can plan and generalize in text-based games using
graph-structured representations learned end-to-end from raw text. We propose a
novel graph-aided transformer agent (GATA) that infers and updates latent
belief graphs during planning to enable effective action selection by capturing
the underlying game dynamics. GATA is trained using a combination of
reinforcement and self-supervised learning. Our work demonstrates that the
learned graph-based representations help agents converge to better policies
than their text-only counterparts and facilitate effective generalization
across game configurations. Experiments on 500+ unique games from the TextWorld
suite show that our best agent outperforms text-based baselines by an average
of 24.2%.
","[{'version': 'v1', 'created': 'Fri, 21 Feb 2020 04:38:37 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Jun 2020 16:22:16 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Oct 2020 20:01:55 GMT'}]",2020-10-26,"[['Adhikari', 'Ashutosh', ''], ['Yuan', 'Xingdi', ''], ['Côté', 'Marc-Alexandre', ''], ['Zelinka', 'Mikuláš', ''], ['Rondeau', 'Marc-Antoine', ''], ['Laroche', 'Romain', ''], ['Poupart', 'Pascal', ''], ['Tang', 'Jian', ''], ['Trischler', 'Adam', ''], ['Hamilton', 'William L.', '']]"
1236729,2001.11316,Akbar Karimi,"Akbar Karimi, Leonardo Rossi, Andrea Prati",Adversarial Training for Aspect-Based Sentiment Analysis with BERT,,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect-Based Sentiment Analysis (ABSA) deals with the extraction of
sentiments and their targets. Collecting labeled data for this task in order to
help neural networks generalize better can be laborious and time-consuming. As
an alternative, similar data to the real-world examples can be produced
artificially through an adversarial process which is carried out in the
embedding space. Although these examples are not real sentences, they have been
shown to act as a regularization method which can make neural networks more
robust. In this work, we apply adversarial training, which was put forward by
Goodfellow et al. (2014), to the post-trained BERT (BERT-PT) language model
proposed by Xu et al. (2019) on the two major tasks of Aspect Extraction and
Aspect Sentiment Classification in sentiment analysis. After improving the
results of post-trained BERT by an ablation study, we propose a novel
architecture called BERT Adversarial Training (BAT) to utilize adversarial
training in ABSA. The proposed model outperforms post-trained BERT in both
tasks. To the best of our knowledge, this is the first study on the application
of adversarial training in ABSA.
","[{'version': 'v1', 'created': 'Thu, 30 Jan 2020 13:53:58 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Jan 2020 12:33:57 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Oct 2020 13:39:32 GMT'}, {'version': 'v4', 'created': 'Fri, 23 Oct 2020 07:39:17 GMT'}]",2020-10-26,"[['Karimi', 'Akbar', ''], ['Rossi', 'Leonardo', ''], ['Prati', 'Andrea', '']]"
1276896,2004.12126,Hongyu Lin,"Hongyu Lin, Yaojie Lu, Jialong Tang, Xianpei Han, Le Sun, Zhicheng
  Wei, Nicholas Jing Yuan","A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained
  Model Lead to the Promised Land?",Accepted to EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fine-tuning pretrained model has achieved promising performance on standard
NER benchmarks. Generally, these benchmarks are blessed with strong name
regularity, high mention coverage and sufficient context diversity.
Unfortunately, when scaling NER to open situations, these advantages may no
longer exist. And therefore it raises a critical question of whether previous
creditable approaches can still work well when facing these challenges. As
there is no currently available dataset to investigate this problem, this paper
proposes to conduct randomization test on standard benchmarks. Specifically, we
erase name regularity, mention coverage and context diversity respectively from
the benchmarks, in order to explore their impact on the generalization ability
of models. To further verify our conclusions, we also construct a new open NER
dataset that focuses on entity types with weaker name regularity and lower
mention coverage to verify our conclusion. From both randomization test and
empirical experiments, we draw the conclusions that 1) name regularity is
critical for the models to generalize to unseen mentions; 2) high mention
coverage may undermine the model generalization ability and 3) context patterns
may not require enormous data to capture when using pretrained encoders.
","[{'version': 'v1', 'created': 'Sat, 25 Apr 2020 12:30:16 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 07:06:06 GMT'}]",2020-10-26,"[['Lin', 'Hongyu', ''], ['Lu', 'Yaojie', ''], ['Tang', 'Jialong', ''], ['Han', 'Xianpei', ''], ['Sun', 'Le', ''], ['Wei', 'Zhicheng', ''], ['Yuan', 'Nicholas Jing', '']]"
1267950,2004.03180,Mamoru Komachi,"Aizhan Imankulova, Masahiro Kaneko, Tosho Hirasawa and Mamoru Komachi",Towards Multimodal Simultaneous Neural Machine Translation,10 pages; WMT 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Simultaneous translation involves translating a sentence before the speaker's
utterance is completed in order to realize real-time understanding in multiple
languages. This task is significantly more challenging than the general full
sentence translation because of the shortage of input information during
decoding. To alleviate this shortage, we propose multimodal simultaneous neural
machine translation (MSNMT), which leverages visual information as an
additional modality. Our experiments with the Multi30k dataset showed that
MSNMT significantly outperforms its text-only counterpart in more timely
translation situations with low latency. Furthermore, we verified the
importance of visual information during decoding by performing an adversarial
evaluation of MSNMT, where we studied how models behaved with incongruent input
modality and analyzed the effect of different word order between source and
target languages.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 08:02:21 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 04:38:38 GMT'}]",2020-10-26,"[['Imankulova', 'Aizhan', ''], ['Kaneko', 'Masahiro', ''], ['Hirasawa', 'Tosho', ''], ['Komachi', 'Mamoru', '']]"
1368731,2010.12401,Gaurish Thakkar Mr,"Gaurish Thakkar, Marcis Pinnis","Pretraining and Fine-Tuning Strategies for Sentiment Analysis of Latvian
  Tweets",,,10.3233/FAIA200602,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present various pre-training strategies that aid in
im-proving the accuracy of the sentiment classification task. We, at first,
pre-trainlanguage representation models using these strategies and then
fine-tune them onthe downstream task. Experimental results on a time-balanced
tweet evaluation setshow the improvement over the previous technique. We
achieve 76% accuracy forsentiment analysis on Latvian tweets, which is a
substantial improvement over pre-vious work
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 13:45:33 GMT'}]",2020-10-26,"[['Thakkar', 'Gaurish', ''], ['Pinnis', 'Marcis', '']]"
1368857,2010.12527,Peng Qi,"Peng Qi, Haejun Lee, Oghenetegiri ""TG"" Sido, Christopher D. Manning","Retrieve, Rerank, Read, then Iterate: Answering Open-Domain Questions of
  Arbitrary Complexity from Text",Peng Qi and Haejun Lee contributed equally,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current approaches to open-domain question answering often make crucial
assumptions that prevent them from generalizing to real-world settings,
including the access to parameterized retrieval systems well-tuned for the
task, access to structured metadata like knowledge bases and web links, or a
priori knowledge of the complexity of questions to be answered (e.g.,
single-hop or multi-hop). To address these limitations, we propose a unified
system to answer open-domain questions of arbitrary complexity directly from
text that works with off-the-shelf retrieval systems on arbitrary text
collections. We employ a single multi-task model to perform all the necessary
subtasks---retrieving supporting facts, reranking them, and predicting the
answer from all retrieved documents---in an iterative fashion. To emulate a
more realistic setting, we also constructed a new unified benchmark by
collecting about 200 multi-hop questions that require three Wikipedia pages to
answer, and combining them with existing datasets. We show that our model not
only outperforms state-of-the-art systems on several existing benchmarks that
exclusively feature single-hop or multi-hop open-domain questions, but also
achieves strong performance on the new benchmark.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 16:51:09 GMT'}]",2020-10-26,"[['Qi', 'Peng', ''], ['Lee', 'Haejun', ''], ['Sido', 'Oghenetegiri ""TG""', ''], ['Manning', 'Christopher D.', '']]"
1368635,2010.12305,Lukas Lange,"Lukas Lange, Heike Adel, Jannik Str\""otgen, Dietrich Klakow",Adversarial Learning of Feature-based Meta-Embeddings,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Certain embedding types outperform others in different scenarios, e.g.,
subword-based embeddings can model rare words well and domain-specific
embeddings can better represent in-domain terms. Therefore, recent works
consider attention-based meta-embeddings to combine different embedding types.
We demonstrate that these methods have two shortcomings: First, the attention
weights are calculated without knowledge of word properties. Second, the
different embedding types can form clusters in the common embedding space,
preventing the computation of a meaningful average of different embeddings and
thus, reducing performance. We propose to solve these problems by using
feature-based meta-embeddings learned with adversarial training. Our
experiments and analysis on sentence classification and sequence tagging tasks
show that our approach is effective. We set the new state of the art on various
datasets across languages and domains.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 11:16:53 GMT'}]",2020-10-26,"[['Lange', 'Lukas', ''], ['Adel', 'Heike', ''], ['Strötgen', 'Jannik', ''], ['Klakow', 'Dietrich', '']]"
1368613,2010.12283,Minjeong Kim,"Minjeong Kim, Gyuwan Kim, Sang-Woo Lee, Jung-Woo Ha","ST-BERT: Cross-modal Language Model Pre-training For End-to-end Spoken
  Language Understanding","5 pages, 2 figures",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language model pre-training has shown promising results in various downstream
tasks. In this context, we introduce a cross-modal pre-trained language model,
called Speech-Text BERT (ST-BERT), to tackle end-to-end spoken language
understanding (E2E SLU) tasks. Taking phoneme posterior and subword-level text
as an input, ST-BERT learns a contextualized cross-modal alignment via our two
proposed pre-training tasks: Cross-modal Masked Language Modeling (CM-MLM) and
Cross-modal Conditioned Language Modeling (CM-CLM). Experimental results on
three benchmarks present that our approach is effective for various SLU
datasets and shows a surprisingly marginal performance degradation even when 1%
of the training data are available. Also, our method shows further SLU
performance gain via domain-adaptive pre-training with domain-specific
speech-text pair data.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 10:28:20 GMT'}]",2020-10-26,"[['Kim', 'Minjeong', ''], ['Kim', 'Gyuwan', ''], ['Lee', 'Sang-Woo', ''], ['Ha', 'Jung-Woo', '']]"
1368602,2010.12272,Zhen Ke,"Zhen Ke, Liang Shi, Erli Meng, Bin Wang, Xipeng Qiu",Pre-trained Model for Chinese Word Segmentation with Meta Learning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent researches show that pre-trained models such as BERT (Devlin et al.,
2019) are beneficial for Chinese Word Segmentation tasks. However, existing
approaches usually finetune pre-trained models directly on a separate
downstream Chinese Word Segmentation corpus. These recent methods don't fully
utilize the prior knowledge of existing segmentation corpora, and don't regard
the discrepancy between the pre-training tasks and the downstream Chinese Word
Segmentation tasks. In this work, we propose a Pre-Trained Model for Chinese
Word Segmentation, which can be abbreviated as PTM-CWS. PTM-CWS model employs a
unified architecture for different segmentation criteria, and is pre-trained on
a joint multi-criteria corpus with meta learning algorithm. Empirical results
show that our PTM-CWS model can utilize the existing prior segmentation
knowledge, reduce the discrepancy between the pre-training tasks and the
downstream Chinese Word Segmentation tasks, and achieve new state-of-the-art
performance on twelve Chinese Word Segmentation corpora.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 10:00:46 GMT'}]",2020-10-26,"[['Ke', 'Zhen', ''], ['Shi', 'Liang', ''], ['Meng', 'Erli', ''], ['Wang', 'Bin', ''], ['Qiu', 'Xipeng', '']]"
1368597,2010.12267,Xinsheng Wang,"Xinsheng Wang, Siyuan Feng, Jihua Zhu, Mark Hasegawa-Johnson, Odette
  Scharenborg",Show and Speak: Directly Synthesize Spoken Description of Images,,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper proposes a new model, referred to as the show and speak (SAS)
model that, for the first time, is able to directly synthesize spoken
descriptions of images, bypassing the need for any text or phonemes. The basic
structure of SAS is an encoder-decoder architecture that takes an image as
input and predicts the spectrogram of speech that describes this image. The
final speech audio is obtained from the predicted spectrogram via WaveNet.
Extensive experiments on the public benchmark database Flickr8k demonstrate
that the proposed SAS is able to synthesize natural spoken descriptions for
images, indicating that synthesizing spoken descriptions for images while
bypassing text and phonemes is feasible.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 09:53:01 GMT'}]",2020-10-26,"[['Wang', 'Xinsheng', ''], ['Feng', 'Siyuan', ''], ['Zhu', 'Jihua', ''], ['Hasegawa-Johnson', 'Mark', ''], ['Scharenborg', 'Odette', '']]"
1368581,2010.12251,Sunghyun Park,"Sunghyun Park, Han Li, Ameen Patel, Sidharth Mudgal, Sungjin Lee,
  Young-Bum Kim, Spyros Matsoukas, Ruhi Sarikaya","A Scalable Framework for Learning From Implicit User Feedback to Improve
  Natural Language Understanding in Large-Scale Conversational AI Systems",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural Language Understanding (NLU) is an established component within a
conversational AI or digital assistant system, and it is responsible for
producing semantic understanding of a user request. We propose a scalable and
automatic approach for improving NLU in a large-scale conversational AI system
by leveraging implicit user feedback, with an insight that user interaction
data and dialog context have rich information embedded from which user
satisfaction and intention can be inferred. In particular, we propose a general
domain-agnostic framework for curating new supervision data for improving NLU
from live production traffic. With an extensive set of experiments, we show the
results of applying the framework and improving NLU for a large-scale
production system and show its impact across 10 domains.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 09:23:44 GMT'}]",2020-10-26,"[['Park', 'Sunghyun', ''], ['Li', 'Han', ''], ['Patel', 'Ameen', ''], ['Mudgal', 'Sidharth', ''], ['Lee', 'Sungjin', ''], ['Kim', 'Young-Bum', ''], ['Matsoukas', 'Spyros', ''], ['Sarikaya', 'Ruhi', '']]"
1368561,2010.12231,Wen-Chin Huang,"Wen-Chin Huang, Yi-Chiao Wu, Tomoki Hayashi, Tomoki Toda","Any-to-One Sequence-to-Sequence Voice Conversion using Self-Supervised
  Discrete Speech Representations",Submitted to ICASSP 2021,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a novel approach to any-to-one (A2O) voice conversion (VC) in a
sequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker,
including those unseen during training, to a fixed target speaker. We utilize
vq-wav2vec (VQW2V), a discretized self-supervised speech representation that
was learned from massive unlabeled data, which is assumed to be
speaker-independent and well corresponds to underlying linguistic contents.
Given a training dataset of the target speaker, we extract VQW2V and acoustic
features to estimate a seq2seq mapping function from the former to the latter.
With the help of a pretraining method and a newly designed postprocessing
technique, our model can be generalized to only 5 min of data, even
outperforming the same model trained with parallel data.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 08:34:52 GMT'}]",2020-10-26,"[['Huang', 'Wen-Chin', ''], ['Wu', 'Yi-Chiao', ''], ['Hayashi', 'Tomoki', ''], ['Toda', 'Tomoki', '']]"
1368528,2010.12198,Abhinav Ramesh Kashyap,"Abhinav Ramesh Kashyap, Devamanyu Hazarika, Min-Yen Kan, Roger
  Zimmermann",Domain Divergences: a Survey and Empirical Analysis,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Domain divergence plays a significant role in estimating the performance of a
model when applied to new domains. While there is significant literature on
divergence measures, choosing an appropriate divergence measures remains
difficult for researchers. We address this shortcoming by both surveying the
literature and through an empirical study. We contribute a taxonomy of
divergence measures consisting of three groups -- Information-theoretic,
Geometric, and Higher-order measures -- and identify the relationships between
them. We then ground the use of divergence measures in three different
application groups -- 1) Data Selection, 2) Learning Representation, and 3)
Decisions in the Wild. From this, we identify that Information-theoretic
measures are prevalent for 1) and 3), and higher-order measures are common for
2). To further help researchers, we validate these uses empirically through a
correlation analysis of performance drops. We consider the current contextual
word representations (CWR) to contrast with the older word distribution based
representations for this analysis. We find that traditional measures over word
distributions still serve as strong baselines, while higher-order measures with
CWR are effective.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 07:12:52 GMT'}]",2020-10-26,"[['Kashyap', 'Abhinav Ramesh', ''], ['Hazarika', 'Devamanyu', ''], ['Kan', 'Min-Yen', ''], ['Zimmermann', 'Roger', '']]"
1368513,2010.12183,Weizhe Lin,"Zhilin Wang, Weizhe Lin, Xiaodong Wu","Identifying Similar Movie Characters Quickly but Effectively Using
  Non-exhaustive Pair-wise Attention",10 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Identifying similar movie characters is a captivating task that can be our
first step to understand the commonalities between human characteristics and
experiences. Here, we seek to identify similar movie character descriptions and
evaluate our findings based on whether they belong to a common fan-curated
trope (theme). Rather than simply comparing the embedding representation of
character description, we use a pair-wise attention model to make use of
complex word/span-level relationships across the two character descriptions to
predict the similarity of the two characters. Naively, such a model would
require the exhaustive comparison of each character to all other characters,
which is an O(n^2) operation with respect to the number of characters, making
it unfeasible to be used in practice. We reduced this into an O(n) operation
using a two-step approach that involves choosing only a tiny fraction of
character-pairs to perform pairwise attention on while still being effective in
this task. Our approach performs at least 9-27% better than methods based on
state-of-the-art paragraph embedding representations.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 06:28:25 GMT'}]",2020-10-26,"[['Wang', 'Zhilin', ''], ['Lin', 'Weizhe', ''], ['Wu', 'Xiaodong', '']]"
1368510,2010.12180,Sanyuan Chen,"Sanyuan Chen, Yu Wu, Zhuo Chen, Takuya Yoshioka, Shujie Liu, Jinyu Li","Don't shoot butterfly with rifles: Multi-channel Continuous Speech
  Separation with Early Exit Transformer",,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With its strong modeling capacity that comes from a multi-head and
multi-layer structure, Transformer is a very powerful model for learning a
sequential representation and has been successfully applied to speech
separation recently. However, multi-channel speech separation sometimes does
not necessarily need such a heavy structure for all time frames especially when
the cross-talker challenge happens only occasionally. For example, in
conversation scenarios, most regions contain only a single active speaker,
where the separation task downgrades to a single speaker enhancement problem.
It turns out that using a very deep network structure for dealing with signals
with a low overlap ratio not only negatively affects the inference efficiency
but also hurts the separation performance. To deal with this problem, we
propose an early exit mechanism, which enables the Transformer model to handle
different cases with adaptive depth. Experimental results indicate that not
only does the early exit mechanism accelerate the inference, but it also
improves the accuracy.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 06:21:11 GMT'}]",2020-10-26,"[['Chen', 'Sanyuan', ''], ['Wu', 'Yu', ''], ['Chen', 'Zhuo', ''], ['Yoshioka', 'Takuya', ''], ['Liu', 'Shujie', ''], ['Li', 'Jinyu', '']]"
1368504,2010.12174,Rubungo Andre Niyongabo,Rubungo Andre Niyongabo and Hong Qu and Julia Kreutzer and Li Huang,"KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for
  Kinyarwanda and Kirundi",COLING 2020,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent progress in text classification has been focused on high-resource
languages such as English and Chinese. For low-resource languages, amongst them
most African languages, the lack of well-annotated data and effective
preprocessing, is hindering the progress and the transfer of successful
methods. In this paper, we introduce two news datasets (KINNEWS and KIRNEWS)
for multi-class classification of news articles in Kinyarwanda and Kirundi, two
low-resource African languages. The two languages are mutually intelligible,
but while Kinyarwanda has been studied in Natural Language Processing (NLP) to
some extent, this work constitutes the first study on Kirundi. Along with the
datasets, we provide statistics, guidelines for preprocessing, and monolingual
and cross-lingual baseline models. Our experiments show that training
embeddings on the relatively higher-resourced Kinyarwanda yields successful
cross-lingual transfer to Kirundi. In addition, the design of the created
datasets allows for a wider use in NLP beyond text classification in future
studies, such as representation learning, cross-lingual learning with more
distant languages, or as base for new annotations for tasks such as parsing,
POS tagging, and NER. The datasets, stopwords, and pre-trained embeddings are
publicly available at https://github.com/Andrews2017/KINNEWS-and-KIRNEWS-Corpus .
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 05:37:42 GMT'}]",2020-10-26,"[['Niyongabo', 'Rubungo Andre', ''], ['Qu', 'Hong', ''], ['Kreutzer', 'Julia', ''], ['Huang', 'Li', '']]"
1368486,2010.12156,Fei Zhao,"Fei Zhao, Zhen Wu, Xinyu Dai",Attention Transfer Network for Aspect-level Sentiment Classification,Accept to COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect-level sentiment classification (ASC) aims to detect the sentiment
polarity of a given opinion target in a sentence. In neural network-based
methods for ASC, most works employ the attention mechanism to capture the
corresponding sentiment words of the opinion target, then aggregate them as
evidence to infer the sentiment of the target. However, aspect-level datasets
are all relatively small-scale due to the complexity of annotation. Data
scarcity causes the attention mechanism sometimes to fail to focus on the
corresponding sentiment words of the target, which finally weakens the
performance of neural models. To address the issue, we propose a novel
Attention Transfer Network (ATN) in this paper, which can successfully exploit
attention knowledge from resource-rich document-level sentiment classification
datasets to improve the attention capability of the aspect-level sentiment
classification task. In the ATN model, we design two different methods to
transfer attention knowledge and conduct experiments on two ASC benchmark
datasets. Extensive experimental results show that our methods consistently
outperform state-of-the-art works. Further analysis also validates the
effectiveness of ATN.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 04:26:33 GMT'}]",2020-10-26,"[['Zhao', 'Fei', ''], ['Wu', 'Zhen', ''], ['Dai', 'Xinyu', '']]"
1368485,2010.12155,Menglong Xu,"Menglong Xu, Shengqiang Li, Xiao-Lei Zhang","Transformer-based End-to-End Speech Recognition with Local Dense
  Synthesizer Attention","5 pages, 3 figures",,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, several studies reported that dot-product selfattention (SA) may
not be indispensable to the state-of-theart Transformer models. Motivated by
the fact that dense synthesizer attention (DSA), which dispenses with dot
products and pairwise interactions, achieved competitive results in many
language processing tasks, in this paper, we first propose a DSA-based speech
recognition, as an alternative to SA. To reduce the computational complexity
and improve the performance, we further propose local DSA (LDSA) to restrict
the attention scope of DSA to a local range around the current central frame
for speech recognition. Finally, we combine LDSA with SA to extract the local
and global information simultaneously. Experimental results on the Ai-shell1
Mandarine speech recognition corpus show that the proposed LDSA-Transformer
achieves a character error rate (CER) of 6.49%, which is slightly better than
that of the SA-Transformer. Meanwhile, the LDSA-Transformer requires less
computation than the SATransformer. The proposed combination method not only
achieves a CER of 6.18%, which significantly outperforms the SA-Transformer,
but also has roughly the same number of parameters and computational complexity
as the latter. The implementation of the multi-head LDSA is available at
https://github.com/mlxu995/multihead-LDSA.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 04:13:44 GMT'}]",2020-10-26,"[['Xu', 'Menglong', ''], ['Li', 'Shengqiang', ''], ['Zhang', 'Xiao-Lei', '']]"
1368478,2010.12148,Yu-Kun Li,"Dongling Xiao, Yu-Kun Li, Han Zhang, Yu Sun, Hao Tian, Hua Wu and
  Haifeng Wang","ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling
  for Natural Language Understanding",work-in-progress,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Coarse-grained linguistic information, such as name entities or phrases,
facilitates adequately representation learning in pre-training. Previous works
mainly focus on extending the objective of BERT's Masked Language Modeling
(MLM) from masking individual tokens to contiguous sequences of n tokens. We
argue that such continuously masking method neglects to model the
inner-dependencies and inter-relation of coarse-grained information. As an
alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to
enhance the integration of coarse-grained information for pre-training. In
ERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram
identities rather than contiguous sequences of tokens. Furthermore, ERNIE-Gram
employs a generator model to sample plausible n-gram identities as optional
n-gram masks and predict them in both coarse-grained and fine-grained manners
to enable comprehensive n-gram prediction and relation modeling. We pre-train
ERNIE-Gram on English and Chinese text corpora and fine-tune on 19 downstream
tasks. Experimental results show that ERNIE-Gram outperforms previous
pre-training models like XLNet and RoBERTa by a large margin, and achieves
comparable results with state-of-the-art methods.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 03:42:20 GMT'}]",2020-10-26,"[['Xiao', 'Dongling', ''], ['Li', 'Yu-Kun', ''], ['Zhang', 'Han', ''], ['Sun', 'Yu', ''], ['Tian', 'Hao', ''], ['Wu', 'Hua', ''], ['Wang', 'Haifeng', '']]"
1368466,2010.12136,Bowen Li,"Bowen Li, Xiaojuan Qi, Philip H. S. Torr, Thomas Lukasiewicz","Lightweight Generative Adversarial Networks for Text-Guided Image
  Manipulation",NeurIPS 2020,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a novel lightweight generative adversarial network for efficient
image manipulation using natural language descriptions. To achieve this, a new
word-level discriminator is proposed, which provides the generator with
fine-grained training feedback at word-level, to facilitate training a
lightweight generator that has a small number of parameters, but can still
correctly focus on specific visual attributes of an image, and then edit them
without affecting other contents that are not described in the text.
Furthermore, thanks to the explicit training signal related to each word, the
discriminator can also be simplified to have a lightweight structure. Compared
with the state of the art, our method has a much smaller number of parameters,
but still achieves a competitive manipulation performance. Extensive
experimental results demonstrate that our method can better disentangle
different visual attributes, then correctly map them to corresponding semantic
words, and thus achieve a more accurate image modification using natural
language descriptions.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 02:43:02 GMT'}]",2020-10-26,"[['Li', 'Bowen', ''], ['Qi', 'Xiaojuan', ''], ['Torr', 'Philip H. S.', ''], ['Lukasiewicz', 'Thomas', '']]"
1368451,2010.12121,Feiliang Ren,"Feiliang Ren, Juchen Li, Huihui Zhang, Shilei Liu, Bochao Li, Ruicheng
  Ming, Yujia Bai",Knowledge Graph Embedding with Atrous Convolution and Residual Learning,"12pages, 2 figures",,,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graph embedding is an important task and it will benefit lots of
downstream applications. Currently, deep neural networks based methods achieve
state-of-the-art performance. However, most of these existing methods are very
complex and need much time for training and inference. To address this issue,
we propose a simple but effective atrous convolution based knowledge graph
embedding method. Compared with existing state-of-the-art methods, our method
has following main characteristics. First, it effectively increases feature
interactions by using atrous convolutions. Second, to address the original
information forgotten issue and vanishing/exploding gradient issue, it uses the
residual learning method. Third, it has simpler structure but much higher
parameter efficiency. We evaluate our method on six benchmark datasets with
different evaluation metrics. Extensive experiments show that our model is very
effective. On these diverse datasets, it achieves better results than the
compared state-of-the-art methods on most of evaluation metrics. The source
codes of our model could be found at https://github.com/neukg/AcrE.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 00:57:23 GMT'}]",2020-10-26,"[['Ren', 'Feiliang', ''], ['Li', 'Juchen', ''], ['Zhang', 'Huihui', ''], ['Liu', 'Shilei', ''], ['Li', 'Bochao', ''], ['Ming', 'Ruicheng', ''], ['Bai', 'Yujia', '']]"
1368434,2010.12104,Siyuan Feng,"Siyuan Feng, Piotr \.Zelasko, Laureano Moro-Vel\'azquez, Ali
  Abavisani, Mark Hasegawa-Johnson, Odette Scharenborg, Najim Dehak",How Phonotactics Affect Multilingual and Zero-shot ASR Performance,"Submitted to ICASSP 2021. The first 2 authors contributed equally to
  this work",,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The idea of combining multiple languages' recordings to train a single
automatic speech recognition (ASR) model brings the promise of the emergence of
universal speech representation. Recently, a Transformer encoder-decoder model
has been shown to leverage multilingual data well in IPA transcriptions of
languages presented during training. However, the representations it learned
were not successful in zero-shot transfer to unseen languages. Because that
model lacks an explicit factorization of the acoustic model (AM) and language
model (LM), it is unclear to what degree the performance suffered from
differences in pronunciation or the mismatch in phonotactics. To gain more
insight into the factors limiting zero-shot ASR transfer, we replace the
encoder-decoder with a hybrid ASR system consisting of a separate AM and LM.
Then, we perform an extensive evaluation of monolingual, multilingual, and
crosslingual (zero-shot) acoustic and language models on a set of 13
phonetically diverse languages. We show that the gain from modeling
crosslingual phonotactics is limited, and imposing a too strong model can hurt
the zero-shot transfer. Furthermore, we find that a multilingual LM hurts a
multilingual ASR system's performance, and retaining only the target language's
phonotactic data in LM training is preferable.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 23:07:24 GMT'}]",2020-10-26,"[['Feng', 'Siyuan', ''], ['Żelasko', 'Piotr', ''], ['Moro-Velázquez', 'Laureano', ''], ['Abavisani', 'Ali', ''], ['Hasegawa-Johnson', 'Mark', ''], ['Scharenborg', 'Odette', ''], ['Dehak', 'Najim', '']]"
1368426,2010.12096,Thibault Doutre,"Thibault Doutre, Wei Han, Min Ma, Zhiyun Lu, Chung-Cheng Chiu, Ruoming
  Pang, Arun Narayanan, Ananya Misra, Yu Zhang, Liangliang Cao","Improving Streaming Automatic Speech Recognition With Non-Streaming
  Model Distillation On Unsupervised Data",,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Streaming end-to-end automatic speech recognition (ASR) models are widely
used on smart speakers and on-device applications. Since these models are
expected to transcribe speech with minimal latency, they are constrained to be
causal with no future context, compared to their non-streaming counterparts.
Consequently, streaming models usually perform worse than non-streaming models.
We propose a novel and effective learning method by leveraging a non-streaming
ASR model as a teacher to generate transcripts on an arbitrarily large data
set, which is then used to distill knowledge into streaming ASR models. This
way, we scale the training of streaming models to up to 3 million hours of
YouTube audio. Experiments show that our approach can significantly reduce the
word error rate (WER) of RNNT models not only on LibriSpeech but also on
YouTube data in four languages. For example, in French, we are able to reduce
the WER by 16.4% relatively to a baseline streaming model by leveraging a
non-streaming teacher model trained on the same amount of labeled data as the
baseline.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 22:41:33 GMT'}]",2020-10-26,"[['Doutre', 'Thibault', ''], ['Han', 'Wei', ''], ['Ma', 'Min', ''], ['Lu', 'Zhiyun', ''], ['Chiu', 'Chung-Cheng', ''], ['Pang', 'Ruoming', ''], ['Narayanan', 'Arun', ''], ['Misra', 'Ananya', ''], ['Zhang', 'Yu', ''], ['Cao', 'Liangliang', '']]"
1368413,2010.12083,Simon Stepputtis,"Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee,
  Chitta Baral, Heni Ben Amor",Language-Conditioned Imitation Learning for Robot Manipulation Tasks,"Accepted to the 34th Conference on Neural Information Processing
  Systems (NeurIPS 2020), Vancouver, Canada as spotlight presentation",,,,cs.RO cs.CL cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Imitation learning is a popular approach for teaching motor skills to robots.
However, most approaches focus on extracting policy parameters from execution
traces alone (i.e., motion trajectories and perceptual data). No adequate
communication channel exists between the human expert and the robot to describe
critical aspects of the task, such as the properties of the target object or
the intended shape of the motion. Motivated by insights into the human teaching
process, we introduce a method for incorporating unstructured natural language
into imitation learning. At training time, the expert can provide
demonstrations along with verbal descriptions in order to describe the
underlying intent (e.g., ""go to the large green bowl""). The training process
then interrelates these two modalities to encode the correlations between
language, perception, and motion. The resulting language-conditioned visuomotor
policies can be conditioned at runtime on new human commands and instructions,
which allows for more fine-grained control over the trained policies while also
reducing situational ambiguity. We demonstrate in a set of simulation
experiments how our approach can learn language-conditioned manipulation
policies for a seven-degree-of-freedom robot arm and compare the results to a
variety of alternative methods.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 21:49:08 GMT'}]",2020-10-26,"[['Stepputtis', 'Simon', ''], ['Campbell', 'Joseph', ''], ['Phielipp', 'Mariano', ''], ['Lee', 'Stefan', ''], ['Baral', 'Chitta', ''], ['Amor', 'Heni Ben', '']]"
1368407,2010.12077,Daiki Shirafuji,"Daiki Shirafuji, Hiromichi Kameya, Rafal Rzepka and Kenji Araki","Summarizing Utterances from Japanese Assembly Minutes using Political
  Sentence-BERT-based Method for QA Lab-PoliInfo-2 Task of NTCIR-15","8 pages, 1 figure, 8 tables, NTCIR-15 conference",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There are many discussions held during political meetings, and a large number
of utterances for various topics is included in their transcripts. We need to
read all of them if we want to follow speakers\' intentions or opinions about a
given topic. To avoid such a costly and time-consuming process to grasp often
longish discussions, NLP researchers work on generating concise summaries of
utterances. Summarization subtask in QA Lab-PoliInfo-2 task of the NTCIR-15
addresses this problem for Japanese utterances in assembly minutes, and our
team (SKRA) participated in this subtask. As a first step for summarizing
utterances, we created a new pre-trained sentence embedding model, i.e. the
Japanese Political Sentence-BERT. With this model, we summarize utterances
without labelled data. This paper describes our approach to solving the task
and discusses its results.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 21:37:28 GMT'}]",2020-10-26,"[['Shirafuji', 'Daiki', ''], ['Kameya', 'Hiromichi', ''], ['Rzepka', 'Rafal', ''], ['Araki', 'Kenji', '']]"
1368639,2010.12309,Lukas Lange,"Michael A. Hedderich, Lukas Lange, Heike Adel, Jannik Str\""otgen,
  Dietrich Klakow","A Survey on Recent Approaches for Natural Language Processing in
  Low-Resource Scenarios",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current developments in natural language processing offer challenges and
opportunities for low-resource languages and domains. Deep neural networks are
known for requiring large amounts of training data which might not be available
in resource-lean scenarios. However, there is also a growing body of works to
improve the performance in low-resource settings. Motivated by fundamental
changes towards neural models and the currently popular pre-train and fine-tune
paradigm, we give an overview of promising approaches for low-resource natural
language processing. After a discussion about the definition of low-resource
scenarios and the different dimensions of data availability, we then examine
methods that enable learning when training data is sparse. This includes
mechanisms to create additional labeled data like data augmentation and distant
supervision as well as transfer learning settings that reduce the need for
target supervision. The survey closes with a brief look into methods suggested
in non-NLP machine learning communities, which might be beneficial for NLP in
low-resource scenarios
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 11:22:01 GMT'}]",2020-10-26,"[['Hedderich', 'Michael A.', ''], ['Lange', 'Lukas', ''], ['Adel', 'Heike', ''], ['Strötgen', 'Jannik', ''], ['Klakow', 'Dietrich', '']]"
1368338,2010.12008,Siamak Shakeri,"Siamak Shakeri, Noah Constant, Mihir Sanjay Kale, Linting Xue","Multilingual Synthetic Question and Answer Generation for Cross-Lingual
  Reading Comprehension",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a simple method to generate large amounts of multilingual question
and answer pairs by a single generative model. These synthetic samples are then
applied to augment the available gold multilingual ones to improve the
performance of multilingual QA models on target languages. Our approach only
requires existence of automatically translated samples from English to the
target domain, thus removing the need for human annotations in the target
languages. Experimental results show our proposed approach achieves significant
gains in a number of multilingual datasets.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 19:59:37 GMT'}]",2020-10-26,"[['Shakeri', 'Siamak', ''], ['Constant', 'Noah', ''], ['Kale', 'Mihir Sanjay', ''], ['Xue', 'Linting', '']]"
1368651,2010.12321,Antoine Tixier,"Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis",BARThez: a Skilled Pretrained French Sequence-to-Sequence Model,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inductive transfer learning, enabled by self-supervised learning, have taken
the entire Natural Language Processing (NLP) field by storm, with models such
as BERT and BART setting new state of the art on countless natural language
understanding tasks. While there are some notable exceptions, most of the
available models and research have been conducted for the English language. In
this work, we introduce BARThez, the first BART model for the French language
(to the best of our knowledge). BARThez was pretrained on a very large
monolingual French corpus from past research that we adapted to suit BART's
perturbation schemes. Unlike already existing BERT-based French language models
such as CamemBERT and FlauBERT, BARThez is particularly well-suited for
generative tasks, since not only its encoder but also its decoder is
pretrained. In addition to discriminative tasks from the FLUE benchmark, we
evaluate BARThez on a novel summarization dataset, OrangeSum, that we release
with this paper. We also continue the pretraining of an already pretrained
multilingual BART on BARThez's corpus, and we show that the resulting model,
which we call mBARTHez, provides a significant boost over vanilla BARThez, and
is on par with or outperforms CamemBERT and FlauBERT.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 11:57:33 GMT'}]",2020-10-26,"[['Eddine', 'Moussa Kamal', ''], ['Tixier', 'Antoine J. -P.', ''], ['Vazirgiannis', 'Michalis', '']]"
1368735,2010.12405,Xin Li,"Xin Li, Lidong Bing, Wenxuan Zhang, Zheng Li, Wai Lam",Unsupervised Cross-lingual Adaptation for Sequence Tagging and Beyond,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cross-lingual adaptation with multilingual pre-trained language models
(mPTLMs) mainly consists of two lines of works: zero-shot approach and
translation-based approach, which have been studied extensively on the
sequence-level tasks. We further verify the efficacy of these cross-lingual
adaptation approaches by evaluating their performances on more fine-grained
sequence tagging tasks. After re-examining their strengths and drawbacks, we
propose a novel framework to consolidate the zero-shot approach and the
translation-based approach for better adaptation performance. Instead of simply
augmenting the source data with the machine-translated data, we tailor-make a
warm-up mechanism to quickly update the mPTLMs with the gradients estimated on
a few translated data. Then, the adaptation approach is applied to the refined
parameters and the cross-lingual transfer is performed in a warm-start way. The
experimental results on nine target languages demonstrate that our method is
beneficial to the cross-lingual adaptation of various sequence tagging tasks.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 13:47:01 GMT'}]",2020-10-26,"[['Li', 'Xin', ''], ['Bing', 'Lidong', ''], ['Zhang', 'Wenxuan', ''], ['Li', 'Zheng', ''], ['Lam', 'Wai', '']]"
1368896,2010.12566,Aditi Chaudhary,"Aditi Chaudhary, Karthik Raman, Krishna Srinivasan, Jiecao Chen","DICT-MLM: Improved Multilingual Pre-Training using Bilingual
  Dictionaries",13 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained multilingual language models such as mBERT have shown immense
gains for several natural language processing (NLP) tasks, especially in the
zero-shot cross-lingual setting. Most, if not all, of these pre-trained models
rely on the masked-language modeling (MLM) objective as the key language
learning objective. The principle behind these approaches is that predicting
the masked words with the help of the surrounding text helps learn potent
contextualized representations. Despite the strong representation learning
capability enabled by MLM, we demonstrate an inherent limitation of MLM for
multilingual representation learning. In particular, by requiring the model to
predict the language-specific token, the MLM objective disincentivizes learning
a language-agnostic representation -- which is a key goal of multilingual
pre-training. Therefore to encourage better cross-lingual representation
learning we propose the DICT-MLM method. DICT-MLM works by incentivizing the
model to be able to predict not just the original masked word, but potentially
any of its cross-lingual synonyms as well. Our empirical analysis on multiple
downstream tasks spanning 30+ languages, demonstrates the efficacy of the
proposed approach and its ability to learn better multilingual representations.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 17:53:11 GMT'}]",2020-10-26,"[['Chaudhary', 'Aditi', ''], ['Raman', 'Karthik', ''], ['Srinivasan', 'Krishna', ''], ['Chen', 'Jiecao', '']]"
1368893,2010.12563,Eric Wallace,"Eric Wallace, Tony Z. Zhao, Shi Feng, Sameer Singh",Customizing Triggers with Concealed Data Poisoning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversarial attacks alter NLP model predictions by perturbing test-time
inputs. However, it is much less understood whether, and how, predictions can
be manipulated with small, concealed changes to the training data. In this
work, we develop a new data poisoning attack that allows an adversary to
control model predictions whenever a desired trigger phrase is present in the
input. For instance, we insert 50 poison examples into a sentiment model's
training set that causes the model to frequently predict Positive whenever the
input contains ""James Bond"". Crucially, we craft these poison examples using a
gradient-based procedure so that they do not mention the trigger phrase. We
also apply our poison attack to language modeling (""Apple iPhone"" triggers
negative generations) and machine translation (""iced coffee"" mistranslated as
""hot coffee""). We conclude by proposing three defenses that can mitigate our
attack at some cost in prediction accuracy or extra human annotation.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 17:47:06 GMT'}]",2020-10-26,"[['Wallace', 'Eric', ''], ['Zhao', 'Tony Z.', ''], ['Feng', 'Shi', ''], ['Singh', 'Sameer', '']]"
1368892,2010.12562,Xiaotao Gu,"Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, Jiawei Han",On the Transformer Growth for Progressive BERT Training,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As the excessive pre-training cost arouses the need to improve efficiency,
considerable efforts have been made to train BERT progressively--start from an
inferior but low-cost model and gradually increase the computational
complexity. Our objective is to help advance the understanding of such
Transformer growth and discover principles that guide progressive training.
First, we find that similar to network architecture selection, Transformer
growth also favors compound scaling. Specifically, while existing methods only
conduct network growth in a single dimension, we observe that it is beneficial
to use compound growth operators and balance multiple dimensions (e.g., depth,
width, and input length of the model). Moreover, we explore alternative growth
operators in each dimension via controlled comparison to give practical
guidance for operator selection. In light of our analyses, the proposed method
CompoundGrow speeds up BERT pre-training by 73.6% and 82.2% for the base and
large models respectively while achieving comparable performances. Code will be
released for reproduction and future studies.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 17:44:59 GMT'}]",2020-10-26,"[['Gu', 'Xiaotao', ''], ['Liu', 'Liyuan', ''], ['Yu', 'Hongkun', ''], ['Li', 'Jing', ''], ['Chen', 'Chen', ''], ['Han', 'Jiawei', '']]"
1368877,2010.12547,Lin Pan,"Lin Pan, Chung-Wei Hang, Haode Qi, Abhishek Shah, Mo Yu, Saloni Potdar",Multilingual BERT Post-Pretraining Alignment,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a simple method to align multilingual contextual embeddings as a
post-pretraining step for improved zero-shot cross-lingual transferability of
the pretrained models. Using parallel data, our method aligns embeddings on the
word level through the recently proposed Translation Language Modeling
objective as well as on the sentence level via contrastive learning and random
input shuffling. We also perform code-switching with English when finetuning on
downstream tasks. On XNLI, our best model (initialized from mBERT) improves
over mBERT by 4.7% in the zero-shot setting and achieves comparable result to
XLM for translate-train while using less than 18% of the same parallel data and
31% less model parameters. On MLQA, our model outperforms XLM-R_Base that has
57% more parameters than ours.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 17:14:41 GMT'}]",2020-10-26,"[['Pan', 'Lin', ''], ['Hang', 'Chung-Wei', ''], ['Qi', 'Haode', ''], ['Shah', 'Abhishek', ''], ['Yu', 'Mo', ''], ['Potdar', 'Saloni', '']]"
1368862,2010.12532,Nicole Peinelt,"Nicole Peinelt, Marek Rei and Maria Liakata","GiBERT: Introducing Linguistic Knowledge into BERT through a Lightweight
  Gated Injection Method",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large pre-trained language models such as BERT have been the driving force
behind recent improvements across many NLP tasks. However, BERT is only trained
to predict missing words - either behind masks or in the next sentence - and
has no knowledge of lexical, syntactic or semantic information beyond what it
picks up through unsupervised pre-training. We propose a novel method to
explicitly inject linguistic knowledge in the form of word embeddings into any
layer of a pre-trained BERT. Our performance improvements on multiple semantic
similarity datasets when injecting dependency-based and counter-fitted
embeddings indicate that such information is beneficial and currently missing
from the original model. Our qualitative analysis shows that counter-fitted
embedding injection particularly helps with cases involving synonym pairs.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 17:00:26 GMT'}]",2020-10-26,"[['Peinelt', 'Nicole', ''], ['Rei', 'Marek', ''], ['Liakata', 'Maria', '']]"
1368853,2010.12523,Yinfei Yang,"Jing Lu, Gustavo Hernandez Abrego, Ji Ma, Jianmo Ni, Yinfei Yang",Neural Passage Retrieval with Improved Negative Contrast,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we explore the effects of negative sampling in dual encoder
models used to retrieve passages for automatic question answering. We explore
four negative sampling strategies that complement the straightforward random
sampling of negatives, typically used to train dual encoder models. Out of the
four strategies, three are based on retrieval and one on heuristics. Our
retrieval-based strategies are based on the semantic similarity and the lexical
overlap between questions and passages. We train the dual encoder models in two
stages: pre-training with synthetic data and fine tuning with domain-specific
data. We apply negative sampling to both stages. The approach is evaluated in
two passage retrieval tasks. Even though it is not evident that there is one
single sampling strategy that works best in all the tasks, it is clear that our
strategies contribute to improving the contrast between the response and all
the other passages. Furthermore, mixing the negatives from different strategies
achieve performance on par with the best performing strategy in all tasks. Our
results establish a new state-of-the-art level of performance on two of the
open-domain question answering datasets that we evaluated.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 16:45:06 GMT'}]",2020-10-26,"[['Lu', 'Jing', ''], ['Abrego', 'Gustavo Hernandez', ''], ['Ma', 'Ji', ''], ['Ni', 'Jianmo', ''], ['Yang', 'Yinfei', '']]"
1368842,2010.12512,Linyi Yang,"Linyi Yang, Eoin M. Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, and
  Ruihai Dong","Generating Plausible Counterfactual Explanations for Deep Transformers
  in Financial Text Classification",Accepted by COLING-20 (Oral),,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Corporate mergers and acquisitions (M&A) account for billions of dollars of
investment globally every year, and offer an interesting and challenging domain
for artificial intelligence. However, in these highly sensitive domains, it is
crucial to not only have a highly robust and accurate model, but be able to
generate useful explanations to garner a user's trust in the automated system.
Regrettably, the recent research regarding eXplainable AI (XAI) in financial
text classification has received little to no attention, and many current
methods for generating textual-based explanations result in highly implausible
explanations, which damage a user's trust in the system. To address these
issues, this paper proposes a novel methodology for producing plausible
counterfactual explanations, whilst exploring the regularization benefits of
adversarial training on language models in the domain of FinTech. Exhaustive
quantitative experiments demonstrate that not only does this approach improve
the model accuracy when compared to the current state-of-the-art and human
performance, but it also generates counterfactual explanations which are
significantly more plausible based on human trials.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 16:29:26 GMT'}]",2020-10-26,"[['Yang', 'Linyi', ''], ['Kenny', 'Eoin M.', ''], ['Ng', 'Tin Lok James', ''], ['Yang', 'Yi', ''], ['Smyth', 'Barry', ''], ['Dong', 'Ruihai', '']]"
1368840,2010.12510,Nafise Sadat Moosavi,"Nafise Sadat Moosavi, Marcel de Boer, Prasetya Ajie Utama, Iryna
  Gurevych","Improving Robustness by Augmenting Training Sentences with
  Predicate-Argument Structures",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing NLP datasets contain various biases, and models tend to quickly
learn those biases, which in turn limits their robustness. Existing approaches
to improve robustness against dataset biases mostly focus on changing the
training objective so that models learn less from biased examples. Besides,
they mostly focus on addressing a specific bias, and while they improve the
performance on adversarial evaluation sets of the targeted bias, they may bias
the model in other ways, and therefore, hurt the overall robustness. In this
paper, we propose to augment the input sentences in the training data with
their corresponding predicate-argument structures, which provide a higher-level
abstraction over different realizations of the same meaning and help the model
to recognize important parts of sentences. We show that without targeting a
specific bias, our sentence augmentation improves the robustness of transformer
models against multiple biases. In addition, we show that models can still be
vulnerable to the lexical overlap bias, even when the training data does not
contain this bias, and that the sentence augmentation also improves the
robustness in this scenario. We will release our adversarial datasets to
evaluate bias in such a scenario as well as our augmentation scripts at
https://github.com/UKPLab/data-augmentation-for-robustness.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 16:22:05 GMT'}]",2020-10-26,"[['Moosavi', 'Nafise Sadat', ''], ['de Boer', 'Marcel', ''], ['Utama', 'Prasetya Ajie', ''], ['Gurevych', 'Iryna', '']]"
1368835,2010.12505,Tim Draws,"Tim Draws, Jody Liu, Nava Tintarev","Helping users discover perspectives: Enhancing opinion mining with joint
  topic models","Accepted at the SENTIRE workshop at ICDM 2020:
  https://sentic.net/sentire/#2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Support or opposition concerning a debated claim such as abortion should be
legal can have different underlying reasons, which we call perspectives. This
paper explores how opinion mining can be enhanced with joint topic modeling, to
identify distinct perspectives within the topic, providing an informative
overview from unstructured text. We evaluate four joint topic models (TAM, JST,
VODUM, and LAM) in a user study assessing human understandability of the
extracted perspectives. Based on the results, we conclude that joint topic
models such as TAM can discover perspectives that align with human judgments.
Moreover, our results suggest that users are not influenced by their
pre-existing stance on the topic of abortion when interpreting the output of
topic models.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 16:13:06 GMT'}]",2020-10-26,"[['Draws', 'Tim', ''], ['Liu', 'Jody', ''], ['Tintarev', 'Nava', '']]"
1368827,2010.12497,Omid Ghahabi,"Omid Ghahabi, Volker Fischer",EML System Description for VoxCeleb Speaker Diarization Challenge 2020,,,,,cs.SD cs.CL eess.AS,http://creativecommons.org/licenses/by/4.0/,"  This technical report describes the EML submission to the first VoxCeleb
speaker diarization challenge. Although the aim of the challenge has been the
offline processing of the signals, the submitted system is basically the EML
online algorithm which decides about the speaker labels in runtime
approximately every 1.2 sec. For the first phase of the challenge, only
VoxCeleb2 dev dataset was used for training. The results on the provided
VoxConverse dev set show much better accuracy in terms of both DER and JER
compared to the offline baseline provided in the challenge. The real-time
factor of the whole diarization process is about 0.01 using a single CPU
machine.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 16:01:28 GMT'}]",2020-10-26,"[['Ghahabi', 'Omid', ''], ['Fischer', 'Volker', '']]"
1368825,2010.12495,Daniel Deutsch,"Daniel Deutsch, Dan Roth","Understanding the Extent to which Summarization Evaluation Metrics
  Measure the Information Quality of Summaries",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reference-based metrics such as ROUGE or BERTScore evaluate the content
quality of a summary by comparing the summary to a reference. Ideally, this
comparison should measure the summary's information quality by calculating how
much information the summaries have in common. In this work, we analyze the
token alignments used by ROUGE and BERTScore to compare summaries and argue
that their scores largely cannot be interpreted as measuring information
overlap, but rather the extent to which they discuss the same topics. Further,
we provide evidence that this result holds true for many other summarization
evaluation metrics. The consequence of this result is that it means the
summarization community has not yet found a reliable automatic metric that
aligns with its research goal, to generate summaries with high-quality
information. Then, we propose a simple and interpretable method of evaluating
summaries which does directly measure information overlap and demonstrate how
it can be used to gain insights into model behavior that could not be provided
by other methods alone.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 15:55:15 GMT'}]",2020-10-26,"[['Deutsch', 'Daniel', ''], ['Roth', 'Dan', '']]"
1368817,2010.12487,Damien Garreau,Dina Mardaoui and Damien Garreau,An Analysis of LIME for Text Data,"29 pages, 17 figures",,,,stat.ML cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text data are increasingly handled in an automated fashion by machine
learning algorithms. But the models handling these data are not always
well-understood due to their complexity and are more and more often referred to
as ""black-boxes."" Interpretability methods aim to explain how these models
operate. Among them, LIME has become one of the most popular in recent years.
However, it comes without theoretical guarantees: even for simple models, we
are not sure that LIME behaves accurately. In this paper, we provide a first
theoretical analysis of LIME for text data. As a consequence of our theoretical
findings, we show that LIME indeed provides meaningful explanations for simple
models, namely decision trees and linear models.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 15:40:13 GMT'}]",2020-10-26,"[['Mardaoui', 'Dina', ''], ['Garreau', 'Damien', '']]"
1368803,2010.12473,Henning Wachsmuth,Henning Wachsmuth and Till Werner,Intrinsic Quality Assessment of Arguments,Accepted at COLING 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Several quality dimensions of natural language arguments have been
investigated. Some are likely to be reflected in linguistic features (e.g., an
argument's arrangement), whereas others depend on context (e.g., relevance) or
topic knowledge (e.g., acceptability). In this paper, we study the intrinsic
computational assessment of 15 dimensions, i.e., only learning from an
argument's text. In systematic experiments with eight feature types on an
existing corpus, we observe moderate but significant learning success for most
dimensions. Rhetorical quality seems hardest to assess, and subjectivity
features turn out strong, although length bias in the corpus impedes full
validity. We also find that human assessors differ more clearly to each other
than to our approach.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 15:16:10 GMT'}]",2020-10-26,"[['Wachsmuth', 'Henning', ''], ['Werner', 'Till', '']]"
1368802,2010.12472,Tommaso Caselli,"Tommaso Caselli, Valerio Basile, Jelena Mitrovi\'c, Michael Granitzer",HateBERT: Retraining BERT for Abusive Language Detection in English,,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In this paper, we introduce HateBERT, a re-trained BERT model for abusive
language detection in English. The model was trained on RAL-E, a large-scale
dataset of Reddit comments in English from communities banned for being
offensive, abusive, or hateful that we have collected and made available to the
public. We present the results of a detailed comparison between a general
pre-trained language model and the abuse-inclined version obtained by
retraining with posts from the banned communities on three English datasets for
offensive, abusive language and hate speech detection tasks. In all datasets,
HateBERT outperforms the corresponding general BERT model. We also discuss a
battery of experiments comparing the portability of the general pre-trained
language model and its corresponding abusive language-inclined counterpart
across the datasets, indicating that portability is affected by compatibility
of the annotated phenomena.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 15:14:14 GMT'}]",2020-10-26,"[['Caselli', 'Tommaso', ''], ['Basile', 'Valerio', ''], ['Mitrović', 'Jelena', ''], ['Granitzer', 'Michael', '']]"
1368763,2010.12433,Gaurish Thakkar Mr,"Diego Alves, Gaurish Thakkar, Marko Tadi\'c","Natural Language Processing Chains Inside a Cross-lingual Event-Centric
  Knowledge Pipeline for European Union Under-resourced Languages",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This article presents the strategy for developing a platform containing
Language Processing Chains for European Union languages, consisting of
Tokenization to Parsing, also including Named Entity recognition andwith
addition ofSentiment Analysis. These chains are part of the first step of an
event-centric knowledge processing pipeline whose aim is to process
multilingual media information about major events that can cause an impactin
Europe and the rest of the world. Due to the differences in terms of
availability of language resources for each language, we have built this
strategy in three steps, starting with processing chains for the well-resourced
languages and finishing with the development of new modules for the
under-resourced ones. In order to classify all European Union official
languages in terms of resources, we have analysed the size of annotated corpora
as well as the existence of pre-trained models in mainstream Language
Processing tools, and we have combined this information with the proposed
classification published at META-NETwhitepaper series.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 14:26:30 GMT'}]",2020-10-26,"[['Alves', 'Diego', ''], ['Thakkar', 'Gaurish', ''], ['Tadić', 'Marko', '']]"
1368758,2010.12428,Gaurish Thakkar Mr,"Diego Alves, Gaurish Thakkar, Marko Tadi\'c","Evaluating Language Tools for Fifteen EU-official Under-resourced
  Languages",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This article presents the results of the evaluation campaign of language
tools available for fifteen EU-official under-resourced languages. The
evaluation was conducted within the MSC ITN CLEOPATRA action that aims at
building the cross-lingual event-centric knowledge processing on top of the
application of linguistic processing chains (LPCs) for at least 24 EU-official
languages. In this campaign, we concentrated on three existing NLP platforms
(Stanford CoreNLP, NLP Cube, UDPipe) that all provide models for
under-resourced languages and in this first run we covered 15 under-resourced
languages for which the models were available. We present the design of the
evaluation campaign and present the results as well as discuss them. We
considered the difference between reported and our tested results within a
single percentage point as being within the limits of acceptable tolerance and
thus consider this result as reproducible. However, for a number of languages,
the results are below what was reported in the literature, and in some cases,
our testing results are even better than the ones reported previously.
Particularly problematic was the evaluation of NERC systems. One of the reasons
is the absence of universally or cross-lingually applicable named entities
classification scheme that would serve the NERC task in different languages
analogous to the Universal Dependency scheme in parsing task. To build such a
scheme has become one of our the future research directions.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 14:21:03 GMT'}]",2020-10-26,"[['Alves', 'Diego', ''], ['Thakkar', 'Gaurish', ''], ['Tadić', 'Marko', '']]"
1368748,2010.12418,Ahmed Al-Ali,"Ahmed Ghanim Al-Ali, Robert Phaal, Donald Sull","Deep Learning Framework for Measuring the Digital Strategy of Companies
  from Earnings Calls","Accepted for The 28th International Conference on Computational
  Linguistics, 9 pages, 1 figure",,,,cs.CL cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Companies today are racing to leverage the latest digital technologies, such
as artificial intelligence, blockchain, and cloud computing. However, many
companies report that their strategies did not achieve the anticipated business
results. This study is the first to apply state of the art NLP models on
unstructured data to understand the different clusters of digital strategy
patterns that companies are Adopting. We achieve this by analyzing earnings
calls from Fortune Global 500 companies between 2015 and 2019. We use
Transformer based architecture for text classification which show a better
understanding of the conversation context. We then investigate digital strategy
patterns by applying clustering analysis. Our findings suggest that Fortune 500
companies use four distinct strategies which are product led, customer
experience led, service led, and efficiency led. This work provides an
empirical baseline for companies and researchers to enhance our understanding
of the field.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 14:07:12 GMT'}]",2020-10-26,"[['Al-Ali', 'Ahmed Ghanim', ''], ['Phaal', 'Robert', ''], ['Sull', 'Donald', '']]"
1368742,2010.12412,Ohad Rubin,Ohad Rubin and Jonathan Berant,SmBoP: Semi-autoregressive Bottom-up Semantic Parsing,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The de-facto standard decoding method for semantic parsing in recent years
has been to autoregressively decode the abstract syntax tree of the target
program using a top-down depth-first traversal. In this work, we propose an
alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that
constructs at decoding step $t$ the top-$K$ sub-trees of height $\leq t$. Our
parser enjoys several benefits compared to top-down autoregressive parsing.
First, since sub-trees in each decoding step are generated in parallel, the
theoretical runtime is logarithmic rather than linear. Second, our bottom-up
approach learns representations with meaningful semantic sub-programs at each
step, rather than semantically vague partial trees. Last, SmBoP includes
Transformer-based layers that contextualize sub-trees with one another,
allowing us, unlike traditional beam-search, to score trees conditioned on
other trees that have been previously explored. We apply SmBoP on Spider, a
challenging zero-shot semantic parsing benchmark, and show that SmBoP is
competitive with top-down autoregressive parsing. On the test set, SmBoP
obtains an EM score of $60.5\%$, similar to the best published score for a
model that does not use database content, which is at $60.6\%$.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 14:02:32 GMT'}]",2020-10-26,"[['Rubin', 'Ohad', ''], ['Berant', 'Jonathan', '']]"
1368736,2010.12406,Gaurish Thakkar Mr,"Diego Alves, Tin Kuculo, Gabriel Amaral, Gaurish Thakkar, and Marko
  Tadic",UNER: Universal Named-Entity RecognitionFramework,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce the Universal Named-Entity Recognition (UNER)framework, a
4-level classification hierarchy, and the methodology that isbeing adopted to
create the first multilingual UNER corpus: the SETimesparallel corpus annotated
for named-entities. First, the English SETimescorpus will be annotated using
existing tools and knowledge bases. Afterevaluating the resulting annotations
through crowdsourcing campaigns,they will be propagated automatically to other
languages within the SE-Times corpora. Finally, as an extrinsic evaluation, the
UNER multilin-gual dataset will be used to train and test available NER tools.
As part offuture research directions, we aim to increase the number of
languages inthe UNER corpus and to investigate possible ways of integrating
UNERwith available knowledge graphs to improve named-entity recognition.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 13:53:31 GMT'}]",2020-10-26,"[['Alves', 'Diego', ''], ['Kuculo', 'Tin', ''], ['Amaral', 'Gabriel', ''], ['Thakkar', 'Gaurish', ''], ['Tadic', 'Marko', '']]"
1368652,2010.12322,Lukas Lange,"Lukas Lange, Xiang Dai, Heike Adel, Jannik Str\""otgen","NLNDE at CANTEMIST: Neural Sequence Labeling and Parsing Approaches for
  Clinical Concept Extraction",IberLEF 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recognition and normalization of clinical information, such as tumor
morphology mentions, is an important, but complex process consisting of
multiple subtasks. In this paper, we describe our system for the CANTEMIST
shared task, which is able to extract, normalize and rank ICD codes from
Spanish electronic health records using neural sequence labeling and parsing
approaches with context-aware embeddings. Our best system achieves 85.3 F1,
76.7 F1, and 77.0 MAP for the three tasks, respectively.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 11:59:28 GMT'}]",2020-10-26,"[['Lange', 'Lukas', ''], ['Dai', 'Xiang', ''], ['Adel', 'Heike', ''], ['Strötgen', 'Jannik', '']]"
1368327,2010.11997,Zhanwen Chen,"Zhanwen Chen, Shiyao Li, Roxanne Rashedi, Xiaoman Zi, Morgan
  Elrod-Erickson, Bryan Hollis, Angela Maliakal, Xinyu Shen, Simeng Zhao,
  Maithilee Kunda","Characterizing Datasets for Social Visual Question Answering, and the
  New TinySocial Dataset","To appear in the Joint IEEE International Conference on Development
  and Learning and on Epigenetic Robotics (ICDL), 2020",,,,cs.HC cs.CL cs.CV cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern social intelligence includes the ability to watch videos and answer
questions about social and theory-of-mind-related content, e.g., for a scene in
Harry Potter, ""Is the father really upset about the boys flying the car?""
Social visual question answering (social VQA) is emerging as a valuable
methodology for studying social reasoning in both humans (e.g., children with
autism) and AI agents. However, this problem space spans enormous variations in
both videos and questions. We discuss methods for creating and characterizing
social VQA datasets, including 1) crowdsourcing versus in-house authoring,
including sample comparisons of two new datasets that we created
(TinySocial-Crowd and TinySocial-InHouse) and the previously existing Social-IQ
dataset; 2) a new rubric for characterizing the difficulty and content of a
given video; and 3) a new rubric for characterizing question types. We close by
describing how having well-characterized social VQA datasets will enhance the
explainability of AI agents and can also inform assessments and educational
interventions for people.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 03:20:23 GMT'}]",2020-10-26,"[['Chen', 'Zhanwen', ''], ['Li', 'Shiyao', ''], ['Rashedi', 'Roxanne', ''], ['Zi', 'Xiaoman', ''], ['Elrod-Erickson', 'Morgan', ''], ['Hollis', 'Bryan', ''], ['Maliakal', 'Angela', ''], ['Shen', 'Xinyu', ''], ['Zhao', 'Simeng', ''], ['Kunda', 'Maithilee', '']]"
1368553,2010.12223,Richard Moot,"Richard Moot (TEXTE, LIRMM, CNRS)",Proof-theoretic aspects of NL$\lambda$,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a proof-theoretic analysis of the logic NL$\lambda$ (Barker \&
Shan 2014, Barker 2019). We notably introduce a novel calculus of proof nets
and prove it is sound and complete with respect to the sequent calculus for the
logic. We study decidability and complexity of the logic using this new
calculus, proving a new upper bound for complexity of the logic (showing it is
in NP) and a new lower bound for the class of formal language generated by the
formalism (mildly context-sensitive languages extended with a permutation
closure operation). Finally, thanks to this new calculus, we present a novel
comparison between NL$\lambda$ and the hybrid type-logical grammars of Kubota
\& Levine (2020). We show there is an unexpected convergence of the natural
language analyses proposed in the two formalism. In addition to studying the
proof-theoretic properties of NL$\lambda$, we greatly extends its linguistic
coverage.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 08:13:39 GMT'}]",2020-10-26,"[['Moot', 'Richard', '', 'TEXTE, LIRMM, CNRS']]"
1368315,2010.11985,Jianing Yang,"Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir
  Zadeh, Soujanya Poria, Louis-Philippe Morency","MTGAT: Multimodal Temporal Graph Attention Networks for Unaligned Human
  Multimodal Language Sequences",,,,,cs.CL cs.CV cs.LG cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human communication is multimodal in nature; it is through multiple
modalities, i.e., language, voice, and facial expressions, that opinions and
emotions are expressed. Data in this domain exhibits complex multi-relational
and temporal interactions. Learning from this data is a fundamentally
challenging research problem. In this paper, we propose Multimodal Temporal
Graph Attention Networks (MTGAT). MTGAT is an interpretable graph-based neural
model that provides a suitable framework for analyzing this type of multimodal
sequential data. We first introduce a procedure to convert unaligned multimodal
sequence data into a graph with heterogeneous nodes and edges that captures the
rich interactions between different modalities through time. Then, a novel
graph operation, called Multimodal Temporal Graph Attention, along with a
dynamic pruning and read-out technique is designed to efficiently process this
multimodal temporal graph. By learning to focus only on the important
interactions within the graph, our MTGAT is able to achieve state-of-the-art
performance on multimodal sentiment analysis and emotion recognition benchmarks
including IEMOCAP and CMU-MOSI, while utilizing significantly fewer
computations.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 18:58:50 GMT'}]",2020-10-26,"[['Yang', 'Jianing', ''], ['Wang', 'Yongxin', ''], ['Yi', 'Ruitao', ''], ['Zhu', 'Yuying', ''], ['Rehman', 'Azaan', ''], ['Zadeh', 'Amir', ''], ['Poria', 'Soujanya', ''], ['Morency', 'Louis-Philippe', '']]"
1352535,2009.11032,Arie Cattan,"Arie Cattan, Alon Eirew, Gabriel Stanovsky, Mandar Joshi, and Ido
  Dagan","Streamlining Cross-Document Coreference Resolution: Evaluation and
  Modeling",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent evaluation protocols for Cross-document (CD) coreference resolution
have often been inconsistent or lenient, leading to incomparable results across
works and overestimation of performance. To facilitate proper future research
on this task, our primary contribution is proposing a pragmatic evaluation
methodology which assumes access to only raw text -- rather than assuming gold
mentions, disregards singleton prediction, and addresses typical targeted
settings in CD coreference resolution. Aiming to set baseline results for
future research that would follow our evaluation methodology, we build the
first end-to-end model for this task. Our model adapts and extends recent
neural models for within-document coreference resolution to address the CD
coreference setting, which outperforms state-of-the-art results by a
significant margin.
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 10:02:10 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 12:18:01 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Oct 2020 13:40:30 GMT'}]",2020-10-26,"[['Cattan', 'Arie', ''], ['Eirew', 'Alon', ''], ['Stanovsky', 'Gabriel', ''], ['Joshi', 'Mandar', ''], ['Dagan', 'Ido', '']]"
1355321,2009.13818,Dinghan Shen,"Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, Weizhu Chen","A Simple but Tough-to-Beat Data Augmentation Approach for Natural
  Language Understanding and Generation",Source code is available at: https://github.com/dinghanshen/cutoff,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversarial training has been shown effective at endowing the learned
representations with stronger generalization ability. However, it typically
requires expensive computation to determine the direction of the injected
perturbations. In this paper, we introduce a set of simple yet effective data
augmentation strategies dubbed cutoff, where part of the information within an
input sentence is erased to yield its restricted views (during the fine-tuning
stage). Notably, this process relies merely on stochastic sampling and thus
adds little computational overhead. A Jensen-Shannon Divergence consistency
loss is further utilized to incorporate these augmented samples into the
training objective in a principled manner. To verify the effectiveness of the
proposed strategies, we apply cutoff to both natural language understanding and
generation problems. On the GLUE benchmark, it is demonstrated that cutoff, in
spite of its simplicity, performs on par or better than several competitive
adversarial-based approaches. We further extend cutoff to machine translation
and observe significant gains in BLEU scores (based upon the Transformer Base
model). Moreover, cutoff consistently outperforms adversarial training and
achieves state-of-the-art results on the IWSLT2014 German-English dataset.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 07:08:35 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 03:19:58 GMT'}]",2020-10-26,"[['Shen', 'Dinghan', ''], ['Zheng', 'Mingzhi', ''], ['Shen', 'Yelong', ''], ['Qu', 'Yanru', ''], ['Chen', 'Weizhu', '']]"
1368318,2010.11988,Bailin Wang,"Bailin Wang, Mirella Lapata and Ivan Titov",Meta-Learning for Domain Generalization in Semantic Parsing,V1.0,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The importance of building semantic parsers which can be applied to new
domains and generate programs unseen at training has long been acknowledged,
and datasets testing out-of-domain performance are becoming increasingly
available. However, little or no attention has been devoted to studying
learning algorithms or objectives which promote domain generalization, with
virtually all existing approaches relying on standard supervised learning. In
this work, we use a meta-learning framework which targets specifically
zero-shot domain generalization for semantic parsing. We apply a model-agnostic
training algorithm that simulates zero-shot parsing by constructing virtual
train and test sets from disjoint domains. The learning objective capitalizes
on the intuition that gradient steps that improve source-domain performance
should also improve target-domain performance, thus encouraging a parser to
generalize well to unseen target domains. Experimental results on the (English)
Spider and Chinese Spider datasets show that the meta-learning objective
significantly boosts the performance of a baseline parser.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 19:00:36 GMT'}]",2020-10-26,"[['Wang', 'Bailin', ''], ['Lapata', 'Mirella', ''], ['Titov', 'Ivan', '']]"
1287478,2005.07683,Victor Sanh,"Victor Sanh, Thomas Wolf, Alexander M. Rush",Movement Pruning: Adaptive Sparsity by Fine-Tuning,"14 pages, 6 figures, 3 tables. Published at NeurIPS2020. Code:
  \url{huggingface.co/mvp}",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Magnitude pruning is a widely used strategy for reducing model size in pure
supervised learning; however, it is less effective in the transfer learning
regime that has become standard for state-of-the-art natural language
processing applications. We propose the use of movement pruning, a simple,
deterministic first-order weight pruning method that is more adaptive to
pretrained model fine-tuning. We give mathematical foundations to the method
and compare it to existing zeroth- and first-order pruning methods. Experiments
show that when pruning large pretrained language models, movement pruning shows
significant improvements in high-sparsity regimes. When combined with
distillation, the approach achieves minimal accuracy loss with down to only 3%
of the model parameters.
","[{'version': 'v1', 'created': 'Fri, 15 May 2020 17:54:15 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 16:14:58 GMT'}]",2020-10-26,"[['Sanh', 'Victor', ''], ['Wolf', 'Thomas', ''], ['Rush', 'Alexander M.', '']]"
1359487,2010.03157,Sheng Bi,"Sheng Bi and Xiya Cheng and Yuan-Fang Li and Yongzhen Wang and Guilin
  Qi","Knowledge-enriched, Type-constrained and Grammar-guided Question
  Generation over Knowledge Bases",Accepted by COLING 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Question generation over knowledge bases (KBQG) aims at generating
natural-language questions about a subgraph, i.e. a set of (connected) triples.
Two main challenges still face the current crop of encoder-decoder-based
methods, especially on small subgraphs: (1) low diversity and poor fluency due
to the limited information contained in the subgraphs, and (2) semantic drift
due to the decoder's oblivion of the semantics of the answer entity. We propose
an innovative knowledge-enriched, type-constrained and grammar-guided KBQG
model, named KTG, to addresses the above challenges. In our model, the encoder
is equipped with auxiliary information from the KB, and the decoder is
constrained with word types during QG. Specifically, entity domain and
description, as well as relation hierarchy information are considered to
construct question contexts, while a conditional copy mechanism is incorporated
to modulate question semantics according to current word types. Besides, a
novel reward function featuring grammatical similarity is designed to improve
both generative richness and syntactic correctness via reinforcement learning.
Extensive experiments show that our proposed model outperforms existing methods
by a significant margin on two widely-used benchmark datasets SimpleQuestion
and PathQuestion.
","[{'version': 'v1', 'created': 'Wed, 7 Oct 2020 04:49:48 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Oct 2020 05:39:58 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Oct 2020 03:32:38 GMT'}]",2020-10-26,"[['Bi', 'Sheng', ''], ['Cheng', 'Xiya', ''], ['Li', 'Yuan-Fang', ''], ['Wang', 'Yongzhen', ''], ['Qi', 'Guilin', '']]"
1362970,2010.06640,Gathika Ratnayaka,"Gathika Ratnayaka, Thushari Atapattu, Mahen Herath, Georgia Zhang,
  Katrina Falkner",Enhancing the Identification of Cyberbullying through Participant Roles,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cyberbullying is a prevalent social problem that inflicts detrimental
consequences to the health and safety of victims such as psychological
distress, anti-social behaviour, and suicide. The automation of cyberbullying
detection is a recent but widely researched problem, with current research
having a strong focus on a binary classification of bullying versus
non-bullying. This paper proposes a novel approach to enhancing cyberbullying
detection through role modeling. We utilise a dataset from ASKfm to perform
multi-class classification to detect participant roles (e.g. victim, harasser).
Our preliminary results demonstrate promising performance including 0.83 and
0.76 of F1-score for cyberbullying and role classification respectively,
outperforming baselines.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 19:13:07 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 01:15:20 GMT'}]",2020-10-26,"[['Ratnayaka', 'Gathika', ''], ['Atapattu', 'Thushari', ''], ['Herath', 'Mahen', ''], ['Zhang', 'Georgia', ''], ['Falkner', 'Katrina', '']]"
1279095,2004.14325,Daniel Loureiro,Daniel Loureiro and Jose Camacho-Collados,"Don't Neglect the Obvious: On the Role of Unambiguous Words in Word
  Sense Disambiguation",Accepted to EMNLP 2020. Website: http://danlou.github.io/uwa,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State-of-the-art methods for Word Sense Disambiguation (WSD) combine two
different features: the power of pre-trained language models and a propagation
method to extend the coverage of such models. This propagation is needed as
current sense-annotated corpora lack coverage of many instances in the
underlying sense inventory (usually WordNet). At the same time, unambiguous
words make for a large portion of all words in WordNet, while being poorly
covered in existing sense-annotated corpora. In this paper, we propose a simple
method to provide annotations for most unambiguous words in a large corpus. We
introduce the UWA (Unambiguous Word Annotations) dataset and show how a
state-of-the-art propagation-based model can use it to extend the coverage and
quality of its word sense embeddings by a significant margin, improving on its
original results on WSD.
","[{'version': 'v1', 'created': 'Wed, 29 Apr 2020 16:51:21 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Oct 2020 08:57:53 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Oct 2020 09:20:11 GMT'}]",2020-10-26,"[['Loureiro', 'Daniel', ''], ['Camacho-Collados', 'Jose', '']]"
1365865,2010.09535,Michelle Yuan,"Michelle Yuan, Hsuan-Tien Lin, Jordan Boyd-Graber",Cold-start Active Learning through Self-supervised Language Modeling,Published in EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Active learning strives to reduce annotation costs by choosing the most
critical examples to label. Typically, the active learning strategy is
contingent on the classification model. For instance, uncertainty sampling
depends on poorly calibrated model confidence scores. In the cold-start
setting, active learning is impractical because of model instability and data
scarcity. Fortunately, modern NLP provides an additional source of information:
pre-trained language models. The pre-training loss can find examples that
surprise the model and should be labeled for efficient fine-tuning. Therefore,
we treat the language modeling loss as a proxy for classification uncertainty.
With BERT, we develop a simple strategy based on the masked language modeling
loss that minimizes labeling costs for text classification. Compared to other
baselines, our approach reaches higher accuracy within less sampling iterations
and computation time.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 14:09:17 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 18:51:04 GMT'}]",2020-10-26,"[['Yuan', 'Michelle', ''], ['Lin', 'Hsuan-Tien', ''], ['Boyd-Graber', 'Jordan', '']]"
1300709,2006.06195,Zhe Gan,"Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu","Large-Scale Adversarial Training for Vision-and-Language Representation
  Learning",NeurIPS 2020 Spotlight paper,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present VILLA, the first known effort on large-scale adversarial training
for vision-and-language (V+L) representation learning. VILLA consists of two
training stages: (i) task-agnostic adversarial pre-training; followed by (ii)
task-specific adversarial finetuning. Instead of adding adversarial
perturbations on image pixels and textual tokens, we propose to perform
adversarial training in the embedding space of each modality. To enable
large-scale training, we adopt the ""free"" adversarial training strategy, and
combine it with KL-divergence-based regularization to promote higher invariance
in the embedding space. We apply VILLA to current best-performing V+L models,
and achieve new state of the art on a wide range of tasks, including Visual
Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval,
Referring Expression Comprehension, Visual Entailment, and NLVR2.
","[{'version': 'v1', 'created': 'Thu, 11 Jun 2020 05:14:35 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 18:12:53 GMT'}]",2020-10-26,"[['Gan', 'Zhe', ''], ['Chen', 'Yen-Chun', ''], ['Li', 'Linjie', ''], ['Zhu', 'Chen', ''], ['Cheng', 'Yu', ''], ['Liu', 'Jingjing', '']]"
1301414,2006.06900,Yue Wu,"Yue Wu, Pan Zhou, Andrew Gordon Wilson, Eric P. Xing, Zhiting Hu","Improving GAN Training with Probability Ratio Clipping and Sample
  Reweighting",NeurIPS 2020 camera ready version,,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite success on a wide range of problems related to vision, generative
adversarial networks (GANs) often suffer from inferior performance due to
unstable training, especially for text generation. To solve this issue, we
propose a new variational GAN training framework which enjoys superior training
stability. Our approach is inspired by a connection of GANs and reinforcement
learning under a variational perspective. The connection leads to (1)
probability ratio clipping that regularizes generator training to prevent
excessively large updates, and (2) a sample re-weighting mechanism that
improves discriminator training by downplaying bad-quality fake samples.
Moreover, our variational GAN framework can provably overcome the training
issue in many GANs that an optimal discriminator cannot provide any informative
gradient to training generator. By plugging the training approach in diverse
state-of-the-art GAN architectures, we obtain significantly improved
performance over a range of tasks, including text generation, text style
transfer, and image generation.
","[{'version': 'v1', 'created': 'Fri, 12 Jun 2020 01:39:48 GMT'}, {'version': 'v2', 'created': 'Tue, 30 Jun 2020 15:02:27 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Oct 2020 03:24:01 GMT'}]",2020-10-26,"[['Wu', 'Yue', ''], ['Zhou', 'Pan', ''], ['Wilson', 'Andrew Gordon', ''], ['Xing', 'Eric P.', ''], ['Hu', 'Zhiting', '']]"
1366371,2010.10041,Chi-Liang Liu,"Chi-Liang Liu and Tsung-Yuan Hsu and Yung-Sung Chuang and Chung-Yi Li
  and Hung-yi Lee","Language Representation in Multilingual BERT and its applications to
  improve Cross-lingual Generalization",preprint,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A token embedding in multilingual BERT (m-BERT) contains both language and
semantic information. We find that representation of a language can be obtained
by simply averaging the embeddings of the tokens of the language. With the
language representation, we can control the output languages of multilingual
BERT by manipulating the token embeddings and achieve unsupervised token
translation. We further propose a computationally cheap but effective approach
to improve the cross-lingual ability of m-BERT based on the observation.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 05:41:35 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 07:26:02 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Oct 2020 05:47:46 GMT'}]",2020-10-26,"[['Liu', 'Chi-Liang', ''], ['Hsu', 'Tsung-Yuan', ''], ['Chuang', 'Yung-Sung', ''], ['Li', 'Chung-Yi', ''], ['Lee', 'Hung-yi', '']]"
1302851,2006.08337,Jinfeng Xiao,"Jinfeng Xiao, Lidan Wang, Franck Dernoncourt, Trung Bui, Tong Sun,
  Jiawei Han",Open-Domain Question Answering with Pre-Constructed Question Spaces,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open-domain question answering aims at solving the task of locating the
answers to user-generated questions in massive collections of documents. There
are two families of solutions available: retriever-readers, and
knowledge-graph-based approaches. A retriever-reader usually first uses
information retrieval methods like TF-IDF to locate some documents or
paragraphs that are likely to be relevant to the question, and then feeds the
retrieved text to a neural network reader to extract the answer. Alternatively,
knowledge graphs can be constructed from the corpus and be queried against to
answer user questions. We propose a novel algorithm with a reader-retriever
structure that differs from both families. Our reader-retriever first uses an
offline reader to read the corpus and generate collections of all answerable
questions associated with their answers, and then uses an online retriever to
respond to user queries by searching the pre-constructed question spaces for
answers that are most likely to be asked in the given way. We further combine
retriever-reader and reader-retriever results into one single answer by
examining the consistency between the two components. We claim that our
algorithm solves some bottlenecks in existing work, and demonstrate that it
achieves superior accuracy on real-world datasets.
","[{'version': 'v1', 'created': 'Tue, 2 Jun 2020 04:31:09 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 02:10:07 GMT'}]",2020-10-26,"[['Xiao', 'Jinfeng', ''], ['Wang', 'Lidan', ''], ['Dernoncourt', 'Franck', ''], ['Bui', 'Trung', ''], ['Sun', 'Tong', ''], ['Han', 'Jiawei', '']]"
1366693,2010.10363,Laurel Orr,"Laurel Orr, Megan Leszczynski, Simran Arora, Sen Wu, Neel Guha, Xiao
  Ling, Christopher Re","Bootleg: Chasing the Tail with Self-Supervised Named Entity
  Disambiguation",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A challenge for named entity disambiguation (NED), the task of mapping
textual mentions to entities in a knowledge base, is how to disambiguate
entities that appear rarely in the training data, termed tail entities. Humans
use subtle reasoning patterns based on knowledge of entity facts, relations,
and types to disambiguate unfamiliar entities. Inspired by these patterns, we
introduce Bootleg, a self-supervised NED system that is explicitly grounded in
reasoning patterns for disambiguation. We define core reasoning patterns for
disambiguation, create a learning procedure to encourage the self-supervised
model to learn the patterns, and show how to use weak supervision to enhance
the signals in the training data. Encoding the reasoning patterns in a simple
Transformer architecture, Bootleg meets or exceeds state-of-the-art on three
NED benchmarks. We further show that the learned representations from Bootleg
successfully transfer to other non-disambiguation tasks that require
entity-based knowledge: we set a new state-of-the-art in the popular TACRED
relation extraction task by 1.0 F1 points and demonstrate up to 8% performance
lift in highly optimized production search and assistant tasks at a major
technology company
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 15:17:49 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 02:19:08 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Oct 2020 16:21:13 GMT'}]",2020-10-26,"[['Orr', 'Laurel', ''], ['Leszczynski', 'Megan', ''], ['Arora', 'Simran', ''], ['Wu', 'Sen', ''], ['Guha', 'Neel', ''], ['Ling', 'Xiao', ''], ['Re', 'Christopher', '']]"
1367073,2010.10743,Jianhao Yan,"Jianhao Yan, Fandong Meng, Jie Zhou",Multi-Unit Transformers for Neural Machine Translation,Accepted as a main conference paper in EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer models achieve remarkable success in Neural Machine Translation.
Many efforts have been devoted to deepening the Transformer by stacking several
units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while
the investigation over multiple parallel units draws little attention. In this
paper, we propose the Multi-Unit Transformers (MUTE), which aim to promote the
expressiveness of the Transformer by introducing diverse and complementary
units. Specifically, we use several parallel units and show that modeling with
multiple units improves model performance and introduces diversity. Further, to
better leverage the advantage of the multi-unit setting, we design biased
module and sequential dependency that guide and encourage complementariness
among different units. Experimental results on three machine translation tasks,
the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18
Chinese-to-English, show that the MUTE models significantly outperform the
Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild
drop in inference speed (about 3.1%). In addition, our methods also surpass the
Transformer-Big model, with only 54\% of its parameters. These results
demonstrate the effectiveness of the MUTE, as well as its efficiency in both
the inference process and parameter usage.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 03:41:49 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 11:33:45 GMT'}]",2020-10-26,"[['Yan', 'Jianhao', ''], ['Meng', 'Fandong', ''], ['Zhou', 'Jie', '']]"
1366215,2010.09885,Seyone Chithrananda,"Seyone Chithrananda, Gabriel Grand and Bharath Ramsundar","ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular
  Property Prediction",Submitted to NeurIPS 2020 ML for Molecules Workshop,,,,cs.LG cs.CL physics.chem-ph q-bio.BM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  GNNs and chemical fingerprints are the predominant approaches to representing
molecules for property prediction. However, in NLP, transformers have become
the de-facto standard for representation learning thanks to their strong
downstream task transfer. In parallel, the software ecosystem around
transformers is maturing rapidly, with libraries like HuggingFace and BertViz
enabling streamlined training and introspection. In this work, we make one of
the first attempts to systematically evaluate transformers on molecular
property prediction tasks via our ChemBERTa model. ChemBERTa scales well with
pretraining dataset size, offering competitive downstream performance on
MoleculeNet and useful attention-based visualization modalities. Our results
suggest that transformers offer a promising avenue of future work for molecular
representation learning and property prediction. To facilitate these efforts,
we release a curated dataset of 77M SMILES from PubChem suitable for
large-scale self-supervised pretraining.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 21:41:41 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 04:22:37 GMT'}]",2020-10-26,"[['Chithrananda', 'Seyone', ''], ['Grand', 'Gabriel', ''], ['Ramsundar', 'Bharath', '']]"
1368312,2010.11982,Avia Efrat,Avia Efrat and Omer Levy,The Turking Test: Can Language Models Understand Instructions?,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Supervised machine learning provides the learner with a set of input-output
examples of the target task. Humans, however, can also learn to perform new
tasks from instructions in natural language. Can machines learn to understand
instructions as well? We present the Turking Test, which examines a model's
ability to follow natural language instructions of varying complexity. These
range from simple tasks, like retrieving the nth word of a sentence, to ones
that require creativity, such as generating examples for SNLI and SQuAD in
place of human intelligence workers (""turkers""). Despite our lenient evaluation
methodology, we observe that a large pretrained language model performs poorly
across all tasks. Analyzing the model's error patterns reveals that the model
tends to ignore explicit instructions and often generates outputs that cannot
be construed as an attempt to solve the task. While it is not yet clear whether
instruction understanding can be captured by traditional language models, the
sheer expressivity of instruction understanding makes it an appealing
alternative to the rising few-shot inference paradigm.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 18:44:16 GMT'}]",2020-10-26,"[['Efrat', 'Avia', ''], ['Levy', 'Omer', '']]"
1368296,2010.11966,David Lowell,"David Lowell, Brian E. Howard, Zachary C. Lipton, Byron C. Wallace","Unsupervised Data Augmentation with Naive Augmentation and without
  Unlabeled Data",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unsupervised Data Augmentation (UDA) is a semi-supervised technique that
applies a consistency loss to penalize differences between a model's
predictions on (a) observed (unlabeled) examples; and (b) corresponding
'noised' examples produced via data augmentation. While UDA has gained
popularity for text classification, open questions linger over which design
decisions are necessary and over how to extend the method to sequence labeling
tasks. This method has recently gained traction for text classification. In
this paper, we re-examine UDA and demonstrate its efficacy on several
sequential tasks. Our main contribution is an empirical study of UDA to
establish which components of the algorithm confer benefits in NLP. Notably,
although prior work has emphasized the use of clever augmentation techniques
including back-translation, we find that enforcing consistency between
predictions assigned to observed and randomly substituted words often yields
comparable (or greater) benefits compared to these complex perturbation models.
Furthermore, we find that applying its consistency loss affords meaningful
gains without any unlabeled data at all, i.e., in a standard supervised
setting. In short: UDA need not be unsupervised, and does not require complex
data augmentation to be effective.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 18:01:51 GMT'}]",2020-10-26,"[['Lowell', 'David', ''], ['Howard', 'Brian E.', ''], ['Lipton', 'Zachary C.', ''], ['Wallace', 'Byron C.', '']]"
1368303,2010.11973,Badr M. Abdullah,"Badr M. Abdullah, Jacek Kudera, Tania Avgustinova, Bernd M\""obius,
  Dietrich Klakow","Rediscovering the Slavic Continuum in Representations Emerging from
  Neural Models of Spoken Language Identification",Accepted in VarDial 2020 Workshop,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Deep neural networks have been employed for various spoken language
recognition tasks, including tasks that are multilingual by definition such as
spoken language identification. In this paper, we present a neural model for
Slavic language identification in speech signals and analyze its emergent
representations to investigate whether they reflect objective measures of
language relatedness and/or non-linguists' perception of language similarity.
While our analysis shows that the language representation space indeed captures
language relatedness to a great extent, we find perceptual confusability
between languages in our study to be the best predictor of the language
representation similarity.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 18:18:19 GMT'}]",2020-10-26,"[['Abdullah', 'Badr M.', ''], ['Kudera', 'Jacek', ''], ['Avgustinova', 'Tania', ''], ['Möbius', 'Bernd', ''], ['Klakow', 'Dietrich', '']]"
1368310,2010.11980,Tuan Manh Lai,"Tuan Manh Lai, Trung Bui, Doo Soon Kim, Quan Hung Tran","A Joint Learning Approach based on Self-Distillation for Keyphrase
  Extraction from Scientific Documents",Accepted to COLING 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Keyphrase extraction is the task of extracting a small set of phrases that
best describe a document. Most existing benchmark datasets for the task
typically have limited numbers of annotated documents, making it challenging to
train increasingly complex neural networks. In contrast, digital libraries
store millions of scientific articles online, covering a wide range of topics.
While a significant portion of these articles contain keyphrases provided by
their authors, most other articles lack such kind of annotations. Therefore, to
effectively utilize these large amounts of unlabeled articles, we propose a
simple and efficient joint learning approach based on the idea of
self-distillation. Experimental results show that our approach consistently
improves the performance of baseline models for keyphrase extraction.
Furthermore, our best models outperform previous methods for the task,
achieving new state-of-the-art results on two public benchmarks: Inspec and
SemEval-2017.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 18:36:31 GMT'}]",2020-10-26,"[['Lai', 'Tuan Manh', ''], ['Bui', 'Trung', ''], ['Kim', 'Doo Soon', ''], ['Tran', 'Quan Hung', '']]"
1368277,2010.11947,Zekun Xu,"Zekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan, Nathanael Teissier","A Differentially Private Text Perturbation Method Using a Regularized
  Mahalanobis Metric","11 pages, 7 figures",,,,cs.CL cs.CR cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Balancing the privacy-utility tradeoff is a crucial requirement of many
practical machine learning systems that deal with sensitive customer data. A
popular approach for privacy-preserving text analysis is noise injection, in
which text data is first mapped into a continuous embedding space, perturbed by
sampling a spherical noise from an appropriate distribution, and then projected
back to the discrete vocabulary space. While this allows the perturbation to
admit the required metric differential privacy, often the utility of downstream
tasks modeled on this perturbed data is low because the spherical noise does
not account for the variability in the density around different words in the
embedding space. In particular, words in a sparse region are likely unchanged
even when the noise scale is large. %Using the global sensitivity of the
mechanism can potentially add too much noise to the words in the dense regions
of the embedding space, causing a high utility loss, whereas using local
sensitivity can leak information through the scale of the noise added.
  In this paper, we propose a text perturbation mechanism based on a carefully
designed regularized variant of the Mahalanobis metric to overcome this
problem. For any given noise scale, this metric adds an elliptical noise to
account for the covariance structure in the embedding space. This heterogeneity
in the noise scale along different directions helps ensure that the words in
the sparse region have sufficient likelihood of replacement without sacrificing
the overall utility. We provide a text-perturbation algorithm based on this
metric and formally prove its privacy guarantees. Additionally, we empirically
show that our mechanism improves the privacy statistics to achieve the same
level of utility as compared to the state-of-the-art Laplace mechanism.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 23:06:44 GMT'}]",2020-10-26,"[['Xu', 'Zekun', ''], ['Aggarwal', 'Abhinav', ''], ['Feyisetan', 'Oluwaseyi', ''], ['Teissier', 'Nathanael', '']]"
1368297,2010.11967,Chenguang Wang,"Chenguang Wang, Xiao Liu, Dawn Song",Language Models are Open Knowledge Graphs,"30 pages, 32 figures, 3 tables",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper shows how to construct knowledge graphs (KGs) from pre-trained
language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs
(e.g, Wikidata, NELL) are built in either a supervised or semi-supervised
manner, requiring humans to create knowledge. Recent deep language models
automatically acquire knowledge from large-scale corpora via pre-training. The
stored knowledge has enabled the language models to improve downstream NLP
tasks, e.g., answering questions, and writing code and articles. In this paper,
we propose an unsupervised method to cast the knowledge contained within
language models into KGs. We show that KGs are constructed with a single
forward pass of the pre-trained language models (without fine-tuning) over the
corpora. We demonstrate the quality of the constructed KGs by comparing to two
KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual
knowledge that is new in the existing KGs. Our code and KGs will be made
publicly available.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 18:01:56 GMT'}]",2020-10-26,"[['Wang', 'Chenguang', ''], ['Liu', 'Xiao', ''], ['Song', 'Dawn', '']]"
1131884,1905.13448,Xuenan Xu,"Xuenan Xu, Heinrich Dinkel, Mengyue Wu, Kai Yu",Audio Caption in a Car Setting with a Sentence-Level Loss,,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Captioning has attracted much attention in image and video understanding
while a small amount of work examines audio captioning. This paper contributes
a Mandarin-annotated dataset for audio captioning within a car scene. A
sentence-level loss is proposed to be used in tandem with a GRU encoder-decoder
model to generate captions with higher semantic similarity to human
annotations. We evaluate the model on the newly-proposed Car dataset, a
previously published Mandarin Hospital dataset and the Joint dataset,
indicating its generalization capability across different scenes. An
improvement in all metrics can be observed, including classical natural
language generation (NLG) metrics, sentence richness and human evaluation
ratings. However, though detailed audio captions can now be automatically
generated, human annotations still outperform model captions on many aspects.
","[{'version': 'v1', 'created': 'Fri, 31 May 2019 07:30:15 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 06:58:36 GMT'}]",2020-10-26,"[['Xu', 'Xuenan', ''], ['Dinkel', 'Heinrich', ''], ['Wu', 'Mengyue', ''], ['Yu', 'Kai', '']]"
1367987,2010.11657,Renyu Wang,"Renyu Wang, Ruilin Tong, Yu Ting Yeung, Xiao Chen","The HUAWEI Speaker Diarisation System for the VoxCeleb Speaker
  Diarisation Challenge","5 pages, 2 figures, A report about our diarisation system for
  VoxCeleb Challenge, Interspeech conference workshop",,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes system setup of our submission to speaker diarisation
track (Track 4) of VoxCeleb Speaker Recognition Challenge 2020. Our diarisation
system consists of a well-trained neural network based speech enhancement model
as pre-processing front-end of input speech signals. We replace conventional
energy-based voice activity detection (VAD) with a neural network based VAD.
The neural network based VAD provides more accurate annotation of speech
segments containing only background music, noise, and other interference, which
is crucial to diarisation performance. We apply agglomerative hierarchical
clustering (AHC) of x-vectors and variational Bayesian hidden Markov model
(VB-HMM) based iterative clustering for speaker clustering. Experimental
results demonstrate that our proposed system achieves substantial improvements
over the baseline system, yielding diarisation error rate (DER) of 10.45%, and
Jacard error rate (JER) of 22.46% on the evaluation set.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 12:42:07 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 07:45:47 GMT'}]",2020-10-26,"[['Wang', 'Renyu', ''], ['Tong', 'Ruilin', ''], ['Yeung', 'Yu Ting', ''], ['Chen', 'Xiao', '']]"
1367808,2010.11478,Minho Ryu,Minho Ryu and Kichun Lee,Knowledge Distillation for BERT Unsupervised Domain Adaptation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A pre-trained language model, BERT, has brought significant performance
improvements across a range of natural language processing tasks. Since the
model is trained on a large corpus of diverse topics, it shows robust
performance for domain shift problems in which data distributions at training
(source data) and testing (target data) differ while sharing similarities.
Despite its great improvements compared to previous models, it still suffers
from performance degradation due to domain shifts. To mitigate such problems,
we propose a simple but effective unsupervised domain adaptation method,
adversarial adaptation with distillation (AAD), which combines the adversarial
discriminative domain adaptation (ADDA) framework with knowledge distillation.
We evaluate our approach in the task of cross-domain sentiment classification
on 30 domain pairs, advancing the state-of-the-art performance for unsupervised
domain adaptation in text sentiment classification.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 06:51:24 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 02:12:06 GMT'}]",2020-10-26,"[['Ryu', 'Minho', ''], ['Lee', 'Kichun', '']]"
1367725,2010.11395,Xie Chen,"Xie Chen, Yu Wu, Zhenghao Wang, Shujie Liu, Jinyu Li","Developing Real-time Streaming Transformer Transducer for Speech
  Recognition on Large-scale Dataset",5 pages,,,,cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, Transformer based end-to-end models have achieved great success in
many areas including speech recognition. However, compared to LSTM models, the
heavy computational cost of the Transformer during inference is a key issue to
prevent their applications. In this work, we explored the potential of
Transformer Transducer (T-T) models for the fist pass decoding with low latency
and fast speed on a large-scale dataset. We combine the idea of Transformer-XL
and chunk-wise streaming processing to design a streamable Transformer
Transducer model. We demonstrate that T-T outperforms the hybrid model, RNN
Transducer (RNN-T), and streamable Transformer attention-based encoder-decoder
model in the streaming scenario. Furthermore, the runtime cost and latency can
be optimized with a relatively small look-ahead.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 03:01:21 GMT'}]",2020-10-26,"[['Chen', 'Xie', ''], ['Wu', 'Yu', ''], ['Wang', 'Zhenghao', ''], ['Liu', 'Shujie', ''], ['Li', 'Jinyu', '']]"
1369125,2010.12795,Navita Goyal,"Navita Goyal, Roodram Paneri, Ayush Agarwal, Udit Kalani, Abhilasha
  Sancheti, Niyati Chhaya",CaM-Gen:Causally-aware Metric-guided Text Generation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Content is created for a well-defined purpose, often described by a metric or
a signal represented in the form of structured information. The relationship
between the metrics or the goal of a target content and the content itself are
non-trivial. While large scale language models show promising text generation
capabilities, guiding and informing the generated text with external metrics is
challenging. These metrics and the content tend to have inherent relationships
and not all of them may directly impact the content. We introduce a CaM-Gen:
Causally-aware Generative Networks guided by user-defined input metrics
incorporating the causal relationships between the metric and the content
features. We leverage causal inference techniques to identify the causally
significant aspects of text that leads to the target metric and then explicitly
guide the generative model towards these by a feedback mechanism. We propose
this mechanism for variational autoencoder-based and transformer-based
generative models. The proposed models beat baselines in terms of the target
metric accuracy while maintaining the fluency and the language quality of the
generated text. To the best of our knowledge, this is one of the early attempts
at incorporating a metric-guide using causal inference towards controlled
generation.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 06:17:35 GMT'}]",2020-10-27,"[['Goyal', 'Navita', ''], ['Paneri', 'Roodram', ''], ['Agarwal', 'Ayush', ''], ['Kalani', 'Udit', ''], ['Sancheti', 'Abhilasha', ''], ['Chhaya', 'Niyati', '']]"
1369124,2010.12794,Zihan Wang,Zihan Wang and Dheeraj Mekala and Jingbo Shang,X-Class: Text Classification with Extremely Weak Supervision,,,,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we explore to conduct text classification with extremely weak
supervision, i.e., only relying on the surface text of class names. This is a
more challenging setting than the seed-driven weak supervision, which allows a
few seed words per class. We opt to attack this problem from a representation
learning perspective -- ideal document representations should lead to very
close results between clustering and the desired classification. In particular,
one can classify the same corpus differently (e.g., based on topics and
locations), so document representations must be adaptive to the given class
names. We propose a novel framework X-Class to realize it. Specifically, we
first estimate comprehensive class representations by incrementally adding the
most similar word to each class until inconsistency appears. Following a
tailored mixture of class attention mechanisms, we obtain the document
representation via a weighted average of contextualized token representations.
We then cluster and align the documents to classes with the prior of each
document assigned to its nearest class. Finally, we pick the most confident
documents from each cluster to train a text classifier. Extensive experiments
demonstrate that X-Class can rival and even outperform seed-driven weakly
supervised methods on 7 benchmark datasets.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 06:09:51 GMT'}]",2020-10-27,"[['Wang', 'Zihan', ''], ['Mekala', 'Dheeraj', ''], ['Shang', 'Jingbo', '']]"
1369119,2010.12789,Limin Zhang,Limin Zhang,"Exploration of NLU: disassemble the information represented by Natural
  Language, based on the understanding of the internal structure of
  information, modeling the storage and processing system of information","13 pages, 8 figures, 11 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Natural language is one of the ways information is encoded and it has highly
abstracted and conceptualized the information. This paper disassembles the
information represented by natural language, analyzes the classification coding
system of attribute information and the abstraction relation between attribute
information and entities in the real world, constructs the storage model of
information, and simulate the attribute information precessing process in one
of the attribute spaces, interprets how the relations which represented by
""Be"", ""Of"", ""Have"", and so on are embodied in the information storage data
structures and the corresponding data reading modes, reclassifies the sentences
types from the perspective of task types and data reading modes. Then,
simulated the understanding process (the information processing process) on a
dialogue example. Finally, the author summarizes the basic conditions of
understanding and gives out the definition of understanding from a personal
point of view. The study in this paper provides a practical, theoretical basis
and research methods for NLU. It also can be applied in large-scale, multi-type
information processing in the artificial intelligence (AI) area.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 05:40:47 GMT'}]",2020-10-27,"[['Zhang', 'Limin', '']]"
1369117,2010.12787,Kung-Hsiang Huang,"Kung-Hsiang Huang, Nanyun Peng","Efficient End-to-end Learning of Cross-event Dependencies for
  Document-level Event Extraction","10 pages, 3 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Document-level event extraction is important for indexing the most important
information in a document to facilitate downstream tasks such as information
retrieval or question answering. However, it is a challenging task because it
requires the understanding of event and entity coreference, and capturing
arguments that span across different sentences. Existing works on event
extraction generally confine on extracting events from single sentences, which
fail to capture the relationships between the event mentions at the scale of a
document, as well as the event arguments that appear in a different sentence
than the event trigger. In this paper, we propose an end-to-end model
leveraging Deep Value Networks (DVN), a structured prediction algorithm, to
efficiently capture cross-event dependencies for document-level event
extraction. Experimental results show that our approach achieves comparable
performance to CRF-based model on ACE05, while enjoys significantly higher
efficiency.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 05:28:16 GMT'}]",2020-10-27,"[['Huang', 'Kung-Hsiang', ''], ['Peng', 'Nanyun', '']]"
1369116,2010.12786,Huda Khayrallah,"Huda Khayrallah, Jo\~ao Sedoc","Measuring the `I don't know' Problem through the Lens of Gricean
  Quantity",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the intrinsic evaluation of neural generative dialog models
through the lens of Grices Maxims of Conversation (1975). Based on the maxim of
Quantity (be informative), we propose Relative Utterance Quantity (RUQ) to
diagnose the `I don't know' problem. The RUQ diagnostic compares the model
score of a generic response to that of the reference response. We find that for
reasonable baseline models, `I don't know' is preferred over the reference more
than half the time, but this can be mitigated with hyperparameter tuning.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 05:16:36 GMT'}]",2020-10-27,"[['Khayrallah', 'Huda', ''], ['Sedoc', 'João', '']]"
1369138,2010.12808,Xiaodong Yu,"Xiaodong Yu, Wenpeng Yin, Dan Roth",Paired Representation Learning for Event and Entity Coreference,"9 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Co-reference of Events and of Entities are commonly formulated as binary
classification problems, given a pair of events or entities as input. Earlier
work addressed the main challenge in these problems -- the representation of
each element in the input pair by: (i) modelling the representation of one
element (event or entity) without considering the other element in the pair;
(ii) encoding all attributes of one element (e.g., arguments of an event) into
a single non-interpretable vector, thus losing the ability to compare
cross-element attributes. In this work we propose paired representation
learning (PairedRL) for coreference resolution. Given a pair of elements
(Events or Entities) our model treats the pair's sentences as a single sequence
so that each element in the pair learns its representation by encoding its own
context as well the other element's context. In addition, when representing
events, PairedRL is structured in that it represents the event's arguments to
facilitate their individual contribution to the final prediction. As we show,
in both (within-document & cross-document) event and entity coreference
benchmarks, our unified approach, PairedRL, outperforms prior state of the art
systems with a large margin.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 06:55:52 GMT'}]",2020-10-27,"[['Yu', 'Xiaodong', ''], ['Yin', 'Wenpeng', ''], ['Roth', 'Dan', '']]"
1369114,2010.12784,Vikram Gupta,"Vikram Gupta, Haoyue Shi, Kevin Gimpel, Mrinmaya Sachan","Clustering Contextualized Representations of Text for Unsupervised
  Syntax Induction",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We explore clustering of contextualized text representations for two
unsupervised syntax induction tasks: part of speech induction (POSI) and
constituency labelling (CoLab). We propose a deep embedded clustering approach
which jointly transforms these representations into a lower dimension cluster
friendly space and clusters them. We further enhance these representations by
augmenting them with task-specific representations. We also explore the
effectiveness of multilingual representations for different tasks and
languages. With this work, we establish the first strong baselines for
unsupervised syntax induction using contextualized text representations. We
report competitive performance on 45-tag POSI, state-of-the-art performance on
12-tag POSI across 10 languages, and competitive results on CoLab.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 05:06:29 GMT'}]",2020-10-27,"[['Gupta', 'Vikram', ''], ['Shi', 'Haoyue', ''], ['Gimpel', 'Kevin', ''], ['Sachan', 'Mrinmaya', '']]"
1369142,2010.12812,Zexuan Zhong,Zexuan Zhong and Danqi Chen,A Frustratingly Easy Approach for Joint Entity and Relation Extraction,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end relation extraction aims to identify named entities and extract
relations between them simultaneously. Most recent work models these two
subtasks jointly, either by unifying them in one structured prediction
framework, or multi-task learning through shared representations. In this work,
we describe a very simple approach for joint entity and relation extraction,
and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05,
and SciERC). Our approach essentially builds on two independent pre-trained
encoders and merely uses the entity model to provide input features for the
relation model. Through a series of careful examinations, we validate the
importance of learning distinct contextual representations for entities and
relations, fusing entity information at the input layer of the relation model,
and incorporating global context. Finally, we also present an efficient
approximation to our approach which requires only one pass of both encoders at
inference time, obtaining a 8-16$\times$ speedup with a small accuracy drop.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 07:14:01 GMT'}]",2020-10-27,"[['Zhong', 'Zexuan', ''], ['Chen', 'Danqi', '']]"
1369130,2010.12800,Xinliang (Frederick) Zhang,"Xinliang Frederick Zhang, Heming Sun, Xiang Yue, Emmett Jesrani, Simon
  Lin, Huan Sun",COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval,,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a large challenging dataset, COUGH, for COVID-19 FAQ retrieval.
Specifically, similar to a standard FAQ dataset, COUGH consists of three parts:
FAQ Bank, User Query Bank and Annotated Relevance Set. FAQ Bank contains ~16K
FAQ items scraped from 55 credible websites (e.g., CDC and WHO). For
evaluation, we introduce User Query Bank and Annotated Relevance Set, where the
former contains 1201 human-paraphrased queries while the latter contains ~32
human-annotated FAQ items for each query. We analyze COUGH by testing different
FAQ retrieval models built on top of BM25 and BERT, among which the best model
achieves 0.29 under P@5, indicating that the dataset presents a great challenge
for future research. Our dataset is freely available at
https://github.com/sunlab-osu/covid-faq.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 06:30:59 GMT'}]",2020-10-27,"[['Zhang', 'Xinliang Frederick', ''], ['Sun', 'Heming', ''], ['Yue', 'Xiang', ''], ['Jesrani', 'Emmett', ''], ['Lin', 'Simon', ''], ['Sun', 'Huan', '']]"
1369100,2010.12770,Jianpeng Cheng J,"Jianpeng Cheng, Devang Agrawal, Hector Martinez Alonso, Shruti
  Bhargava, Joris Driesen, Federico Flego, Dain Kaplan, Dimitri Kartsaklis, Lin
  Li, Dhivya Piraviperumal, Jason D Williams, Hong Yu, Diarmuid O Seaghdha,
  Anders Johannsen",Conversational Semantic Parsing for Dialog State Tracking,Publish as a conference paper at EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We consider a new perspective on dialog state tracking (DST), the task of
estimating a user's goal through the course of a dialog. By formulating DST as
a semantic parsing task over hierarchical representations, we can incorporate
semantic compositionality, cross-domain knowledge sharing and co-reference. We
present TreeDST, a dataset of 27k conversations annotated with tree-structured
dialog states and system acts. We describe an encoder-decoder framework for DST
with hierarchical representations, which leads to 20% improvement over
state-of-the-art DST approaches that operate on a flat meaning space of
slot-value pairs.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 04:10:32 GMT'}]",2020-10-27,"[['Cheng', 'Jianpeng', ''], ['Agrawal', 'Devang', ''], ['Alonso', 'Hector Martinez', ''], ['Bhargava', 'Shruti', ''], ['Driesen', 'Joris', ''], ['Flego', 'Federico', ''], ['Kaplan', 'Dain', ''], ['Kartsaklis', 'Dimitri', ''], ['Li', 'Lin', ''], ['Piraviperumal', 'Dhivya', ''], ['Williams', 'Jason D', ''], ['Yu', 'Hong', ''], ['Seaghdha', 'Diarmuid O', ''], ['Johannsen', 'Anders', '']]"
1369110,2010.12780,Yan Zeng,Yan Zeng and Jian-Yun Nie,Open-Domain Dialogue Generation Based on Pre-trained Language Models,"[v0], 10 pages, 4 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models have been successfully used in response
generation for open-domain dialogue. Four main frameworks have been proposed:
(1) Transformer-ED using Transformer encoder and decoder separately for source
and target sentences; (2) Transformer-Dec using Transformer decoder for both
source and target sentences; (3) Transformer-MLM using Transformer decoder that
applies bi-directional attention on the source side and left-to-right attention
on the target side with masked language model objective; and (4) Transformer-AR
that uses auto-regressive objective instead. In this study, we compare these
frameworks on 3 datasets, and our comparison reveals that the best framework
uses bidirectional attention on the source side and does not separate encoder
and decoder. We also examine model discrepancy, and our experiments confirm
that the performance of a model is directly impacted by the underlying
discrepancies. We then propose two correction methods to reduce the
discrepancies, and both improve the model performance. These results show that
discrepancies is an important factor to consider when we use a pre-trained
model, and a reduction in discrepancies can lead to improved performance.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 04:52:28 GMT'}]",2020-10-27,"[['Zeng', 'Yan', ''], ['Nie', 'Jian-Yun', '']]"
1369109,2010.12779,Aida Mostafazadeh Davani,"Aida Mostafazadeh Davani, Ali Omrani, Brendan Kennedy, Mohammad Atari,
  Xiang Ren, Morteza Dehghani","Fair Hate Speech Detection through Evaluation of Social Group
  Counterfactuals",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Approaches for mitigating bias in supervised models are designed to reduce
models' dependence on specific sensitive features of the input data, e.g.,
mentioned social groups. However, in the case of hate speech detection, it is
not always desirable to equalize the effects of social groups because of their
essential role in distinguishing outgroup-derogatory hate, such that particular
types of hateful rhetoric carry the intended meaning only when contextualized
around certain social group tokens. Counterfactual token fairness for a
mentioned social group evaluates the model's predictions as to whether they are
the same for (a) the actual sentence and (b) a counterfactual instance, which
is generated by changing the mentioned social group in the sentence. Our
approach assures robust model predictions for counterfactuals that imply
similar meaning as the actual sentence. To quantify the similarity of a
sentence and its counterfactual, we compare their likelihood score calculated
by generative language models. By equalizing model behaviors on each sentence
and its counterfactuals, we mitigate bias in the proposed model while
preserving the overall classification performance.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 04:51:47 GMT'}]",2020-10-27,"[['Davani', 'Aida Mostafazadeh', ''], ['Omrani', 'Ali', ''], ['Kennedy', 'Brendan', ''], ['Atari', 'Mohammad', ''], ['Ren', 'Xiang', ''], ['Dehghani', 'Morteza', '']]"
1369107,2010.12777,Hyung Won Chung,"Hyung Won Chung, Dan Garrette, Kiat Chuan Tan, Jason Riesa",Improving Multilingual Models with Language-Clustered Vocabularies,Published in the main conference of EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State-of-the-art multilingual models depend on vocabularies that cover all of
the languages the model will expect to see at inference time, but the standard
methods for generating those vocabularies are not ideal for massively
multilingual applications. In this work, we introduce a novel procedure for
multilingual vocabulary generation that combines the separately trained
vocabularies of several automatically derived language clusters, thus balancing
the trade-off between cross-lingual subword sharing and language-specific
vocabularies. Our experiments show improvements across languages on key
multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1\%), and WikiAnn NER
(+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without
increasing the size of the model or data.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 04:49:15 GMT'}]",2020-10-27,"[['Chung', 'Hyung Won', ''], ['Garrette', 'Dan', ''], ['Tan', 'Kiat Chuan', ''], ['Riesa', 'Jason', '']]"
1369106,2010.12776,Yanda Chen,"Yanda Chen (1), Md Arafat Sultan (2), Vittorio Castelli (2) ((1)
  Department of Computer Science, Columbia University, (2) IBM Research AI,
  T.J. Watson Research Center, New York, USA)",Improved Synthetic Training for Reading Comprehension,"11 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatically generated synthetic training examples have been shown to
improve performance in machine reading comprehension (MRC). Compared to human
annotated gold standard data, synthetic training data has unique properties,
such as high availability at the possible expense of quality. In view of such
differences, in this paper, we explore novel applications of synthetic examples
to MRC. Our proposed pre-training and knowledge distillation strategies show
significant improvements over existing methods. In a particularly surprising
discovery, we observe that synthetic distillation often yields students that
can outperform the teacher model.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 04:41:30 GMT'}]",2020-10-27,"[['Chen', 'Yanda', ''], ['Sultan', 'Md Arafat', ''], ['Castelli', 'Vittorio', '']]"
1253799,2003.03444,Yuval Pinter,Yuval Pinter and Cassandra L. Jacobs and Max Bittker,NYTWIT: A Dataset of Novel Words in the New York Times,COLING 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We present the New York Times Word Innovation Types dataset, or NYTWIT, a
collection of over 2,500 novel English words published in the New York Times
between November 2017 and March 2019, manually annotated for their class of
novelty (such as lexical derivation, dialectal variation, blending, or
compounding). We present baseline results for both uncontextual and contextual
prediction of novelty class, showing that there is room for improvement even
for state-of-the-art NLP systems. We hope this resource will prove useful for
linguists and NLP practitioners by providing a real-world environment of novel
word appearance.
","[{'version': 'v1', 'created': 'Fri, 6 Mar 2020 21:19:44 GMT'}, {'version': 'v2', 'created': 'Tue, 26 May 2020 03:54:35 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Oct 2020 18:54:48 GMT'}]",2020-10-27,"[['Pinter', 'Yuval', ''], ['Jacobs', 'Cassandra L.', ''], ['Bittker', 'Max', '']]"
1369103,2010.12773,Xiang Deng,"Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr
  Polozov, Huan Sun, Matthew Richardson",Structure-Grounded Pretraining for Text-to-SQL,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning to capture text-table alignment is essential for table related tasks
like text-to-SQL. The model needs to correctly recognize natural language
references to columns and values and to ground them in the given database
schema. In this paper, we present a novel weakly supervised Structure-Grounded
pretraining framework (StruG) for text-to-SQL that can effectively learn to
capture text-table alignment based on a parallel text-table corpus. We identify
a set of novel prediction tasks: column grounding, value grounding and
column-value mapping, and train them using weak supervision without requiring
complex SQL annotation. Additionally, to evaluate the model under a more
realistic setting, we create a new evaluation set Spider-Realistic based on
Spider with explicit mentions of column names removed, and adopt two existing
single-database text-to-SQL datasets. StruG significantly outperforms
BERT-LARGE on Spider and the realistic evaluation sets, while bringing
consistent improvement on the large-scale WikiSQL benchmark.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 04:35:35 GMT'}]",2020-10-27,"[['Deng', 'Xiang', ''], ['Awadallah', 'Ahmed Hassan', ''], ['Meek', 'Christopher', ''], ['Polozov', 'Oleksandr', ''], ['Sun', 'Huan', ''], ['Richardson', 'Matthew', '']]"
1369085,2010.12755,Xinyu Zhao,"Xinyu Zhao, Shih-ting Lin, Greg Durrett",Effective Distant Supervision for Temporal Relation Extraction,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A principal barrier to training temporal relation extraction models in new
domains is the lack of varied, high quality examples and the challenge of
collecting more. We present a method of automatically collecting
distantly-supervised examples of temporal relations. We scrape and
automatically label event pairs where the temporal relations are made explicit
in text, then mask out those explicit cues, forcing a model trained on this
data to learn other signals. We demonstrate that a pre-trained Transformer
model is able to transfer from the weakly labeled examples to human-annotated
benchmarks in both zero-shot and few-shot settings, and that the masking scheme
is important in improving generalization.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 03:17:31 GMT'}]",2020-10-27,"[['Zhao', 'Xinyu', ''], ['Lin', 'Shih-ting', ''], ['Durrett', 'Greg', '']]"
1369143,2010.12813,Kevin Lin,"Catherine Chen, Kevin Lin, Dan Klein",Inducing Taxonomic Knowledge from Pretrained Transformers,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a method for inducing taxonomic trees from pretrained
transformers. Given a set of input terms, we assign a score for the likelihood
that each pair of terms forms a parent-child relation. To produce a tree from
pairwise parent-child edge scores, we treat this as a graph optimization
problem and output the maximum spanning tree. We train the model by finetuning
it on parent-child relations from subtrees of WordNet and test on
non-overlapping subtrees. In addition, we incorporate semi-structured
definitions from the web to further improve performance. On the task of
inducing subtrees of WordNet, the model achieves 66.0 ancestor F_1, a 10.4
point absolute increase over the previous best published result on this task.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 07:16:21 GMT'}]",2020-10-27,"[['Chen', 'Catherine', ''], ['Lin', 'Kevin', ''], ['Klein', 'Dan', '']]"
1369101,2010.12771,Yixin Liu,"Yixin Liu, Graham Neubig, John Wieting",On Learning Text Style Transfer with Direct Rewards,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In most cases, the lack of parallel corpora makes it impossible to directly
train supervised models for text style transfer task. In this paper, we explore
training algorithms that instead optimize reward functions that explicitly
consider different aspects of the style-transferred outputs. In particular, we
leverage semantic similarity metrics originally used for fine-tuning neural
machine translation models to explicitly assess the preservation of content
between system outputs and input texts. We also investigate the potential
weaknesses of the existing automatic metrics and propose efficient strategies
of using these metrics for training. The experimental results show that our
model provides significant gains in both automatic and human evaluation over
strong baselines, indicating the effectiveness of our proposed methods and
training strategies.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 04:30:02 GMT'}]",2020-10-27,"[['Liu', 'Yixin', ''], ['Neubig', 'Graham', ''], ['Wieting', 'John', '']]"
1369087,2010.12757,Kai Sun,"Kai Sun, Seungwhan Moon, Paul Crook, Stephen Roller, Becka Silvert,
  Bing Liu, Zhiguang Wang, Honglei Liu, Eunjoon Cho, Claire Cardie",Adding Chit-Chats to Enhance Task-Oriented Dialogues,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The existing dialogue corpora and models are typically designed under two
disjoint motives: while task-oriented systems focus on achieving functional
goals (e.g., booking hotels), open-domain chatbots aim at making socially
engaging conversations. In this work, we propose to integrate both types of
systems by Adding Chit-Chats to ENhance Task-ORiented dialogues (ACCENTOR),
with the goal of making virtual assistant conversations more engaging and
interactive. Specifically, we propose a flexible approach for generating
diverse chit-chat responses to augment task-oriented dialogues with minimal
annotation effort. We then present our new chit-chat annotations to 23.8K
dialogues from the popular task-oriented datasets (Schema-Guided Dialogue and
MultiWOZ 2.1) and demonstrate their advantage over the originals via human
evaluation. Lastly, we propose three new models for ACCENTOR explicitly trained
to predict user goals and to generate contextually relevant chit-chat
responses. Automatic and human evaluations show that, compared with the
state-of-the-art task-oriented baseline, our models can code-switch between
task and chit-chat to be more engaging, interesting, knowledgeable, and
humanlike, while maintaining competitive task performance.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 03:22:43 GMT'}]",2020-10-27,"[['Sun', 'Kai', ''], ['Moon', 'Seungwhan', ''], ['Crook', 'Paul', ''], ['Roller', 'Stephen', ''], ['Silvert', 'Becka', ''], ['Liu', 'Bing', ''], ['Wang', 'Zhiguang', ''], ['Liu', 'Honglei', ''], ['Cho', 'Eunjoon', ''], ['Cardie', 'Claire', '']]"
1369088,2010.12758,Zhiyu Chen,"Zhiyu Chen, Honglei Liu, Hu Xu, Seungwhan Moon, Hao Zhou, Bing Liu","NUANCED: Natural Utterance Annotation for Nuanced Conversation with
  Estimated Distributions",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing conversational systems are mostly agent-centric, which assumes the
user utterances would closely follow the system ontology (for NLU or dialogue
state tracking). However, in real-world scenarios, it is highly desirable that
the users can speak freely in their own way. It is extremely hard, if not
impossible, for the users to adapt to the unknown system ontology. In this
work, we attempt to build a user-centric dialogue system. As there is no clean
mapping for a user's free form utterance to an ontology, we first model the
user preferences as estimated distributions over the system ontology and map
the users' utterances to such distributions. Learning such a mapping poses new
challenges on reasoning over existing knowledge, ranging from factoid
knowledge, commonsense knowledge to the users' own situations. To this end, we
build a new dataset named NUANCED that focuses on such realistic settings for
conversational recommendation. Collected via dialogue simulation and
paraphrasing, NUANCED contains 5.1k dialogues, 26k turns of high-quality user
responses. We conduct experiments, showing both the usefulness and challenges
of our problem setting. We believe NUANCED can serve as a valuable resource to
push existing research from the agent-centric system to the user-centric
system. The code and data will be made publicly available.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 03:23:14 GMT'}]",2020-10-27,"[['Chen', 'Zhiyu', ''], ['Liu', 'Honglei', ''], ['Xu', 'Hu', ''], ['Moon', 'Seungwhan', ''], ['Zhou', 'Hao', ''], ['Liu', 'Bing', '']]"
1369092,2010.12762,Sarah Wiegreffe,"Sarah Wiegreffe, Ana Marasovic, Noah A. Smith",Measuring Association Between Labels and Free-Text Rationales,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Interpretable NLP has taking increasing interest in ensuring that
explanations are faithful to the model's decision-making process. This property
is crucial for machine learning researchers and practitioners using
explanations to better understand models. While prior work focuses primarily on
extractive rationales (a subset of the input elements), we investigate their
less-studied counterpart: free-text natural language rationales. We demonstrate
that existing models for faithful interpretability do not extend cleanly to
tasks where free-text rationales are needed. We turn to models that jointly
predict and rationalize, a common class of models for free-text rationalization
whose faithfulness is not yet established. We propose measurements of
label-rationale association, a necessary property of faithful rationales, for
these models. Using our measurements, we show that a state-of-the-art joint
model based on T5 has strengths and weaknesses for producing faithful
rationales.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 03:40:56 GMT'}]",2020-10-27,"[['Wiegreffe', 'Sarah', ''], ['Marasovic', 'Ana', ''], ['Smith', 'Noah A.', '']]"
1369094,2010.12764,Rodolfo Corona,"Rodolfo Corona, Daniel Fried, Coline Devin, Dan Klein, Trevor Darrell",Modularity Improves Out-of-Domain Instruction Following,,,,,cs.CL cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a modular architecture for following natural language instructions
that describe sequences of diverse subgoals, such as navigating to landmarks or
picking up objects. Standard, non-modular, architectures used in instruction
following do not exploit subgoal compositionality and often struggle on
out-of-distribution tasks and environments. In our approach, subgoal modules
each carry out natural language instructions for a specific subgoal type. A
sequence of modules to execute is chosen by learning to segment the
instructions and predicting a subgoal type for each segment. When compared to
standard sequence-to-sequence approaches on ALFRED, a challenging instruction
following benchmark, we find that modularization improves generalization to
environments unseen in training and to novel tasks.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 03:48:45 GMT'}]",2020-10-27,"[['Corona', 'Rodolfo', ''], ['Fried', 'Daniel', ''], ['Devin', 'Coline', ''], ['Klein', 'Dan', ''], ['Darrell', 'Trevor', '']]"
1369150,2010.12820,Emily Sheng,"Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng","""Nice Try, Kiddo"": Ad Hominems in Dialogue Systems",14 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ad hominem attacks are those that attack some feature of a person's character
instead of the position the person is maintaining. As a form of toxic and
abusive language, ad hominems contain harmful language that could further
amplify the skew of power inequality for marginalized populations. Since
dialogue systems are designed to respond directly to user input, it is
important to study ad hominems in these system responses. In this work, we
propose categories of ad hominems that allow us to analyze human and dialogue
system responses to Twitter posts. We specifically compare responses to Twitter
posts about marginalized communities (#BlackLivesMatter, #MeToo) and other
topics (#Vegan, #WFH). Furthermore, we propose a constrained decoding technique
that uses salient $n$-gram similarity to apply soft constraints to top-$k$
sampling and can decrease the amount of ad hominems generated by dialogue
systems. Our results indicate that 1) responses composed by both humans and
DialoGPT contain more ad hominems for discussions around marginalized
communities versus other topics, 2) different amounts of ad hominems in the
training data can influence the likelihood of the model generating ad hominems,
and 3) we can thus carefully choose training data and use constrained decoding
techniques to decrease the amount of ad hominems generated by dialogue systems.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 07:37:49 GMT'}]",2020-10-27,"[['Sheng', 'Emily', ''], ['Chang', 'Kai-Wei', ''], ['Natarajan', 'Premkumar', ''], ['Peng', 'Nanyun', '']]"
1369211,2010.12881,Arturo Oncevay,Arturo Oncevay and Kervy Rivas Rojas,Revisiting Neural Language Modelling with Syllables,"5 pages (main paper), 4 pages of Appendix",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language modelling is regularly analysed at word, subword or character units,
but syllables are seldom used. Syllables provide shorter sequences than
characters, they can be extracted with rules, and their segmentation typically
requires less specialised effort than identifying morphemes. We reconsider
syllables for an open-vocabulary generation task in 20 languages. We use
rule-based syllabification methods for five languages and address the rest with
a hyphenation tool, which behaviour as syllable proxy is validated. With a
comparable perplexity, we show that syllables outperform characters, annotated
morphemes and unsupervised subwords. Finally, we also study the overlapping of
syllables concerning other subword pieces and discuss some limitations and
opportunities.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 11:44:41 GMT'}]",2020-10-27,"[['Oncevay', 'Arturo', ''], ['Rojas', 'Kervy Rivas', '']]"
1369155,2010.12825,Rochelle Choenni,"Rochelle Choenni, Ekaterina Shutova","Cross-neutralising: Probing for joint encoding of linguistic information
  in multilingual models",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual sentence encoders are widely used to transfer NLP models across
languages. The success of this transfer is, however, dependent on the model's
ability to encode the patterns of cross-lingual similarity and variation. Yet,
little is known as to how these models are able to do this. We propose a simple
method to study how relationships between languages are encoded in two
state-of-the-art multilingual models (i.e. M-BERT and XLM-R). The results
provide insight into their information sharing mechanisms and suggest that
linguistic properties are encoded jointly across typologically-similar
languages in these models.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 07:55:32 GMT'}]",2020-10-27,"[['Choenni', 'Rochelle', ''], ['Shutova', 'Ekaterina', '']]"
1369161,2010.12831,Liunian Harold Li,"Liunian Harold Li, Haoxuan You, Zhecan Wang, Alireza Zareian, Shih-Fu
  Chang, Kai-Wei Chang","Weakly-supervised VisualBERT: Pre-training without Parallel Images and
  Captions",,,,,cs.CL cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained contextual vision-and-language (V&L) models have brought
impressive performance improvement on various benchmarks. However, the paired
text-image data required for pre-training are hard to collect and scale up. We
investigate if a strong V&L representation model can be learned without
text-image pairs. We propose Weakly-supervised VisualBERT with the key idea of
conducting ""mask-and-predict"" pre-training on language-only and image-only
corpora. Additionally, we introduce the object tags detected by an object
recognition model as anchor points to bridge two modalities. Evaluation on four
V&L benchmarks shows that Weakly-supervised VisualBERT achieves similar
performance with a model pre-trained with paired data. Besides, pre-training on
more image-only data further improves a model that already has access to
aligned data, suggesting the possibility of utilizing billions of raw images
available to enhance V&L models.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 08:17:54 GMT'}]",2020-10-27,"[['Li', 'Liunian Harold', ''], ['You', 'Haoxuan', ''], ['Wang', 'Zhecan', ''], ['Zareian', 'Alireza', ''], ['Chang', 'Shih-Fu', ''], ['Chang', 'Kai-Wei', '']]"
1369164,2010.12834,Saadia Gabriel,"Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao",Go Figure! A Meta Evaluation of Factuality in Summarization,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text generation models can generate factually inconsistent text containing
distorted or fabricated facts about the source text. Recent work has focused on
building evaluation models to verify the factual correctness of semantically
constrained text generation tasks such as document summarization. While the
field of factuality evaluation is growing fast, we don't have well-defined
criteria for measuring the effectiveness, generalizability, reliability, or
sensitivity of the factuality metrics. Focusing on these aspects, in this
paper, we introduce a meta-evaluation framework for evaluating factual
consistency metrics. We introduce five necessary, common-sense conditions for
effective factuality metrics and experiment with nine recent factuality metrics
using synthetic and human-labeled factuality data from short news, long news
and dialogue summarization domains. Our framework enables assessing the
efficiency of any new factual consistency metric on a variety of dimensions
over multiple summarization domains and can be easily extended with new
meta-evaluation criteria. We also present our conclusions towards standardizing
the factuality evaluation metrics.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 08:30:20 GMT'}]",2020-10-27,"[['Gabriel', 'Saadia', ''], ['Celikyilmaz', 'Asli', ''], ['Jha', 'Rahul', ''], ['Choi', 'Yejin', ''], ['Gao', 'Jianfeng', '']]"
1369166,2010.12836,Alexander Fabbri,"Alexander R. Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan
  Ghazvininejad, Shafiq Joty, Dragomir Radev, Yashar Mehdad","Improving Zero and Few-Shot Abstractive Summarization with Intermediate
  Fine-tuning and Data Augmentation",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Models pretrained with self-supervised objectives on large text corpora
achieve state-of-the-art performance on text summarization tasks. However,
these models are typically fine-tuned on hundreds of thousands of data points,
an infeasible requirement when applying summarization to new, niche domains. In
this work, we introduce a general method, called WikiTransfer, for fine-tuning
pretrained models for summarization in an unsupervised, dataset-specific manner
which makes use of characteristics of the target dataset such as the length and
abstractiveness of the desired summaries. We achieve state-of-the-art,
zero-shot abstractive summarization performance on the CNN-DailyMail dataset
and demonstrate the effectiveness of our approach on three additional, diverse
datasets. The models fine-tuned in this unsupervised manner are more robust to
noisy data and also achieve better few-shot performance using 10 and 100
training examples. We perform ablation studies on the effect of the components
of our unsupervised fine-tuning data and analyze the performance of these
models in few-shot scenarios along with data augmentation techniques using both
automatic and human evaluation.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 08:36:49 GMT'}]",2020-10-27,"[['Fabbri', 'Alexander R.', ''], ['Han', 'Simeng', ''], ['Li', 'Haoyuan', ''], ['Li', 'Haoran', ''], ['Ghazvininejad', 'Marjan', ''], ['Joty', 'Shafiq', ''], ['Radev', 'Dragomir', ''], ['Mehdad', 'Yashar', '']]"
1369083,2010.12753,Ben Zhou,"Ben Zhou and Kyle Richardson and Qiang Ning and Tushar Khot and Ashish
  Sabharwal and Dan Roth",Temporal Reasoning on Implicit Events from Distant Supervision,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing works on temporal reasoning among events described in text focus on
modeling relationships between explicitly mentioned events and do not handle
event end time effectively. However, human readers can infer from natural
language text many implicit events that help them better understand the
situation and, consequently, better reason about time. This work proposes a new
crowd-sourced dataset, TRACIE, which evaluates systems' understanding of
implicit events - events that are not mentioned explicitly in the text but can
be inferred from it. This is done via textual entailment instances querying
both start and end times of events. We show that TRACIE is challenging for
state-of-the-art language models. Our proposed model, SymTime, exploits distant
supervision signals from the text itself and reasons over events' start time
and duration to infer events' end time points. We show that our approach
improves over baseline language models, gaining 5% on the i.i.d. split and 9%
on an out-of-distribution test split. Our approach is also general to other
annotation schemes, gaining 2%-8% on MATRES, an extrinsic temporal relation
benchmark.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 03:12:27 GMT'}]",2020-10-27,"[['Zhou', 'Ben', ''], ['Richardson', 'Kyle', ''], ['Ning', 'Qiang', ''], ['Khot', 'Tushar', ''], ['Sabharwal', 'Ashish', ''], ['Roth', 'Dan', '']]"
1369174,2010.12844,Sahisnu Mazumder,"Sahisnu Mazumder, Oriana Riva",FLIN: A Flexible Natural Language Interface for Web Navigation,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  AI assistants have started carrying out tasks on a user's behalf by
interacting directly with the web. However, training an interface that maps
natural language (NL) commands to web actions is challenging for existing
semantic parsing approaches due to the variable and unknown set of actions that
characterize websites. We propose FLIN, a natural language interface for web
navigation that maps NL commands to concept-level actions rather than low-level
UI interactions, thus being able to flexibly adapt to different websites and
handle their transient nature. We frame this as a ranking problem where, given
a user command and a webpage, FLIN learns to score the most appropriate
navigation instruction (involving action and parameter values). To train and
evaluate FLIN, we collect a dataset using nine popular websites from three
different domains. Quantitative results show that FLIN is capable of adapting
to new websites in a given domain.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 09:11:26 GMT'}]",2020-10-27,"[['Mazumder', 'Sahisnu', ''], ['Riva', 'Oriana', '']]"
1369180,2010.12850,Semih Yavuz,"Shiyang Li, Semih Yavuz, Kazuma Hashimoto, Jia Li, Tong Niu, Nazneen
  Rajani, Xifeng Yan, Yingbo Zhou and Caiming Xiong","CoCo: Controllable Counterfactuals for Evaluating Dialogue State
  Trackers",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialogue state trackers have made significant progress on benchmark datasets,
but their generalization capability to novel and realistic scenarios beyond the
held-out conversations is less understood. We propose controllable
counterfactuals (CoCo) to bridge this gap and evaluate dialogue state tracking
(DST) models on novel scenarios, i.e., would the system successfully tackle the
request if the user responded differently but still consistently with the
dialogue flow? CoCo leverages turn-level belief states as counterfactual
conditionals to produce novel conversation scenarios in two steps: (i)
counterfactual goal generation at turn-level by dropping and adding slots
followed by replacing slot values, (ii) counterfactual conversation generation
that is conditioned on (i) and consistent with the dialogue flow. Evaluating
state-of-the-art DST models on MultiWOZ dataset with CoCo-generated
counterfactuals results in a significant performance drop of up to 30.8% (from
49.4% to 18.6%) in absolute joint goal accuracy. In comparison, widely used
techniques like paraphrasing only affect the accuracy by at most 2%. Human
evaluations show that CoCo-generated conversations perfectly reflect the
underlying user goal with more than 95% accuracy and are as human-like as the
original conversations, further strengthening its reliability and promise to be
adopted as part of the robustness evaluation of DST models.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 09:39:35 GMT'}]",2020-10-27,"[['Li', 'Shiyang', ''], ['Yavuz', 'Semih', ''], ['Hashimoto', 'Kazuma', ''], ['Li', 'Jia', ''], ['Niu', 'Tong', ''], ['Rajani', 'Nazneen', ''], ['Yan', 'Xifeng', ''], ['Zhou', 'Yingbo', ''], ['Xiong', 'Caiming', '']]"
1369184,2010.12854,Tushar Khot,Shih-Ting Lin and Ashish Sabharwal and Tushar Khot,ReadOnce Transformers: Reusable Representations of Text for Transformers,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While large-scale language models are extremely effective when directly
fine-tuned on many end-tasks, such models learn to extract information and
solve the task simultaneously from end-task supervision. This is wasteful, as
the general problem of gathering information from a document is mostly
task-independent and need not be re-learned from scratch each time. Moreover,
once the information has been captured in a computable representation, it can
now be re-used across examples, leading to faster training and evaluation of
models. We present a transformer-based approach, ReadOnce Transformers, that is
trained to build such information-capturing representations of text. Our model
compresses the document into a variable-length task-independent representation
that can now be re-used in different examples and tasks, thereby requiring a
document to only be read once. Additionally, we extend standard text-to-text
models to consume our ReadOnce Representations along with text to solve
multiple downstream tasks. We show our task-independent representations can be
used for multi-hop QA, abstractive QA, and summarization. We observe 2x-5x
speedups compared to standard text-to-text models, while also being able to
handle long documents that would normally exceed the length limit of current
models.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 09:53:16 GMT'}]",2020-10-27,"[['Lin', 'Shih-Ting', ''], ['Sabharwal', 'Ashish', ''], ['Khot', 'Tushar', '']]"
1369188,2010.12858,Benjamin Muller,"Benjamin Muller and Antonis Anastasopoulos and Beno\^it Sagot and
  Djam\'e Seddah","When Being Unseen from mBERT is just the Beginning: Handling New
  Languages With Multilingual Language Models",,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Transfer learning based on pretraining language models on a large amount of
raw data has become a new norm to reach state-of-the-art performance in NLP.
Still, it remains unclear how this approach should be applied for unseen
languages that are not covered by any available large-scale multilingual
language model and for which only a small amount of raw data is generally
available. In this work, by comparing multilingual and monolingual models, we
show that such models behave in multiple ways on unseen languages. Some
languages greatly benefit from transfer learning and behave similarly to
closely related high resource languages whereas others apparently do not.
Focusing on the latter, we show that this failure to transfer is largely
related to the impact of the script used to write such languages.
Transliterating those languages improves very significantly the ability of
large-scale multilingual language models on downstream tasks.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 10:15:03 GMT'}]",2020-10-27,"[['Muller', 'Benjamin', ''], ['Anastasopoulos', 'Antonis', ''], ['Sagot', 'Benoît', ''], ['Seddah', 'Djamé', '']]"
1369159,2010.12829,Juan Pino,"Chau Tran, Changhan Wang, Yuqing Tang, Yun Tang, Juan Pino, Xian Li","Cross-Modal Transfer Learning for Multilingual Speech-to-Text
  Translation",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose an effective approach to utilize pretrained speech and text models
to perform speech-to-text translation (ST). Our recipe to achieve cross-modal
and cross-lingual transfer learning (XMTL) is simple and generalizable: using
an adaptor module to bridge the modules pretrained in different modalities, and
an efficient finetuning step which leverages the knowledge from pretrained
modules yet making it work on a drastically different downstream task. With
this approach, we built a multilingual speech-to-text translation model with
pretrained audio encoder (wav2vec) and multilingual text decoder (mBART), which
achieves new state-of-the-art on CoVoST 2 ST benchmark [1] for English into 15
languages as well as 6 Romance languages into English with on average +2.8 BLEU
and +3.9 BLEU, respectively. On low-resource languages (with less than 10 hours
training data), our approach significantly improves the quality of
speech-to-text translation with +9.0 BLEU on Portuguese-English and +5.2 BLEU
on Dutch-English.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 08:15:08 GMT'}]",2020-10-27,"[['Tran', 'Chau', ''], ['Wang', 'Changhan', ''], ['Tang', 'Yuqing', ''], ['Tang', 'Yun', ''], ['Pino', 'Juan', ''], ['Li', 'Xian', '']]"
1369158,2010.12828,Haoyu Zhang,"Haoyu Zhang, Dingkun Long, Guangwei Xu, Pengjun Xie, Fei Huang, Ji
  Wang","Keyphrase Extraction with Dynamic Graph Convolutional Networks and
  Diversified Inference",11 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Keyphrase extraction (KE) aims to summarize a set of phrases that accurately
express a concept or a topic covered in a given document. Recently,
Sequence-to-Sequence (Seq2Seq) based generative framework is widely used in KE
task, and it has obtained competitive performance on various benchmarks. The
main challenges of Seq2Seq methods lie in acquiring informative latent document
representation and better modeling the compositionality of the target
keyphrases set, which will directly affect the quality of generated keyphrases.
In this paper, we propose to adopt the Dynamic Graph Convolutional Networks
(DGCN) to solve the above two problems simultaneously. Concretely, we explore
to integrate dependency trees with GCN for latent representation learning.
Moreover, the graph structure in our model is dynamically modified during the
learning process according to the generated keyphrases. To this end, our
approach is able to explicitly learn the relations within the keyphrases
collection and guarantee the information interchange between encoder and
decoder in both directions. Extensive experiments on various KE benchmark
datasets demonstrate the effectiveness of our approach.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 08:11:23 GMT'}]",2020-10-27,"[['Zhang', 'Haoyu', ''], ['Long', 'Dingkun', ''], ['Xu', 'Guangwei', ''], ['Xie', 'Pengjun', ''], ['Huang', 'Fei', ''], ['Wang', 'Ji', '']]"
1369151,2010.12821,Hyung Won Chung,"Hyung Won Chung, Thibault F\'evry, Henry Tsai, Melvin Johnson,
  Sebastian Ruder",Rethinking embedding coupling in pre-trained language models,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We re-evaluate the standard practice of sharing weights between input and
output embeddings in state-of-the-art pre-trained language models. We show that
decoupled embeddings provide increased modeling flexibility, allowing us to
significantly improve the efficiency of parameter allocation in the input
embedding of multilingual models. By reallocating the input embedding
parameters in the Transformer layers, we achieve dramatically better
performance on standard natural language understanding tasks with the same
number of parameters during fine-tuning. We also show that allocating
additional capacity to the output embedding provides benefits to the model that
persist through the fine-tuning stage even though the output embedding is
discarded after pre-training. Our analysis shows that larger output embeddings
prevent the model's last layers from overspecializing to the pre-training task
and encourage Transformer representations to be more general and more
transferable to other tasks and languages. Harnessing these findings, we are
able to train models that achieve strong performance on the XTREME benchmark
without increasing the number of parameters at the fine-tuning stage.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 07:43:00 GMT'}]",2020-10-27,"[['Chung', 'Hyung Won', ''], ['Févry', 'Thibault', ''], ['Tsai', 'Henry', ''], ['Johnson', 'Melvin', ''], ['Ruder', 'Sebastian', '']]"
1369198,2010.12868,Yongchang Hao,"Yongchang Hao, Shilin He, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu and
  Xing Wang","Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine
  Translation",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Non-Autoregressive machine Translation (NAT) models have demonstrated
significant inference speedup but suffer from inferior translation accuracy.
The common practice to tackle the problem is transferring the Autoregressive
machine Translation (AT) knowledge to NAT models, e.g., with knowledge
distillation. In this work, we hypothesize and empirically verify that AT and
NAT encoders capture different linguistic properties and representations of
source sentences. Therefore, we propose to adopt the multi-task learning to
transfer the AT knowledge to NAT models through the encoder sharing.
Specifically, we take the AT model as an auxiliary task to enhance NAT model
performance. Experimental results on WMT14 English->German and WMT16
English->Romanian datasets show that the proposed multi-task NAT achieves
significant improvements over the baseline NAT models. In addition,
experimental results demonstrate that our multi-task NAT is complementary to
the standard knowledge transfer method, knowledge distillation. Code is
publicly available at https://github.com/yongchanghao/multi-task-nat
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 11:00:58 GMT'}]",2020-10-27,"[['Hao', 'Yongchang', ''], ['He', 'Shilin', ''], ['Jiao', 'Wenxiang', ''], ['Tu', 'Zhaopeng', ''], ['Lyu', 'Michael', ''], ['Wang', 'Xing', '']]"
1369202,2010.12872,Aaron Chan,"Mrigank Raman, Siddhant Agarwal, Peifeng Wang, Aaron Chan, Hansen
  Wang, Sungchul Kim, Ryan Rossi, Handong Zhao, Nedim Lipka, Xiang Ren","Learning to Deceive Knowledge Graph Augmented Models via Targeted
  Perturbation","13 pages, 9 figures",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Symbolic knowledge (e.g., entities, relations, and facts in a knowledge
graph) has become an increasingly popular component of neural-symbolic models
applied to machine learning tasks, such as question answering and recommender
systems. Besides improving downstream performance, these symbolic structures
(and their associated attention weights) are often used to help explain the
model's predictions and provide ""insights"" to practitioners. In this paper, we
question the faithfulness of such symbolic explanations. We demonstrate that,
through a learned strategy (or even simple heuristics), one can produce
deceptively perturbed symbolic structures which maintain the downstream
performance of the original structure while significantly deviating from the
original semantics. In particular, we train a reinforcement learning policy to
manipulate relation types or edge connections in a knowledge graph, such that
the resulting downstream performance is maximally preserved. Across multiple
models and tasks, our approach drastically alters knowledge graphs with little
to no drop in performance. These results raise doubts about the faithfulness of
explanations provided by learned symbolic structures and the reliability of
current neural-symbolic models in leveraging symbolic knowledge.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 11:04:45 GMT'}]",2020-10-27,"[['Raman', 'Mrigank', ''], ['Agarwal', 'Siddhant', ''], ['Wang', 'Peifeng', ''], ['Chan', 'Aaron', ''], ['Wang', 'Hansen', ''], ['Kim', 'Sungchul', ''], ['Rossi', 'Ryan', ''], ['Zhao', 'Handong', ''], ['Lipka', 'Nedim', ''], ['Ren', 'Xiang', '']]"
1369203,2010.12873,Jun Yan,"Jun Yan, Mrigank Raman, Tianyu Zhang, Ryan Rossi, Handong Zhao,
  Sungchul Kim, Nedim Lipka, Xiang Ren",Learning Contextualized Knowledge Structures for Commonsense Reasoning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, neural-symbolic architectures have achieved success on commonsense
reasoning through effectively encoding relational structures retrieved from
external knowledge graphs (KGs) and obtained state-of-the-art results in tasks
such as (commonsense) question answering and natural language inference.
However, these methods rely on quality and contextualized knowledge structures
(i.e., fact triples) that are retrieved at the pre-processing stage but
overlook challenges caused by incompleteness of a KG, limited expressiveness of
its relations, and retrieved facts irrelevant to the reasoning context. In this
paper, we present a novel neural-symbolic model, named Hybrid Graph Network
(HGN), which jointly generates feature representations for new triples (as a
complement to existing edges in the KG), determines the relevance of the
triples to the reasoning context, and learns graph module parameters for
encoding the relational information. Our model learns a compact graph structure
(comprising both extracted and generated edges) through filtering edges that
are unhelpful to the reasoning process. We show marked improvement on three
commonsense reasoning benchmarks and demonstrate the superiority of the learned
graph structures with user studies.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 11:09:16 GMT'}]",2020-10-27,"[['Yan', 'Jun', ''], ['Raman', 'Mrigank', ''], ['Zhang', 'Tianyu', ''], ['Rossi', 'Ryan', ''], ['Zhao', 'Handong', ''], ['Kim', 'Sungchul', ''], ['Lipka', 'Nedim', ''], ['Ren', 'Xiang', '']]"
1369212,2010.12882,Mingyang Chen,"Mingyang Chen, Wen Zhang, Zonggang Yuan, Yantao Jia, Huajun Chen",FedE: Embedding Knowledge Graphs in Federated Setting,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graphs (KGs) consisting of triples are always incomplete, so it's
important to do Knowledge Graph Completion (KGC) by predicting missing triples.
Multi-Source KG is a common situation in real KG applications which can be
viewed as a set of related individual KGs where different KGs contains
relations of different aspects of entities. It's intuitive that, for each
individual KG, its completion could be greatly contributed by the triples
defined and labeled in other ones. However, because of the data privacy and
sensitivity, a set of relevant knowledge graphs cannot complement each other's
KGC by just collecting data from different knowledge graphs together.
Therefore, in this paper, we introduce federated setting to keep their privacy
without triple transferring between KGs and apply it in embedding knowledge
graph, a typical method which have proven effective for KGC in the past decade.
We propose a Federated Knowledge Graph Embedding framework FedE, focusing on
learning knowledge graph embeddings by aggregating locally-computed updates.
Finally, we conduct extensive experiments on datasets derived from KGE
benchmark datasets and results show the effectiveness of our proposed FedE.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 11:52:05 GMT'}]",2020-10-27,"[['Chen', 'Mingyang', ''], ['Zhang', 'Wen', ''], ['Yuan', 'Zonggang', ''], ['Jia', 'Yantao', ''], ['Chen', 'Huajun', '']]"
1369214,2010.12884,Ximing Lu,"Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra
  Bhagavatula, Yejin Choi","NeuroLogic Decoding: (Un)supervised Neural Text Generation with
  Predicate Logic Constraints",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conditional text generation often requires lexical constraints, i.e., which
words should or shouldn't be included in the output text. While the dominant
recipe for conditional text generation has been large-scale pretrained language
models that are finetuned on the task-specific training data, such models do
not learn to follow the underlying constraints reliably, even when supervised
with large amounts of task-specific examples.
  We propose NeuroLogic Decoding, a simple yet effective algorithm that enables
neural language models -- supervised or not -- to generate fluent text while
satisfying complex lexical constraints. Our approach is powerful yet efficient.
It handles any set of lexical constraints that is expressible under predicate
logic, while its asymptotic runtime is equivalent to conventional beam search.
  Empirical results on four benchmarks show that NeuroLogic Decoding
outperforms previous approaches, including algorithms that handle a subset of
our constraints. Moreover, we find that unsupervised models with NeuroLogic
Decoding often outperform supervised models with conventional decoding, even
when the latter is based on considerably larger networks. Our results suggest
the limit of large-scale neural networks for fine-grained controllable
generation and the promise of inference-time algorithms.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 11:55:22 GMT'}]",2020-10-27,"[['Lu', 'Ximing', ''], ['West', 'Peter', ''], ['Zellers', 'Rowan', ''], ['Bras', 'Ronan Le', ''], ['Bhagavatula', 'Chandra', ''], ['Choi', 'Yejin', '']]"
1369157,2010.12827,Amane Sugiyama,Amane Sugiyama and Naoki Yoshinaga,"Context-aware Decoder for Neural Machine Translation using a Target-side
  Document-Level Language Model",Under Review,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although many context-aware neural machine translation models have been
proposed to incorporate contexts in translation, most of those models are
trained end-to-end on parallel documents aligned in sentence-level. Because
only a few domains (and language pairs) have such document-level parallel data,
we cannot perform accurate context-aware translation in most domains. We
therefore present a simple method to turn a sentence-level translation model
into a context-aware model by incorporating a document-level language model
into the decoder. Our context-aware decoder is built upon only a sentence-level
parallel corpora and monolingual corpora; thus no document-level parallel data
is needed. In a theoretical viewpoint, the core part of this work is the novel
representation of contextual information using point-wise mutual information
between context and the current sentence. We show the effectiveness of our
approach in three language pairs, English to French, English to Russian, and
Japanese to English, by evaluation in \textsc{bleu} and contrastive tests for
context-aware translation.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 08:06:18 GMT'}]",2020-10-27,"[['Sugiyama', 'Amane', ''], ['Yoshinaga', 'Naoki', '']]"
1369215,2010.12885,Tong Niu,"Tong Niu, Semih Yavuz, Yingbo Zhou, Huan Wang, Nitish Shirish Keskar,
  Caiming Xiong",Unsupervised Paraphrase Generation via Dynamic Blocking,10 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose Dynamic Blocking, a decoding algorithm which enables large-scale
pretrained autoregressive models (such as BART, T5, GPT-2 and XLNet) to
generate high-quality paraphrases in an unsupervised setting. In order to
obtain an alternative surface form, whenever the language model emits a token
that is present in the source sequence, we prevent the model from generating
the subsequent source token for the next time step. We show that our approach
achieves state-of-the-art results on benchmark datasets when compared to
previous unsupervised approaches, and is even comparable with strong
supervised, in-domain models. We also propose a new automatic metric based on
self-BLEU and BERTscore which not only discourages the model from copying the
input through, but also evaluates text similarity based on distributed
representations, hence avoiding reliance on exact keyword matching. In
addition, we demonstrate that our model generalizes across languages without
any additional training.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 11:55:28 GMT'}]",2020-10-27,"[['Niu', 'Tong', ''], ['Yavuz', 'Semih', ''], ['Zhou', 'Yingbo', ''], ['Wang', 'Huan', ''], ['Keskar', 'Nitish Shirish', ''], ['Xiong', 'Caiming', '']]"
1369242,2010.12912,Camilo Thorne,Camilo Thorne and Saber Akhondi,Word Embeddings for Chemical Patent Natural Language Processing,"Extended version of an extended abstract presented (and reviewed) at
  the Latinx Workshop at ICML 2020",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  We evaluate chemical patent word embeddings against known biomedical
embeddings and show that they outperform the latter extrinsically and
intrinsically. We also show that using contextualized embeddings can induce
predictive models of reasonable performance for this domain over a relatively
small gold standard.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 15:03:20 GMT'}]",2020-10-27,"[['Thorne', 'Camilo', ''], ['Akhondi', 'Saber', '']]"
1369249,2010.12919,Reid Pryzant,"Reid Pryzant, Dallas Card, Dan Jurafsky, Victor Veitch, Dhanya Sridhar",Causal Effects of Linguistic Properties,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of estimating the causal effects of linguistic
properties on downstream outcomes. For example, does writing a complaint
politely lead to a faster response time? How much will a positive product
review increase sales? This paper focuses on two challenges related to the
problem. First, we formalize the causal quantity of interest as the effect of a
writer's intent, and establish the assumptions necessary to identify this from
observational data. Second, in practice we only have access to noisy proxies
for these linguistic properties---e.g., predictions from classifiers and
lexicons. We propose an estimator for this setting and prove that its bias is
bounded when we perform an adjustment for the text. The method leverages (1) a
pre-trained language model (BERT) to adjust for the text, and (2) distant
supervision to improve the quality of noisy proxies. We show that our algorithm
produces better causal estimates than related methods on two datasets:
predicting the effect of music review sentiment on sales, and complaint
politeness on response time.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 15:43:37 GMT'}]",2020-10-27,"[['Pryzant', 'Reid', ''], ['Card', 'Dallas', ''], ['Jurafsky', 'Dan', ''], ['Veitch', 'Victor', ''], ['Sridhar', 'Dhanya', '']]"
1369255,2010.12925,Camilo Thorne,Dhruba Pujary and Camilo Thorne and Wilker Aziz,Disease Normalization with Graph Embeddings,"This is a pre-print of a paper to appear in the proceedings of the
  IntelliSys 2020 conference",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The detection and normalization of diseases in biomedical texts are key
biomedical natural language processing tasks. Disease names need not only be
identified, but also normalized or linked to clinical taxonomies describing
diseases such as MeSH. In this paper we describe deep learning methods that
tackle both tasks. We train and test our methods on the known NCBI disease
benchmark corpus. We propose to represent disease names by leveraging MeSH's
graphical structure together with the lexical information available in the
taxonomy using graph embeddings. We also show that combining neural named
entity recognition models with our graph-based entity linking methods via
multitask learning leads to improved disease recognition in the NCBI corpus.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 16:25:05 GMT'}]",2020-10-27,"[['Pujary', 'Dhruba', ''], ['Thorne', 'Camilo', ''], ['Aziz', 'Wilker', '']]"
1369156,2010.12826,Felix Faltings,"Felix Faltings and Michel Galley and Gerold Hintz and Chris Brockett
  and Chris Quirk and Jianfeng Gao and Bill Dolan",Text Editing by Command,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A prevailing paradigm in neural text generation is one-shot generation, where
text is produced in a single step. The one-shot setting is inadequate, however,
when the constraints the user wishes to impose on the generated text are
dynamic, especially when authoring longer documents. We address this limitation
with an interactive text generation setting in which the user interacts with
the system by issuing commands to edit existing text. To this end, we propose a
novel text editing task, and introduce WikiDocEdits, a dataset of
single-sentence edits crawled from Wikipedia. We show that our Interactive
Editor, a transformer-based model trained on this dataset, outperforms
baselines and obtains positive results in both automatic and human evaluations.
We present empirical and qualitative analyses of this model's performance.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 08:00:30 GMT'}]",2020-10-27,"[['Faltings', 'Felix', ''], ['Galley', 'Michel', ''], ['Hintz', 'Gerold', ''], ['Brockett', 'Chris', ''], ['Quirk', 'Chris', ''], ['Gao', 'Jianfeng', ''], ['Dolan', 'Bill', '']]"
1369201,2010.12871,Zein Shaheen,"Zein Shaheen, Gerhard Wohlgenannt, Erwin Filtz",Large Scale Legal Text Classification Using Transformer Models,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large multi-label text classification is a challenging Natural Language
Processing (NLP) problem that is concerned with text classification for
datasets with thousands of labels. We tackle this problem in the legal domain,
where datasets, such as JRC-Acquis and EURLEX57K labeled with the EuroVoc
vocabulary were created within the legal information systems of the European
Union. The EuroVoc taxonomy includes around 7000 concepts. In this work, we
study the performance of various recent transformer-based models in combination
with strategies such as generative pretraining, gradual unfreezing and
discriminative learning rates in order to reach competitive classification
performance, and present new state-of-the-art results of 0.661 (F1) for
JRC-Acquis and 0.754 for EURLEX57K. Furthermore, we quantify the impact of
individual steps, such as language model fine-tuning or gradual unfreezing in
an ablation study, and provide reference dataset splits created with an
iterative stratification algorithm.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 11:03:01 GMT'}]",2020-10-27,"[['Shaheen', 'Zein', ''], ['Wohlgenannt', 'Gerhard', ''], ['Filtz', 'Erwin', '']]"
1369072,2010.12742,Zhiqiang Hu,"Zhiqiang Hu, Roy Ka-Wei Lee, Charu C. Aggarwal",Text Style Transfer: A Review and Experiment Evaluation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The stylistic properties of text have intrigued computational linguistics
researchers in recent years. Specifically, researchers have investigated the
Text Style Transfer (TST) task, which aims to change the stylistic properties
of the text while retaining its style independent content. Over the last few
years, many novel TST algorithms have been developed, while the industry has
leveraged these algorithms to enable exciting TST applications. The field of
TST research has burgeoned because of this symbiosis. This article aims to
provide a comprehensive review of recent research efforts on text style
transfer. More concretely, we create a taxonomy to organize the TST models and
provide a comprehensive summary of the state of the art. We review the existing
evaluation methodologies for TST tasks and conduct a large-scale
reproducibility study where we experimentally benchmark 19 state-of-the-art TST
algorithms on two publicly available datasets. Finally, we expand on current
trends and provide new perspectives on the new and exciting developments in the
TST field.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 02:02:58 GMT'}]",2020-10-27,"[['Hu', 'Zhiqiang', ''], ['Lee', 'Roy Ka-Wei', ''], ['Aggarwal', 'Charu C.', '']]"
1369194,2010.12864,Xisen Jin,"Xisen Jin, Francesco Barbieri, Aida Mostafazadeh Davani, Brendan
  Kennedy, Leonardo Neves, Xiang Ren",Efficiently Mitigating Classification Bias via Transfer Learning,10 pages,,,,cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prediction bias in machine learning models refers to unintended model
behaviors that discriminate against inputs mentioning or produced by certain
groups; for example, hate speech classifiers predict more false positives for
neutral text mentioning specific social groups. Mitigating bias for each task
or domain is inefficient, as it requires repetitive model training, data
annotation (e.g., demographic information), and evaluation. In pursuit of a
more accessible solution, we propose the Upstream Bias Mitigation for
Downstream Fine-Tuning (UBM) framework, which mitigate one or multiple bias
factors in downstream classifiers by transfer learning from an upstream model.
In the upstream bias mitigation stage, explanation regularization and
adversarial training are applied to mitigate multiple bias factors. In the
downstream fine-tuning stage, the classifier layer of the model is
re-initialized, and the entire model is fine-tuned to downstream tasks in
potentially novel domains without any further bias mitigation. We expect
downstream classifiers to be less biased by transfer learning from de-biased
upstream models. We conduct extensive experiments varying the similarity
between the source and target data, as well as varying the number of dimensions
of bias (e.g., discrimination against specific social groups or dialects). Our
results indicate the proposed UBM framework can effectively reduce bias in
downstream classifiers.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 10:36:11 GMT'}]",2020-10-27,"[['Jin', 'Xisen', ''], ['Barbieri', 'Francesco', ''], ['Davani', 'Aida Mostafazadeh', ''], ['Kennedy', 'Brendan', ''], ['Neves', 'Leonardo', ''], ['Ren', 'Xiang', '']]"
1369060,2010.12730,Gustavo Aguilar,"Gustavo Aguilar, Bryan McCann, Tong Niu, Nazneen Rajani, Nitish
  Keskar, Thamar Solorio","Char2Subword: Extending the Subword Embedding Space from Pre-trained
  Models Using Robust Character Compositionality",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword
tokenization process of language models. BPE provides multiple benefits, such
as handling the out-of-vocabulary problem and reducing vocabulary sparsity.
However, this process is defined from the pre-training data statistics, making
the tokenization on different domains susceptible to infrequent spelling
sequences (e.g., misspellings as in social media or character-level adversarial
attacks). On the other hand, pure character-level models, though robust to
misspellings, often lead to unreasonably large sequence lengths and make it
harder for the model to learn meaningful contiguous characters. To alleviate
these challenges, we propose a character-based subword transformer module
(char2subword) that learns the subword embedding table in pre-trained models
like BERT. Our char2subword module builds representations from characters out
of the subword vocabulary, and it can be used as a drop-in replacement of the
subword embedding table. The module is robust to character-level alterations
such as misspellings, word inflection, casing, and punctuation. We integrate it
further with BERT through pre-training while keeping BERT transformer
parameters fixed. We show our method's effectiveness by outperforming a vanilla
multilingual BERT on the linguistic code-switching evaluation (LinCE)
benchmark.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 01:08:28 GMT'}]",2020-10-27,"[['Aguilar', 'Gustavo', ''], ['McCann', 'Bryan', ''], ['Niu', 'Tong', ''], ['Rajani', 'Nazneen', ''], ['Keskar', 'Nitish', ''], ['Solorio', 'Thamar', '']]"
1368943,2010.12613,Julia Siekiera,"Julia Siekiera, Marius K\""oppel, Edwin Simpson, Kevin Stowe, Iryna
  Gurevych, Stefan Kramer",Ranking Creative Language Characteristics in Small Data Scenarios,"10 pages, 3 figures",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The ability to rank creative natural language provides an important general
tool for downstream language understanding and generation. However, current
deep ranking models require substantial amounts of labeled data that are
difficult and expensive to obtain for different domains, languages and creative
characteristics. A recent neural approach, the DirectRanker, promises to reduce
the amount of training data needed but its application to text isn't fully
explored. We therefore adapt the DirectRanker to provide a new deep model for
ranking creative language with small data. We compare DirectRanker with a
Bayesian approach, Gaussian process preference learning (GPPL), which has
previously been shown to work well with sparse data. Our experiments with
sparse training data show that while the performance of standard neural ranking
approaches collapses with small training datasets, DirectRanker remains
effective. We find that combining DirectRanker with GPPL increases performance
across different settings by leveraging the complementary benefits of both
models. Our combined approach outperforms the previous state-of-the-art on
humor and metaphor novelty tasks, increasing Spearman's $\rho$ by 14% and 16%
on average.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 18:57:47 GMT'}]",2020-10-27,"[['Siekiera', 'Julia', ''], ['Köppel', 'Marius', ''], ['Simpson', 'Edwin', ''], ['Stowe', 'Kevin', ''], ['Gurevych', 'Iryna', ''], ['Kramer', 'Stefan', '']]"
1369267,2010.12937,Arun Kumar Singh,"Arun Kumar Singh, Sushant Dave, Dr. Prathosh A. P., Prof. Brejesh Lall
  and Shresth Mehta","A Benchmark Corpus and Neural Approach for Sanskrit Derivative Nouns
  Analysis","6 pages, 2 figures, EACL 2021 Submission",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents first benchmark corpus of Sanskrit Pratyaya (suffix) and
inflectional words (padas) formed due to suffixes along with neural network
based approaches to process the formation and splitting of inflectional words.
Inflectional words spans the primary and secondary derivative nouns as the
scope of current work. Pratyayas are an important dimension of morphological
analysis of Sanskrit texts. There have been Sanskrit Computational Linguistics
tools for processing and analyzing Sanskrit texts. Unfortunately there has not
been any work to standardize & validate these tools specifically for derivative
nouns analysis. In this work, we prepared a Sanskrit suffix benchmark corpus
called Pratyaya-Kosh to evaluate the performance of tools. We also present our
own neural approach for derivative nouns analysis while evaluating the same on
most prominent Sanskrit Morphological Analysis tools. This benchmark will be
freely dedicated and available to researchers worldwide and we hope it will
motivate all to improve morphological analysis in Sanskrit Language.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 17:22:44 GMT'}]",2020-10-27,"[['Singh', 'Arun Kumar', ''], ['Dave', 'Sushant', ''], ['P.', 'Dr. Prathosh A.', ''], ['Lall', 'Prof. Brejesh', ''], ['Mehta', 'Shresth', '']]"
1237621,2002.00198,Kun Zhou,"Kun Zhou, Berrak Sisman, Haizhou Li","Transforming Spectrum and Prosody for Emotional Voice Conversion with
  Non-Parallel Training Data","accepted by Speaker Odyssey 2020 in Tokyo, Japan",,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Emotional voice conversion aims to convert the spectrum and prosody to change
the emotional patterns of speech, while preserving the speaker identity and
linguistic content. Many studies require parallel speech data between different
emotional patterns, which is not practical in real life. Moreover, they often
model the conversion of fundamental frequency (F0) with a simple linear
transform. As F0 is a key aspect of intonation that is hierarchical in nature,
we believe that it is more adequate to model F0 in different temporal scales by
using wavelet transform. We propose a CycleGAN network to find an optimal
pseudo pair from non-parallel training data by learning forward and inverse
mappings simultaneously using adversarial and cycle-consistency losses. We also
study the use of continuous wavelet transform (CWT) to decompose F0 into ten
temporal scales, that describes speech prosody at different time resolution,
for effective F0 conversion. Experimental results show that our proposed
framework outperforms the baselines both in objective and subjective
evaluations.
","[{'version': 'v1', 'created': 'Sat, 1 Feb 2020 12:36:55 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Apr 2020 12:43:26 GMT'}, {'version': 'v3', 'created': 'Tue, 7 Apr 2020 07:25:24 GMT'}, {'version': 'v4', 'created': 'Wed, 13 May 2020 05:21:37 GMT'}, {'version': 'v5', 'created': 'Sat, 24 Oct 2020 06:37:42 GMT'}]",2020-10-27,"[['Zhou', 'Kun', ''], ['Sisman', 'Berrak', ''], ['Li', 'Haizhou', '']]"
1368751,2010.12421,Jose Camacho-Collados,"Francesco Barbieri and Jose Camacho-Collados and Leonardo Neves and
  Luis Espinosa-Anke","TweetEval: Unified Benchmark and Comparative Evaluation for Tweet
  Classification","Findings of EMNLP 2020. TweetEval benchmark available at
  https://github.com/cardiffnlp/tweeteval",,,,cs.CL cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The experimental landscape in natural language processing for social media is
too fragmented. Each year, new shared tasks and datasets are proposed, ranging
from classics like sentiment analysis to irony detection or emoji prediction.
Therefore, it is unclear what the current state of the art is, as there is no
standardized evaluation protocol, neither a strong set of baselines trained on
such domain-specific data. In this paper, we propose a new evaluation framework
(TweetEval) consisting of seven heterogeneous Twitter-specific classification
tasks. We also provide a strong set of baselines as starting point, and compare
different language modeling pre-training strategies. Our initial experiments
show the effectiveness of starting off with existing pre-trained generic
language models, and continue training them on Twitter corpora.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 14:11:04 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 09:14:54 GMT'}]",2020-10-27,"[['Barbieri', 'Francesco', ''], ['Camacho-Collados', 'Jose', ''], ['Neves', 'Leonardo', ''], ['Espinosa-Anke', 'Luis', '']]"
1368264,2010.11934,Colin Raffel,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou,
  Aditya Siddhant, Aditya Barua, Colin Raffel",mT5: A massively multilingual pre-trained text-to-text transformer,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recent ""Text-to-Text Transfer Transformer"" (T5) leveraged a unified
text-to-text format and scale to attain state-of-the-art results on a wide
variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based
dataset covering 101 languages. We describe the design and modified training of
mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. All of the code and model checkpoints used in this work are
publicly available.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 17:58:14 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 21:25:28 GMT'}]",2020-10-27,"[['Xue', 'Linting', ''], ['Constant', 'Noah', ''], ['Roberts', 'Adam', ''], ['Kale', 'Mihir', ''], ['Al-Rfou', 'Rami', ''], ['Siddhant', 'Aditya', ''], ['Barua', 'Aditya', ''], ['Raffel', 'Colin', '']]"
1368186,2010.11856,Akari Asai,"Akari Asai, Jungo Kasai, Jonathan H. Clark, Kenton Lee, Eunsol Choi
  and Hannaneh Hajishirzi",XOR QA: Cross-lingual Open-Retrieval Question Answering,"Our data and code are available at
  https://nlp.cs.washington.edu/xorqa",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multilingual question answering tasks typically assume answers exist in the
same language as the question. Yet in practice, many languages face both
information scarcity---where languages have few reference articles---and
information asymmetry---where questions reference concepts from other cultures.
This work extends open-retrieval question answering to a cross-lingual setting
enabling questions from one language to be answered via answer content from
another language. We construct a large-scale dataset built on questions from
TyDi QA lacking same-language answers. Our task formulation, called
Cross-lingual Open Retrieval Question Answering (XOR QA), includes 40k
information-seeking questions from across 7 diverse non-English languages.
Based on this dataset, we introduce three new tasks that involve cross-lingual
document retrieval using multi-lingual and English resources. We establish
baselines with state-of-the-art machine translation systems and cross-lingual
pretrained models. Experimental results suggest that XOR QA is a challenging
task that will facilitate the development of novel techniques for multilingual
question answering. Our data and code are available at
https://nlp.cs.washington.edu/xorqa.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 16:47:17 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 10:00:22 GMT'}]",2020-10-27,"[['Asai', 'Akari', ''], ['Kasai', 'Jungo', ''], ['Clark', 'Jonathan H.', ''], ['Lee', 'Kenton', ''], ['Choi', 'Eunsol', ''], ['Hajishirzi', 'Hannaneh', '']]"
1367758,2010.11428,Qiujia Li,"Qiujia Li, David Qiu, Yu Zhang, Bo Li, Yanzhang He, Philip C.
  Woodland, Liangliang Cao, Trevor Strohman","Confidence Estimation for Attention-based Sequence-to-sequence Models
  for Speech Recognition",Submitted to ICASSP 2021,,,,eess.AS cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  For various speech-related tasks, confidence scores from a speech recogniser
are a useful measure to assess the quality of transcriptions. In traditional
hidden Markov model-based automatic speech recognition (ASR) systems,
confidence scores can be reliably obtained from word posteriors in decoding
lattices. However, for an ASR system with an auto-regressive decoder, such as
an attention-based sequence-to-sequence model, computing word posteriors is
difficult. An obvious alternative is to use the decoder softmax probability as
the model confidence. In this paper, we first examine how some commonly used
regularisation methods influence the softmax-based confidence scores and study
the overconfident behaviour of end-to-end models. Then we propose a lightweight
and effective approach named confidence estimation module (CEM) on top of an
existing end-to-end ASR model. Experiments on LibriSpeech show that CEM can
mitigate the overconfidence problem and can produce more reliable confidence
scores with and without shallow fusion of a language model. Further analysis
shows that CEM generalises well to speech from a moderately mismatched domain
and can potentially improve downstream tasks such as semi-supervised learning.
","[{'version': 'v1', 'created': 'Thu, 22 Oct 2020 04:02:27 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 18:49:07 GMT'}]",2020-10-27,"[['Li', 'Qiujia', ''], ['Qiu', 'David', ''], ['Zhang', 'Yu', ''], ['Li', 'Bo', ''], ['He', 'Yanzhang', ''], ['Woodland', 'Philip C.', ''], ['Cao', 'Liangliang', ''], ['Strohman', 'Trevor', '']]"
1367634,2010.11304,Wenxuan Zhou,"Wenxuan Zhou, Kevin Huang, Tengyu Ma, Jing Huang","Document-Level Relation Extraction with Adaptive Thresholding and
  Localized Context Pooling",Code available at https://github.com/wzhouad/ATLOP,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Document-level relation extraction (RE) poses new challenges compared to its
sentence-level RE counterpart. One document commonly contains multiple entity
pairs, and one entity pair occurs multiple times in the document associated
with multiple possible relations. In this paper, we propose two novel
techniques, adaptive thresholding and localized context pooling, to solve the
multilabel and multi-entity problems. The adaptive thresholding replaces the
global threshold for multi-label classification in the prior work by a
learnable entities-dependent threshold. The localized context pooling directly
transfers attention from pre-trained language models to locate relevant context
that is useful to decide the relation. We experiment on three document-level RE
benchmark datasets: DocRED, a recently released large-scale RE dataset, and two
datasets CDR and GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding
and Localized cOntext Pooling) model achieves an F1 score of 63.4; and also
significantly outperforms existing models on both CDR and GDA.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 20:41:23 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 21:18:24 GMT'}]",2020-10-27,"[['Zhou', 'Wenxuan', ''], ['Huang', 'Kevin', ''], ['Ma', 'Tengyu', ''], ['Huang', 'Jing', '']]"
1367328,2010.10998,Aditya Kalyanpur,"Aditya Kalyanpur, Or Biran, Tom Breloff, Jennifer Chu-Carroll, Ariel
  Diertani, Owen Rambow, Mark Sammons",Open-Domain Frame Semantic Parsing Using Transformers,11 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Frame semantic parsing is a complex problem which includes multiple
underlying subtasks. Recent approaches have employed joint learning of subtasks
(such as predicate and argument detection), and multi-task learning of related
tasks (such as syntactic and semantic parsing). In this paper, we explore
multi-task learning of all subtasks with transformer-based models. We show that
a purely generative encoder-decoder architecture handily beats the previous
state of the art in FrameNet 1.7 parsing, and that a mixed decoding multi-task
approach achieves even better performance. Finally, we show that the multi-task
model also outperforms recent state of the art systems for PropBank SRL parsing
on the CoNLL 2012 benchmark.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 13:38:04 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Oct 2020 23:37:12 GMT'}]",2020-10-27,"[['Kalyanpur', 'Aditya', ''], ['Biran', 'Or', ''], ['Breloff', 'Tom', ''], ['Chu-Carroll', 'Jennifer', ''], ['Diertani', 'Ariel', ''], ['Rambow', 'Owen', ''], ['Sammons', 'Mark', '']]"
1366264,2010.09934,Chengzhi Zhang,Yingyi Zhang and Chengzhi Zhang,Enhancing Keyphrase Extraction from Microblogs using Human Reading Time,,"Journal of the Association for Information Science and
  Technology,2021",10.1002/ASI.24430,,cs.CL cs.HC cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The premise of manual keyphrase annotation is to read the corresponding
content of an annotated object. Intuitively, when we read, more important words
will occupy a longer reading time. Hence, by leveraging human reading time, we
can find the salient words in the corresponding content. However, previous
studies on keyphrase extraction ignore human reading features. In this article,
we aim to leverage human reading time to extract keyphrases from microblog
posts. There are two main tasks in this study. One is to determine how to
measure the time spent by a human on reading a word. We use eye fixation
durations extracted from an open source eye-tracking corpus (OSEC). Moreover,
we propose strategies to make eye fixation duration more effective on keyphrase
extraction. The other task is to determine how to integrate human reading time
into keyphrase extraction models. We propose two novel neural network models.
The first is a model in which the human reading time is used as the ground
truth of the attention mechanism. In the second model, we use human reading
time as the external feature. Quantitative and qualitative experiments show
that our proposed models yield better performance than the baseline models on
two microblog datasets.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 00:18:44 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Oct 2020 11:24:18 GMT'}]",2020-10-27,"[['Zhang', 'Yingyi', ''], ['Zhang', 'Chengzhi', '']]"
1365563,2010.09233,Dang Pham Nhu Hai,"Dang Pham, Tuan M.V.Le",Auto-Encoding Variational Bayes for Inferring Topics and Visualization,"Accepted at the 28th International Conference on Computational
  Linguistics (COLING 2020)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Visualization and topic modeling are widely used approaches for text
analysis. Traditional visualization methods find low-dimensional
representations of documents in the visualization space (typically 2D or 3D)
that can be displayed using a scatterplot. In contrast, topic modeling aims to
discover topics from text, but for visualization, one needs to perform a
post-hoc embedding using dimensionality reduction methods. Recent approaches
propose using a generative model to jointly find topics and visualization,
allowing the semantics to be infused in the visualization space for a
meaningful interpretation. A major challenge that prevents these methods from
being used practically is the scalability of their inference algorithms. We
present, to the best of our knowledge, the first fast Auto-Encoding Variational
Bayes based inference method for jointly inferring topics and visualization.
Since our method is black box, it can handle model changes efficiently with
little mathematical rederivation effort. We demonstrate the efficiency and
effectiveness of our method on real-world large datasets and compare it with
existing baselines.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 05:57:11 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Oct 2020 19:37:56 GMT'}]",2020-10-27,"[['Pham', 'Dang', ''], ['Le', 'Tuan M. V.', '']]"
1365524,2010.09194,Pan Xie,"Pan Xie, Zhi Cui, Xiuyin Chen, Xiaohui Hu, Jianwei Cui, Bin Wang","Infusing Sequential Information into Conditional Masked Translation
  Model with Self-Review Mechanism",accepted to coling 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Non-autoregressive models generate target words in a parallel way, which
achieve a faster decoding speed but at the sacrifice of translation accuracy.
To remedy a flawed translation by non-autoregressive models, a promising
approach is to train a conditional masked translation model (CMTM), and refine
the generated results within several iterations. Unfortunately, such approach
hardly considers the \textit{sequential dependency} among target words, which
inevitably results in a translation degradation. Hence, instead of solely
training a Transformer-based CMTM, we propose a Self-Review Mechanism to infuse
sequential information into it. Concretely, we insert a left-to-right mask to
the same decoder of CMTM, and then induce it to autoregressively review whether
each generated word from CMTM is supposed to be replaced or kept. The
experimental results (WMT14 En$\leftrightarrow$De and WMT16
En$\leftrightarrow$Ro) demonstrate that our model uses dramatically less
training computations than the typical CMTM, as well as outperforms several
state-of-the-art non-autoregressive models by over 1 BLEU. Through knowledge
distillation, our model even surpasses a typical left-to-right Transformer
model, while significantly speeding up decoding.
","[{'version': 'v1', 'created': 'Mon, 19 Oct 2020 03:38:56 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 13:22:06 GMT'}]",2020-10-27,"[['Xie', 'Pan', ''], ['Cui', 'Zhi', ''], ['Chen', 'Xiuyin', ''], ['Hu', 'Xiaohui', ''], ['Cui', 'Jianwei', ''], ['Wang', 'Bin', '']]"
1364910,2010.08580,Zeyu Liu,"Chuanrong Li, Lin Shengshuo, Leo Z. Liu, Xinyi Wu, Xuhui Zhou, Shane
  Steinert-Threlkeld","Linguistically-Informed Transformations (LIT): A Method forAutomatically
  Generating Contrast Sets",Appears at EMNLP BlackboxNLP Workshop 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although large-scale pretrained language models, such as BERT and RoBERTa,
have achieved superhuman performance on in-distribution test sets, their
performance suffers on out-of-distribution test sets (e.g., on contrast sets).
Building contrast sets often re-quires human-expert annotation, which is
expensive and hard to create on a large scale. In this work, we propose a
Linguistically-Informed Transformation (LIT) method to automatically generate
contrast sets, which enables practitioners to explore linguistic phenomena of
interests as well as compose different phenomena. Experimenting with our method
on SNLI and MNLI shows that current pretrained language models, although being
claimed to contain sufficient linguistic knowledge, struggle on our
automatically generated contrast sets. Furthermore, we improve models'
performance on the contrast sets by apply-ing LIT to augment the training data,
without affecting performance on the original data.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 18:23:05 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Oct 2020 01:39:23 GMT'}]",2020-10-27,"[['Li', 'Chuanrong', ''], ['Shengshuo', 'Lin', ''], ['Liu', 'Leo Z.', ''], ['Wu', 'Xinyi', ''], ['Zhou', 'Xuhui', ''], ['Steinert-Threlkeld', 'Shane', '']]"
1364763,2010.08433,Andrey Kormilitzin,"Andrey Kormilitzin, Nemanja Vaci, Qiang Liu, Hao Ni, Goran Nenadic,
  Alejo Nevado-Holgado",An efficient representation of chronological events in medical texts,"4 pages, 2 figures, 7 tables",,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work we addressed the problem of capturing sequential information
contained in longitudinal electronic health records (EHRs). Clinical notes,
which is a particular type of EHR data, are a rich source of information and
practitioners often develop clever solutions how to maximise the sequential
information contained in free-texts. We proposed a systematic methodology for
learning from chronological events available in clinical notes. The proposed
methodological {\it path signature} framework creates a non-parametric
hierarchical representation of sequential events of any type and can be used as
features for downstream statistical learning tasks. The methodology was
developed and externally validated using the largest in the UK secondary care
mental health EHR data on a specific task of predicting survival risk of
patients diagnosed with Alzheimer's disease. The signature-based model was
compared to a common survival random forest model. Our results showed a
15.4$\%$ increase of risk prediction AUC at the time point of 20 months after
the first admission to a specialist memory clinic and the signature method
outperformed the baseline mixed-effects model by 13.2 $\%$.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 14:54:29 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 21:52:03 GMT'}]",2020-10-27,"[['Kormilitzin', 'Andrey', ''], ['Vaci', 'Nemanja', ''], ['Liu', 'Qiang', ''], ['Ni', 'Hao', ''], ['Nenadic', 'Goran', ''], ['Nevado-Holgado', 'Alejo', '']]"
1364576,2010.08246,Johannes Bjerva,"Johannes Bjerva and Elizabeth Salesky and Sabrina J. Mielke and Aditi
  Chaudhary and Giuseppe G. A. Celano and Edoardo M. Ponti and Ekaterina
  Vylomova and Ryan Cotterell and Isabelle Augenstein",SIGTYP 2020 Shared Task: Prediction of Typological Features,SigTyp 2020 Shared Task Description Paper @ EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013)
contain information about linguistic properties of the world's languages. They
have been shown to be useful for downstream applications, including
cross-lingual transfer learning and linguistic probing. A major drawback
hampering broader adoption of typological KBs is that they are sparsely
populated, in the sense that most languages only have annotations for some
features, and skewed, in that few features have wide coverage. As typological
features often correlate with one another, it is possible to predict them and
thus automatically populate typological KBs, which is also the focus of this
shared task. Overall, the task attracted 8 submissions from 5 teams, out of
which the most successful methods make use of such feature correlations.
However, our error analysis reveals that even the strongest submitted systems
struggle with predicting feature values for languages where few features are
known.
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 08:47:24 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 07:29:45 GMT'}]",2020-10-27,"[['Bjerva', 'Johannes', ''], ['Salesky', 'Elizabeth', ''], ['Mielke', 'Sabrina J.', ''], ['Chaudhary', 'Aditi', ''], ['Celano', 'Giuseppe G. A.', ''], ['Ponti', 'Edoardo M.', ''], ['Vylomova', 'Ekaterina', ''], ['Cotterell', 'Ryan', ''], ['Augenstein', 'Isabelle', '']]"
1359306,2010.02976,Albert Webson,"Albert Webson, Zhizhong Chen, Carsten Eickhoff, Ellie Pavlick","Are ""Undocumented Workers"" the Same as ""Illegal Aliens""? Disentangling
  Denotation and Connotation in Vector Spaces","Published at EMNLP 2020. Recorded talk available at
  https://youtu.be/V2pdS6Y_8n0 . Code and data available at
  https://github.com/awebson/congressional_adversary",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In politics, neologisms are frequently invented for partisan objectives. For
example, ""undocumented workers"" and ""illegal aliens"" refer to the same group of
people (i.e., they have the same denotation), but they carry clearly different
connotations. Examples like these have traditionally posed a challenge to
reference-based semantic theories and led to increasing acceptance of
alternative theories (e.g., Two-Factor Semantics) among philosophers and
cognitive scientists. In NLP, however, popular pretrained models encode both
denotation and connotation as one entangled representation. In this study, we
propose an adversarial neural network that decomposes a pretrained
representation as independent denotation and connotation representations. For
intrinsic interpretability, we show that words with the same denotation but
different connotations (e.g., ""immigrants"" vs. ""aliens"", ""estate tax"" vs.
""death tax"") move closer to each other in denotation space while moving further
apart in connotation space. For extrinsic application, we train an information
retrieval system with our disentangled representations and show that the
denotation vectors improve the viewpoint diversity of document rankings.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 19:09:03 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Oct 2020 15:43:44 GMT'}]",2020-10-27,"[['Webson', 'Albert', ''], ['Chen', 'Zhizhong', ''], ['Eickhoff', 'Carsten', ''], ['Pavlick', 'Ellie', '']]"
1357359,2010.01029,Chengjin Xu,"Chengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi,
  Jens Lehmann",TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation,This paper is accepted by COLING2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the last few years, there has been a surge of interest in learning
representations of entitiesand relations in knowledge graph (KG). However, the
recent availability of temporal knowledgegraphs (TKGs) that contain time
information for each fact created the need for reasoning overtime in such TKGs.
In this regard, we present a new approach of TKG embedding, TeRo, which defines
the temporal evolution of entity embedding as a rotation from the initial time
to the currenttime in the complex vector space. Specially, for facts involving
time intervals, each relation isrepresented as a pair of dual complex
embeddings to handle the beginning and the end of therelation, respectively. We
show our proposed model overcomes the limitations of the existing KG embedding
models and TKG embedding models and has the ability of learning and
inferringvarious relation patterns over time. Experimental results on four
different TKGs show that TeRo significantly outperforms existing
state-of-the-art models for link prediction. In addition, we analyze the effect
of time granularity on link prediction over TKGs, which as far as we know
hasnot been investigated in previous literature.
","[{'version': 'v1', 'created': 'Fri, 2 Oct 2020 14:35:27 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 22:42:26 GMT'}]",2020-10-27,"[['Xu', 'Chengjin', ''], ['Nayyeri', 'Mojtaba', ''], ['Alkhoury', 'Fouad', ''], ['Yazdi', 'Hamed Shariat', ''], ['Lehmann', 'Jens', '']]"
1354770,2009.13267,Pedram Rooshenas,"Subhajit Naskar, Amirmohammad Rooshenas, Simeng Sun, Mohit Iyyer,
  Andrew McCallum","Energy-Based Reranking: Improving Neural Machine Translation Using
  Energy-Based Models",,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The discrepancy between maximum likelihood estimation (MLE) and task measures
such as BLEU score has been studied before for autoregressive neural machine
translation (NMT) and resulted in alternative training algorithms (Ranzato et
al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However,
MLE training remains the de facto approach for autoregressive NMT because of
its computational efficiency and stability. Despite this mismatch between the
training objective and task measure, we notice that the samples drawn from an
MLE-based trained NMT support the desired distribution -- there are samples
with much higher BLEU score comparing to the beam decoding output. To benefit
from this observation, we train an energy-based model to mimic the behavior of
the task measure (i.e., the energy-based model assigns lower energy to samples
with higher BLEU score), which is resulted in a re-ranking algorithm based on
the samples drawn from NMT: energy-based re-ranking (EBR). Our EBR consistently
improves the performance of the Transformer-based NMT: +3 BLEU points on
Sinhala-English, +2.0 BLEU points on IWSLT'17 French-English, and +1.7 BLEU
points on WMT'19 German-English tasks.
","[{'version': 'v1', 'created': 'Sun, 20 Sep 2020 02:50:52 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 04:57:59 GMT'}]",2020-10-27,"[['Naskar', 'Subhajit', ''], ['Rooshenas', 'Amirmohammad', ''], ['Sun', 'Simeng', ''], ['Iyyer', 'Mohit', ''], ['McCallum', 'Andrew', '']]"
1353847,2009.12344,"Tommi Gr\""ondahl","Mika Juuti, Tommi Gr\""ondahl, Adrian Flanagan and N. Asokan","A little goes a long way: Improving toxic language classification
  despite data scarcity",To appear in Findings of ACL: EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Detection of some types of toxic language is hampered by extreme scarcity of
labeled training data. Data augmentation - generating new synthetic data from a
labeled seed dataset - can help. The efficacy of data augmentation on toxic
language classification has not been fully explored. We present the first
systematic study on how data augmentation techniques impact performance across
toxic language classifiers, ranging from shallow logistic regression
architectures to BERT - a state-of-the-art pre-trained Transformer network. We
compare the performance of eight techniques on very scarce seed datasets. We
show that while BERT performed the best, shallow classifiers performed
comparably when trained on data augmented with a combination of three
techniques, including GPT-2-generated sentences. We discuss the interplay of
performance and computational overhead, which can inform the choice of
techniques under different constraints.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 17:04:17 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 19:31:34 GMT'}]",2020-10-27,"[['Juuti', 'Mika', ''], ['Gröndahl', 'Tommi', ''], ['Flanagan', 'Adrian', ''], ['Asokan', 'N.', '']]"
1352187,2009.10684,Bruno Taill\'e,"Bruno Taill\'e, Vincent Guigue, Geoffrey Scoutheeten and Patrick
  Gallinari",Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!,Accepted at EMNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite efforts to distinguish three different evaluation setups (Bekoulis et
al., 2018), numerous end-to-end Relation Extraction (RE) articles present
unreliable performance comparison to previous work. In this paper, we first
identify several patterns of invalid comparisons in published papers and
describe them to avoid their propagation. We then propose a small empirical
study to quantify the impact of the most common mistake and evaluate it leads
to overestimating the final RE performance by around 5% on ACE05. We also seize
this opportunity to study the unexplored ablations of two recent developments:
the use of language model pretraining (specifically BERT) and span-level NER.
This meta-analysis emphasizes the need for rigor in the report of both the
evaluation setting and the datasets statistics and we call for unifying the
evaluation setting in end-to-end RE.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 16:59:15 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 16:43:35 GMT'}]",2020-10-27,"[['Taillé', 'Bruno', ''], ['Guigue', 'Vincent', ''], ['Scoutheeten', 'Geoffrey', ''], ['Gallinari', 'Patrick', '']]"
1350056,2009.08553,Yuning Mao,"Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao,
  Jiawei Han, Weizhu Chen",Generation-Augmented Retrieval for Open-domain Question Answering,"Added experiments with a generative reader. Current performance:
  EM=41.8 (43.8 +DPR) on NQ and 62.7 on Trivia with BERT-base (extractive);
  EM=38.1 (45.3 +DPR) on NQ and 61.8 on Trivia with BART-large (generative)",,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and
efficient, but solely rely on lexical overlap without semantic matching. Recent
dense retrieval methods learn latent representations to tackle the lexical
mismatch problem, while being more computationally expensive and insufficient
for exact matching as they embed the text sequence into a single vector with
limited capacity. In this paper, we present Generation-Augmented Retrieval
(GAR), a query expansion method that augments a query with relevant contexts
through text generation. We demonstrate on open-domain question answering that
the generated contexts significantly enrich the semantics of the queries and
thus GAR with sparse representations (BM25) achieves comparable or better
performance than the state-of-the-art dense methods such as DPR
\cite{karpukhin2020dense}. We show that generating various contexts of a query
is beneficial as fusing their results consistently yields better retrieval
accuracy. Moreover, as sparse and dense representations are often
complementary, GAR can be easily combined with DPR to achieve even better
performance. Furthermore, GAR achieves the state-of-the-art performance on the
Natural Questions and TriviaQA datasets under the extractive setting when
equipped with an extractive reader, and consistently outperforms other
retrieval methods when the same generative reader is used.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 23:08:01 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 03:23:27 GMT'}]",2020-10-27,"[['Mao', 'Yuning', ''], ['He', 'Pengcheng', ''], ['Liu', 'Xiaodong', ''], ['Shen', 'Yelong', ''], ['Gao', 'Jianfeng', ''], ['Han', 'Jiawei', ''], ['Chen', 'Weizhu', '']]"
1347990,2009.06487,Chengyu Wang,"Chengyu Wang, Mengli Cheng, Xu Hu, Jun Huang","EasyASR: A Distributed Machine Learning Platform for End-to-end
  Automatic Speech Recognition",aaai 2021 demo paper,,,,cs.CL cs.AI cs.DC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present EasyASR, a distributed machine learning platform for training and
serving large-scale Automatic Speech Recognition (ASR) models, as well as
collecting and processing audio data at scale. Our platform is built upon the
Machine Learning Platform for AI of Alibaba Cloud. Its main functionality is to
support efficient learning and inference for end-to-end ASR models on
distributed GPU clusters. It allows users to learn ASR models with either
pre-defined or user-customized network architectures via simple user interface.
On EasyASR, we have produced state-of-the-art results over several public
datasets for Mandarin speech recognition.
","[{'version': 'v1', 'created': 'Mon, 14 Sep 2020 14:47:02 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 09:44:27 GMT'}]",2020-10-27,"[['Wang', 'Chengyu', ''], ['Cheng', 'Mengli', ''], ['Hu', 'Xu', ''], ['Huang', 'Jun', '']]"
1347389,2009.05886,Dylan Slack,Gavin Kerrigan and Dylan Slack and Jens Tuyls,Differentially Private Language Models Benefit from Public Pre-training,,,,,cs.LG cs.CL cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language modeling is a keystone task in natural language processing. When
training a language model on sensitive information, differential privacy (DP)
allows us to quantify the degree to which our private data is protected.
However, training algorithms which enforce differential privacy often lead to
degradation in model quality. We study the feasibility of learning a language
model which is simultaneously high-quality and privacy preserving by tuning a
public base model on a private corpus. We find that DP fine-tuning boosts the
performance of language models in the private domain, making the training of
such models possible.
","[{'version': 'v1', 'created': 'Sun, 13 Sep 2020 00:50:44 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 16:04:43 GMT'}]",2020-10-27,"[['Kerrigan', 'Gavin', ''], ['Slack', 'Dylan', ''], ['Tuyls', 'Jens', '']]"
1368953,2010.12623,Wenhu Chen,"Liangming Pan, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, William Yang
  Wang",Unsupervised Multi-hop Question Answering by Question Generation,Technical Report,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Obtaining training data for Multi-hop Question Answering (QA) is extremely
time-consuming and resource-intensive. To address this, we propose the problem
of \textit{unsupervised} multi-hop QA, assuming that no human-labeled multi-hop
question-answer pairs are available. We propose MQA-QG, an unsupervised
question answering framework that can generate human-like multi-hop training
pairs from both homogeneous and heterogeneous data sources. Our model generates
questions by first selecting or generating relevant information from each data
source and then integrating the multiple information to form a multi-hop
question. We find that we can train a competent multi-hop QA model with only
generated data. The F1 gap between the unsupervised and fully-supervised models
is less than 20 in both the HotpotQA and the HybridQA dataset. Further
experiments reveal that an unsupervised pretraining with the QA data generated
by our model would greatly reduce the demand for human-annotated training data
for multi-hop QA.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 19:13:47 GMT'}]",2020-10-27,"[['Pan', 'Liangming', ''], ['Chen', 'Wenhu', ''], ['Xiong', 'Wenhan', ''], ['Kan', 'Min-Yen', ''], ['Wang', 'William Yang', '']]"
1369071,2010.12741,Jo\~ao Sedoc,"Seolhwa Lee, Heuiseok Lim, Jo\~ao Sedoc",An Evaluation Protocol for Generative Conversational Systems,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is a multitude of novel generative models for open-domain
conversational systems; however, there is no systematic evaluation of different
systems. Systematic comparisons require consistency in experimental design,
evaluation sets, conversational systems and their outputs, and statistical
analysis. We lay out a protocol for the evaluation of conversational models
using head-to-head pairwise comparison. We analyze ten recent models that claim
state-of-the-art performance using a paired head-to-head performance
(win-loss-tie) on five evaluation datasets. Our findings show that DialoGPT and
Blender are superior systems using Bradley-Terry model and TrueSkill ranking
methods. These findings demonstrate the feasibility of our protocol to evaluate
conversational agents and evaluation sets. Finally, we make all code and
evaluations publicly available for researchers to compare their model to other
state-of-the-art dialog models.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 01:59:49 GMT'}]",2020-10-27,"[['Lee', 'Seolhwa', ''], ['Lim', 'Heuiseok', ''], ['Sedoc', 'João', '']]"
1368956,2010.12626,Laure Thompson,"Laure Thompson, David Mimno",Topic Modeling with Contextualized Word Representation Clusters,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Clustering token-level contextualized word representations produces output
that shares many similarities with topic models for English text collections.
Unlike clusterings of vocabulary-level word embeddings, the resulting models
more naturally capture polysemy and can be used as a way of organizing
documents. We evaluate token clusterings trained from several different output
layers of popular contextualized language models. We find that BERT and GPT-2
produce high quality clusterings, but RoBERTa does not. These cluster models
are simple, reliable, and can perform as well as, if not better than, LDA topic
models, maintaining high topic quality even when the number of topics is large
relative to the size of the local collection.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 19:16:59 GMT'}]",2020-10-27,"[['Thompson', 'Laure', ''], ['Mimno', 'David', '']]"
1368964,2010.12634,Yusen Zhang,"Yusen Zhang, Xiangyu Dong, Shuaichen Chang, Tao Yu, Peng Shi and Rui
  Zhang","Did You Ask a Good Question? A Cross-Domain Question Intention
  Classification Benchmark for Text-to-SQL","8 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural models have achieved significant results on the text-to-SQL task, in
which most current work assumes all the input questions are legal and generates
a SQL query for any input. However, in the real scenario, users can input any
text that may not be able to be answered by a SQL query. In this work, we
propose TriageSQL, the first cross-domain text-to-SQL question intention
classification benchmark that requires models to distinguish four types of
unanswerable questions from answerable questions. The baseline RoBERTa model
achieves a 60% F1 score on the test set, demonstrating the need for further
improvement on this task. Our dataset is available at
https://github.com/chatc/TriageSQL.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 19:36:57 GMT'}]",2020-10-27,"[['Zhang', 'Yusen', ''], ['Dong', 'Xiangyu', ''], ['Chang', 'Shuaichen', ''], ['Yu', 'Tao', ''], ['Shi', 'Peng', ''], ['Zhang', 'Rui', '']]"
1369059,2010.12729,Adina Williams,"Adina Williams, Tristan Thrush, Douwe Kiela",ANLIzing the Adversarial Natural Language Inference Dataset,"33 pages, 1 figure, 24 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We perform an in-depth error analysis of Adversarial NLI (ANLI), a recently
introduced large-scale human-and-model-in-the-loop natural language inference
dataset collected over multiple rounds. We propose a fine-grained annotation
scheme of the different aspects of inference that are responsible for the gold
classification labels, and use it to hand-code all three of the ANLI
development sets. We use these annotations to answer a variety of interesting
questions: which inference types are most common, which models have the highest
performance on each reasoning type, and which types are the most challenging
for state of-the-art models? We hope that our annotations will enable more
fine-grained evaluation of models trained on ANLI, provide us with a deeper
understanding of where models fail and succeed, and help us determine how to
train better models in future.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 01:03:51 GMT'}]",2020-10-27,"[['Williams', 'Adina', ''], ['Thrush', 'Tristan', ''], ['Kiela', 'Douwe', '']]"
1369055,2010.12725,Peter Shaw,"Peter Shaw, Ming-Wei Chang, Panupong Pasupat, Kristina Toutanova","Compositional Generalization and Natural Language Variation: Can a
  Semantic Parsing Approach Handle Both?",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sequence-to-sequence models excel at handling natural language variation, but
have been shown to struggle with out-of-distribution compositional
generalization. This has motivated new specialized architectures with stronger
compositional biases, but most of these approaches have only been evaluated on
synthetically-generated datasets, which are not representative of natural
language variation. In this work we ask: can we develop a semantic parsing
approach that handles both natural language variation and compositional
generalization? To better assess this capability, we propose new train and test
splits of non-synthetic datasets. We demonstrate that strong existing semantic
parsing approaches do not yet perform well across a broad set of evaluations.
We also propose NQG-T5, a hybrid model that combines a high-precision
grammar-based approach with a pre-trained sequence-to-sequence model. It
outperforms existing approaches across several compositional generalization
challenges, while also being competitive with the state-of-the-art on standard
evaluations. While still far from solving this problem, our study highlights
the importance of diverse evaluations and the open challenge of handling both
compositional generalization and natural language variation in semantic
parsing.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 00:38:27 GMT'}]",2020-10-27,"[['Shaw', 'Peter', ''], ['Chang', 'Ming-Wei', ''], ['Pasupat', 'Panupong', ''], ['Toutanova', 'Kristina', '']]"
1369053,2010.12723,Yuning Mao,"Yuning Mao, Xiang Ren, Heng Ji, Jiawei Han","Constrained Abstractive Summarization: Preserving Factual Consistency
  with Constrained Generation",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Summaries generated by abstractive summarization are supposed to only contain
statements entailed by the source documents. However, state-of-the-art
abstractive methods are still prone to hallucinate content inconsistent with
the source documents. In this paper, we propose constrained abstractive
summarization (CAS), a general setup that preserves the factual consistency of
abstractive summarization by specifying tokens as constraints that must be
present in the summary. We explore the feasibility of using lexically
constrained decoding, a technique applicable to any abstractive method with
beam search decoding, to fulfill CAS and conduct experiments in two scenarios:
(1) Standard summarization without human involvement, where keyphrase
extraction is used to extract constraints from source documents; (2)
Interactive summarization with human feedback, which is simulated by taking
missing tokens in the reference summaries as constraints. Automatic and human
evaluations on two benchmark datasets demonstrate that CAS improves the quality
of abstractive summaries, especially on factual consistency. In particular, we
observe up to 11.2 ROUGE-2 gains when several ground-truth tokens are used as
constraints in the interactive summarization scenario.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 00:27:44 GMT'}]",2020-10-27,"[['Mao', 'Yuning', ''], ['Ren', 'Xiang', ''], ['Ji', 'Heng', ''], ['Han', 'Jiawei', '']]"
1369049,2010.12719,Falcon Dai,Falcon Z. Dai,Word2vec Conjecture and A Limitative Result,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Being inspired by the success of \texttt{word2vec}
\citep{mikolov2013distributed} in capturing analogies, we study the conjecture
that analogical relations can be represented by vector spaces. Unlike many
previous works that focus on the distributional semantic aspect of
\texttt{word2vec}, we study the purely \emph{representational} question: can
\emph{all} semantic word-word relations be represented by differences (or
directions) of vectors? We call this the word2vec conjecture and point out some
of its desirable implications. However, we will exhibit a class of relations
that cannot be represented in this way, thus falsifying the conjecture and
establishing a limitative result for the representability of semantic relations
by vector spaces over fields of characteristic 0, e.g., real or complex
numbers.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 00:14:04 GMT'}]",2020-10-27,"[['Dai', 'Falcon Z.', '']]"
1369042,2010.12712,Shuguang Chen,"Shuguang Chen, Gustavo Aguilar, Leonardo Neves, Thamar Solorio","A Caption Is Worth A Thousand Images: Investigating Image Captions for
  Multimodal Named Entity Recognition","8 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal named entity recognition (MNER) requires to bridge the gap between
language understanding and visual context. Due to advances in natural language
processing (NLP) and computer vision (CV), many neural techniques have been
proposed to incorporate images into the NER task. In this work, we conduct a
detailed analysis of current state-of-the-art fusion techniques for MNER and
describe scenarios where adding information from the image does not always
result in boosts in performance. We also study the use of captions as a way to
enrich the context for MNER. We provide extensive empirical analysis and an
ablation study on three datasets from popular social platforms to expose the
situations where the approach is beneficial.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 23:41:51 GMT'}]",2020-10-27,"[['Chen', 'Shuguang', ''], ['Aguilar', 'Gustavo', ''], ['Neves', 'Leonardo', ''], ['Solorio', 'Thamar', '']]"
1369040,2010.12710,Maria Phillips,"Debajyoti Datta, Maria Phillips, Jennifer Chiu, Ginger S. Watson,
  James P. Bywater, Laura Barnes, and Donald Brown","Improving Classification through Weak Supervision in Context-specific
  Conversational Agent Development for Teacher Education",Preprint: Under Review,,,,cs.CL cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning techniques applied to the Natural Language Processing (NLP)
component of conversational agent development show promising results for
improved accuracy and quality of feedback that a conversational agent can
provide. The effort required to develop an educational scenario specific
conversational agent is time consuming as it requires domain experts to label
and annotate noisy data sources such as classroom videos. Previous approaches
to modeling annotations have relied on labeling thousands of examples and
calculating inter-annotator agreement and majority votes in order to model the
necessary scenarios. This method, while proven successful, ignores individual
annotator strengths in labeling a data point and under-utilizes examples that
do not have a majority vote for labeling. We propose using a multi-task weak
supervision method combined with active learning to address these concerns.
This approach requires less labeling than traditional methods and shows
significant improvements in precision, efficiency, and time-requirements than
the majority vote method (Ratner 2019). We demonstrate the validity of this
method on the Google Jigsaw data set and then propose a scenario to apply this
method using the Instructional Quality Assessment(IQA) to define the categories
for labeling. We propose using probabilistic modeling of annotator labeling to
generate active learning examples to further label the data. Active learning is
able to iteratively improve the training performance and accuracy of the
original classification model. This approach combines state-of-the art labeling
techniques of weak supervision and active learning to optimize results in the
educational domain and could be further used to lessen the data requirements
for expanded scenarios within the education domain through transfer learning.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 23:39:40 GMT'}]",2020-10-27,"[['Datta', 'Debajyoti', ''], ['Phillips', 'Maria', ''], ['Chiu', 'Jennifer', ''], ['Watson', 'Ginger S.', ''], ['Bywater', 'James P.', ''], ['Barnes', 'Laura', ''], ['Brown', 'Donald', '']]"
1369037,2010.12707,Dorottya Demszky,"Dorottya Demszky, Devyani Sharma, Jonathan H. Clark, Vinodkumar
  Prabhakaran, Jacob Eisenstein",Learning to Recognize Dialect Features,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Linguists characterize dialects by the presence, absence, and frequency of
dozens of interpretable features. Detecting these features in text has
applications to social science and dialectology, and can be used to assess the
robustness of natural language processing systems to dialect differences. For
most dialects, large-scale annotated corpora for these features are
unavailable, making it difficult to train recognizers. Linguists typically
define dialect features by providing a small number of minimal pairs, which are
paired examples distinguished only by whether the feature is present, while
holding everything else constant. In this paper, we present two multitask
learning architectures for recognizing dialect features, both based on
pretrained transformers. We evaluate these models on two test sets of Indian
English, annotated for a total of 22 dialect features. We find these models
learn to recognize many features with high accuracy; crucially, a few minimal
pairs can be nearly as effective for training as thousands of labeled examples.
We also demonstrate the downstream applicability of our dialect feature
detection model as a dialect density measure and as a dialect classifier.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 23:25:00 GMT'}]",2020-10-27,"[['Demszky', 'Dorottya', ''], ['Sharma', 'Devyani', ''], ['Clark', 'Jonathan H.', ''], ['Prabhakaran', 'Vinodkumar', ''], ['Eisenstein', 'Jacob', '']]"
1369029,2010.12699,"Stefan Gr\""unewald","Stefan Gr\""unewald, Annemarie Friedrich, Jonas Kuhn","Graph-Based Universal Dependency Parsing in the Age of the Transformer:
  What Works, and What Doesn't",14 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current state-of-the-art graph-based dependency parsers differ on various
dimensions. Among others, these include (a) the choice of pre-trained word
embeddings or language models used for representing token, (b) training setups
performing only parsing or additional tasks such as part-of-speech-tagging, and
(c) their mechanism of constructing trees or graphs from edge scores. Because
of this, it is difficult to estimate the impact of these architectural
decisions when comparing parsers.
  In this paper, we perform a series of experiments on STEPS, a new modular
graph-based parser for basic and enhanced Universal Dependencies, analyzing the
effects of architectural configurations. We find that pre-trained embeddings
have by far the greatest and most clear-cut impact on parser performance. The
choice of factorized vs. unfactorized architectures and a multi-task training
setup affect parsing accuracy in more subtle ways, depending on target language
and output representation (trees vs. graphs). Our parser achieves new
state-of-the-art results for a wide range of languages on both basic as well as
enhanced Universal Dependencies, using a unified and comparatively simple
architecture for both parsing tasks.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 22:58:26 GMT'}]",2020-10-27,"[['Grünewald', 'Stefan', ''], ['Friedrich', 'Annemarie', ''], ['Kuhn', 'Jonas', '']]"
1369024,2010.12694,Sayali Kulkarni,"Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, Eugene Ie","AQuaMuSe: Automatically Generating Datasets for Query-Based
  Multi-Document Summarization",,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Summarization is the task of compressing source document(s) into coherent and
succinct passages. This is a valuable tool to present users with concise and
accurate sketch of the top ranked documents related to their queries.
Query-based multi-document summarization (qMDS) addresses this pervasive need,
but the research is severely limited due to lack of training and evaluation
datasets as existing single-document and multi-document summarization datasets
are inadequate in form and scale. We propose a scalable approach called
AQuaMuSe to automatically mine qMDS examples from question answering datasets
and large document corpora. Our approach is unique in the sense that it can
general a dual dataset -- for extractive and abstractive summaries both. We
publicly release a specific instance of an AQuaMuSe dataset with 5,519
query-based summaries, each associated with an average of 6 input documents
selected from an index of 355M documents from Common Crawl. Extensive
evaluation of the dataset along with baseline summarization model experiments
are provided.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 22:38:18 GMT'}]",2020-10-27,"[['Kulkarni', 'Sayali', ''], ['Chammas', 'Sheide', ''], ['Zhu', 'Wan', ''], ['Sha', 'Fei', ''], ['Ie', 'Eugene', '']]"
1369023,2010.12693,Nadezhda Chirkova,Nadezhda Chirkova,Neural Code Completion with Anonymized Variable Names,,,,,cs.SE cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Source code processing heavily relies on the methods widely used in natural
language processing (NLP), but involves specifics that need to be taken into
account to achieve higher quality. An example of this specificity is that
renaming variables does not change the semantics of what the code does. In this
work, we develop a recurrent architecture that processes code with all variable
names anonymized, i. e. replaced with unique placeholders. The proposed
architecture outperforms standard NLP baselines on code completion task by a
large margin in the anonymized setting, and improves the base model in the
non-anonymized setting, being ensembled with it.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 22:32:11 GMT'}]",2020-10-27,"[['Chirkova', 'Nadezhda', '']]"
1369018,2010.12688,Oshin Agarwal,"Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou","Large Scale Knowledge Graph Based Synthetic Corpus Generation for
  Knowledge-Enhanced Language Model Pre-training",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating natural sentences from Knowledge Graph (KG) triples, known as
Data-To-Text Generation, is a task with many datasets for which numerous
complex systems have been developed. However, no prior work has attempted to
perform this generation at scale by converting an entire KG into natural text.
In this paper, we verbalize the entire Wikidata KG, and create a KG-Text
aligned corpus in the training process. We discuss the challenges in
verbalizing an entire KG versus verbalizing smaller datasets. We further show
that verbalizing an entire KG can be used to integrate structured and natural
language data. In contrast to the many architectures that have been developed
to integrate the structural differences between these two sources, our approach
converts the KG into the same format as natural text allowing it to be
seamlessly plugged into existing natural language systems. We evaluate this
approach by augmenting the retrieval corpus in REALM and showing improvements,
both on the LAMA knowledge probe and open domain QA.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 22:14:50 GMT'}]",2020-10-27,"[['Agarwal', 'Oshin', ''], ['Ge', 'Heming', ''], ['Shakeri', 'Siamak', ''], ['Al-Rfou', 'Rami', '']]"
1369014,2010.12684,Valentin Hofmann,"Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\""utze",Dynamic Contextualized Word Embeddings,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Static word embeddings that represent words by a single vector cannot capture
the variability of word meaning in different linguistic and extralinguistic
contexts. Building on prior work on contextualized and dynamic word embeddings,
we introduce dynamic contextualized word embeddings that represent words as a
function of both linguistic and extralinguistic context. Based on a pretrained
language model (PLM), dynamic contextualized word embeddings model time and
social space jointly, which makes them attractive for various tasks in the
computational social sciences. We highlight potential applications by means of
qualitative and quantitative analyses.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 22:02:40 GMT'}]",2020-10-27,"[['Hofmann', 'Valentin', ''], ['Pierrehumbert', 'Janet B.', ''], ['Schütze', 'Hinrich', '']]"
1369013,2010.12683,Jyun-Yu Jiang,"Jyun-Yu Jiang, Chenyan Xiong, Chia-Jung Lee and Wei Wang",Long Document Ranking with Query-Directed Sparse Transformer,"Accepted by EMNLP 2020, 12 pages, 5 figures",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The computing cost of transformer self-attention often necessitates breaking
long documents to fit in pretrained models in document ranking tasks. In this
paper, we design Query-Directed Sparse attention that induces IR-axiomatic
structures in transformer self-attention. Our model, QDS-Transformer, enforces
the principle properties desired in ranking: local contextualization,
hierarchical representation, and query-oriented proximity matching, while it
also enjoys efficiency from sparsity. Experiments on one fully supervised and
three few-shot TREC document ranking benchmarks demonstrate the consistent and
robust advantage of QDS-Transformer over previous approaches, as they either
retrofit long documents into BERT or use sparse attention without emphasizing
IR principles. We further quantify the computing complexity and demonstrates
that our sparse attention with TVM implementation is twice more efficient than
the fully-connected self-attention. All source codes, trained model, and
predictions of this work are available at
https://github.com/hallogameboy/QDS-Transformer.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 21:57:56 GMT'}]",2020-10-27,"[['Jiang', 'Jyun-Yu', ''], ['Xiong', 'Chenyan', ''], ['Lee', 'Chia-Jung', ''], ['Wang', 'Wei', '']]"
1369011,2010.12681,Armineh Nourbakhsh,"Natraj Raman, Armineh Nourbakhsh, Sameena Shah, Manuela Veloso",Robust Document Representations using Latent Topics and Metadata,"9 pages, 7 figures",,,,cs.CL cs.AI cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task specific fine-tuning of a pre-trained neural language model using a
custom softmax output layer is the de facto approach of late when dealing with
document classification problems. This technique is not adequate when labeled
examples are not available at training time and when the metadata artifacts in
a document must be exploited. We address these challenges by generating
document representations that capture both text and metadata artifacts in a
task agnostic manner. Instead of traditional auto-regressive or auto-encoding
based training, our novel self-supervised approach learns a soft-partition of
the input space when generating text embeddings. Specifically, we employ a
pre-learned topic model distribution as surrogate labels and construct a loss
function based on KL divergence. Our solution also incorporates metadata
explicitly rather than just augmenting them with text. The generated document
embeddings exhibit compositional characteristics and are directly used by
downstream classification tasks to create decision boundaries from a small
number of labeled examples, thereby eschewing complicated recognition methods.
We demonstrate through extensive evaluation that our proposed cross-model
fusion solution outperforms several competitive baselines on multiple datasets.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 21:52:38 GMT'}]",2020-10-27,"[['Raman', 'Natraj', ''], ['Nourbakhsh', 'Armineh', ''], ['Shah', 'Sameena', ''], ['Veloso', 'Manuela', '']]"
1369006,2010.12676,Chunchuan Lyu Mr.,"Chunchuan Lyu, Shay B. Cohen, Ivan Titov","A Differentiable Relaxation of Graph Segmentation and Alignment for AMR
  Parsing",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Abstract Meaning Representations (AMR) are a broad-coverage semantic
formalism which represents sentence meaning as a directed acyclic graph. To
train most AMR parsers, one needs to segment the graph into subgraphs and align
each such subgraph to a word in a sentence; this is normally done at
preprocessing, relying on hand-crafted rules. In contrast, we treat both
alignment and segmentation as latent variables in our model and induce them as
part of end-to-end training.
  As marginalizing over the structured latent variables is infeasible, we use
the variational autoencoding framework.
  To ensure end-to-end differentiable optimization, we introduce a continuous
differentiable relaxation of the segmentation and alignment problems. We
observe that inducing segmentation yields substantial gains over using a
`greedy' segmentation heuristic. The performance of our method also approaches
that of a model that relies on \citet{Lyu2018AMRPA}'s segmentation rules, which
were hand-crafted to handle individual AMR constructions.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 21:22:50 GMT'}]",2020-10-27,"[['Lyu', 'Chunchuan', ''], ['Cohen', 'Shay B.', ''], ['Titov', 'Ivan', '']]"
1369005,2010.12675,David Gaddy,"David Gaddy, Alex Kouzemtchenko, Pavan Kumar Reddy, Prateek Kolhar,
  and Rushin Shah",Overcoming Conflicting Data for Model Updates,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we explore how to use a small amount of new data to update a
model when the desired output for some examples has changed. When making
updates in this way, one potential problem that arises is the presence of
conflicting data, or out-of-date labels in the original training set. To
evaluate the impact of this problem, we propose an experimental setup for
simulating changes to a neural semantic parser. We show that the presence of
conflicting data greatly hinders learning of an update, then explore several
methods to mitigate its effect. Our methods lead to large improvements in model
accuracy compared to a naive mixing strategy, and our best method closes 86% of
the accuracy gap between this baseline and an oracle upper bound.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 21:19:03 GMT'}]",2020-10-27,"[['Gaddy', 'David', ''], ['Kouzemtchenko', 'Alex', ''], ['Reddy', 'Pavan Kumar', ''], ['Kolhar', 'Prateek', ''], ['Shah', 'Rushin', '']]"
1369003,2010.12673,Liang Lu,"Liang Lu, Zhong Meng, Naoyuki Kanda, Jinyu Li, and Yifan Gong","On Minimum Word Error Rate Training of the Hybrid Autoregressive
  Transducer","5 pages, submitted to ICASSP 2021",,,,cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Hybrid Autoregressive Transducer (HAT) is a recently proposed end-to-end
acoustic model that extends the standard Recurrent Neural Network Transducer
(RNN-T) for the purpose of the external language model (LM) fusion. In HAT, the
blank probability and the label probability are estimated using two separate
probability distributions, which provides a more accurate solution for internal
LM score estimation, and thus works better when combining with an external LM.
Previous work mainly focuses on HAT model training with the negative
log-likelihood loss, while in this paper, we study the minimum word error rate
(MWER) training of HAT -- a criterion that is closer to the evaluation metric
for speech recognition, and has been successfully applied to other types of
end-to-end models such as sequence-to-sequence (S2S) and RNN-T models. From
experiments with around 30,000 hours of training data, we show that MWER
training can improve the accuracy of HAT models, while at the same time,
improving the robustness of the model against the decoding hyper-parameters
such as length normalization and decoding beam during inference.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 21:16:30 GMT'}]",2020-10-27,"[['Lu', 'Liang', ''], ['Meng', 'Zhong', ''], ['Kanda', 'Naoyuki', ''], ['Li', 'Jinyu', ''], ['Gong', 'Yifan', '']]"
1368988,2010.12658,Cheng Zhang,"Cheng Zhang, Yicheng Sun, Hejia Chen, Jie Wang",Generating Adequate Distractors for Multiple-Choice Questions,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a novel approach to automatic generation of adequate
distractors for a given question-answer pair (QAP) generated from a given
article to form an adequate multiple-choice question (MCQ). Our method is a
combination of part-of-speech tagging, named-entity tagging, semantic-role
labeling, regular expressions, domain knowledge bases, word embeddings, word
edit distance, WordNet, and other algorithms. We use the US SAT (Scholastic
Assessment Test) practice reading tests as a dataset to produce QAPs and
generate three distractors for each QAP to form an MCQ. We show that, via
experiments and evaluations by human judges, each MCQ has at least one adequate
distractor and 84\% of MCQs have three adequate distractors.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 20:47:58 GMT'}]",2020-10-27,"[['Zhang', 'Cheng', ''], ['Sun', 'Yicheng', ''], ['Chen', 'Hejia', ''], ['Wang', 'Jie', '']]"
1368982,2010.12652,Orhan Firat,"Mahdis Mahdieh, Mia Xu Chen, Yuan Cao, Orhan Firat",Rapid Domain Adaptation for Machine Translation with Monolingual Data,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One challenge of machine translation is how to quickly adapt to unseen
domains in face of surging events like COVID-19, in which case timely and
accurate translation of in-domain information into multiple languages is
critical but little parallel data is available yet. In this paper, we propose
an approach that enables rapid domain adaptation from the perspective of
unsupervised translation. Our proposed approach only requires in-domain
monolingual data and can be quickly applied to a preexisting translation system
trained on general domain, reaching significant gains on in-domain translation
quality with little or no drop on general-domain. We also propose an effective
procedure of simultaneous adaptation for multiple domains and languages. To the
best of our knowledge, this is the first attempt that aims to address
unsupervised multilingual domain adaptation.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 20:31:37 GMT'}]",2020-10-27,"[['Mahdieh', 'Mahdis', ''], ['Chen', 'Mia Xu', ''], ['Cao', 'Yuan', ''], ['Firat', 'Orhan', '']]"
1368973,2010.12643,Jacopo Staiano,"Arij Riabi, Thomas Scialom, Rachel Keraron, Beno\^it Sagot, Djam\'e
  Seddah, Jacopo Staiano","Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question
  Answering",7 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Coupled with the availability of large scale datasets, deep learning
architectures have enabled rapid progress on the Question Answering task.
However, most of those datasets are in English, and the performances of
state-of-the-art multilingual models are significantly lower when evaluated on
non-English data. Due to high data collection costs, it is not realistic to
obtain annotated data for each language one desires to support.
  We propose a method to improve the Cross-lingual Question Answering
performance without requiring additional annotated data, leveraging Question
Generation models to produce synthetic samples in a cross-lingual fashion. We
show that the proposed method allows to significantly outperform the baselines
trained on English data only. We report a new state-of-the-art on four
multilingual datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 20:09:01 GMT'}]",2020-10-27,"[['Riabi', 'Arij', ''], ['Scialom', 'Thomas', ''], ['Keraron', 'Rachel', ''], ['Sagot', 'Benoît', ''], ['Seddah', 'Djamé', ''], ['Staiano', 'Jacopo', '']]"
1368969,2010.12639,Jesse Thomason,"Shurjo Banerjee, Jesse Thomason, Jason J. Corso","The RobotSlang Benchmark: Dialog-guided Robot Localization and
  Navigation",Conference on Robot Learning 2020,,,,cs.RO cs.AI cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Autonomous robot systems for applications from search and rescue to assistive
guidance should be able to engage in natural language dialog with people. To
study such cooperative communication, we introduce Robot Simultaneous
Localization and Mapping with Natural Language (RobotSlang), a benchmark of 169
natural language dialogs between a human Driver controlling a robot and a human
Commander providing guidance towards navigation goals. In each trial, the pair
first cooperates to localize the robot on a global map visible to the
Commander, then the Driver follows Commander instructions to move the robot to
a sequence of target objects. We introduce a Localization from Dialog History
(LDH) and a Navigation from Dialog History (NDH) task where a learned agent is
given dialog and visual observations from the robot platform as input and must
localize in the global map or navigate towards the next target object,
respectively. RobotSlang is comprised of nearly 5k utterances and over 1k
minutes of robot camera and control streams. We present an initial model for
the NDH task, and show that an agent trained in simulation can follow the
RobotSlang dialog-based navigation instructions for controlling a physical
robot platform. Code and data are available at https://umrobotslang.github.io/.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 19:58:17 GMT'}]",2020-10-27,"[['Banerjee', 'Shurjo', ''], ['Thomason', 'Jesse', ''], ['Corso', 'Jason J.', '']]"
1368968,2010.12638,Hao Cheng,"Hao Cheng, Xiaodong Liu, Lis Pereira, Yaoliang Yu, Jianfeng Gao","Posterior Differential Regularization with f-divergence for Improving
  Model Robustness",,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We address the problem of enhancing model robustness through regularization.
Specifically, we focus on methods that regularize the model posterior
difference between clean and noisy inputs. Theoretically, we provide a
connection of two recent methods, Jacobian Regularization and Virtual
Adversarial Training, under this framework. Additionally, we generalize the
posterior differential regularization to the family of $f$-divergences and
characterize the overall regularization framework in terms of Jacobian matrix.
Empirically, we systematically compare those regularizations and standard BERT
training on a diverse set of tasks to provide a comprehensive profile of their
effect on model in-domain and out-of-domain generalization. For both fully
supervised and semi-supervised settings, our experiments show that regularizing
the posterior differential with $f$-divergence can result in well-improved
model robustness. In particular, with a proper $f$-divergence, a BERT-base
model can achieve comparable generalization as its BERT-large counterpart for
in-domain, adversarial and domain shift scenarios, indicating the great
potential of the proposed framework for boosting model generalization for NLP
models.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 19:58:01 GMT'}]",2020-10-27,"[['Cheng', 'Hao', ''], ['Liu', 'Xiaodong', ''], ['Pereira', 'Lis', ''], ['Yu', 'Yaoliang', ''], ['Gao', 'Jianfeng', '']]"
1368967,2010.12637,Dhivya Chandrasekaran,Dhivya Chandrasekaran and Vijay Mago,Domain Specific Complex Sentence (DCSC) Semantic Similarity Dataset,"12 pages, 4 figures, submitted to ""IEEE Transactions on Knowledge and
  Data Engineering",,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semantic textual similarity is one of the open research challenges in the
field of Natural Language Processing. Extensive research has been carried out
in this field and near-perfect results are achieved by recent transformed based
models in existing benchmark datasets like STS dataset and SICK dataset. In
this paper, we study the sentences in these datasets and analyze the
sensitivity of various word embeddings with respect to the complexity of the
sentences. We propose a new benchmark dataset -- the Domain Specific Complex
Sentences (DSCS) dataset comprising of 50 sentence pairs with associated
semantic similarity values provided by 15 human annotators. Readability
analysis is performed to highlight the increase in complexity of the sentences
in the existing benchmark datasets and those in the proposed dataset. Further,
we perform a comparative analysis of the performance of various word embeddings
and the results justify the hypothesis that the performance of the word
embeddings decrease with an increase in complexity of the sentences.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 19:55:11 GMT'}]",2020-10-27,"[['Chandrasekaran', 'Dhivya', ''], ['Mago', 'Vijay', '']]"
1368957,2010.12627,Tobias Eder,"Tobias Eder, Viktor Hangya, Alexander Fraser",Anchor-based Bilingual Word Embeddings for Low-Resource Languages,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Bilingual word embeddings (BWEs) are useful for many cross-lingual
applications, such as bilingual lexicon induction (BLI) and cross-lingual
transfer learning. While recent methods have led to good quality BWEs for
different language pairs using only weak bilingual signals, they still rely on
an abundance of monolingual training data in both languages for their
performance. This becomes a problem especially in the case of low resource
languages where neither parallel bilingual corpora nor large monolingual
training data are available. This paper proposes a new approach for building
BWEs in which the vector space of the high resource source language is used as
a starting point for training an embedding space for the low resource target
language. By using the source vectors as anchors the vector spaces are
automatically aligned. We evaluate the resulting BWEs on BLI and show the
proposed method outperforms previous approaches in the low-resource setting by
a large margin. We show strong results on the standard English-German test pair
(using German to simulate low resource). We also show we can build useful BWEs
for English-Hiligaynon, a true low-resource language, where previous approaches
failed.
","[{'version': 'v1', 'created': 'Fri, 23 Oct 2020 19:17:00 GMT'}]",2020-10-27,"[['Eder', 'Tobias', ''], ['Hangya', 'Viktor', ''], ['Fraser', 'Alexander', '']]"
1369270,2010.12940,Arun Kumar Singh,"Sushant Dave, Arun Kumar Singh, Dr. Prathosh A. P. and Prof. Brejesh
  Lall","Neural Compound-Word (Sandhi) Generation and Splitting in Sanskrit
  Language","6 pages, 3 figures, CODS-COMAD 2021, IIIT Bangalore, India",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes neural network based approaches to the process of the
formation and splitting of word-compounding, respectively known as the Sandhi
and Vichchhed, in Sanskrit language. Sandhi is an important idea essential to
morphological analysis of Sanskrit texts. Sandhi leads to word transformations
at word boundaries. The rules of Sandhi formation are well defined but complex,
sometimes optional and in some cases, require knowledge about the nature of the
words being compounded. Sandhi split or Vichchhed is an even more difficult
task given its non uniqueness and context dependence. In this work, we propose
the route of formulating the problem as a sequence to sequence prediction task,
using modern deep learning techniques. Being the first fully data driven
technique, we demonstrate that our model has an accuracy better than the
existing methods on multiple standard datasets, despite not using any
additional lexical or morphological resources. The code is being made available
at https://github.com/IITD-DataScience/Sandhi_Prakarana
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 18:02:40 GMT'}]",2020-10-27,"[['Dave', 'Sushant', ''], ['Singh', 'Arun Kumar', ''], ['P.', 'Dr. Prathosh A.', ''], ['Lall', 'Prof. Brejesh', '']]"
1279698,2004.14928,Christos Baziotis,"Christos Baziotis, Barry Haddow, Alexandra Birch",Language Model Prior for Low-Resource Neural Machine Translation,Accepted at EMNLP 2020. Camera-ready version,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The scarcity of large parallel corpora is an important obstacle for neural
machine translation. A common solution is to exploit the knowledge of language
models (LM) trained on abundant monolingual data. In this work, we propose a
novel approach to incorporate a LM as prior in a neural translation model (TM).
Specifically, we add a regularization term, which pushes the output
distributions of the TM to be probable under the LM prior, while avoiding wrong
predictions when the TM ""disagrees"" with the LM. This objective relates to
knowledge distillation, where the LM can be viewed as teaching the TM about the
target language. The proposed approach does not compromise decoding speed,
because the LM is used only at training time, unlike previous work that
requires it during inference. We present an analysis of the effects that
different methods have on the distributions of the TM. Results on two
low-resource machine translation datasets show clear improvements even with
limited monolingual data.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 16:29:56 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Oct 2020 21:39:55 GMT'}, {'version': 'v3', 'created': 'Mon, 26 Oct 2020 08:56:46 GMT'}]",2020-10-27,"[['Baziotis', 'Christos', ''], ['Haddow', 'Barry', ''], ['Birch', 'Alexandra', '']]"
1369971,2010.13641,Timo Schick,"Timo Schick, Helmut Schmid, Hinrich Sch\""utze","Automatically Identifying Words That Can Serve as Labels for Few-Shot
  Text Classification",To appear at COLING 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A recent approach for few-shot text classification is to convert textual
inputs to cloze questions that contain some form of task description, process
them with a pretrained language model and map the predicted words to labels.
Manually defining this mapping between words and labels requires both domain
expertise and an understanding of the language model's abilities. To mitigate
this issue, we devise an approach that automatically finds such a mapping given
small amounts of training data. For a number of tasks, the mapping found by our
approach performs almost as well as hand-crafted label-to-word mappings.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 14:56:22 GMT'}]",2020-10-27,"[['Schick', 'Timo', ''], ['Schmid', 'Helmut', ''], ['Schütze', 'Hinrich', '']]"
1369734,2010.13404,Rifat Rahman,Rifat Rahman,"Robust and Consistent Estimation of Word Embedding for Bangla Language
  by fine-tuning Word2Vec Model","6 pages, 8 figures, submitted to 23rd International Conference of
  Computer and Information Technology (ICCIT). IEEE, 2020",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Word embedding or vector representation of word holds syntactical and
semantic characteristics of word which can be an informative feature for any
machine learning based models of natural language processing. There are several
deep learning based models for the vectorization of words like word2vec,
fasttext, gensim, glove etc. In this study, we analysis word2vec model for
learning word vectors by tuning different hyper-parameters and present the most
effective word embedding for Bangla language. For testing the performances of
different word embeddings induced by fine-tuning of word2vec model, we perform
both intrinsic and extrinsic evaluations. We cluster the word vectors to
examine the relational similarity of words and also use different word
embeddings as the feature of news article classifier for extrinsic evaluation.
From our experiment, we discover that the word vectors with 300 dimension,
generated from 'skip-gram' method of word2vec model using the sliding window
size of 4, are giving the most robust vector representations for Bangla
language.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 08:00:48 GMT'}]",2020-10-27,"[['Rahman', 'Rifat', '']]"
1369745,2010.13415,Yucheng Wang,"Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu and
  Limin Sun","TPLinker: Single-stage Joint Extraction of Entities and Relations
  Through Token Pair Linking",COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Extracting entities and relations from unstructured text has attracted
increasing attention in recent years but remains challenging, due to the
intrinsic difficulty in identifying overlapping relations with shared entities.
Prior works show that joint learning can result in a noticeable performance
gain. However, they usually involve sequential interrelated steps and suffer
from the problem of exposure bias. At training time, they predict with the
ground truth conditions while at inference it has to make extraction from
scratch. This discrepancy leads to error accumulation. To mitigate the issue,
we propose in this paper a one-stage joint extraction model, namely, TPLinker,
which is capable of discovering overlapping relations sharing one or both
entities while immune from the exposure bias. TPLinker formulates joint
extraction as a token pair linking problem and introduces a novel handshaking
tagging scheme that aligns the boundary tokens of entity pairs under each
relation type. Experiment results show that TPLinker performs significantly
better on overlapping and multiple relation extraction, and achieves
state-of-the-art performance on two public datasets.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 08:35:06 GMT'}]",2020-10-27,"[['Wang', 'Yucheng', ''], ['Yu', 'Bowen', ''], ['Zhang', 'Yueyang', ''], ['Liu', 'Tingwen', ''], ['Zhu', 'Hongsong', ''], ['Sun', 'Limin', '']]"
1281590,2005.01795,Kundan Krishna,"Kundan Krishna, Sopan Khosla, Jeffrey P. Bigham, Zachary C. Lipton",Generating SOAP Notes from Doctor-Patient Conversations,,,,,cs.CL cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Following each patient visit, physicians must draft a detailed clinical
summary called a SOAP note. Moreover, with electronic health records, these
notes must be digitized. Despite the benefits of this documentation, their
creation remains an onerous process, contributing to increasing physician
burnout. In this paper, we present the first study to evaluate complete
pipelines to train summarization models to generate these notes from
conversations between physicians and patients. We benefit from a dataset that,
along with transcripts and paired SOAP notes, consists of annotations marking
noteworthy utterances that support each summary sentence. We decompose the
problem into extractive and abstractive subtasks, exploring a spectrum of
approaches according to how much they demand from each component. We observe
that the performance improves constantly as the extractive subtask is made more
complex - an observation that we also replicate on the well-known AMI meeting
summarization dataset. Our best performing method first (i) extracts noteworthy
utterances via multi-label classification, assigning each to summary
section(s); (ii) clusters noteworthy utterances on a per-section basis; and
(iii) generates the summary sentences by conditioning on the corresponding
cluster and the subsection of the SOAP sentence to be generated. Compared to an
end-to-end approach that generates the full SOAP note from the full
conversation, our approach improves by around 8 ROUGE-1 points.
","[{'version': 'v1', 'created': 'Mon, 4 May 2020 19:10:26 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 04:09:10 GMT'}]",2020-10-27,"[['Krishna', 'Kundan', ''], ['Khosla', 'Sopan', ''], ['Bigham', 'Jeffrey P.', ''], ['Lipton', 'Zachary C.', '']]"
1369845,2010.13515,Andrea Asperti,Andrea Asperti and Stefano Dal Bianco,Syllabification of the Divine Comedy,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We provide a syllabification algorithm for the Divine Comedy using techniques
from probabilistic and constraint programming. We particularly focus on the
synalephe, addressed in terms of the ""propensity"" of a word to take part in a
synalephe with adjacent words. We jointly provide an online vocabulary
containing, for each word, information about its syllabification, the location
of the tonic accent, and the aforementioned synalephe propensity, on the left
and right sides. The algorithm is intrinsically nondeterministic, producing
different possible syllabifications for each verse, with different likelihoods;
metric constraints relative to accents on the 10th, 4th and 6th syllables are
used to further reduce the solution space. The most likely syllabification is
hence returned as output. We believe that this work could be a major milestone
for a lot of different investigations. From the point of view of digital
humanities it opens new perspectives on computer assisted analysis of digital
sources, comprising automated detection of anomalous and problematic cases,
metric clustering of verses and their categorization, or more foundational
investigations addressing e.g. the phonetic roles of consonants and vowels.
From the point of view of text processing and deep learning, information about
syllabification and the location of accents opens a wide range of exciting
perspectives, from the possibility of automatic learning syllabification of
words and verses, to the improvement of generative models, aware of metric
issues, and more respectful of the expected musicality.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 12:14:14 GMT'}]",2020-10-27,"[['Asperti', 'Andrea', ''], ['Bianco', 'Stefano Dal', '']]"
1369874,2010.13544,Zhenzhen Li,"Zhenzhen Li, Jian-Yun Nie, Benyou Wang, Pan Du, Yuhan Zhang, Lixin
  Zou, and Dongsheng Li","Meta-Learning for Neural Relation Classification with Distant
  Supervision","10 pages, 7 figures; corrected one encoding error in CIKM pdf","In Proceedings of CIKM, pp. 815-824. 2020",10.1145/3340531.3412039,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Distant supervision provides a means to create a large number of weakly
labeled data at low cost for relation classification. However, the resulting
labeled instances are very noisy, containing data with wrong labels. Many
approaches have been proposed to select a subset of reliable instances for
neural model training, but they still suffer from noisy labeling problem or
underutilization of the weakly-labeled data. To better select more reliable
training instances, we introduce a small amount of manually labeled data as
reference to guide the selection process. In this paper, we propose a
meta-learning based approach, which learns to reweight noisy training data
under the guidance of reference data. As the clean reference data is usually
very small, we propose to augment it by dynamically distilling the most
reliable elite instances from the noisy data. Experiments on several datasets
demonstrate that the reference data can effectively guide the selection of
training data, and our augmented approach consistently improves the performance
of relation classification comparing to the existing state-of-the-art methods.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 12:52:28 GMT'}]",2020-10-27,"[['Li', 'Zhenzhen', ''], ['Nie', 'Jian-Yun', ''], ['Wang', 'Benyou', ''], ['Du', 'Pan', ''], ['Zhang', 'Yuhan', ''], ['Zou', 'Lixin', ''], ['Li', 'Dongsheng', '']]"
1369886,2010.13556,Yu Zhang,"Yu Zhang, Xiusi Chen, Yu Meng, Jiawei Han","Hierarchical Metadata-Aware Document Categorization under Weak
  Supervision",9 pages; Accepted to WSDM 2021,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Categorizing documents into a given label hierarchy is intuitively appealing
due to the ubiquity of hierarchical topic structures in massive text corpora.
Although related studies have achieved satisfying performance in fully
supervised hierarchical document classification, they usually require massive
human-annotated training data and only utilize text information. However, in
many domains, (1) annotations are quite expensive where very few training
samples can be acquired; (2) documents are accompanied by metadata information.
Hence, this paper studies how to integrate the label hierarchy, metadata, and
text signals for document categorization under weak supervision. We develop
HiMeCat, an embedding-based generative framework for our task. Specifically, we
propose a novel joint representation learning module that allows simultaneous
modeling of category dependencies, metadata information and textual semantics,
and we introduce a data augmentation module that hierarchically synthesizes
training documents to complement the original, small-scale training set. Our
experiments demonstrate a consistent improvement of HiMeCat over competitive
baselines and validate the contribution of our representation learning and data
augmentation modules.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 13:07:56 GMT'}]",2020-10-27,"[['Zhang', 'Yu', ''], ['Chen', 'Xiusi', ''], ['Meng', 'Yu', ''], ['Han', 'Jiawei', '']]"
1369915,2010.13585,Reza Marzban,"Reza Marzban, Christopher John Crick",Interpreting convolutional networks trained on textual data,"9 pages, 6 figures, 5 tables",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There have been many advances in the artificial intelligence field due to the
emergence of deep learning. In almost all sub-fields, artificial neural
networks have reached or exceeded human-level performance. However, most of the
models are not interpretable. As a result, it is hard to trust their decisions,
especially in life and death scenarios. In recent years, there has been a
movement toward creating explainable artificial intelligence, but most work to
date has concentrated on image processing models, as it is easier for humans to
perceive visual patterns. There has been little work in other fields like
natural language processing. In this paper, we train a convolutional model on
textual data and analyze the global logic of the model by studying its filter
values. In the end, we find the most important words in our corpus to our
models logic and remove the rest (95%). New models trained on just the 5% most
important words can achieve the same performance as the original model while
reducing training time by more than half. Approaches such as this will help us
to understand NLP models, explain their decisions according to their word
choices, and improve them by finding blind spots and biases.
","[{'version': 'v1', 'created': 'Tue, 20 Oct 2020 20:12:05 GMT'}]",2020-10-27,"[['Marzban', 'Reza', ''], ['Crick', 'Christopher John', '']]"
1369918,2010.13588,Ozan Caglayan,"Ozan Caglayan, Pranava Madhyastha, Lucia Specia","Curious Case of Language Generation Evaluation Metrics: A Cautionary
  Tale","7 pages, accepted to COLING 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic evaluation of language generation systems is a well-studied problem
in Natural Language Processing. While novel metrics are proposed every year, a
few popular metrics remain as the de facto metrics to evaluate tasks such as
image captioning and machine translation, despite their known limitations. This
is partly due to ease of use, and partly because researchers expect to see them
and know how to interpret them. In this paper, we urge the community for more
careful consideration of how they automatically evaluate their models by
demonstrating important failure cases on multiple datasets, language pairs and
tasks. Our experiments show that metrics (i) usually prefer system outputs to
human-authored texts, (ii) can be insensitive to correct translations of rare
words, (iii) can yield surprisingly high scores when given a single sentence as
system output for the entire test set.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 13:57:20 GMT'}]",2020-10-27,"[['Caglayan', 'Ozan', ''], ['Madhyastha', 'Pranava', ''], ['Specia', 'Lucia', '']]"
1305141,2006.10627,Qian Liu,"Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao,
  Bin Zhou, Nanning Zheng, Dongmei Zhang",Compositional Generalization by Learning Analytical Expressions,To appear in NeurIPS 2020 (Spotlight),,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Compositional generalization is a basic and essential intellective capability
of human beings, which allows us to recombine known parts readily. However,
existing neural network based models have been proven to be extremely deficient
in such a capability. Inspired by work in cognition which argues
compositionality can be captured by variable slots with symbolic functions, we
present a refreshing view that connects a memory-augmented neural model with
analytical expressions, to achieve compositional generalization. Our model
consists of two cooperative neural modules, Composer and Solver, fitting well
with the cognitive argument while being able to be trained in an end-to-end
manner via a hierarchical reinforcement learning algorithm. Experiments on the
well-known benchmark SCAN demonstrate that our model seizes a great ability of
compositional generalization, solving all challenges addressed by previous
works with 100% accuracies.
","[{'version': 'v1', 'created': 'Thu, 18 Jun 2020 15:50:57 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 03:47:49 GMT'}]",2020-10-27,"[['Liu', 'Qian', ''], ['An', 'Shengnan', ''], ['Lou', 'Jian-Guang', ''], ['Chen', 'Bei', ''], ['Lin', 'Zeqi', ''], ['Gao', 'Yan', ''], ['Zhou', 'Bin', ''], ['Zheng', 'Nanning', ''], ['Zhang', 'Dongmei', '']]"
1306638,2006.12124,Anne Wu,"Anne Wu, Changhan Wang, Juan Pino, Jiatao Gu",Self-Supervised Representations Improve End-to-End Speech Translation,Accepted to INTERSPEECH 2020,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end speech-to-text translation can provide a simpler and smaller
system but is facing the challenge of data scarcity. Pre-training methods can
leverage unlabeled data and have been shown to be effective on data-scarce
settings. In this work, we explore whether self-supervised pre-trained speech
representations can benefit the speech translation task in both high- and
low-resource settings, whether they can transfer well to other languages, and
whether they can be effectively combined with other common methods that help
improve low-resource end-to-end speech translation such as using a pre-trained
high-resource speech recognition system. We demonstrate that self-supervised
pre-trained features can consistently improve the translation performance, and
cross-lingual transfer allows to extend to a variety of languages without or
with little tuning.
","[{'version': 'v1', 'created': 'Mon, 22 Jun 2020 10:28:38 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Oct 2020 03:31:15 GMT'}]",2020-10-27,"[['Wu', 'Anne', ''], ['Wang', 'Changhan', ''], ['Pino', 'Juan', ''], ['Gu', 'Jiatao', '']]"
1369967,2010.13637,Peng Gao,"Peng Gao, Fei Shao, Xiaoyuan Liu, Xusheng Xiao, Zheng Qin, Fengyuan
  Xu, Prateek Mittal, Sanjeev R. Kulkarni, Dawn Song",Enabling Efficient Cyber Threat Hunting With Cyber Threat Intelligence,,,,,cs.CR cs.CL cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Log-based cyber threat hunting has emerged as an important solution to
counter sophisticated cyber attacks. However, existing approaches require
non-trivial efforts of manual query construction and have overlooked the rich
external knowledge about threat behaviors provided by open-source Cyber Threat
Intelligence (OSCTI). To bridge the gap, we propose EffHunter, a system that
facilitates cyber threat hunting in computer systems using OSCTI. Built upon
mature system auditing frameworks, EffHunter provides (1) an unsupervised,
light-weight, and accurate NLP pipeline that extracts structured threat
behaviors from unstructured OSCTI text, (2) a concise and expressive
domain-specific query language, TBQL, to hunt for malicious system activities,
(3) a query synthesis mechanism that automatically synthesizes a TBQL query for
threat hunting from the extracted threat behaviors, and (4) an efficient query
execution engine to search the big audit logging data. Evaluations on a broad
set of attack cases demonstrate the accuracy and efficiency of EffHunter in
enabling practical threat hunting.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 14:54:01 GMT'}]",2020-10-27,"[['Gao', 'Peng', ''], ['Shao', 'Fei', ''], ['Liu', 'Xiaoyuan', ''], ['Xiao', 'Xusheng', ''], ['Qin', 'Zheng', ''], ['Xu', 'Fengyuan', ''], ['Mittal', 'Prateek', ''], ['Kulkarni', 'Sanjeev R.', ''], ['Song', 'Dawn', '']]"
1369988,2010.13658,Baosong Yang,"Tianchi Bi and Liang Yao and Baosong Yang and Haibo Zhang and Weihua
  Luo and Boxing Chen","Constraint Translation Candidates: A Bridge between Neural Query
  Translation and Cross-lingual Information Retrieval",SIGIR eCom 2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Query translation (QT) is a key component in cross-lingual information
retrieval system (CLIR). With the help of deep learning, neural machine
translation (NMT) has shown promising results on various tasks. However, NMT is
generally trained with large-scale out-of-domain data rather than in-domain
query translation pairs. Besides, the translation model lacks a mechanism at
the inference time to guarantee the generated words to match the search index.
The two shortages of QT result in readable texts for human but inadequate
candidates for the downstream retrieval task. In this paper, we propose a novel
approach to alleviate these problems by limiting the open target vocabulary
search space of QT to a set of important words mined from search index
database. The constraint translation candidates are employed at both of
training and inference time, thus guiding the translation model to learn and
generate well performing target queries. The proposed methods are exploited and
examined in a real-word CLIR system--Aliexpress e-Commerce search engine.
Experimental results demonstrate that our approach yields better performance on
both translation quality and retrieval accuracy than the strong NMT baseline.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 15:27:51 GMT'}]",2020-10-27,"[['Bi', 'Tianchi', ''], ['Yao', 'Liang', ''], ['Yang', 'Baosong', ''], ['Zhang', 'Haibo', ''], ['Luo', 'Weihua', ''], ['Chen', 'Boxing', '']]"
1369989,2010.13659,Baosong Yang,"Liang Yao and Baosong Yang and Haibo Zhang and Weihua Luo and Boxing
  Chen","Exploiting Neural Query Translation into Cross Lingual Information
  Retrieval",SIGIR eCom 2020,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As a crucial role in cross-language information retrieval (CLIR), query
translation has three main challenges: 1) the adequacy of translation; 2) the
lack of in-domain parallel training data; and 3) the requisite of low latency.
To this end, existing CLIR systems mainly exploit statistical-based machine
translation (SMT) rather than the advanced neural machine translation (NMT),
limiting the further improvements on both translation and retrieval quality. In
this paper, we investigate how to exploit neural query translation model into
CLIR system. Specifically, we propose a novel data augmentation method that
extracts query translation pairs according to user clickthrough data, thus to
alleviate the problem of domain-adaptation in NMT. Then, we introduce an
asynchronous strategy which is able to leverage the advantages of the real-time
in SMT and the veracity in NMT. Experimental results reveal that the proposed
approach yields better retrieval quality than strong baselines and can be well
applied into a real-world CLIR system, i.e. Aliexpress e-Commerce search
engine. Readers can examine and test their cases on our website:
https://aliexpress.com .
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 15:28:19 GMT'}]",2020-10-27,"[['Yao', 'Liang', ''], ['Yang', 'Baosong', ''], ['Zhang', 'Haibo', ''], ['Luo', 'Weihua', ''], ['Chen', 'Boxing', '']]"
1370004,2010.13674,Christina Niklaus,"Thiemo Wambsganss, Christina Niklaus, Matthias S\""ollner, Siegfried
  Handschuh, Jan Marco Leimeister",A Corpus for Argumentative Writing Support in German,"to be published in The 28th International Conference on Computational
  Linguistics (COLING 2020)",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we present a novel annotation approach to capture claims and
premises of arguments and their relations in student-written persuasive peer
reviews on business models in German language. We propose an annotation scheme
based on annotation guidelines that allows to model claims and premises as well
as support and attack relations for capturing the structure of argumentative
discourse in student-written peer reviews. We conduct an annotation study with
three annotators on 50 persuasive essays to evaluate our annotation scheme. The
obtained inter-rater agreement of $\alpha=0.57$ for argument components and
$\alpha=0.49$ for argumentative relations indicates that the proposed
annotation scheme successfully guides annotators to moderate agreement.
Finally, we present our freely available corpus of 1,000 persuasive
student-written peer reviews on business models and our annotation guidelines
to encourage future research on the design and development of argumentative
writing support systems for students.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 15:52:12 GMT'}]",2020-10-27,"[['Wambsganss', 'Thiemo', ''], ['Niklaus', 'Christina', ''], ['Söllner', 'Matthias', ''], ['Handschuh', 'Siegfried', ''], ['Leimeister', 'Jan Marco', '']]"
1370018,2010.13688,Alexander Kalinowski,"Alexander Kalinowski, Yuan An","A Survey of Embedding Space Alignment Methods for Language and Knowledge
  Graphs","27 pages, 2 figures",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural embedding approaches have become a staple in the fields of computer
vision, natural language processing, and more recently, graph analytics. Given
the pervasive nature of these algorithms, the natural question becomes how to
exploit the embedding spaces to map, or align, embeddings of different data
sources. To this end, we survey the current research landscape on word,
sentence and knowledge graph embedding algorithms. We provide a classification
of the relevant alignment techniques and discuss benchmark datasets used in
this field of research. By gathering these diverse approaches into a singular
survey, we hope to further motivate research into alignment of embedding spaces
of varied data types and sources.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 16:08:13 GMT'}]",2020-10-27,"[['Kalinowski', 'Alexander', ''], ['An', 'Yuan', '']]"
1279583,2004.14813,Liying Cheng,"Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu,
  Luo Si",ENT-DESC: Entity Description Generation by Exploring Knowledge Graph,"11 pages, 6 figures, accepted by EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous works on knowledge-to-text generation take as input a few RDF
triples or key-value pairs conveying the knowledge of some entities to generate
a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and
E2E, basically have a good alignment between an input triple/pair set and its
output text. However, in practice, the input knowledge could be more than
enough, since the output description may only cover the most significant
knowledge. In this paper, we introduce a large-scale and challenging dataset to
facilitate the study of such a practical scenario in KG-to-text. Our dataset
involves retrieving abundant knowledge of various types of main entities from a
large knowledge graph (KG), which makes the current graph-to-sequence models
severely suffer from the problems of information loss and parameter explosion
while generating the descriptions. We address these challenges by proposing a
multi-graph structure that is able to represent the original graph information
more comprehensively. Furthermore, we also incorporate aggregation methods that
learn to extract the rich graph information. Extensive experiments demonstrate
the effectiveness of our model architecture.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 14:16:19 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 07:33:32 GMT'}]",2020-10-27,"[['Cheng', 'Liying', ''], ['Wu', 'Dekun', ''], ['Bing', 'Lidong', ''], ['Zhang', 'Yan', ''], ['Jie', 'Zhanming', ''], ['Lu', 'Wei', ''], ['Si', 'Luo', '']]"
1295328,2006.00814,Maha Elbayad,"Maha Elbayad, Michael Ustaszewski, Emmanuelle Esperan\c{c}a-Rodier,
  Francis Brunet Manquat, Laurent Besacier","Online Versus Offline NMT Quality: An In-depth Analysis on
  English-German and German-English",Accepted at COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We conduct in this work an evaluation study comparing offline and online
neural machine translation architectures. Two sequence-to-sequence models:
convolutional Pervasive Attention (Elbayad et al. 2018) and attention-based
Transformer (Vaswani et al. 2017) are considered. We investigate, for both
architectures, the impact of online decoding constraints on the translation
quality through a carefully designed human evaluation on English-German and
German-English language pairs, the latter being particularly sensitive to
latency constraints. The evaluation results allow us to identify the strengths
and shortcomings of each model when we shift to the online setup.
","[{'version': 'v1', 'created': 'Mon, 1 Jun 2020 09:43:54 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 13:36:00 GMT'}]",2020-10-27,"[['Elbayad', 'Maha', ''], ['Ustaszewski', 'Michael', ''], ['Esperança-Rodier', 'Emmanuelle', ''], ['Manquat', 'Francis Brunet', ''], ['Besacier', 'Laurent', '']]"
1369303,2010.12973,Andros Tjandra,"Andros Tjandra, Ruoming Pang, Yu Zhang, Shigeki Karita","Unsupervised Learning of Disentangled Speech Content and Style
  Representation",Submitted to ICASSP 2021,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present an approach for unsupervised learning of speech representation
disentangling contents and styles. Our model consists of: (1) a local encoder
that captures per-frame information; (2) a global encoder that captures
per-utterance information; and (3) a conditional decoder that reconstructs
speech given local and global latent variables. Our experiments show that (1)
the local latent variables encode speech contents, as reconstructed speech can
be recognized by ASR with low word error rates (WER), even with a different
global encoding; (2) the global latent variables encode speaker style, as
reconstructed speech shares speaker identity with the source utterance of the
global encoding. Additionally, we demonstrate an useful application from our
pre-trained model, where we can train a speaker recognition model from the
global latent variables and achieve high accuracy by fine-tuning with as few
data as one label per speaker.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 20:16:03 GMT'}]",2020-10-27,"[['Tjandra', 'Andros', ''], ['Pang', 'Ruoming', ''], ['Zhang', 'Yu', ''], ['Karita', 'Shigeki', '']]"
1279693,2004.14923,Arturo Oncevay,"Arturo Oncevay, Barry Haddow, Alexandra Birch","Bridging Linguistic Typology and Multilingual Machine Translation with
  Multi-View Language Representations",Accepted at EMNLP 2020. Camera-ready version,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sparse language vectors from linguistic typology databases and learned
embeddings from tasks like multilingual machine translation have been
investigated in isolation, without analysing how they could benefit from each
other's language characterisation. We propose to fuse both views using singular
vector canonical correlation analysis and study what kind of information is
induced from each source. By inferring typological features and language
phylogenies, we observe that our representations embed typology and strengthen
correlations with language relationships. We then take advantage of our
multi-view language vector space for multilingual machine translation, where we
achieve competitive overall translation accuracy in tasks that require
information about language similarities, such as language clustering and
ranking candidates for multilingual transfer. With our method, which is also
released as a tool, we can easily project and assess new languages without
expensive retraining of massive multilingual or ranking models, which are major
disadvantages of related approaches.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 16:25:39 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Oct 2020 20:51:46 GMT'}]",2020-10-27,"[['Oncevay', 'Arturo', ''], ['Haddow', 'Barry', ''], ['Birch', 'Alexandra', '']]"
1286997,2005.07202,Shoya Wada,"Shoya Wada, Toshihiro Takeda, Shiro Manabe, Shozo Konishi, Jun
  Kamohara, and Yasushi Matsumura","A pre-training technique to localize medical BERT and to enhance
  biomedical BERT","We made the pre-trained weights of ouBioBERT and the source code for
  fine-tuning freely available at
  https://github.com/sy-wada/blue_benchmark_with_transformers",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Bidirectional Encoder Representations from Transformers (BERT) models for
medical specialties, such as BioBERT and clinicalBERT, have significantly
improved in performing biomedical text mining tasks and have enabled extracting
valuable information from biomedical literature; however, only English speakers
benefit due to the significant scarcity of high-quality medical documents, such
as PubMed, in each language. Therefore, we propose a method to train a
high-performance BERT model using a small corpus. We introduce the method to
train a BERT model on a small medical corpus both in English and in Japanese,
and we present the evaluation of each model in terms of the biomedical language
understanding evaluation (BLUE) benchmark and the medical document
classification task in Japanese, respectively. After confirming their
satisfactory performances, we applied our method to develop a model comparable
to the publicly available models. OuBioBERT, short for Bidirectional Encoder
Representations from Transformers for Biomedical Text Mining by Osaka
University, achieved the best score in terms of the BLUE benchmark. The total
score is 1.1 points above that of BioBERT and 0.3 points above that of the
ablated model trained without our proposed method. This proposed technique is
an effective approach to develop localized medical BERT models and to enhance
domain-specific models in the biomedical domain.
","[{'version': 'v1', 'created': 'Thu, 14 May 2020 18:00:01 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Oct 2020 04:22:24 GMT'}]",2020-10-27,"[['Wada', 'Shoya', ''], ['Takeda', 'Toshihiro', ''], ['Manabe', 'Shiro', ''], ['Konishi', 'Shozo', ''], ['Kamohara', 'Jun', ''], ['Matsumura', 'Yasushi', '']]"
1369982,2010.13652,Thomas Winters,"Thomas Winters, Pieter Delobelle",Dutch Humor Detection by Generating Negative Examples,"Accepted at the Proceedings of the 32st Benelux Conference on
  Artificial Intelligence (BNAIC 2020) and the 29th Belgian Dutch Conference on
  Machine Learning (Benelearn 2020)",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Detecting if a text is humorous is a hard task to do computationally, as it
usually requires linguistic and common sense insights. In machine learning,
humor detection is usually modeled as a binary classification task, trained to
predict if the given text is a joke or another type of text. Rather than using
completely different non-humorous texts, we propose using text generation
algorithms for imitating the original joke dataset to increase the difficulty
for the learning algorithm. We constructed several different joke and non-joke
datasets to test the humor detection abilities of different language
technologies. In particular, we compare the humor detection capabilities of
classic neural network approaches with the state-of-the-art Dutch language
model RobBERT. In doing so, we create and compare the first Dutch humor
detection systems. We found that while other language models perform well when
the non-jokes came from completely different domains, RobBERT was the only one
that was able to distinguish jokes from generated negative examples. This
performance illustrates the usefulness of using text generation to create
negative datasets for humor recognition, and also shows that transformer models
are a large step forward in humor detection.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 15:15:10 GMT'}]",2020-10-27,"[['Winters', 'Thomas', ''], ['Delobelle', 'Pieter', '']]"
1369721,2010.13391,Amir Pouran Ben Veyseh,"Amir Pouran Ben Veyseh, Tuan Ngo Nguyen, Thien Huu Nguyen","Graph Transformer Networks with Syntactic and Semantic Structures for
  Event Argument Extraction",accepted at EMNLP 2020 findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The goal of Event Argument Extraction (EAE) is to find the role of each
entity mention for a given event trigger word. It has been shown in the
previous works that the syntactic structures of the sentences are helpful for
the deep learning models for EAE. However, a major problem in such prior works
is that they fail to exploit the semantic structures of the sentences to induce
effective representations for EAE. Consequently, in this work, we propose a
novel model for EAE that exploits both syntactic and semantic structures of the
sentences with the Graph Transformer Networks (GTNs) to learn more effective
sentence structures for EAE. In addition, we introduce a novel inductive bias
based on information bottleneck to improve generalization of the EAE models.
Extensive experiments are performed to demonstrate the benefits of the proposed
model, leading to state-of-the-art performance for EAE on standard datasets.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 07:41:40 GMT'}]",2020-10-27,"[['Veyseh', 'Amir Pouran Ben', ''], ['Nguyen', 'Tuan Ngo', ''], ['Nguyen', 'Thien Huu', '']]"
1369787,2010.13457,Henry Turner,"Henry Turner, Giulio Lovisotto and Ivan Martinovic","Speaker Anonymization with Distribution-Preserving X-Vector Generation
  for the VoicePrivacy Challenge 2020",,,,,cs.SD cs.CL cs.CR eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we present a Distribution-Preserving Voice Anonymization
technique, as our submission to the VoicePrivacy Challenge 2020. We notice that
the challenge baseline system generates fake X-vectors which are very similar
to each other, significantly more so than those extracted from organic
speakers. This difference arises from averaging many X-vectors from a pool of
speakers in the anonymization processs, causing a loss of information. We
propose a new method to generate fake X-vectors which overcomes these
limitations by preserving the distributional properties of X-vectors and their
intra-similarity. We use population data to learn the properties of the
X-vector space, before fitting a generative model which we use to sample fake
X-vectors. We show how this approach generates X-vectors that more closely
follow the expected intra-similarity distribution of organic speaker X-vectors.
Our method can be easily integrated with others as the anonymization component
of the system and removes the need to distribute a pool of speakers to use
during the anonymization. Our approach leads to an increase in EER of up to
16.8\% in males and 8.4\% in females in scenarios where enrollment and trial
utterances are anonymized versus the baseline solution, demonstrating the
diversity of our generated voices.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 09:53:56 GMT'}]",2020-10-27,"[['Turner', 'Henry', ''], ['Lovisotto', 'Giulio', ''], ['Martinovic', 'Ivan', '']]"
1369712,2010.13382,Young Jin Kim,Young Jin Kim and Hany Hassan Awadalla,"FastFormers: Highly Efficient Transformer Models for Natural Language
  Understanding",Accepted to SustaiNLP 2020 at EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer-based models are the state-of-the-art for Natural Language
Understanding (NLU) applications. Models are getting bigger and better on
various tasks. However, Transformer models remain computationally challenging
since they are not efficient at inference-time compared to traditional
approaches. In this paper, we present FastFormers, a set of recipes to achieve
efficient inference-time performance for Transformer-based models on various
NLU tasks. We show how carefully utilizing knowledge distillation, structured
pruning and numerical optimization can lead to drastic improvements on
inference efficiency. We provide effective recipes that can guide practitioners
to choose the best settings for various NLU tasks and pretrained models.
Applying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x
up to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also
achieve up to 12.4x speed-up with the presented methods. We show that
FastFormers can drastically reduce cost of serving 100 million requests from
4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a
sustainable runtime by reducing energy consumption 6.9x - 125.8x according to
the metrics used in the SustaiNLP 2020 shared task.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 07:25:15 GMT'}]",2020-10-27,"[['Kim', 'Young Jin', ''], ['Awadalla', 'Hany Hassan', '']]"
1369339,2010.13009,Jianguo Zhang,"Jian-Guo Zhang, Kazuma Hashimoto, Wenhao Liu, Chien-Sheng Wu, Yao Wan,
  Philip S. Yu, Richard Socher, Caiming Xiong","Discriminative Nearest Neighbor Few-Shot Intent Detection by
  Transferring Natural Language Inference","19 pages, accepted by EMNLP 2020 main conference as a long paper.
  Code will be available at https://github.com/salesforce/DNNC-few-shot-intent",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Intent detection is one of the core components of goal-oriented dialog
systems, and detecting out-of-scope (OOS) intents is also a practically
important skill. Few-shot learning is attracting much attention to mitigate
data scarcity, but OOS detection becomes even more challenging. In this paper,
we present a simple yet effective approach, discriminative nearest neighbor
classification with deep self-attention. Unlike softmax classifiers, we
leverage BERT-style pairwise encoding to train a binary classifier that
estimates the best matched training example for a user input. We propose to
boost the discriminative ability by transferring a natural language inference
(NLI) model. Our extensive experiments on a large-scale multi-domain intent
detection task show that our method achieves more stable and accurate in-domain
and OOS detection accuracy than RoBERTa-based classifiers and embedding-based
nearest neighbor approaches. More notably, the NLI transfer enables our 10-shot
model to perform competitively with 50-shot or even full-shot classifiers,
while we can keep the inference time constant by leveraging a faster embedding
retrieval model.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 00:39:32 GMT'}]",2020-10-27,"[['Zhang', 'Jian-Guo', ''], ['Hashimoto', 'Kazuma', ''], ['Liu', 'Wenhao', ''], ['Wu', 'Chien-Sheng', ''], ['Wan', 'Yao', ''], ['Yu', 'Philip S.', ''], ['Socher', 'Richard', ''], ['Xiong', 'Caiming', '']]"
1369358,2010.13028,Sayyed Zahiri,Sayyed M. Zahiri and Ali Ahmadvand,"CRAB: Class Representation Attentive BERT for Hate Speech Identification
  in Social Media",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, social media platforms have hosted an explosion of hate
speech and objectionable content. The urgent need for effective automatic hate
speech detection models have drawn remarkable investment from companies and
researchers. Social media posts are generally short and their semantics could
drastically be altered by even a single token. Thus, it is crucial for this
task to learn context-aware input representations, and consider relevancy
scores between input embeddings and class representations as an additional
signal. To accommodate these needs, this paper introduces CRAB (Class
Representation Attentive BERT), a neural model for detecting hate speech in
social media. The model benefits from two semantic representations: (i)
trainable token-wise and sentence-wise class representations, and (ii)
contextualized input embeddings from state-of-the-art BERT encoder. To
investigate effectiveness of CRAB, we train our model on Twitter data and
compare it against strong baselines. Our results show that CRAB achieves 1.89%
relative improved Macro-averaged F1 over state-of-the-art baseline. The results
of this research open an opportunity for the future research on automated
abusive behavior detection in social media
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 04:11:30 GMT'}]",2020-10-27,"[['Zahiri', 'Sayyed M.', ''], ['Ahmadvand', 'Ali', '']]"
1328086,2008.00364,Qian Li,"Qian Li, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun,
  Philip S. Yu, Lifang He",A Survey on Text Classification: From Shallow to Deep Learning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text classification is the most fundamental and essential task in natural
language processing. The last decade has seen a surge of research in this area
due to the unprecedented success of deep learning. Numerous methods, datasets,
and evaluation metrics have been proposed in the literature, raising the need
for a comprehensive and updated survey. This paper fills the gap by reviewing
the state of the art approaches from 1961 to 2020, focusing on models from
shallow to deep learning. We create a taxonomy for text classification
according to the text involved and the models used for feature extraction and
classification. We then discuss each of these categories in detail, dealing
with both the technical developments and benchmark datasets that support tests
of predictions. A comprehensive comparison between different techniques, as
well as identifying the pros and cons of various evaluation metrics are also
provided in this survey. Finally, we conclude by summarizing key implications,
future research directions, and the challenges facing the research area.
","[{'version': 'v1', 'created': 'Sun, 2 Aug 2020 00:09:03 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Aug 2020 06:13:59 GMT'}, {'version': 'v3', 'created': 'Sun, 11 Oct 2020 04:15:57 GMT'}, {'version': 'v4', 'created': 'Tue, 13 Oct 2020 07:05:36 GMT'}, {'version': 'v5', 'created': 'Mon, 26 Oct 2020 02:46:42 GMT'}]",2020-10-27,"[['Li', 'Qian', ''], ['Peng', 'Hao', ''], ['Li', 'Jianxin', ''], ['Xia', 'Congying', ''], ['Yang', 'Renyu', ''], ['Sun', 'Lichao', ''], ['Yu', 'Philip S.', ''], ['He', 'Lifang', '']]"
1369361,2010.13031,Jian Du,"Xiaoying Li, Suyuan Peng, Jian Du","Towards Medical Knowmetrics: Representing and Computing Medical
  Knowledge using Semantic Predications as the Knowledge Unit and the
  Uncertainty as the Knowledge Context",,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In China, Prof. Hongzhou Zhao and Zeyuan Liu are the pioneers of the concept
""knowledge unit"" and ""knowmetrics"" for measuring knowledge. However, the
definition of ""computable knowledge object"" remains controversial so far in
different fields. For example, it is defined as 1) quantitative scientific
concept in natural science and engineering, 2) knowledge point in the field of
education research, and 3) semantic predications, i.e.,
Subject-Predicate-Object (SPO) triples in biomedical fields. The Semantic
MEDLINE Database (SemMedDB), a high-quality public repository of SPO triples
extracted from medical literature, provides a basic data infrastructure for
measuring medical knowledge. In general, the study of extracting SPO triples as
computable knowledge unit from unstructured scientific text has been
overwhelmingly focusing on scientific knowledge per se. Since the SPO triples
would be possibly extracted from hypothetical, speculative statements or even
conflicting and contradictory assertions, the knowledge status (i.e., the
uncertainty), which serves as an integral and critical part of scientific
knowledge has been largely overlooked. This article aims to put forward a
framework for Medical Knowmetrics using the SPO triples as the knowledge unit
and the uncertainty as the knowledge context. The lung cancer publications
dataset is used to validate the proposed framework. The uncertainty of medical
knowledge and how its status evolves over time indirectly reflect the strength
of competing knowledge claims, and the probability of certainty for a given SPO
triple. We try to discuss the new insights using the uncertainty-centric
approaches to detect research fronts, and identify knowledge claims with high
certainty level, in order to improve the efficacy of knowledge-driven decision
support.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 04:27:43 GMT'}]",2020-10-27,"[['Li', 'Xiaoying', ''], ['Peng', 'Suyuan', ''], ['Du', 'Jian', '']]"
1369370,2010.13040,Zhaoning Li,Zhaoning Li and Jiangtao Ren,Fine-tuning ERNIE for chest abnormal imaging signs extraction,"30 pages, 5 figures, 8 tables",J. Biomed. Inform. 108 (2020),10.1016/j.jbi.2020.103492,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chest imaging reports describe the results of chest radiography procedures.
Automatic extraction of abnormal imaging signs from chest imaging reports has a
pivotal role in clinical research and a wide range of downstream medical tasks.
However, there are few studies on information extraction from Chinese chest
imaging reports. In this paper, we formulate chest abnormal imaging sign
extraction as a sequence tagging and matching problem. On this basis, we
propose a transferred abnormal imaging signs extractor with pretrained ERNIE as
the backbone, named EASON (fine-tuning ERNIE with CRF for Abnormal Signs
ExtractiON), which can address the problem of data insufficiency. In addition,
to assign the attributes (the body part and degree) to corresponding abnormal
imaging signs from the results of the sequence tagging model, we design a
simple but effective tag2relation algorithm based on the nature of chest
imaging report text. We evaluate our method on the corpus provided by a medical
big data company, and the experimental results demonstrate that our method
achieves significant and consistent improvement compared to other baselines.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 05:18:14 GMT'}]",2020-10-27,"[['Li', 'Zhaoning', ''], ['Ren', 'Jiangtao', '']]"
1369377,2010.13047,Hirofumi Inaguma,"Hirofumi Inaguma, Yosuke Higuchi, Kevin Duh, Tatsuya Kawahara, Shinji
  Watanabe","Orthros: Non-autoregressive End-to-end Speech Translation with
  Dual-decoder",,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fast inference speed is an important goal towards real-world deployment of
speech translation (ST) systems. End-to-end (E2E) models based on the
encoder-decoder architecture are more suitable for this goal than traditional
cascaded systems, but their effectiveness regarding decoding speed has not been
explored so far. Inspired by recent progress in non-autoregressive (NAR)
methods in text-based translation, which generates target tokens in parallel by
eliminating conditional dependencies, we study the problem of NAR decoding for
E2E-ST. We propose a novel NAR E2E-ST framework, Orthoros, in which both NAR
and autoregressive (AR) decoders are jointly trained on the shared speech
encoder. The latter is used for selecting better translation among various
length candidates generated from the former, which dramatically improves the
effectiveness of a large length beam with negligible overhead. We further
investigate effective length prediction methods from speech inputs and the
impact of vocabulary sizes. Experiments on four benchmarks show the
effectiveness of the proposed method in improving inference speed while
maintaining competitive translation quality compared to state-of-the-art AR
E2E-ST systems.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 06:35:30 GMT'}]",2020-10-27,"[['Inaguma', 'Hirofumi', ''], ['Higuchi', 'Yosuke', ''], ['Duh', 'Kevin', ''], ['Kawahara', 'Tatsuya', ''], ['Watanabe', 'Shinji', '']]"
1369719,2010.13389,Amir Pouran Ben Veyseh,"Amir Pouran Ben Veyseh, Nasim Nour, Franck Dernoncourt, Quan Hung
  Tran, Dejing Dou, Thien Huu Nguyen","Improving Aspect-based Sentiment Analysis with Gated Graph Convolutional
  Networks and Syntax-based Regulation",accepted at EMNLP 2020 findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Aspect-based Sentiment Analysis (ABSA) seeks to predict the sentiment
polarity of a sentence toward a specific aspect. Recently, it has been shown
that dependency trees can be integrated into deep learning models to produce
the state-of-the-art performance for ABSA. However, these models tend to
compute the hidden/representation vectors without considering the aspect terms
and fail to benefit from the overall contextual importance scores of the words
that can be obtained from the dependency tree for ABSA. In this work, we
propose a novel graph-based deep learning model to overcome these two issues of
the prior work on ABSA. In our model, gate vectors are generated from the
representation vectors of the aspect terms to customize the hidden vectors of
the graph-based models toward the aspect terms. In addition, we propose a
mechanism to obtain the importance scores for each word in the sentences based
on the dependency trees that are then injected into the model to improve the
representation vectors for ABSA. The proposed model achieves the
state-of-the-art performance on three benchmark datasets.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 07:36:24 GMT'}]",2020-10-27,"[['Veyseh', 'Amir Pouran Ben', ''], ['Nour', 'Nasim', ''], ['Dernoncourt', 'Franck', ''], ['Tran', 'Quan Hung', ''], ['Dou', 'Dejing', ''], ['Nguyen', 'Thien Huu', '']]"
1369387,2010.13057,Sathvik Nair,"Sathvik Nair, Mahesh Srinivasan, Stephan Meylan","Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense
  Knowledge","To appear in proceedings of the Cognitive Aspects of the Lexicon
  Workshop at the 28th International Conference on Computational Linguistics
  (COLING)",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Understanding context-dependent variation in word meanings is a key aspect of
human language comprehension supported by the lexicon. Lexicographic resources
(e.g., WordNet) capture only some of this context-dependent variation; for
example, they often do not encode how closely senses, or discretized word
meanings, are related to one another. Our work investigates whether recent
advances in NLP, specifically contextualized word embeddings, capture
human-like distinctions between English word senses, such as polysemy and
homonymy. We collect data from a behavioral, web-based experiment, in which
participants provide judgments of the relatedness of multiple WordNet senses of
a word in a two-dimensional spatial arrangement task. We find that
participants' judgments of the relatedness between senses are correlated with
distances between senses in the BERT embedding space. Homonymous senses (e.g.,
bat as mammal vs. bat as sports equipment) are reliably more distant from one
another in the embedding space than polysemous ones (e.g., chicken as animal
vs. chicken as meat). Our findings point towards the potential utility of
continuous-space representations of sense meanings.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 07:56:52 GMT'}]",2020-10-27,"[['Nair', 'Sathvik', ''], ['Srinivasan', 'Mahesh', ''], ['Meylan', 'Stephan', '']]"
1369392,2010.13062,Zhixiang Li,"Mengzhe Li, Yudan Wang, Ying Zhao and Zhixiang Li","Transgender Community Sentiment Analysis from Social Media Data: A
  Natural Language Processing Approach","5 pages, 1 figures",,,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  Transgender community is experiencing a huge disparity in mental health
conditions compared with the general population. Interpreting the social medial
data posted by transgender people may help us understand the sentiments of
these sexual minority groups better and apply early interventions. In this
study, we manually categorize 300 social media comments posted by transgender
people to the sentiment of negative, positive, and neutral. 5 machine learning
algorithms and 2 deep neural networks are adopted to build sentiment analysis
classifiers based on the annotated data. Results show that our annotations are
reliable with a high Cohen's Kappa score over 0.8 across all three classes.
LSTM model yields an optimal performance of accuracy over 0.85 and AUC of
0.876. Our next step will focus on using advanced natural language processing
algorithms on a larger annotated dataset.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 08:13:34 GMT'}]",2020-10-27,"[['Li', 'Mengzhe', ''], ['Wang', 'Yudan', ''], ['Zhao', 'Ying', ''], ['Li', 'Zhixiang', '']]"
1369379,2010.13049,Gongqi Lin,"Gongqi Lin, Yuan Miao, Xiaoyong Yang, Wenwu Ou, Lizhen Cui, Wei Guo,
  Chunyan Miao",Commonsense knowledge adversarial dataset that challenges ELECTRA,To appear in ICARCV2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Commonsense knowledge is critical in human reading comprehension. While
machine comprehension has made significant progress in recent years, the
ability in handling commonsense knowledge remains limited. Synonyms are one of
the most widely used commonsense knowledge. Constructing adversarial dataset is
an important approach to find weak points of machine comprehension models and
support the design of solutions. To investigate machine comprehension models'
ability in handling the commonsense knowledge, we created a Question and Answer
Dataset with common knowledge of Synonyms (QADS). QADS are questions generated
based on SQuAD 2.0 by applying commonsense knowledge of synonyms. The synonyms
are extracted from WordNet. Words often have multiple meanings and synonyms. We
used an enhanced Lesk algorithm to perform word sense disambiguation to
identify synonyms for the context. ELECTRA achieves the state-of-art result on
the SQuAD 2.0 dataset in 2019. With scale, ELECTRA can achieve similar
performance as BERT does. However, QADS shows that ELECTRA has little ability
to handle commonsense knowledge of synonyms. In our experiment, ELECTRA-small
can achieve 70% accuracy on SQuAD 2.0, but only 20% on QADS. ELECTRA-large did
not perform much better. Its accuracy on SQuAD 2.0 is 88% but dropped
significantly to 26% on QADS. In our earlier experiments, BERT, although also
failed badly on QADS, was not as bad as ELECTRA. The result shows that even
top-performing NLP models have little ability to handle commonsense knowledge
which is essential in reading comprehension.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 07:17:45 GMT'}]",2020-10-27,"[['Lin', 'Gongqi', ''], ['Miao', 'Yuan', ''], ['Yang', 'Xiaoyong', ''], ['Ou', 'Wenwu', ''], ['Cui', 'Lizhen', ''], ['Guo', 'Wei', ''], ['Miao', 'Chunyan', '']]"
1369600,2010.13270,Yosuke Higuchi,"Yosuke Higuchi, Hirofumi Inaguma, Shinji Watanabe, Tetsuji Ogawa,
  Tetsunori Kobayashi",Improved Mask-CTC for Non-Autoregressive End-to-End ASR,Submitted to ICASSP2021,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  For real-world deployment of automatic speech recognition (ASR), the system
is desired to be capable of fast inference while relieving the requirement of
computational resources. The recently proposed end-to-end ASR system based on
mask-predict with connectionist temporal classification (CTC), Mask-CTC,
fulfills this demand by generating tokens in a non-autoregressive fashion.
While Mask-CTC achieves remarkably fast inference speed, its recognition
performance falls behind that of conventional autoregressive (AR) systems. To
boost the performance of Mask-CTC, we first propose to enhance the encoder
network architecture by employing a recently proposed architecture called
Conformer. Next, we propose new training and decoding methods by introducing
auxiliary objective to predict the length of a partial target sequence, which
allows the model to delete or insert tokens during inference. Experimental
results on different ASR tasks show that the proposed approaches improve
Mask-CTC significantly, outperforming a standard CTC model (15.5% $\rightarrow$
9.1% WER on WSJ). Moreover, Mask-CTC now achieves competitive results to AR
models with no degradation of inference speed ($<$ 0.1 RTF using CPU). We also
show a potential application of Mask-CTC to end-to-end speech translation.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 01:22:35 GMT'}]",2020-10-27,"[['Higuchi', 'Yosuke', ''], ['Inaguma', 'Hirofumi', ''], ['Watanabe', 'Shinji', ''], ['Ogawa', 'Tetsuji', ''], ['Kobayashi', 'Tetsunori', '']]"
1321824,2007.10310,Changhan Wang,"Changhan Wang, Anne Wu, Juan Pino",CoVoST 2 and Massively Multilingual Speech-to-Text Translation,,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Speech translation has recently become an increasingly popular topic of
research, partly due to the development of benchmark datasets. Nevertheless,
current datasets cover a limited number of languages. With the aim to foster
research in massive multilingual speech translation and speech translation for
low resource language pairs, we release CoVoST 2, a large-scale multilingual
speech translation corpus covering translations from 21 languages into English
and from English into 15 languages. This represents the largest open dataset
available to date from total volume and language coverage perspective. Data
sanity checks provide evidence about the quality of the data, which is released
under CC0 license. We also provide extensive speech recognition, bilingual and
multilingual machine translation and speech translation baselines with
open-source implementation.
","[{'version': 'v1', 'created': 'Mon, 20 Jul 2020 17:53:35 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Aug 2020 17:53:10 GMT'}, {'version': 'v3', 'created': 'Sat, 24 Oct 2020 06:07:01 GMT'}]",2020-10-27,"[['Wang', 'Changhan', ''], ['Wu', 'Anne', ''], ['Pino', 'Juan', '']]"
1369708,2010.13378,Amir Pouran Ben Veyseh,"Amir Pouran Ben Veyseh, Nasim Nouri, Franck Dernoncourt, Dejing Dou,
  Thien Huu Nguyen","Introducing Syntactic Structures into Target Opinion Word Extraction
  with Deep Learning",accepted at EMNLP 2020 main conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Targeted opinion word extraction (TOWE) is a sub-task of aspect based
sentiment analysis (ABSA) which aims to find the opinion words for a given
aspect-term in a sentence. Despite their success for TOWE, the current deep
learning models fail to exploit the syntactic information of the sentences that
have been proved to be useful for TOWE in the prior research. In this work, we
propose to incorporate the syntactic structures of the sentences into the deep
learning models for TOWE, leveraging the syntax-based opinion possibility
scores and the syntactic connections between the words. We also introduce a
novel regularization technique to improve the performance of the deep learning
models based on the representation distinctions between the words in TOWE. The
proposed model is extensively analyzed and achieves the state-of-the-art
performance on four benchmark datasets.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 07:13:17 GMT'}]",2020-10-27,"[['Veyseh', 'Amir Pouran Ben', ''], ['Nouri', 'Nasim', ''], ['Dernoncourt', 'Franck', ''], ['Dou', 'Dejing', ''], ['Nguyen', 'Thien Huu', '']]"
1274534,2004.09764,Fang Xianghong,"Xianghong Fang and Haoli Bai and Zenglin Xu and Michael Lyu and Irwin
  King",Discrete Auto-regressive Variational Attention Models for Text Modeling,"10 pages, 4 figures",,,,cs.LG cs.CL stat.ML,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.
","[{'version': 'v1', 'created': 'Tue, 21 Apr 2020 05:49:04 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Oct 2020 08:02:24 GMT'}]",2020-10-27,"[['Fang', 'Xianghong', ''], ['Bai', 'Haoli', ''], ['Xu', 'Zenglin', ''], ['Lyu', 'Michael', ''], ['King', 'Irwin', '']]"
1326856,2007.15342,Ramon Ferrer i Cancho,"Ramon Ferrer-i-Cancho, Carlos G\'omez-Rodr\'iguez, Juan Luis Esteban
  and Llu\'is Alemany-Puig",The optimality of syntactic dependency distances,"results on the zeta score have been corrected; format of the article
  has changed; some figures/tables have been resized; typos corrected",,,,cs.CL cs.DM physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is often stated that human languages, as other biological systems, are
shaped by cost-cutting pressures but, to what extent? Attempts to quantify the
degree of optimality of languages by means of an optimality score have been
scarce and focused mostly on English. Here we recast the problem of the
optimality of the word order of a sentence as an optimization problem on a
spatial network where the vertices are words, arcs indicate syntactic
dependencies and the space is defined by the linear order of the words in the
sentence. We introduce a new score to quantify the cognitive pressure to reduce
the distance between linked words in a sentence. The analysis of sentences from
93 languages representing 19 linguistic families reveals that half of languages
are optimized to a 70% or more. The score indicates that distances are not
significantly reduced in a few languages and confirms two theoretical
predictions, i.e. that longer sentences are more optimized and that distances
are more likely to be longer than expected by chance in short sentences. We
present a new hierarchical ranking of languages by their degree of
optimization. The statistical advantages of the new score call for a
reevaluation of the evolution of dependency distance over time in languages as
well as the relationship between dependency distance and linguistic competence.
Finally, the principles behind the design of the score can be extended to
develop more powerful normalizations of topological distances or physical
distances in more dimensions.
","[{'version': 'v1', 'created': 'Thu, 30 Jul 2020 09:40:41 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Oct 2020 22:15:22 GMT'}]",2020-10-27,"[['Ferrer-i-Cancho', 'Ramon', ''], ['Gómez-Rodríguez', 'Carlos', ''], ['Esteban', 'Juan Luis', ''], ['Alemany-Puig', 'Lluís', '']]"
1369522,2010.13192,Alexandra Chronopoulou,"Alexandra Chronopoulou, Dario Stojanovski, Viktor Hangya, Alexander
  Fraser","The LMU Munich System for the WMT 2020 Unsupervised Machine Translation
  Shared Task",WMT Unsupervised Shared Task 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes the submission of LMU Munich to the WMT 2020
unsupervised shared task, in two language directions, German<->Upper Sorbian.
Our core unsupervised neural machine translation (UNMT) system follows the
strategy of Chronopoulou et al. (2020), using a monolingual pretrained language
generation model (on German) and fine-tuning it on both German and Upper
Sorbian, before initializing a UNMT model, which is trained with online
backtranslation. Pseudo-parallel data obtained from an unsupervised statistical
machine translation (USMT) system is used to fine-tune the UNMT model. We also
apply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more
robust system. We additionally experiment with residual adapters and find them
useful in the Upper Sorbian->German direction. We explore sampling during
backtranslation and curriculum learning to use SMT translations in a more
principled way. Finally, we ensemble our best-performing systems and reach a
BLEU score of 32.4 on German->Upper Sorbian and 35.2 on Upper Sorbian->German.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 19:04:03 GMT'}]",2020-10-27,"[['Chronopoulou', 'Alexandra', ''], ['Stojanovski', 'Dario', ''], ['Hangya', 'Viktor', ''], ['Fraser', 'Alexander', '']]"
1280356,2005.00561,Anna Rogers,"Sai Prasanna, Anna Rogers, Anna Rumshisky","When BERT Plays the Lottery, All Tickets Are Winning",EMNLP 2020 camera-ready,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Transformer-based models were shown to be reducible to a smaller number
of self-attention heads and layers. We consider this phenomenon from the
perspective of the lottery ticket hypothesis, using both structured and
magnitude pruning. For fine-tuned BERT, we show that (a) it is possible to find
subnetworks achieving performance that is comparable with that of the full
model, and (b) similarly-sized subnetworks sampled from the rest of the model
perform worse. Strikingly, with structured pruning even the worst possible
subnetworks remain highly trainable, indicating that most pre-trained BERT
weights are potentially useful. We also study the ""good"" subnetworks to see if
their success can be attributed to superior linguistic knowledge, but find them
unstable, and not explained by meaningful self-attention patterns.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 18:24:42 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Oct 2020 10:15:27 GMT'}]",2020-10-27,"[['Prasanna', 'Sai', ''], ['Rogers', 'Anna', ''], ['Rumshisky', 'Anna', '']]"
1369498,2010.13168,Tenzin Singhay Bhotia,"Vaibhav Kumar, Tenzin Singhay Bhotia, Vaibhav Kumar","Fair Embedding Engine: A Library for Analyzing and Mitigating Gender
  Bias in Word Embeddings","6 pages, 3 figures",,,,cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Non-contextual word embedding models have been shown to inherit human-like
stereotypical biases of gender, race and religion from the training corpora. To
counter this issue, a large body of research has emerged which aims to mitigate
these biases while keeping the syntactic and semantic utility of embeddings
intact. This paper describes Fair Embedding Engine (FEE), a library for
analysing and mitigating gender bias in word embeddings. FEE combines various
state of the art techniques for quantifying, visualising and mitigating gender
bias in word embeddings under a standard abstraction. FEE will aid
practitioners in fast track analysis of existing debiasing methods on their
embedding models. Further, it will allow rapid prototyping of new methods by
evaluating their performance on a suite of standard metrics.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 17:31:12 GMT'}]",2020-10-27,"[['Kumar', 'Vaibhav', ''], ['Bhotia', 'Tenzin Singhay', ''], ['Kumar', 'Vaibhav', '']]"
1369458,2010.13128,Mokanarangan Thayaparan,"Mokanarangan Thayaparan, Marco Valentino, Andr\'e Freitas","ExplanationLP: Abductive Reasoning for Explainable Science Question
  Answering",,,,,cs.AI cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a novel approach for answering and explaining multiple-choice
science questions by reasoning on grounding and abstract inference chains. This
paper frames question answering as an abductive reasoning problem, constructing
plausible explanations for each choice and then selecting the candidate with
the best explanation as the final answer. Our system, ExplanationLP, elicits
explanations by constructing a weighted graph of relevant facts for each
candidate answer and extracting the facts that satisfy certain structural and
semantic constraints. To extract the explanations, we employ a linear
programming formalism designed to select the optimal subgraph. The graphs'
weighting function is composed of a set of parameters, which we fine-tune to
optimize answer selection performance. We carry out our experiments on the
WorldTree and ARC-Challenge corpus to empirically demonstrate the following
conclusions: (1) Grounding-Abstract inference chains provides the semantic
control to perform explainable abductive reasoning (2) Efficiency and
robustness in learning with a fewer number of parameters by outperforming
contemporary explainable and transformer-based approaches in a similar setting
(3) Generalisability by outperforming SOTA explainable approaches on general
science question sets.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 14:49:24 GMT'}]",2020-10-27,"[['Thayaparan', 'Mokanarangan', ''], ['Valentino', 'Marco', ''], ['Freitas', 'André', '']]"
1369435,2010.13105,Gyuwan Kim,"Seongbin Kim, Gyuwan Kim, Seongjin Shin, Sangmin Lee","Two-stage Textual Knowledge Distillation to Speech Encoder for Spoken
  Language Understanding","Preprint; 5 pages, 1 figure",,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end approaches open a new way for more accurate and efficient spoken
language understanding (SLU) systems by alleviating the drawbacks of
traditional pipeline systems. Previous works exploit textual information for an
SLU model via pre-training with automatic speech recognition or fine-tuning
with knowledge distillation. To utilize textual information more effectively,
this work proposes a two-stage textual knowledge distillation method that
matches utterance-level representations and predicted logits of two modalities
during pre-training and fine-tuning, sequentially. We use vq-wav2vec BERT as a
speech encoder because it captures general and rich features. Furthermore, we
improve the performance, especially in a low-resource scenario, with data
augmentation methods by randomly masking spans of discrete audio tokens and
contextualized hidden representations. Consequently, we push the
state-of-the-art on the Fluent Speech Commands, achieving 99.7% test accuracy
in the full dataset setting and 99.5% in the 10% subset setting. Throughout the
ablation studies, we empirically verify that all used methods are crucial to
the final performance, providing the best practice for spoken language
understanding. Code to reproduce our results will be available upon
publication.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 12:36:05 GMT'}]",2020-10-27,"[['Kim', 'Seongbin', ''], ['Kim', 'Gyuwan', ''], ['Shin', 'Seongjin', ''], ['Lee', 'Sangmin', '']]"
1267434,2004.02664,Qingyu Zhou,"Qingyu Zhou, Furu Wei, Ming Zhou","At Which Level Should We Extract? An Empirical Analysis on Extractive
  Document Summarization",To appear at COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Extractive methods have been proven effective in automatic document
summarization. Previous works perform this task by identifying informative
contents at sentence level. However, it is unclear whether performing
extraction at sentence level is the best solution. In this work, we show that
unnecessity and redundancy issues exist when extracting full sentences, and
extracting sub-sentential units is a promising alternative. Specifically, we
propose extracting sub-sentential units based on the constituency parsing tree.
A neural extractive model which leverages the sub-sentential information and
extracts them is presented. Extensive experiments and analyses show that
extracting sub-sentential units performs competitively comparing to full
sentence extraction under the evaluation of both automatic and human
evaluations. Hopefully, our work could provide some inspiration of the basic
extraction units in extractive summarization for future research.
","[{'version': 'v1', 'created': 'Mon, 6 Apr 2020 13:35:10 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 08:35:19 GMT'}]",2020-10-27,"[['Zhou', 'Qingyu', ''], ['Wei', 'Furu', ''], ['Zhou', 'Ming', '']]"
1370795,2010.14465,Marta R. Costa-juss\`a,Marta R. Costa-juss\`a and Christine Basta and Gerard I. G\'allego,Evaluating Gender Bias in Speech Translation,"Preprint, Submitted to ICASSP 2021",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The scientific community is more and more aware of the necessity to embrace
pluralism and consistently represent major and minor social groups. In this
direction, there is an urgent need to provide evaluation sets and protocols to
measure existing biases in our automatic systems. This paper introduces WinoST,
a new freely available challenge set for evaluating gender bias in speech
translation. WinoST is the speech version of WinoMT which is an MT challenge
set and both follow an evaluation protocol to measure gender accuracy. Using a
state-of-the-art end-to-end speech translation system, we report the gender
bias evaluation on 4 language pairs, and we show that gender accuracy in speech
translation is more than 23% lower than in MT.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 17:24:27 GMT'}]",2020-10-28,"[['Costa-jussà', 'Marta R.', ''], ['Basta', 'Christine', ''], ['Gállego', 'Gerard I.', '']]"
1370809,2010.14479,Sugat Chaturvedi,"Rochana Chaturvedi, Sugat Chaturvedi",It's All in the Name: A Character Based Approach To Infer Religion,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Demographic inference from text has received a surge of attention in the
field of natural language processing in the last decade. In this paper, we use
personal names to infer religion in South Asia - where religion is a salient
social division, and yet, disaggregated data on it remains scarce. Existing
work predicts religion using dictionary based method, and therefore, can not
classify unseen names. We use character based models which learn character
patterns and, therefore, can classify unseen names as well with high accuracy.
These models are also much faster and can easily be scaled to large data sets.
We improve our classifier by combining the name of an individual with that of
their parent/spouse and achieve remarkably high accuracy. Finally, we trace the
classification decisions of a convolutional neural network model using
layer-wise relevance propagation which can explain the predictions of complex
non-linear classifiers and circumvent their purported black box nature. We show
how character patterns learned by the classifier are rooted in the linguistic
origins of names.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 17:38:11 GMT'}]",2020-10-28,"[['Chaturvedi', 'Rochana', ''], ['Chaturvedi', 'Sugat', '']]"
960680,1803.10547,Nurendra Choudhary,"Nurendra Choudhary, Rajat Singh, Ishita Bindlish and Manish
  Shrivastava",Neural Network Architecture for Credibility Assessment of Textual Claims,"Best Paper Award at 19th International Conference on Computational
  Linguistics and Intelligent Text Processing, March 2018, Hanoi, Vietnam",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text articles with false claims, especially news, have recently become
aggravating for the Internet users. These articles are in wide circulation and
readers face difficulty discerning fact from fiction. Previous work on
credibility assessment has focused on factual analysis and linguistic features.
The task's main challenge is the distinction between the features of true and
false articles. In this paper, we propose a novel approach called Credibility
Outcome (CREDO) which aims at scoring the credibility of an article in an open
domain setting.
  CREDO consists of different modules for capturing various features
responsible for the credibility of an article. These features includes
credibility of the article's source and author, semantic similarity between the
article and related credible articles retrieved from a knowledge base, and
sentiments conveyed by the article. A neural network architecture learns the
contribution of each of these modules to the overall credibility of an article.
Experiments on Snopes dataset reveals that CREDO outperforms the
state-of-the-art approaches based on linguistic features.
","[{'version': 'v1', 'created': 'Wed, 28 Mar 2018 11:50:32 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Mar 2018 10:42:04 GMT'}, {'version': 'v3', 'created': 'Mon, 26 Oct 2020 21:30:25 GMT'}]",2020-10-28,"[['Choudhary', 'Nurendra', ''], ['Singh', 'Rajat', ''], ['Bindlish', 'Ishita', ''], ['Shrivastava', 'Manish', '']]"
1370811,2010.14481,Biao Zhang,"Biao Zhang, Ivan Titov, Rico Sennrich",Fast Interleaved Bidirectional Sequence Generation,"WMT2020, source code is at
  https://github.com/bzhangGo/zero/tree/master/docs/interleaved_bidirectional_transformer",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Independence assumptions during sequence generation can speed up inference,
but parallel generation of highly inter-dependent tokens comes at a cost in
quality. Instead of assuming independence between neighbouring tokens
(semi-autoregressive decoding, SA), we take inspiration from bidirectional
sequence generation and introduce a decoder that generates target words from
the left-to-right and right-to-left directions simultaneously. We show that we
can easily convert a standard architecture for unidirectional decoding into a
bidirectional decoder by simply interleaving the two directions and adapting
the word positions and self-attention masks. Our interleaved bidirectional
decoder (IBDecoder) retains the model simplicity and training efficiency of the
standard Transformer, and on five machine translation tasks and two document
summarization tasks, achieves a decoding speedup of ~2X compared to
autoregressive decoding with comparable quality. Notably, it outperforms
left-to-right SA because the independence assumptions in IBDecoder are more
felicitous. To achieve even higher speedups, we explore hybrid models where we
either simultaneously predict multiple neighbouring tokens per direction, or
perform multi-directional decoding by partitioning the target sequence. These
methods achieve speedups to 4X-11X across different tasks at the cost of <1
BLEU or <0.5 ROUGE (on average). Source code is released at
https://github.com/bzhangGo/zero.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 17:38:51 GMT'}]",2020-10-28,"[['Zhang', 'Biao', ''], ['Titov', 'Ivan', ''], ['Sennrich', 'Rico', '']]"
1246301,2002.08878,Clement Moulin-Frier,Cl\'ement Moulin-Frier and Pierre-Yves Oudeyer,"Multi-Agent Reinforcement Learning as a Computational Tool for Language
  Evolution Research: Historical Context and Future Challenges",,"Challenges and Opportunities for Multi-Agent Reinforcement
  Learning (COMARL AAAI 2020-2021), AAAI Spring Symposium Series, Stanford
  University, Palo Alto, California, USA",,,cs.MA cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Computational models of emergent communication in agent populations are
currently gaining interest in the machine learning community due to recent
advances in Multi-Agent Reinforcement Learning (MARL). Current contributions
are however still relatively disconnected from the earlier theoretical and
computational literature aiming at understanding how language might have
emerged from a prelinguistic substance. The goal of this paper is to position
recent MARL contributions within the historical context of language evolution
research, as well as to extract from this theoretical and computational
background a few challenges for future research.
","[{'version': 'v1', 'created': 'Thu, 20 Feb 2020 17:26:46 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 13:54:46 GMT'}]",2020-10-28,"[['Moulin-Frier', 'Clément', ''], ['Oudeyer', 'Pierre-Yves', '']]"
1352508,2009.11005,Khang Nguyen,Khang Phuoc-Quy Nguyen and Kiet Van Nguyen,"Exploiting Vietnamese Social Media Characteristics for Textual Emotion
  Recognition in Vietnamese","6 pages, 9 tables, 2 figures of table, conference",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Textual emotion recognition has been a promising research topic in recent
years. Many researchers aim to build more accurate and robust emotion detection
systems. In this paper, we conduct several experiments to indicate how data
pre-processing affects a machine learning method on textual emotion
recognition. These experiments are performed on the Vietnamese Social Media
Emotion Corpus (UIT-VSMEC) as the benchmark dataset. We explore Vietnamese
social media characteristics to propose different pre-processing techniques,
and key-clause extraction with emotional context to improve the machine
performance on UIT-VSMEC. Our experimental evaluation shows that with
appropriate pre-processing techniques based on Vietnamese social media
characteristics, Multinomial Logistic Regression (MLR) achieves the best
F1-score of 64.40%, a significant improvement of 4.66% over the CNN model built
by the authors of UIT-VSMEC (59.74%).
","[{'version': 'v1', 'created': 'Wed, 23 Sep 2020 08:49:39 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Sep 2020 15:46:49 GMT'}, {'version': 'v3', 'created': 'Tue, 27 Oct 2020 14:39:40 GMT'}]",2020-10-28,"[['Nguyen', 'Khang Phuoc-Quy', ''], ['Van Nguyen', 'Kiet', '']]"
1280128,2005.00333,Edoardo Maria Ponti,"Edoardo Maria Ponti, Goran Glava\v{s}, Olga Majewska, Qianchu Liu,
  Ivan Vuli\'c and Anna Korhonen",XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In order to simulate human language capacity, natural language processing
systems must be able to reason about the dynamics of everyday situations,
including their possible causes and effects. Moreover, they should be able to
generalise the acquired world knowledge to new languages, modulo cultural
differences. Advances in machine reasoning and cross-lingual transfer depend on
the availability of challenging evaluation benchmarks. Motivated by both
demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a
typologically diverse multilingual dataset for causal commonsense reasoning in
11 languages, which includes resource-poor languages like Eastern Apur\'imac
Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on
this novel dataset, revealing that the performance of current methods based on
multilingual pretraining and zero-shot fine-tuning falls short compared to
translation-based transfer. Finally, we propose strategies to adapt
multilingual models to out-of-sample resource-lean languages where only a small
corpus or a bilingual dictionary is available, and report substantial
improvements over the random baseline. The XCOPA dataset is freely available at
github.com/cambridgeltl/xcopa.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 12:22:33 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 23:23:58 GMT'}]",2020-10-28,"[['Ponti', 'Edoardo Maria', ''], ['Glavaš', 'Goran', ''], ['Majewska', 'Olga', ''], ['Liu', 'Qianchu', ''], ['Vulić', 'Ivan', ''], ['Korhonen', 'Anna', '']]"
1353816,2009.12313,Iacer Calixto,Victor Milewski and Marie-Francine Moens and Iacer Calixto,Are scene graphs good enough to improve Image Captioning?,"Published at AACL-IJCNLP 2020. 12 pages, 5 figures",,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many top-performing image captioning models rely solely on object features
computed with an object detection model to generate image descriptions.
However, recent studies propose to directly use scene graphs to introduce
information about object relations into captioning, hoping to better describe
interactions between objects. In this work, we thoroughly investigate the use
of scene graphs in image captioning. We empirically study whether using
additional scene graph encoders can lead to better image descriptions and
propose a conditional graph attention network (C-GAT), where the image
captioning decoder state is used to condition the graph updates. Finally, we
determine to what extent noise in the predicted scene graphs influence caption
quality. Overall, we find no significant difference between models that use
scene graph features and models that only use object detection features across
different captioning metrics, which suggests that existing scene graph
generation models are still too noisy to be useful in image captioning.
Moreover, although the quality of predicted scene graphs is very low in
general, when using high quality scene graphs we obtain gains of up to 3.3
CIDEr compared to a strong Bottom-Up Top-Down baseline. We open source code to
reproduce all our experiments in
https://github.com/iacercalixto/butd-image-captioning.
","[{'version': 'v1', 'created': 'Fri, 25 Sep 2020 16:09:08 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 17:55:55 GMT'}]",2020-10-28,"[['Milewski', 'Victor', ''], ['Moens', 'Marie-Francine', ''], ['Calixto', 'Iacer', '']]"
1370778,2010.14448,Xavier Ferrer Aran,"Xavier Ferrer-Aran, Tom van Nuenen, Natalia Criado, Jose M. Such",Discovering and Interpreting Conceptual Biases in Online Communities,,,,,cs.CL cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language carries implicit human biases, functioning both as a reflection and
a perpetuation of stereotypes that people carry with them. Recently, ML-based
NLP methods such as word embeddings have been shown to learn such language
biases with striking accuracy. This capability of word embeddings has been
successfully exploited as a tool to quantify and study human biases. However,
previous studies only consider a predefined set of conceptual biases to attest
(e.g., whether gender is more or less associated with particular jobs), or just
discover biased words without helping to understand their meaning at the
conceptual level. As such, these approaches are either unable to find
conceptual biases that have not been defined in advance, or the biases they
find are difficult to interpret and study. This makes existing approaches
unsuitable to discover and interpret biases in online communities, as such
communities may carry different biases than those in mainstream culture. This
paper proposes a general, data-driven approach to automatically discover and
help interpret conceptual biases encoded in word embeddings. We apply this
approach to study the conceptual biases present in the language used in online
communities and experimentally show the validity and stability of our method.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 17:07:12 GMT'}]",2020-10-28,"[['Ferrer-Aran', 'Xavier', ''], ['van Nuenen', 'Tom', ''], ['Criado', 'Natalia', ''], ['Such', 'Jose M.', '']]"
1370794,2010.14464,Lukasz Borchmann,"{\L}ukasz Borchmann, Dawid Jurkiewicz, Filip Grali\'nski, Tomasz
  G\'orecki","Dynamic Boundary Time Warping for Sub-sequence Matching with Few
  Examples",,,,,cs.DS cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The paper presents a novel method of finding a fragment in a long temporal
sequence similar to the set of shorter sequences. We are the first to propose
an algorithm for such a search that does not rely on computing the average
sequence from query examples. Instead, we use query examples as is, utilizing
all of them simultaneously. The introduced method based on the Dynamic Time
Warping (DTW) technique is suited explicitly for few-shot query-by-example
retrieval tasks. We evaluate it on two different few-shot problems from the
field of Natural Language Processing. The results show it either outperforms
baselines and previous approaches or achieves comparable results when a low
number of examples is available.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 17:23:18 GMT'}]",2020-10-28,"[['Borchmann', 'Łukasz', ''], ['Jurkiewicz', 'Dawid', ''], ['Graliński', 'Filip', ''], ['Górecki', 'Tomasz', '']]"
998122,1807.00914,Edoardo Maria Ponti,"Edoardo Maria Ponti, Helen O'Horan, Yevgeni Berzak, Ivan Vuli\'c, Roi
  Reichart, Thierry Poibeau, Ekaterina Shutova, Anna Korhonen","Modeling Language Variation and Universals: A Survey on Typological
  Linguistics for Natural Language Processing",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Linguistic typology aims to capture structural and semantic variation across
the world's languages. A large-scale typology could provide excellent guidance
for multilingual Natural Language Processing (NLP), particularly for languages
that suffer from the lack of human labeled resources. We present an extensive
literature survey on the use of typological information in the development of
NLP techniques. Our survey demonstrates that to date, the use of information in
existing typological databases has resulted in consistent but modest
improvements in system performance. We show that this is due to both intrinsic
limitations of databases (in terms of coverage and feature granularity) and
under-employment of the typological features included in them. We advocate for
a new approach that adapts the broad and discrete nature of typological
categories to the contextual and continuous nature of machine learning
algorithms used in contemporary NLP. In particular, we suggest that such
approach could be facilitated by recent developments in data-driven induction
of typological knowledge.
","[{'version': 'v1', 'created': 'Mon, 2 Jul 2018 22:09:59 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Feb 2019 19:55:28 GMT'}, {'version': 'v3', 'created': 'Mon, 26 Oct 2020 23:23:45 GMT'}]",2020-10-28,"[['Ponti', 'Edoardo Maria', ''], [""O'Horan"", 'Helen', ''], ['Berzak', 'Yevgeni', ''], ['Vulić', 'Ivan', ''], ['Reichart', 'Roi', ''], ['Poibeau', 'Thierry', ''], ['Shutova', 'Ekaterina', ''], ['Korhonen', 'Anna', '']]"
1355762,2009.14259,Peter Jansen,Peter A. Jansen,"Visually-Grounded Planning without Vision: Language Models Infer
  Detailed Plans from High-level Instructions","Accepted to Findings of EMNLP. V2: corrected typo Table 1; margins
  Table 3",,,,cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  The recently proposed ALFRED challenge task aims for a virtual robotic agent
to complete complex multi-step everyday tasks in a virtual home environment
from high-level natural language directives, such as ""put a hot piece of bread
on a plate"". Currently, the best-performing models are able to complete less
than 5% of these tasks successfully. In this work we focus on modeling the
translation problem of converting natural language directives into detailed
multi-step sequences of actions that accomplish those goals in the virtual
environment. We empirically demonstrate that it is possible to generate gold
multi-step plans from language directives alone without any visual input in 26%
of unseen cases. When a small amount of visual information is incorporated,
namely the starting location in the virtual environment, our best-performing
GPT-2 model successfully generates gold command sequences in 58% of cases. Our
results suggest that contextualized language models may provide strong visual
semantic planning modules for grounded virtual agents.
","[{'version': 'v1', 'created': 'Tue, 29 Sep 2020 18:52:39 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 19:16:00 GMT'}]",2020-10-28,"[['Jansen', 'Peter A.', '']]"
776112,1610.00765,Edoardo Maria Ponti,"Edoardo Maria Ponti, Elisabetta Jezek, Bernardo Magnini","Distributed Representations of Lexical Sets and Prototypes in Causal
  Alternation Verbs","5 pages, 4 figures, accepted at: Third Italian Conference on
  Computational Linguistics (CLIC-it). 5-6 December 2016, Napoli (Italy)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lexical sets contain the words filling an argument slot of a verb, and are in
part determined by selectional preferences. The purpose of this paper is to
unravel the properties of lexical sets through distributional semantics. We
investigate 1) whether lexical set behave as prototypical categories with a
centre and a periphery; 2) whether they are polymorphic, i.e. composed by
subcategories; 3) whether the distance between lexical sets of different
arguments is explanatory of verb properties. In particular, our case study are
lexical sets of causative-inchoative verbs in Italian. Having studied several
vector models, we find that 1) based on spatial distance from the centroid,
object fillers are scattered uniformly across the category, whereas
intransitive subject fillers lie on its edge; 2) a correlation exists between
the amount of verb senses and that of clusters discovered automatically,
especially for intransitive subjects; 3) the distance between the centroids of
object and intransitive subject is correlated with other properties of verbs,
such as their cross-lingual tendency to appear in the intransitive pattern
rather than transitive one. This paper is noncommittal with respect to the
hypothesis that this connection is underpinned by a semantic reason, namely the
spontaneity of the event denoted by the verb.
","[{'version': 'v1', 'created': 'Mon, 3 Oct 2016 21:50:27 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 23:48:14 GMT'}]",2020-10-28,"[['Ponti', 'Edoardo Maria', ''], ['Jezek', 'Elisabetta', ''], ['Magnini', 'Bernardo', '']]"
1362519,2010.06189,Zhengbao Jiang,"Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, Graham
  Neubig","X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained
  Language Models",EMNLP 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language models (LMs) have proven surprisingly successful at capturing
factual knowledge by completing cloze-style fill-in-the-blank questions such as
""Punta Cana is located in _."" However, while knowledge is both written and
queried in many languages, studies on LMs' factual representation ability have
almost invariably been performed on English. To assess factual knowledge
retrieval in LMs in different languages, we create a multilingual benchmark of
cloze-style probes for 23 typologically diverse languages. To properly handle
language variations, we expand probing methods from single- to multi-word
entities, and develop several decoding algorithms to generate multi-token
predictions. Extensive experimental results provide insights about how well (or
poorly) current state-of-the-art LMs perform at this task in languages with
more or fewer available resources. We further propose a code-switching-based
method to improve the ability of multilingual LMs to access knowledge, and
verify its effectiveness on several benchmark languages. Benchmark data and
code have been released at https://x-factr.github.io.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 05:29:56 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Oct 2020 22:23:17 GMT'}, {'version': 'v3', 'created': 'Tue, 27 Oct 2020 15:30:03 GMT'}]",2020-10-28,"[['Jiang', 'Zhengbao', ''], ['Anastasopoulos', 'Antonios', ''], ['Araki', 'Jun', ''], ['Ding', 'Haibo', ''], ['Neubig', 'Graham', '']]"
1362726,2010.06396,Ekta Sood,"Ekta Sood, Simon Tannert, Diego Frassinelli, Andreas Bulling and Ngoc
  Thang Vu","Interpreting Attention Models with Human Visual Attention in Machine
  Reading Comprehension",CoNLL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While neural networks with attention mechanisms have achieved superior
performance on many natural language processing tasks, it remains unclear to
which extent learned attention resembles human visual attention. In this paper,
we propose a new method that leverages eye-tracking data to investigate the
relationship between human visual attention and neural attention in machine
reading comprehension. To this end, we introduce a novel 23 participant eye
tracking dataset - MQA-RC, in which participants read movie plots and answered
pre-defined questions. We compare state of the art networks based on long
short-term memory (LSTM), convolutional neural models (CNN) and XLNet
Transformer architectures. We find that higher similarity to human attention
and performance significantly correlates to the LSTM and CNN models. However,
we show this relationship does not hold true for the XLNet models -- despite
the fact that the XLNet performs best on this challenging task. Our results
suggest that different architectures seem to learn rather different neural
attention strategies and similarity of neural to human attention does not
guarantee best performance.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 13:51:57 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 14:47:51 GMT'}]",2020-10-28,"[['Sood', 'Ekta', ''], ['Tannert', 'Simon', ''], ['Frassinelli', 'Diego', ''], ['Bulling', 'Andreas', ''], ['Vu', 'Ngoc Thang', '']]"
1364221,2010.07891,Ekta Sood,"Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling","Improving Natural Language Processing Tasks with Human Gaze-Guided
  Neural Attention",NeurIPS 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A lack of corpora has so far limited advances in integrating human gaze data
as a supervisory signal in neural attention mechanisms for natural language
processing(NLP). We propose a novel hybrid text saliency model(TSM) that, for
the first time, combines a cognitive model of reading with explicit human gaze
supervision in a single machine learning framework. On four different corpora
we demonstrate that our hybrid TSM duration predictions are highly correlated
with human gaze ground truth. We further propose a novel joint modeling
approach to integrate TSM predictions into the attention layer of a network
designed for a specific upstream NLP task without the need for any
task-specific human gaze data. We demonstrate that our joint model outperforms
the state of the art in paraphrase generation on the Quora Question Pairs
corpus by more than 10% in BLEU-4 and achieves state of the art performance for
sentence compression on the challenging Google Sentence Compression corpus. As
such, our work introduces a practical approach for bridging between data-driven
and cognitive models and demonstrates a new way to integrate human gaze-guided
neural attention into NLP tasks.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 17:14:09 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 16:16:18 GMT'}]",2020-10-28,"[['Sood', 'Ekta', ''], ['Tannert', 'Simon', ''], ['Mueller', 'Philipp', ''], ['Bulling', 'Andreas', '']]"
1177919,1909.07881,Palakorn Achananuparp,"Helena Lee, Palakorn Achananuparp, Yue Liu, Ee-Peng Lim, Lav R.
  Varshney","Estimating Glycemic Impact of Cooking Recipes via Online Crowdsourcing
  and Machine Learning","To appear in the Proceedings of Digital Public Health 2019 as short
  paper",,10.1145/3357729.3357748,,cs.CY cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Consumption of diets with low glycemic impact is highly recommended for
diabetics and pre-diabetics as it helps maintain their blood glucose levels.
However, laboratory analysis of dietary glycemic potency is time-consuming and
expensive. In this paper, we explore a data-driven approach utilizing online
crowdsourcing and machine learning to estimate the glycemic impact of cooking
recipes. We show that a commonly used healthiness metric may not always be
effective in determining recipes suitable for diabetics, thus emphasizing the
importance of the glycemic-impact estimation task. Our best classification
model, trained on nutritional and crowdsourced data obtained from Amazon
Mechanical Turk (AMT), can accurately identify recipes which are unhealthful
for diabetics.
","[{'version': 'v1', 'created': 'Tue, 17 Sep 2019 15:14:51 GMT'}]",2020-10-28,"[['Lee', 'Helena', ''], ['Achananuparp', 'Palakorn', ''], ['Liu', 'Yue', ''], ['Lim', 'Ee-Peng', ''], ['Varshney', 'Lav R.', '']]"
1299720,2006.05206,Jayden Macklin-Cordes,"Jayden L. Macklin-Cordes, Erich R. Round",Re-evaluating phoneme frequencies,"29pp (3 figures, 3 tables). This article has been provisionally
  accepted for publication (Frontiers in Psychology, Language Sciences).
  Supplementary information, data and code available at
  http://doi.org/10.5281/zenodo.3886212",,10.3389/fpsyg.2020.570895,,cs.CL physics.soc-ph stat.AP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Causal processes can give rise to distinctive distributions in the linguistic
variables that they affect. Consequently, a secure understanding of a
variable's distribution can hold a key to understanding the forces that have
causally shaped it. A storied distribution in linguistics has been Zipf's law,
a kind of power law. In the wake of a major debate in the sciences around
power-law hypotheses and the unreliability of earlier methods of evaluating
them, here we re-evaluate the distributions claimed to characterize phoneme
frequencies. We infer the fit of power laws and three alternative distributions
to 166 Australian languages, using a maximum likelihood framework. We find
evidence supporting earlier results, but also nuancing them and increasing our
understanding of them. Most notably, phonemic inventories appear to have a
Zipfian-like frequency structure among their most-frequent members (though
perhaps also a lognormal structure) but a geometric (or exponential) structure
among the least-frequent. We compare these new insights the kinds of causal
processes that affect the evolution of phonemic inventories over time, and
identify a potential account for why, despite there being an important role for
phonetic substance in phonemic change, we could still expect inventories with
highly diverse phonetic content to share similar distributions of phoneme
frequencies. We conclude with priorities for future work in this promising
program of research.
","[{'version': 'v1', 'created': 'Tue, 9 Jun 2020 12:05:10 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 03:56:14 GMT'}]",2020-10-28,"[['Macklin-Cordes', 'Jayden L.', ''], ['Round', 'Erich R.', '']]"
1367182,2010.10852,Huy To Quoc,"Huy Quoc To, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen, Anh Gia-Tuan
  Nguyen","Gender Prediction Based on Vietnamese Names with Machine Learning
  Techniques","6 pages, 6 figures",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As biological gender is one of the aspects of presenting individual human,
much work has been done on gender classification based on people names. The
proposals for English and Chinese languages are tremendous; still, there have
been few works done for Vietnamese so far. We propose a new dataset for gender
prediction based on Vietnamese names. This dataset comprises over 26,000 full
names annotated with genders. This dataset is available on our website for
research purposes. In addition, this paper describes six machine learning
algorithms (Support Vector Machine, Multinomial Naive Bayes, Bernoulli Naive
Bayes, Decision Tree, Random Forrest and Logistic Regression) and a deep
learning model (LSTM) with fastText word embedding for gender prediction on
Vietnamese names. We create a dataset and investigate the impact of each name
component on detecting gender. As a result, the best F1-score that we have
achieved is up to 96\% on LSTM model and we generate a web API based on our
trained model.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 09:25:48 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 02:21:32 GMT'}, {'version': 'v3', 'created': 'Tue, 27 Oct 2020 01:29:35 GMT'}]",2020-10-28,"[['To', 'Huy Quoc', ''], ['Van Nguyen', 'Kiet', ''], ['Nguyen', 'Ngan Luu-Thuy', ''], ['Nguyen', 'Anh Gia-Tuan', '']]"
1236866,2001.11453,Edoardo Maria Ponti,"Edoardo M. Ponti, Ivan Vuli\'c, Ryan Cotterell, Marinela Parovic, Roi
  Reichart and Anna Korhonen","Parameter Space Factorization for Zero-Shot Learning across Tasks and
  Languages",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most combinations of NLP tasks and language varieties lack in-domain examples
for supervised training because of the paucity of annotated data. How can
neural models make sample-efficient generalizations from task--language
combinations with available data to low-resource ones? In this work, we propose
a Bayesian generative model for the space of neural parameters. We assume that
this space can be factorized into latent variables for each language and each
task. We infer the posteriors over such latent variables based on data from
seen task--language combinations through variational inference. This enables
zero-shot classification on unseen combinations at prediction time. For
instance, given training data for named entity recognition (NER) in Vietnamese
and for part-of-speech (POS) tagging in Wolof, our model can perform accurate
predictions for NER in Wolof. In particular, we experiment with a typologically
diverse sample of 33 languages from 4 continents and 11 families, and show that
our model yields comparable or better results than state-of-the-art, zero-shot
cross-lingual transfer methods. Moreover, we demonstrate that approximate
Bayesian model averaging results in smoother predictive distributions, whose
entropy strongly correlates with accuracy. Hence, the proposed framework also
offers robust estimates of uncertainty.
","[{'version': 'v1', 'created': 'Thu, 30 Jan 2020 16:58:56 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 23:30:01 GMT'}]",2020-10-28,"[['Ponti', 'Edoardo M.', ''], ['Vulić', 'Ivan', ''], ['Cotterell', 'Ryan', ''], ['Parovic', 'Marinela', ''], ['Reichart', 'Roi', ''], ['Korhonen', 'Anna', '']]"
1304040,2006.09526,Chau Tran,"Chau Tran, Yuqing Tang, Xian Li, Jiatao Gu",Cross-lingual Retrieval for Iterative Self-Supervised Training,,NeurIPS 2020,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have demonstrated the cross-lingual alignment ability of
multilingual pretrained language models. In this work, we found that the
cross-lingual alignment can be further improved by training seq2seq models on
sentence pairs mined using their own encoder outputs. We utilized these
findings to develop a new approach -- cross-lingual retrieval for iterative
self-supervised training (CRISS), where mining and training processes are
applied iteratively, improving cross-lingual alignment and translation ability
at the same time. Using this method, we achieved state-of-the-art unsupervised
machine translation results on 9 language directions with an average
improvement of 2.4 BLEU, and on the Tatoeba sentence retrieval task in the
XTREME benchmark on 16 languages with an average improvement of 21.5% in
absolute accuracy. Furthermore, CRISS also brings an additional 1.8 BLEU
improvement on average compared to mBART, when finetuned on supervised machine
translation downstream tasks.
","[{'version': 'v1', 'created': 'Tue, 16 Jun 2020 21:30:51 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Oct 2020 23:25:31 GMT'}]",2020-10-28,"[['Tran', 'Chau', ''], ['Tang', 'Yuqing', ''], ['Li', 'Xian', ''], ['Gu', 'Jiatao', '']]"
1367362,2010.11032,Leshem Choshen,"Leshem Choshen, Dmitry Nikolaev, Yevgeni Berzak, Omri Abend",Classifying Syntactic Errors in Learner Language,CoNLL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a method for classifying syntactic errors in learner language,
namely errors whose correction alters the morphosyntactic structure of a
sentence.
  The methodology builds on the established Universal Dependencies syntactic
representation scheme, and provides complementary information to other
error-classification systems.
  Unlike existing error classification methods, our method is applicable across
languages, which we showcase by producing a detailed picture of syntactic
errors in learner English and learner Russian. We further demonstrate the
utility of the methodology for analyzing the outputs of leading Grammatical
Error Correction (GEC) systems.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 14:28:22 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 14:58:14 GMT'}]",2020-10-28,"[['Choshen', 'Leshem', ''], ['Nikolaev', 'Dmitry', ''], ['Berzak', 'Yevgeni', ''], ['Abend', 'Omri', '']]"
1370769,2010.14439,Bill Yuchen Lin,"Bill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Xiang
  Ren, William W. Cohen",Differentiable Open-Ended Commonsense Reasoning,Work in progress. Project page: https://yuchenlin.xyz/opencsr/,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current commonsense reasoning research mainly focuses on developing models
that use commonsense knowledge to answer multiple-choice questions. However,
systems designed to answer multiple-choice questions may not be useful in
applications that do not provide a small list of possible candidate answers to
choose from. As a step towards making commonsense reasoning research more
realistic, we propose to study open-ended commonsense reasoning (OpenCSR) --
the task of answering a commonsense question without any pre-defined choices,
using as a resource only a corpus of commonsense facts written in natural
language. The task is challenging due to a much larger decision space, and
because many commonsense questions require multi-hop reasoning. We propose an
efficient differentiable model for multi-hop reasoning over knowledge facts,
named DrFact. We evaluate our approach on a collection of re-formatted,
open-ended versions of popular tests targeting commonsense reasoning, and show
that our approach outperforms strong baseline methods by a large margin.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 10:07:00 GMT'}]",2020-10-28,"[['Lin', 'Bill Yuchen', ''], ['Sun', 'Haitian', ''], ['Dhingra', 'Bhuwan', ''], ['Zaheer', 'Manzil', ''], ['Ren', 'Xiang', ''], ['Cohen', 'William W.', '']]"
1329286,2008.01564,Bruce Lee,"Bruce W. Lee, Jason Hyung-Jong Lee","LXPER Index: a curriculum-specific text readability assessment model for
  EFL students in Korea","8 pages, 2 figures, 7 tables",,10.14569/IJACSA.2020.0110801,,cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Automatic readability assessment is one of the most important applications of
Natural Language Processing (NLP) in education. Since automatic readability
assessment allows the fast selection of appropriate reading material for
readers at all levels of proficiency, it can be particularly useful for the
English education of English as Foreign Language (EFL) students around the
world. Most readability assessment models are developed for the native readers
of English and have low accuracy for texts in the non-native English Language
Training (ELT) curriculum. We introduce LXPER Index, which is a readability
assessment model for non-native EFL readers in the ELT curriculum of Korea. Our
experiments show that our new model, trained with CoKEC-text (Text Corpus of
the Korean ELT Curriculum), significantly improves the accuracy of automatic
readability assessment for texts in the Korean ELT curriculum.
","[{'version': 'v1', 'created': 'Sat, 1 Aug 2020 11:55:03 GMT'}]",2020-10-28,"[['Lee', 'Bruce W.', ''], ['Lee', 'Jason Hyung-Jong', '']]"
1280414,2005.00619,Gabriel Ilharco,"Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi","Probing Contextual Language Models for Common Ground with Visual
  Representations",,,,,cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While large-scale contextual language models have enjoyed great success
recently, much remains to be understood about what is encoded in their
representations. In this work, we characterize how contextual representations
of concrete nouns extracted by trained language models relate to the physical
properties of the objects they refer to. Our approach uses a probing model that
examines how effective these language representations are in discerning between
different visual representations. We show that many recent language models
yield representations that are useful in retrieving semantically aligned image
patches, and explore the role of context in this process. Much weaker results
are found in control experiments, attesting the selectivity of the probe. All
examined models greatly under-perform humans in retrieval, highlighting
substantial room for future progress. Altogether, our findings shed new
empirical insights on language grounding and its materialization in contextual
language models.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 21:28:28 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 17:19:20 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Oct 2020 22:12:40 GMT'}, {'version': 'v4', 'created': 'Tue, 27 Oct 2020 16:40:01 GMT'}]",2020-10-28,"[['Ilharco', 'Gabriel', ''], ['Zellers', 'Rowan', ''], ['Farhadi', 'Ali', ''], ['Hajishirzi', 'Hannaneh', '']]"
1370672,2010.14342,Guanyi Chen,"Guanyi Chen, Yinhe Zheng, Yupei Du",Listener's Social Identity Matters in Personalised Response Generation,Long paper accepted at INLG 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Personalised response generation enables generating human-like responses by
means of assigning the generator a social identity. However, pragmatics theory
suggests that human beings adjust the way of speaking based on not only who
they are but also whom they are talking to. In other words, when modelling
personalised dialogues, it might be favourable if we also take the listener's
social identity into consideration. To validate this idea, we use gender as a
typical example of a social variable to investigate how the listener's identity
influences the language used in Chinese dialogues on social media. Also, we
build personalised generators. The experiment results demonstrate that the
listener's identity indeed matters in the language use of responses and that
the response generator can capture such differences in language use. More
interestingly, by additionally modelling the listener's identity, the
personalised response generator performs better in its own identity.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 14:57:21 GMT'}]",2020-10-28,"[['Chen', 'Guanyi', ''], ['Zheng', 'Yinhe', ''], ['Du', 'Yupei', '']]"
1370434,2010.14104,"Bj\""orn Bebensee","Bj\""orn Bebensee, Byoung-Tak Zhang",Co-attentional Transformers for Story-Based Video Understanding,"10 pages, 2 figures, submitted to ICASSP 2021",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inspired by recent trends in vision and language learning, we explore
applications of attention mechanisms for visio-lingual fusion within an
application to story-based video understanding. Like other video-based QA
tasks, video story understanding requires agents to grasp complex temporal
dependencies. However, as it focuses on the narrative aspect of video it also
requires understanding of the interactions between different characters, as
well as their actions and their motivations. We propose a novel co-attentional
transformer model to better capture long-term dependencies seen in visual
stories such as dramas and measure its performance on the video question
answering task. We evaluate our approach on the recently introduced DramaQA
dataset which features character-centered video story understanding questions.
Our model outperforms the baseline model by 8 percentage points overall, at
least 4.95 and up to 12.8 percentage points on all difficulty levels and
manages to beat the winner of the DramaQA challenge.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 07:17:09 GMT'}]",2020-10-28,"[['Bebensee', 'Björn', ''], ['Zhang', 'Byoung-Tak', '']]"
1370601,2010.14271,Ming Gong,"Junhao Liu, Linjun Shou, Jian Pei, Ming Gong, Min Yang, Daxin Jiang","Cross-lingual Machine Reading Comprehension with Language Branch
  Knowledge Distillation",Accepted as long paper in COLING 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Cross-lingual Machine Reading Comprehension (CLMRC) remains a challenging
problem due to the lack of large-scale annotated datasets in low-source
languages, such as Arabic, Hindi, and Vietnamese. Many previous approaches use
translation data by translating from a rich-source language, such as English,
to low-source languages as auxiliary supervision. However, how to effectively
leverage translation data and reduce the impact of noise introduced by
translation remains onerous. In this paper, we tackle this challenge and
enhance the cross-lingual transferring performance by a novel augmentation
approach named Language Branch Machine Reading Comprehension (LBMRC). A
language branch is a group of passages in one single language paired with
questions in all target languages. We train multiple machine reading
comprehension (MRC) models proficient in individual language based on LBMRC.
Then, we devise a multilingual distillation approach to amalgamate knowledge
from multiple language branch models to a single model for all target
languages. Combining the LBMRC and multilingual distillation can be more robust
to the data noises, therefore, improving the model's cross-lingual ability.
Meanwhile, the produced single multilingual model is applicable to all target
languages, which saves the cost of training, inference, and maintenance for
multiple models. Extensive experiments on two CLMRC benchmarks clearly show the
effectiveness of our proposed method.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 13:12:17 GMT'}]",2020-10-28,"[['Liu', 'Junhao', ''], ['Shou', 'Linjun', ''], ['Pei', 'Jian', ''], ['Gong', 'Ming', ''], ['Yang', 'Min', ''], ['Jiang', 'Daxin', '']]"
1370144,2010.13814,Constantin Orasan,"Hadeel Saadany, Constantin Orasan","Is it Great or Terrible? Preserving Sentiment in Neural Machine
  Translation of Arabic Reviews",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Since the advent of Neural Machine Translation (NMT) approaches there has
been a tremendous improvement in the quality of automatic translation. However,
NMT output still lacks accuracy in some low-resource languages and sometimes
makes major errors that need extensive post-editing. This is particularly
noticeable with texts that do not follow common lexico-grammatical standards,
such as user generated content (UGC). In this paper we investigate the
challenges involved in translating book reviews from Arabic into English, with
particular focus on the errors that lead to incorrect translation of sentiment
polarity. Our study points to the special characteristics of Arabic UGC,
examines the sentiment transfer errors made by Google Translate of Arabic UGC
to English, analyzes why the problem occurs, and proposes an error typology
specific of the translation of Arabic UGC. Our analysis shows that the output
of online translation tools of Arabic UGC can either fail to transfer the
sentiment at all by producing a neutral target text, or completely flips the
sentiment polarity of the target word or phrase and hence delivers a wrong
affect message. We address this problem by fine-tuning an NMT model with
respect to sentiment polarity showing that this approach can significantly help
with correcting sentiment errors detected in the online translation of Arabic
UGC.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 18:01:52 GMT'}]",2020-10-28,"[['Saadany', 'Hadeel', ''], ['Orasan', 'Constantin', '']]"
1370169,2010.13839,Subhajit Chaudhury,"Thomas Carta, Subhajit Chaudhury, Kartik Talamadupula and Michiaki
  Tatsubori","VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement
  Learning",Code is available at http://ibm.biz/VisualHints,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present VisualHints, a novel environment for multimodal reinforcement
learning (RL) involving text-based interactions along with visual hints
(obtained from the environment). Real-life problems often demand that agents
interact with the environment using both natural language information and
visual perception towards solving a goal. However, most traditional RL
environments either solve pure vision-based tasks like Atari games or
video-based robotic manipulation; or entirely use natural language as a mode of
interaction, like Text-based games and dialog systems. In this work, we aim to
bridge this gap and unify these two approaches in a single environment for
multimodal RL. We introduce an extension of the TextWorld cooking environment
with the addition of visual clues interspersed throughout the environment. The
goal is to force an RL agent to use both text and visual features to predict
natural language action commands for solving the final task of cooking a meal.
We enable variations and difficulties in our environment to emulate various
interactive real-world scenarios. We present a baseline multimodal agent for
solving such problems using CNN-based feature extraction from visual hints and
LSTMs for textual feature extraction. We believe that our proposed
visual-lingual environment will facilitate novel problem settings for the RL
community.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 18:51:02 GMT'}]",2020-10-28,"[['Carta', 'Thomas', ''], ['Chaudhury', 'Subhajit', ''], ['Talamadupula', 'Kartik', ''], ['Tatsubori', 'Michiaki', '']]"
1370186,2010.13856,Ciprian Chelba,"Ciprian Chelba, Junpei Zhou, Yuezhang (Music) Li, Hideto Kazawa, Jeff
  Klingner, Mengmeng Niu","Data Troubles in Sentence Level Confidence Estimation for Machine
  Translation",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The paper investigates the feasibility of confidence estimation for neural
machine translation models operating at the high end of the performance
spectrum. As a side product of the data annotation process necessary for
building such models we propose sentence level accuracy $SACC$ as a simple,
self-explanatory evaluation metric for quality of translation.
  Experiments on two different annotator pools, one comprised of non-expert
(crowd-sourced) and one of expert (professional) translators show that $SACC$
can vary greatly depending on the translation proficiency of the annotators,
despite the fact that both pools are about equally reliable according to
Krippendorff's alpha metric; the relatively low values of inter-annotator
agreement confirm the expectation that sentence-level binary labeling $good$ /
$needs\ work$ for translation out of context is very hard.
  For an English-Spanish translation model operating at $SACC = 0.89$ according
to a non-expert annotator pool we can derive a confidence estimate that labels
0.5-0.6 of the $good$ translations in an ""in-domain"" test set with 0.95
Precision. Switching to an expert annotator pool decreases $SACC$ dramatically:
$0.61$ for English-Spanish, measured on the exact same data as above. This
forces us to lower the CE model operating point to 0.9 Precision while labeling
correctly about 0.20-0.25 of the $good$ translations in the data.
  We find surprising the extent to which CE depends on the level of proficiency
of the annotator pool used for labeling the data. This leads to an important
recommendation we wish to make when tackling CE modeling in practice: it is
critical to match the end-user expectation for translation quality in the
desired domain with the demands of annotators assigning binary quality labels
to CE training data.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 19:20:29 GMT'}]",2020-10-28,"[['Chelba', 'Ciprian', '', 'Music'], ['Zhou', 'Junpei', '', 'Music'], ['Yuezhang', '', '', 'Music'], ['Li', '', ''], ['Kazawa', 'Hideto', ''], ['Klingner', 'Jeff', ''], ['Niu', 'Mengmeng', '']]"
1256634,2003.06279,Diego Amancio,Laura V. C. Quispe and Jorge A. V. Tohalino and Diego R. Amancio,"Using word embeddings to improve the discriminability of co-occurrence
  text networks",,,10.1016/j.physa.2020.125344,,cs.CL cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Word co-occurrence networks have been employed to analyze texts both in the
practical and theoretical scenarios. Despite the relative success in several
applications, traditional co-occurrence networks fail in establishing links
between similar words whenever they appear distant in the text. Here we
investigate whether the use of word embeddings as a tool to create virtual
links in co-occurrence networks may improve the quality of classification
systems. Our results revealed that the discriminability in the stylometry task
is improved when using Glove, Word2Vec and FastText. In addition, we found that
optimized results are obtained when stopwords are not disregarded and a simple
global thresholding strategy is used to establish virtual links. Because the
proposed approach is able to improve the representation of texts as complex
networks, we believe that it could be extended to study other natural language
processing tasks. Likewise, theoretical languages studies could benefit from
the adopted enriched representation of word co-occurrence networks.
","[{'version': 'v1', 'created': 'Fri, 13 Mar 2020 13:35:44 GMT'}]",2020-10-28,"[['Quispe', 'Laura V. C.', ''], ['Tohalino', 'Jorge A. V.', ''], ['Amancio', 'Diego R.', '']]"
1369939,2010.13609,Dumitru-Clementin Cercel,"Mircea-Adrian Tanase, Dumitru-Clementin Cercel and Costin-Gabriel
  Chiru","UPB at SemEval-2020 Task 12: Multilingual Offensive Language Detection
  on Social Media by Fine-tuning a Variety of BERT-based Models",,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Offensive language detection is one of the most challenging problem in the
natural language processing field, being imposed by the rising presence of this
phenomenon in online social media. This paper describes our Transformer-based
solutions for identifying offensive language on Twitter in five languages
(i.e., English, Arabic, Danish, Greek, and Turkish), which was employed in
Subtask A of the Offenseval 2020 shared task. Several neural architectures
(i.e., BERT, mBERT, Roberta, XLM-Roberta, and ALBERT), pre-trained using both
single-language and multilingual corpora, were fine-tuned and compared using
multiple combinations of datasets. Finally, the highest-scoring models were
used for our submissions in the competition, which ranked our team 21st of 85,
28th of 53, 19th of 39, 16th of 37, and 10th of 46 for English, Arabic, Danish,
Greek, and Turkish, respectively.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 14:28:29 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 09:21:21 GMT'}]",2020-10-28,"[['Tanase', 'Mircea-Adrian', ''], ['Cercel', 'Dumitru-Clementin', ''], ['Chiru', 'Costin-Gabriel', '']]"
1370200,2010.13870,Leon Bergen,"Charles Yu, Ryan Sie, Nico Tedeschi, Leon Bergen",Word Frequency Does Not Predict Grammatical Knowledge in Language Models,EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Neural language models learn, to varying degrees of accuracy, the grammatical
properties of natural languages. In this work, we investigate whether there are
systematic sources of variation in the language models' accuracy. Focusing on
subject-verb agreement and reflexive anaphora, we find that certain nouns are
systematically understood better than others, an effect which is robust across
grammatical tasks and different language models. Surprisingly, we find that
across four orders of magnitude, corpus frequency is unrelated to a noun's
performance on grammatical tasks. Finally, we find that a novel noun's
grammatical properties can be few-shot learned from various types of training
data. The results present a paradox: there should be less variation in
grammatical performance than is actually observed.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 19:51:36 GMT'}]",2020-10-28,"[['Yu', 'Charles', ''], ['Sie', 'Ryan', ''], ['Tedeschi', 'Nico', ''], ['Bergen', 'Leon', '']]"
1370208,2010.13878,Suyoun Kim,"Suyoun Kim, Yuan Shangguan, Jay Mahadeokar, Antoine Bruguier,
  Christian Fuegen, Michael L. Seltzer, Duc Le","Improved Neural Language Model Fusion for Streaming Recurrent Neural
  Network Transducer",submitted to ICASSP 2021,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recurrent Neural Network Transducer (RNN-T), like most end-to-end speech
recognition model architectures, has an implicit neural network language model
(NNLM) and cannot easily leverage unpaired text data during training. Previous
work has proposed various fusion methods to incorporate external NNLMs into
end-to-end ASR to address this weakness. In this paper, we propose extensions
to these techniques that allow RNN-T to exploit external NNLMs during both
training and inference time, resulting in 13-18% relative Word Error Rate
improvement on Librispeech compared to strong baselines. Furthermore, our
methods do not incur extra algorithmic latency and allow for flexible
plug-and-play of different NNLMs without re-training. We also share in-depth
analysis to better understand the benefits of the different NNLM fusion
methods. Our work provides a reliable technique for leveraging unpaired text
data to significantly improve RNN-T while keeping the system streamable,
flexible, and lightweight.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 20:10:12 GMT'}]",2020-10-28,"[['Kim', 'Suyoun', ''], ['Shangguan', 'Yuan', ''], ['Mahadeokar', 'Jay', ''], ['Bruguier', 'Antoine', ''], ['Fuegen', 'Christian', ''], ['Seltzer', 'Michael L.', ''], ['Le', 'Duc', '']]"
1370242,2010.13912,Chien-Sheng Wu,Chien-Sheng Wu and Caiming Xiong,Probing Task-Oriented Dialogue Representation from Language Models,EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper investigates pre-trained language models to find out which model
intrinsically carries the most informative representation for task-oriented
dialogue tasks. We approach the problem from two aspects: supervised classifier
probe and unsupervised mutual information probe. We fine-tune a feed-forward
layer as the classifier probe on top of a fixed pre-trained language model with
annotated labels in a supervised way. Meanwhile, we propose an unsupervised
mutual information probe to evaluate the mutual dependence between a real
clustering and a representation clustering. The goals of this empirical paper
are to 1) investigate probing techniques, especially from the unsupervised
mutual information aspect, 2) provide guidelines of pre-trained language model
selection for the dialogue research community, 3) find insights of pre-training
factors for dialogue application that may be the key to success.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 21:34:39 GMT'}]",2020-10-28,"[['Wu', 'Chien-Sheng', ''], ['Xiong', 'Caiming', '']]"
1370648,2010.14318,Peidong Wang,"Peidong Wang, Tara N. Sainath, Ron J. Weiss",Multitask Training with Text Data for End-to-End Speech Recognition,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a multitask training method for attention-based end-to-end speech
recognition models to better incorporate language level information. We
regularize the decoder in a sequence-to-sequence architecture by multitask
training it on both the speech recognition task and a next-token prediction
language modeling task. Trained on either the 100 hour subset of LibriSpeech or
the full 960 hour dataset, the proposed method leads to an 11% relative
performance improvement over the baseline and is comparable to language model
shallow fusion, without requiring an additional neural network during decoding.
Analyses of sample output sentences and the word error rate on rare words
demonstrate that the proposed method can incorporate language level information
effectively.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 14:29:28 GMT'}]",2020-10-28,"[['Wang', 'Peidong', ''], ['Sainath', 'Tara N.', ''], ['Weiss', 'Ron J.', '']]"
1252853,2003.02498,Palakorn Achananuparp,"Helena H. Lee, Ke Shu, Palakorn Achananuparp, Philips Kokoh Prasetyo,
  Yue Liu, Ee-Peng Lim, Lav R. Varshney","RecipeGPT: Generative Pre-training Based Cooking Recipe Generation and
  Evaluation System",Accepted to WWW 2020. Demo track paper,,10.1145/3366424.3383536,,cs.CL cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Interests in the automatic generation of cooking recipes have been growing
steadily over the past few years thanks to a large amount of online cooking
recipes. We present RecipeGPT, a novel online recipe generation and evaluation
system. The system provides two modes of text generations: (1) instruction
generation from given recipe title and ingredients; and (2) ingredient
generation from recipe title and cooking instructions. Its back-end text
generation module comprises a generative pre-trained language model GPT-2
fine-tuned on a large cooking recipe dataset. Moreover, the recipe evaluation
module allows the users to conveniently inspect the quality of the generated
recipe contents and store the results for future reference. RecipeGPT can be
accessed online at https://recipegpt.org/.
","[{'version': 'v1', 'created': 'Thu, 5 Mar 2020 09:25:30 GMT'}]",2020-10-28,"[['Lee', 'Helena H.', ''], ['Shu', 'Ke', ''], ['Achananuparp', 'Palakorn', ''], ['Prasetyo', 'Philips Kokoh', ''], ['Liu', 'Yue', ''], ['Lim', 'Ee-Peng', ''], ['Varshney', 'Lav R.', '']]"
1370250,2010.13920,Chien-Sheng Wu,Chien-Sheng Wu and Steven Hoi and Caiming Xiong,Improving Limited Labeled Dialogue State Tracking with Self-Supervision,EMNLP 2020 (findings),,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing dialogue state tracking (DST) models require plenty of labeled data.
However, collecting high-quality labels is costly, especially when the number
of domains increases. In this paper, we address a practical DST problem that is
rarely discussed, i.e., learning efficiently with limited labeled data. We
present and investigate two self-supervised objectives: preserving latent
consistency and modeling conversational behavior. We encourage a DST model to
have consistent latent distributions given a perturbed input, making it more
robust to an unseen scenario. We also add an auxiliary utterance generation
task, modeling a potential correlation between conversational behavior and
dialogue states. The experimental results show that our proposed
self-supervised signals can improve joint goal accuracy by 8.95\% when only 1\%
labeled data is used on the MultiWOZ dataset. We can achieve an additional
1.76\% improvement if some unlabeled data is jointly trained as semi-supervised
learning. We analyze and visualize how our proposed self-supervised signals
help the DST task and hope to stimulate future data-efficient DST research.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 21:57:42 GMT'}]",2020-10-28,"[['Wu', 'Chien-Sheng', ''], ['Hoi', 'Steven', ''], ['Xiong', 'Caiming', '']]"
1370274,2010.13944,Khyathi Raghavi Chandu,"Khyathi Raghavi Chandu, Ruo-Ping Dong, Alan Black",Reading Between the Lines: Exploring Infilling in Visual Narratives,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating long form narratives such as stories and procedures from multiple
modalities has been a long standing dream for artificial intelligence. In this
regard, there is often crucial subtext that is derived from the surrounding
contexts. The general seq2seq training methods render the models shorthanded
while attempting to bridge the gap between these neighbouring contexts. In this
paper, we tackle this problem by using \textit{infilling} techniques involving
prediction of missing steps in a narrative while generating textual
descriptions from a sequence of images. We also present a new large scale
\textit{visual procedure telling} (ViPT) dataset with a total of 46,200
procedures and around 340k pairwise images and textual descriptions that is
rich in such contextual dependencies. Generating steps using infilling
technique demonstrates the effectiveness in visual procedures with more
coherent texts. We conclusively show a METEOR score of 27.51 on procedures
which is higher than the state-of-the-art on visual storytelling. We also
demonstrate the effects of interposing new text with missing images during
inference. The code and the dataset will be publicly available at
https://visual-narratives.github.io/Visual-Narratives/.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 23:09:09 GMT'}]",2020-10-28,"[['Chandu', 'Khyathi Raghavi', ''], ['Dong', 'Ruo-Ping', ''], ['Black', 'Alan', '']]"
1370312,2010.13982,Hung-Ting Chen,"Hung-Ting Chen, Yu-Chieh Chao, Ta-Hsuan Chao, Wei-Yun Ma",Predict and Use Latent Patterns for Short-Text Conversation,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many neural network models nowadays have achieved promising performances in
Chit-chat settings. The majority of them rely on an encoder for understanding
the post and a decoder for generating the response. Without given assigned
semantics, the models lack the fine-grained control over responses as the
semantic mapping between posts and responses is hidden on the fly within the
end-to-end manners. Some previous works utilize sampled latent words as a
controllable semantic form to drive the generated response around the work, but
few works attempt to use more complex semantic forms to guide the generation.
In this paper, we propose to use more detailed semantic forms, including latent
responses and part-of-speech sequences sampled from the corresponding
distributions, as the controllable semantics to guide the generation. Our
experimental results show that the richer semantics are not only able to
provide informative and diverse responses, but also increase the overall
performance of response quality, including fluency and coherence.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 01:31:42 GMT'}]",2020-10-28,"[['Chen', 'Hung-Ting', ''], ['Chao', 'Yu-Chieh', ''], ['Chao', 'Ta-Hsuan', ''], ['Ma', 'Wei-Yun', '']]"
1370146,2010.13816,Maarten Sap,"Xinyao Ma, Maarten Sap, Hannah Rashkin, Yejin Choi","PowerTransformer: Unsupervised Controllable Revision for Biased Language
  Correction",EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unconscious biases continue to be prevalent in modern text and media, calling
for algorithms that can assist writers with bias correction. For example, a
female character in a story is often portrayed as passive and powerless (""She
daydreams about being a doctor"") while a man is portrayed as more proactive and
powerful (""He pursues his dream of being a doctor"").
  We formulate *Controllable Debiasing*, a new revision task that aims to
rewrite a given text to correct the implicit and potentially undesirable bias
in character portrayals. We then introduce PowerTransformer as an approach that
debiases text through the lens of connotation frames (Sap et al., 2017), which
encode pragmatic knowledge of implied power dynamics with respect to verb
predicates. One key challenge of our task is the lack of parallel corpora. To
address this challenge, we adopt an unsupervised approach using auxiliary
supervision with related tasks such as paraphrasing and self-supervision based
on a reconstruction loss, building on pretrained language models.
  Through comprehensive experiments based on automatic and human evaluations,
we demonstrate that our approach outperforms ablations and existing methods
from related tasks. Furthermore, we demonstrate the use of PowerTransformer as
a step toward mitigating the well-documented gender bias in character portrayal
in movie scripts.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 18:05:48 GMT'}]",2020-10-28,"[['Ma', 'Xinyao', ''], ['Sap', 'Maarten', ''], ['Rashkin', 'Hannah', ''], ['Choi', 'Yejin', '']]"
1369704,2010.13374,Bruce W. Lee,Bruce W. Lee and Jason Hyung-Jong Lee,"LXPER Index 2.0: Improving Text Readability Assessment for L2 English
  Learners in South Korea","NLP-TEA, Association for Computational Linguistics",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Most text readability assessment models are developed for the native readers
of English and have low accuracy for texts in foreign English Language Training
(ELT) curriculum. In this paper, we investigate a text readability assessment
model for L2 English learners in Korea. In accordance, we improve and expand
the Text Corpus of the Korean ELT curriculum (CoKEC-text). Each text is labeled
with its target grade level. We train our model with CoKEC-text and
significantly improve the accuracy of readability assessment for texts in the
Korean ELT curriculum.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 07:03:14 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 17:04:03 GMT'}]",2020-10-28,"[['Lee', 'Bruce W.', ''], ['Lee', 'Jason Hyung-Jong', '']]"
1159525,1908.01355,Johan Bos,Johan Bos,Separating Argument Structure from Logical Structure in AMR,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The AMR (Abstract Meaning Representation) formalism for representing meaning
of natural language sentences was not designed to deal with scope and
quantifiers. By extending AMR with indices for contexts and formulating
constraints on these contexts, a formalism is derived that makes correct
prediction for inferences involving negation and bound variables. The
attractive core predicate-argument structure of AMR is preserved. The resulting
framework is similar to that of Discourse Representation Theory.
","[{'version': 'v1', 'created': 'Sun, 4 Aug 2019 14:46:35 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 14:54:01 GMT'}]",2020-10-28,"[['Bos', 'Johan', '']]"
1369424,2010.13094,Masahiro Kaneko,Masahiro Kaneko and Danushka Bollegala,Autoencoding Improves Pre-trained Word Embeddings,COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Prior work investigating the geometry of pre-trained word embeddings have
shown that word embeddings to be distributed in a narrow cone and by centering
and projecting using principal component vectors one can increase the accuracy
of a given set of pre-trained word embeddings. However, theoretically, this
post-processing step is equivalent to applying a linear autoencoder to minimise
the squared l2 reconstruction error. This result contradicts prior work (Mu and
Viswanath, 2018) that proposed to remove the top principal components from
pre-trained embeddings. We experimentally verify our theoretical claims and
show that retaining the top principal components is indeed useful for improving
pre-trained word embeddings, without requiring access to additional linguistic
resources or labelled data.
","[{'version': 'v1', 'created': 'Sun, 25 Oct 2020 11:30:05 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 07:51:34 GMT'}]",2020-10-28,"[['Kaneko', 'Masahiro', ''], ['Bollegala', 'Danushka', '']]"
1370585,2010.14255,Jianing Wang,Jianing Wang and Chong Su,"Improving Reinforcement Learning for Neural Relation Extraction with
  Hierarchical Memory Extractor","9 pages, 7 figures, WWW2021 submission paper",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Distant supervision relation extraction (DSRE) is an efficient method to
extract semantic relations on a large-scale heuristic labeling corpus. However,
it usually brings in a massive noisy data. In order to alleviate this problem,
many recent approaches adopt reinforcement learning (RL), which aims to select
correct data autonomously before relation classification. Although these RL
methods outperform conventional multi-instance learning-based methods, there
are still two neglected problems: 1) the existing RL methods ignore the
feedback of noisy data, 2) the reduction of training corpus exacerbates
long-tail problem. In this paper, we propose a novel framework to solve the two
problems mentioned above. Firstly, we design a novel reward function to obtain
feedback from both correct and noisy data. In addition, we use implicit
relations information to improve RL. Secondly, we propose the hierarchical
memory extractor (HME), which utilizes the gating mechanism to share the
semantics from correlative instances between data-rich and data-poor classes.
Moreover, we define a hierarchical weighted ranking loss function to implement
top-down search processing. Extensive experiments conducted on the widely used
NYT dataset show significant improvement over state-of-the-art baseline
methods.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 12:50:27 GMT'}]",2020-10-28,"[['Wang', 'Jianing', ''], ['Su', 'Chong', '']]"
1370565,2010.14235,Yao Lu,"Yao Lu, Yue Dong, Laurent Charlin","Multi-XScience: A Large-scale Dataset for Extreme Multi-document
  Summarization of Scientific Articles",EMNLP 2020,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-document summarization is a challenging task for which there exists
little large-scale datasets. We propose Multi-XScience, a large-scale
multi-document summarization dataset created from scientific articles.
Multi-XScience introduces a challenging multi-document summarization task:
writing the related-work section of a paper based on its abstract and the
articles it references. Our work is inspired by extreme summarization, a
dataset construction protocol that favours abstractive modeling approaches.
Descriptive statistics and empirical results---using several state-of-the-art
models trained on the Multi-XScience dataset---reveal that Multi-XScience is
well suited for abstractive models.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 12:10:19 GMT'}]",2020-10-28,"[['Lu', 'Yao', ''], ['Dong', 'Yue', ''], ['Charlin', 'Laurent', '']]"
1370564,2010.14234,Muvazima Mansoor,"Muvazima Mansoor, Kirthika Gurumurthy, Anantharam R U, V R Badri
  Prasad",Global Sentiment Analysis Of COVID-19 Tweets Over Time,"7 pages, 20 figures, Submitted to ICDSBDA 2020",,,,cs.CL cs.LG cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Coronavirus pandemic has affected the normal course of life. People
around the world have taken to social media to express their opinions and
general emotions regarding this phenomenon that has taken over the world by
storm. The social networking site, Twitter showed an unprecedented increase in
tweets related to the novel Coronavirus in a very short span of time. This
paper presents the global sentiment analysis of tweets related to Coronavirus
and how the sentiment of people in different countries has changed over time.
Furthermore, to determine the impact of Coronavirus on daily aspects of life,
tweets related to Work From Home (WFH) and Online Learning were scraped and the
change in sentiment over time was observed. In addition, various Machine
Learning models such as Long Short Term Memory (LSTM) and Artificial Neural
Networks (ANN) were implemented for sentiment classification and their
accuracies were determined. Exploratory data analysis was also performed for a
dataset providing information about the number of confirmed cases on a per-day
basis in a few of the worst-hit countries to provide a comparison between the
change in sentiment with the change in cases since the start of this pandemic
till June 2020.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 12:10:10 GMT'}]",2020-10-28,"[['Mansoor', 'Muvazima', ''], ['Gurumurthy', 'Kirthika', ''], ['U', 'Anantharam R', ''], ['Prasad', 'V R Badri', '']]"
1020672,1809.00656,Larry Moss,Alex Kruckman and Lawrence S. Moss,Exploring the Landscape of Relational Syllogistic Logics,,,10.1017/S1755020320000386,,math.LO cs.CL cs.LO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper explores relational syllogistic logics, a family of logical
systems related to reasoning about relations in extensions of the classical
syllogistic. These are all decidable logical systems. We prove completeness
theorems and complexity results for a natural subfamily of relational
syllogistic logics, parametrized by constructors for terms and for sentences.
","[{'version': 'v1', 'created': 'Mon, 3 Sep 2018 16:57:54 GMT'}]",2020-10-28,"[['Kruckman', 'Alex', ''], ['Moss', 'Lawrence S.', '']]"
1370563,2010.14233,Ethan Chi,"Ethan A. Chi, Julian Salazar, and Katrin Kirchhoff","Align-Refine: Non-Autoregressive Speech Recognition via Iterative
  Realignment",,,,,eess.AS cs.CL cs.LG cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Non-autoregressive models greatly improve decoding speed over typical
sequence-to-sequence models, but suffer from degraded performance. Infilling
and iterative refinement models make up some of this gap by editing the outputs
of a non-autoregressive model, but are constrained in the edits that they can
make. We propose iterative realignment, where refinements occur over latent
alignments rather than output sequence space. We demonstrate this in speech
recognition with Align-Refine, an end-to-end Transformer-based model which
refines connectionist temporal classification (CTC) alignments to allow
length-changing insertions and deletions. Align-Refine outperforms Imputer and
Mask-CTC, matching an autoregressive baseline on WSJ at 1/14th the real-time
factor and attaining a LibriSpeech test-other WER of 9.0% without an LM. Our
model is strong even in one iteration with a shallower decoder.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 09:35:37 GMT'}]",2020-10-28,"[['Chi', 'Ethan A.', ''], ['Salazar', 'Julian', ''], ['Kirchhoff', 'Katrin', '']]"
831602,1703.08098,Dat Quoc Nguyen,Dat Quoc Nguyen,"A survey of embedding models of entities and relationships for knowledge
  graph completion","In Proceedings of the 14th Workshop on Graph-Based Natural Language
  Processing (TextGraphs 2020); 16 pages, 2 figures, 6 tables",,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graphs (KGs) of real-world facts about entities and their
relationships are useful resources for a variety of natural language processing
tasks. However, because knowledge graphs are typically incomplete, it is useful
to perform knowledge graph completion or link prediction, i.e. predict whether
a relationship not in the knowledge graph is likely to be true. This paper
serves as a comprehensive survey of embedding models of entities and
relationships for knowledge graph completion, summarizing up-to-date
experimental results on standard benchmark datasets and pointing out potential
future research directions.
","[{'version': 'v1', 'created': 'Thu, 23 Mar 2017 15:15:26 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Mar 2017 15:28:08 GMT'}, {'version': 'v3', 'created': 'Sat, 3 Feb 2018 04:39:45 GMT'}, {'version': 'v4', 'created': 'Tue, 9 Apr 2019 02:26:26 GMT'}, {'version': 'v5', 'created': 'Sat, 27 Apr 2019 13:33:30 GMT'}, {'version': 'v6', 'created': 'Fri, 28 Feb 2020 07:06:36 GMT'}, {'version': 'v7', 'created': 'Wed, 22 Apr 2020 11:58:35 GMT'}, {'version': 'v8', 'created': 'Mon, 10 Aug 2020 08:35:07 GMT'}, {'version': 'v9', 'created': 'Tue, 27 Oct 2020 04:11:25 GMT'}]",2020-10-28,"[['Nguyen', 'Dat Quoc', '']]"
1370453,2010.14123,Viet Lai,"Viet Dac Lai, Tuan Ngo Nguyen, Thien Huu Nguyen","Event Detection: Gate Diversity and Syntactic Importance Scoresfor Graph
  Convolution Neural Networks",EMNLP 2020,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Recent studies on event detection (ED) haveshown that the syntactic
dependency graph canbe employed in graph convolution neural net-works (GCN) to
achieve state-of-the-art per-formance. However, the computation of thehidden
vectors in such graph-based models isagnostic to the trigger candidate words,
po-tentially leaving irrelevant information for thetrigger candidate for event
prediction. In addi-tion, the current models for ED fail to exploitthe overall
contextual importance scores of thewords, which can be obtained via the
depen-dency tree, to boost the performance. In thisstudy, we propose a novel
gating mechanismto filter noisy information in the hidden vec-tors of the GCN
models for ED based on theinformation from the trigger candidate. Wealso
introduce novel mechanisms to achievethe contextual diversity for the gates and
theimportance score consistency for the graphsand models in ED. The experiments
show thatthe proposed model achieves state-of-the-artperformance on two ED
datasets
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 08:28:28 GMT'}]",2020-10-28,"[['Lai', 'Viet Dac', ''], ['Nguyen', 'Tuan Ngo', ''], ['Nguyen', 'Thien Huu', '']]"
1370314,2010.13984,Siwon Kim,"Siwon Kim, Jihun Yi, Eunji Kim, and Sungroh Yoon",Interpretation of NLP models through input marginalization,"10 pages, 5 figures, to be published in the 2020 Conference on
  Empirical Methods in Natural Language Processing (EMNLP 2020)",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To demystify the ""black box"" property of deep neural networks for natural
language processing (NLP), several methods have been proposed to interpret
their predictions by measuring the change in prediction probability after
erasing each token of an input. Since existing methods replace each token with
a predefined value (i.e., zero), the resulting sentence lies out of the
training data distribution, yielding misleading interpretations. In this study,
we raise the out-of-distribution problem induced by the existing interpretation
methods and present a remedy; we propose to marginalize each token out. We
interpret various NLP models trained for sentiment analysis and natural
language inference using the proposed method.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 01:40:41 GMT'}]",2020-10-28,"[['Kim', 'Siwon', ''], ['Yi', 'Jihun', ''], ['Kim', 'Eunji', ''], ['Yoon', 'Sungroh', '']]"
1370156,2010.13826,Cheng-I Lai,"Cheng-I Lai, Yung-Sung Chuang, Hung-Yi Lee, Shang-Wen Li, James Glass","Semi-Supervised Spoken Language Understanding via Self-Supervised Speech
  and Language Model Pretraining",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Much recent work on Spoken Language Understanding (SLU) is limited in at
least one of three ways: models were trained on oracle text input and neglected
ASR errors, models were trained to predict only intents without the slot
values, or models were trained on a large amount of in-house data. In this
paper, we propose a clean and general framework to learn semantics directly
from speech with semi-supervision from transcribed or untranscribed speech to
address these issues. Our framework is built upon pretrained end-to-end (E2E)
ASR and self-supervised language models, such as BERT, and fine-tuned on a
limited amount of target SLU data. We study two semi-supervised settings for
the ASR component: supervised pretraining on transcribed speech, and
unsupervised pretraining by replacing the ASR encoder with self-supervised
speech representations, such as wav2vec. In parallel, we identify two essential
criteria for evaluating SLU models: environmental noise-robustness and E2E
semantics evaluation. Experiments on ATIS show that our SLU framework with
speech as input can perform on par with those using oracle text as input in
semantics understanding, even though environmental noise is present and a
limited amount of labeled semantics data is available for training.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 18:21:27 GMT'}]",2020-10-28,"[['Lai', 'Cheng-I', ''], ['Chuang', 'Yung-Sung', ''], ['Lee', 'Hung-Yi', ''], ['Li', 'Shang-Wen', ''], ['Glass', 'James', '']]"
1370432,2010.14102,Wen Wu,"Wen Wu, Chao Zhang, Philip C. Woodland","Emotion recognition by fusing time synchronous and time asynchronous
  representations",,,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, a novel two-branch neural network model structure is proposed
for multimodal emotion recognition, which consists of a time synchronous branch
(TSB) and a time asynchronous branch (TAB). To capture correlations between
each word and its acoustic realisation, the TSB combines speech and text
modalities at each input window frame and then does pooling across time to form
a single embedding vector. The TAB, by contrast, provides cross-utterance
information by integrating sentence text embeddings from a number of context
utterances into another embedding vector. The final emotion classification uses
both the TSB and the TAB embeddings. Experimental results on the IEMOCAP
dataset demonstrate that the two-branch structure achieves state-of-the-art
results in 4-way classification with all common test setups. When using
automatic speech recognition (ASR) output instead of manually transcribed
reference text, it is shown that the cross-utterance information considerably
improves the robustness against ASR errors. Furthermore, by incorporating an
extra class for all the other emotions, the final 5-way classification system
with ASR hypotheses can be viewed as a prototype for more realistic emotion
recognition systems.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 07:14:31 GMT'}]",2020-10-28,"[['Wu', 'Wen', ''], ['Zhang', 'Chao', ''], ['Woodland', 'Philip C.', '']]"
1370391,2010.14061,Yan Zeng,Yan Zeng and Jian-Yun Nie,"Multi-Domain Dialogue State Tracking -- A Purely Transformer-Based
  Generative Approach","[v0], 8 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:2010.11137",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate the problem of multi-domain Dialogue State Tracking (DST) with
open vocabulary. Existing approaches exploit BERT encoder and copy-based RNN
decoder, where the encoder first predicts the state operation, and then the
decoder generates new slot values. However, in this stacked encoder-decoder
structure, the operation prediction objective only affects the BERT encoder and
the value generation objective mainly affects the RNN decoder. In this paper,
we propose a purely Transformer-based framework that uses BERT as both encoder
and decoder. In so doing, the operation prediction objective and the value
generation objective can jointly optimize our model for DST. At the decoding
step, we re-use the hidden states of the encoder in the self-attention
mechanism of the corresponding decoder layer to construct a flat model
structure for effective parameter updating. Experimental results show that our
approach substantially outperforms the existing state-of-the-art framework, and
it also achieves very competitive performance to the best ontology-based
approaches.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 04:54:52 GMT'}]",2020-10-28,"[['Zeng', 'Yan', ''], ['Nie', 'Jian-Yun', '']]"
1370372,2010.14042,Kasturi Bhattacharjee,"Kasturi Bhattacharjee, Miguel Ballesteros, Rishita Anubhai, Smaranda
  Muresan, Jie Ma, Faisal Ladhak, Yaser Al-Onaizan","To BERT or Not to BERT: Comparing Task-specific and Task-agnostic
  Semi-Supervised Approaches for Sequence Tagging","Accepted in the Proceedings of 2020 Conference on Empirical Methods
  in Natural Language Processing (EMNLP
  2020)(https://2020.emnlp.org/papers/main)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Leveraging large amounts of unlabeled data using Transformer-like
architectures, like BERT, has gained popularity in recent times owing to their
effectiveness in learning general representations that can then be further
fine-tuned for downstream tasks to much success. However, training these models
can be costly both from an economic and environmental standpoint. In this work,
we investigate how to effectively use unlabeled data: by exploring the
task-specific semi-supervised approach, Cross-View Training (CVT) and comparing
it with task-agnostic BERT in multiple settings that include domain and task
relevant English data. CVT uses a much lighter model architecture and we show
that it achieves similar performance to BERT on a set of sequence tagging
tasks, with lesser financial and environmental impact.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 04:03:47 GMT'}]",2020-10-28,"[['Bhattacharjee', 'Kasturi', ''], ['Ballesteros', 'Miguel', ''], ['Anubhai', 'Rishita', ''], ['Muresan', 'Smaranda', ''], ['Ma', 'Jie', ''], ['Ladhak', 'Faisal', ''], ['Al-Onaizan', 'Yaser', '']]"
1370359,2010.14029,Runxin Xu,"Runxin Xu, Zhuo Zhi, Jun Cao, Mingxuan Wang, Lei Li",Volctrans Parallel Corpus Filtering System for WMT 2020,WMT 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we describe our submissions to the WMT20 shared task on
parallel corpus filtering and alignment for low-resource conditions. The task
requires the participants to align potential parallel sentence pairs out of the
given document pairs, and score them so that low-quality pairs can be filtered.
Our system, Volctrans, is made of two modules, i.e., a mining module and a
scoring module. Based on the word alignment model, the mining module adopts an
iterative mining strategy to extract latent parallel sentences. In the scoring
module, an XLM-based scorer provides scores, followed by reranking mechanisms
and ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x
for km-en and ps-en on From Scratch/Fine-Tune conditions, which is the highest
among all submissions.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 03:20:04 GMT'}]",2020-10-28,"[['Xu', 'Runxin', ''], ['Zhi', 'Zhuo', ''], ['Cao', 'Jun', ''], ['Wang', 'Mingxuan', ''], ['Li', 'Lei', '']]"
1370321,2010.13991,Wei Zou,"Dongwei Jiang, Wubo Li, Miao Cao, Ruixiong Zhang, Wei Zou, Kun Han,
  Xiangang Li","Speech SIMCLR: Combining Contrastive and Reconstruction Objective for
  Self-supervised Speech Representation Learning",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Self-supervised visual pretraining has shown significant progress recently.
Among those methods, SimCLR greatly advanced the state of the art in
self-supervised and semi-supervised learning on ImageNet. The input feature
representations for speech and visual tasks are both continuous, so it is
natural to consider applying similar objective on speech representation
learning. In this paper, we propose Speech SimCLR, a new self-supervised
objective for speech representation learning. During training, Speech SimCLR
applies augmentation on raw speech and its spectrogram. Its objective is the
combination of contrastive loss that maximizes agreement between differently
augmented samples in the latent space and reconstruction loss of input
representation. The proposed method achieved competitive results on speech
emotion recognition and speech recognition. When used as feature extractor, our
best model achieved 5.89% word error rate on LibriSpeech test-clean set using
LibriSpeech 960 hours as pretraining data and LibriSpeech train-clean-100 set
as fine-tuning data, which is the lowest error rate obtained in this setup to
the best of our knowledge.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 02:09:06 GMT'}]",2020-10-28,"[['Jiang', 'Dongwei', ''], ['Li', 'Wubo', ''], ['Cao', 'Miao', ''], ['Zhang', 'Ruixiong', ''], ['Zou', 'Wei', ''], ['Han', 'Kun', ''], ['Li', 'Xiangang', '']]"
1339591,2008.11869,Xinsong Zhang,Xinsong Zhang and Hang Li,AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models such as BERT have exhibited remarkable
performances in many tasks in natural language understanding (NLU). The tokens
in the models are usually fine-grained in the sense that for languages like
English they are words or sub-words and for languages like Chinese they are
characters. In English, for example, there are multi-word expressions which
form natural lexical units and thus the use of coarse-grained tokenization also
appears to be reasonable. In fact, both fine-grained and coarse-grained
tokenizations have advantages and disadvantages for learning of pre-trained
language models. In this paper, we propose a novel pre-trained language model,
referred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained
and coarse-grained tokenizations. For English, AMBERT takes both the sequence
of words (fine-grained tokens) and the sequence of phrases (coarse-grained
tokens) as input after tokenization, employs one encoder for processing the
sequence of words and the other encoder for processing the sequence of the
phrases, utilizes shared parameters between the two encoders, and finally
creates a sequence of contextualized representations of the words and a
sequence of contextualized representations of the phrases. Experiments have
been conducted on benchmark datasets for Chinese and English, including CLUE,
GLUE, SQuAD and RACE. The results show that AMBERT outperforms the existing
best performing models in almost all cases, particularly the improvements are
significant for Chinese.
","[{'version': 'v1', 'created': 'Thu, 27 Aug 2020 00:23:48 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Sep 2020 05:29:27 GMT'}, {'version': 'v3', 'created': 'Tue, 27 Oct 2020 06:53:33 GMT'}]",2020-10-28,"[['Zhang', 'Xinsong', ''], ['Li', 'Hang', '']]"
1279334,2004.14564,Brian Thompson,Brian Thompson and Matt Post,"Automatic Machine Translation Evaluation in Many Languages via Zero-Shot
  Paraphrasing",EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We frame the task of machine translation evaluation as one of scoring machine
translation output with a sequence-to-sequence paraphraser, conditioned on a
human reference. We propose training the paraphraser as a multilingual NMT
system, treating paraphrasing as a zero-shot translation task (e.g., Czech to
Czech). This results in the paraphraser's output mode being centered around a
copy of the input sequence, which represents the best case scenario where the
MT system output matches a human reference. Our method is simple and intuitive,
and does not require human judgements for training. Our single model (trained
in 39 languages) outperforms or statistically ties with all prior metrics on
the WMT 2019 segment-level shared metrics task in all languages (excluding
Gujarati where the model had no training data). We also explore using our model
for the task of quality estimation as a metric--conditioning on the source
instead of the reference--and find that it significantly outperforms every
submission to the WMT 2019 shared task on quality estimation in every language
pair.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 03:32:34 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 23:54:02 GMT'}]",2020-10-29,"[['Thompson', 'Brian', ''], ['Post', 'Matt', '']]"
1187463,1910.03544,Jianguo Zhang,"Jian-Guo Zhang, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wan, Philip S.
  Yu, Richard Socher, Caiming Xiong","Find or Classify? Dual Strategy for Slot-Value Predictions on
  Multi-Domain Dialog State Tracking","14 pages, accepted at the 9th Joint Conference on Lexical and
  Computational Semantics (*SEM 2020). This version fixes small errors",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialog state tracking (DST) is a core component in task-oriented dialog
systems. Existing approaches for DST mainly fall into one of two categories,
namely, ontology-based and ontology-free methods. An ontology-based method
selects a value from a candidate-value list for each target slot, while an
ontology-free method extracts spans from dialog contexts. Recent work
introduced a BERT-based model to strike a balance between the two methods by
pre-defining categorical and non-categorical slots. However, it is not clear
enough which slots are better handled by either of the two slot types, and the
way to use the pre-trained model has not been well investigated. In this paper,
we propose a simple yet effective dual-strategy model for DST, by adapting a
single BERT-style reading comprehension model to jointly handle both the
categorical and non-categorical slots. Our experiments on the MultiWOZ datasets
show that our method significantly outperforms the BERT-based counterpart,
finding that the key is a deep interaction between the domain-slot and context
information. When evaluated on noisy (MultiWOZ 2.0) and cleaner (MultiWOZ 2.1)
settings, our method performs competitively and robustly across the two
different settings. Our method sets the new state of the art in the noisy
setting, while performing more robustly than the best model in the cleaner
setting. We also conduct a comprehensive error analysis on the dataset,
including the effects of the dual strategy for each slot, to facilitate future
research.
","[{'version': 'v1', 'created': 'Tue, 8 Oct 2019 17:08:39 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Oct 2019 08:04:12 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Sep 2020 08:37:44 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Oct 2020 10:07:01 GMT'}]",2020-10-29,"[['Zhang', 'Jian-Guo', ''], ['Hashimoto', 'Kazuma', ''], ['Wu', 'Chien-Sheng', ''], ['Wan', 'Yao', ''], ['Yu', 'Philip S.', ''], ['Socher', 'Richard', ''], ['Xiong', 'Caiming', '']]"
1342828,2009.01325,Ryan Lowe T.,"Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe,
  Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano",Learning to summarize from human feedback,NeurIPS 2020 camera ready,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As language models become more powerful, training and evaluation are
increasingly bottlenecked by the data and metrics used for a particular task.
For example, summarization models are often trained to predict human reference
summaries and evaluated using ROUGE, but both of these metrics are rough
proxies for what we really care about---summary quality. In this work, we show
that it is possible to significantly improve summary quality by training a
model to optimize for human preferences. We collect a large, high-quality
dataset of human comparisons between summaries, train a model to predict the
human-preferred summary, and use that model as a reward function to fine-tune a
summarization policy using reinforcement learning. We apply our method to a
version of the TL;DR dataset of Reddit posts and find that our models
significantly outperform both human reference summaries and much larger models
fine-tuned with supervised learning alone. Our models also transfer to CNN/DM
news articles, producing summaries nearly as good as the human reference
without any news-specific fine-tuning. We conduct extensive analyses to
understand our human feedback dataset and fine-tuned models We establish that
our reward model generalizes to new datasets, and that optimizing our reward
model results in better summaries than optimizing ROUGE according to humans. We
hope the evidence from our paper motivates machine learning researchers to pay
closer attention to how their training loss affects the model behavior they
actually want.
","[{'version': 'v1', 'created': 'Wed, 2 Sep 2020 19:54:41 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 22:19:53 GMT'}]",2020-10-29,"[['Stiennon', 'Nisan', ''], ['Ouyang', 'Long', ''], ['Wu', 'Jeff', ''], ['Ziegler', 'Daniel M.', ''], ['Lowe', 'Ryan', ''], ['Voss', 'Chelsea', ''], ['Radford', 'Alec', ''], ['Amodei', 'Dario', ''], ['Christiano', 'Paul', '']]"
1201327,1911.02733,Xue Mengge,"Xue Mengge, Yu Bowen, Liu Tingwen, Zhang Yue, Meng Erli, Wang Bin",Porous Lattice-based Transformer Encoder for Chinese NER,"9 pages, 4 figures",COLING 2020,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Incorporating lattices into character-level Chinese named entity recognition
is an effective method to exploit explicit word information. Recent works
extend recurrent and convolutional neural networks to model lattice inputs.
However, due to the DAG structure or the variable-sized potential word set for
lattice inputs, these models prevent the convenient use of batched computation,
resulting in serious inefficient. In this paper, we propose a porous
lattice-based transformer encoder for Chinese named entity recognition, which
is capable to better exploit the GPU parallelism and batch the computation
owing to the mask mechanism in transformer. We first investigate the
lattice-aware self-attention coupled with relative position representations to
explore effective word information in the lattice structure. Besides, to
strengthen the local dependencies among neighboring tokens, we propose a novel
porous structure during self-attentional computation processing, in which every
two non-neighboring tokens are connected through a shared pivot node.
Experimental results on four datasets show that our model performs up to 9.47
times faster than state-of-the-art models, while is roughly on a par with its
performance. The source code of this paper can be obtained from
https://github.com/xxx/xxx.
","[{'version': 'v1', 'created': 'Thu, 7 Nov 2019 02:58:17 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Apr 2020 14:46:51 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Oct 2020 12:52:24 GMT'}]",2020-10-29,"[['Mengge', 'Xue', ''], ['Bowen', 'Yu', ''], ['Tingwen', 'Liu', ''], ['Yue', 'Zhang', ''], ['Erli', 'Meng', ''], ['Bin', 'Wang', '']]"
1290078,2005.10283,Bryan Eikema,Bryan Eikema and Wilker Aziz,"Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural
  Machine Translation",COLING 2020 camera-ready,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recent studies have revealed a number of pathologies of neural machine
translation (NMT) systems. Hypotheses explaining these mostly suggest there is
something fundamentally wrong with NMT as a model or its training algorithm,
maximum likelihood estimation (MLE). Most of this evidence was gathered using
maximum a posteriori (MAP) decoding, a decision rule aimed at identifying the
highest-scoring translation, i.e. the mode. We argue that the evidence
corroborates the inadequacy of MAP decoding more than casts doubt on the model
and its training algorithm. In this work, we show that translation
distributions do reproduce various statistics of the data well, but that beam
search strays from such statistics. We show that some of the known pathologies
and biases of NMT are due to MAP decoding and not to NMT's statistical
assumptions nor MLE. In particular, we show that the most likely translations
under the model accumulate so little probability mass that the mode can be
considered essentially arbitrary. We therefore advocate for the use of decision
rules that take into account the translation distribution holistically. We show
that an approximation to minimum Bayes risk decoding gives competitive results
confirming that NMT models do capture important aspects of translation well in
expectation.
","[{'version': 'v1', 'created': 'Wed, 20 May 2020 18:05:51 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 11:29:52 GMT'}]",2020-10-29,"[['Eikema', 'Bryan', ''], ['Aziz', 'Wilker', '']]"
1303020,2006.08506,Tobias Watzel,"Tobias Watzel, Ludwig K\""urzinger, Lujun Li, Gerhard Rigoll",Regularized Forward-Backward Decoder for Attention Models,,,,,eess.AS cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Nowadays, attention models are one of the popular candidates for speech
recognition. So far, many studies mainly focus on the encoder structure or the
attention module to enhance the performance of these models. However, mostly
ignore the decoder. In this paper, we propose a novel regularization technique
incorporating a second decoder during the training phase. This decoder is
optimized on time-reversed target labels beforehand and supports the standard
decoder during training by adding knowledge from future context. Since it is
only added during training, we are not changing the basic structure of the
network or adding complexity during decoding. We evaluate our approach on the
smaller TEDLIUMv2 and the larger LibriSpeech dataset, achieving consistent
improvements on both of them.
","[{'version': 'v1', 'created': 'Mon, 15 Jun 2020 16:04:16 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 14:00:52 GMT'}]",2020-10-29,"[['Watzel', 'Tobias', ''], ['Kürzinger', 'Ludwig', ''], ['Li', 'Lujun', ''], ['Rigoll', 'Gerhard', '']]"
1295146,2006.00632,Barbara Plank,Alan Ramponi and Barbara Plank,Neural Unsupervised Domain Adaptation in NLP---A Survey,"COLING 2020. Accompanying repository:
  https://github.com/bplank/awesome-neural-adaptation-in-NLP",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks excel at learning from labeled data and achieve
state-of-the-art resultson a wide array of Natural Language Processing tasks.
In contrast, learning from unlabeled data, especially under domain shift,
remains a challenge. Motivated by the latest advances, in this survey we review
neural unsupervised domain adaptation techniques which do not require labeled
target domain data. This is a more challenging yet a more widely applicable
setup. We outline methods, from early traditional non-neural methods to
pre-trained model transfer. We also revisit the notion of domain, and we
uncover a bias in the type of Natural Language Processing tasks which received
most attention. Lastly, we outline future directions, particularly the broader
need for out-of-distribution generalization of future NLP.
","[{'version': 'v1', 'created': 'Sun, 31 May 2020 22:34:14 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 08:24:14 GMT'}]",2020-10-29,"[['Ramponi', 'Alan', ''], ['Plank', 'Barbara', '']]"
1292163,2005.12368,Marija Stepanovi\'c,"Andreas Kirkedal, Marija Stepanovi\'c, Barbara Plank",FT Speech: Danish Parliament Speech Corpus,Accepted at Interspeech 2020,,10.21437/Interspeech.2020-3164,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces FT Speech, a new speech corpus created from the
recorded meetings of the Danish Parliament, otherwise known as the Folketing
(FT). The corpus contains over 1,800 hours of transcribed speech by a total of
434 speakers. It is significantly larger in duration, vocabulary, and amount of
spontaneous speech than the existing public speech corpora for Danish, which
are largely limited to read-aloud and dictation data. We outline design
considerations, including the preprocessing methods and the alignment
procedure. To evaluate the quality of the corpus, we train automatic speech
recognition systems on the new resource and compare them to the systems trained
on the Danish part of Spr\r{a}kbanken, the largest public ASR corpus for Danish
to date. Our baseline results show that we achieve a 14.01 WER on the new
corpus. A combination of FT Speech with in-domain language data provides
comparable results to models trained specifically on Spr\r{a}kbanken, showing
that FT Speech transfers well to this data set. Interestingly, our results
demonstrate that the opposite is not the case. This shows that FT Speech
provides a valuable resource for promoting research on Danish ASR with more
spontaneous speech.
","[{'version': 'v1', 'created': 'Mon, 25 May 2020 19:51:18 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 13:36:44 GMT'}]",2020-10-29,"[['Kirkedal', 'Andreas', ''], ['Stepanović', 'Marija', ''], ['Plank', 'Barbara', '']]"
1364540,2010.08210,Xue Mengge,"Mengge Xue, Bowen Yu, Zhenyu Zhang, Tingwen Liu, Yue Zhang, Bin Wang",Coarse-to-Fine Pre-training for Named Entity Recognition,,EMNLP 2020,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  More recently, Named Entity Recognition hasachieved great advances aided by
pre-trainingapproaches such as BERT. However, currentpre-training techniques
focus on building lan-guage modeling objectives to learn a gen-eral
representation, ignoring the named entity-related knowledge. To this end, we
proposea NER-specific pre-training framework to in-ject coarse-to-fine
automatically mined entityknowledge into pre-trained models. Specifi-cally, we
first warm-up the model via an en-tity span identification task by training it
withWikipedia anchors, which can be deemed asgeneral-typed entities. Then we
leverage thegazetteer-based distant supervision strategy totrain the model
extract coarse-grained typedentities. Finally, we devise a
self-supervisedauxiliary task to mine the fine-grained namedentity knowledge
via clustering.Empiricalstudies on three public NER datasets demon-strate that
our framework achieves significantimprovements against several pre-trained
base-lines, establishing the new state-of-the-art per-formance on three
benchmarks. Besides, weshow that our framework gains promising re-sults without
using human-labeled trainingdata, demonstrating its effectiveness in label-few
and low-resource scenarios
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 07:39:20 GMT'}]",2020-10-29,"[['Xue', 'Mengge', ''], ['Yu', 'Bowen', ''], ['Zhang', 'Zhenyu', ''], ['Liu', 'Tingwen', ''], ['Zhang', 'Yue', ''], ['Wang', 'Bin', '']]"
1361978,2010.05648,Steffen Eger,Steffen Eger and Yannik Benz,From Hero to Z\'eroe: A Benchmark of Low-Level Adversarial Attacks,"Authors accidentally in wrong order; cannot be undone due to
  conference constraints. Accepted for publication at AACL 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversarial attacks are label-preserving modifications to inputs of machine
learning classifiers designed to fool machines but not humans. Natural Language
Processing (NLP) has mostly focused on high-level attack scenarios such as
paraphrasing input texts. We argue that these are less realistic in typical
application scenarios such as in social media, and instead focus on low-level
attacks on the character-level. Guided by human cognitive abilities and human
robustness, we propose the first large-scale catalogue and benchmark of
low-level adversarial attacks, which we dub Z\'eroe, encompassing nine
different attack modes including visual and phonetic adversaries. We show that
RoBERTa, NLP's current workhorse, fails on our attacks. Our dataset provides a
benchmark for testing robustness of future more human-like NLP models.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 12:35:36 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 12:53:05 GMT'}]",2020-10-29,"[['Eger', 'Steffen', ''], ['Benz', 'Yannik', '']]"
1359177,2010.02847,Haiyang Zhang,"Haiyang Zhang, Alison Sneyd and Mark Stevenson","Robustness and Reliability of Gender Bias Assessment in Word Embeddings:
  The Role of Base Pairs",Accepted at AACL-IJCNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It has been shown that word embeddings can exhibit gender bias, and various
methods have been proposed to quantify this. However, the extent to which the
methods are capturing social stereotypes inherited from the data has been
debated. Bias is a complex concept and there exist multiple ways to define it.
Previous work has leveraged gender word pairs to measure bias and extract
biased analogies. We show that the reliance on these gendered pairs has strong
limitations: bias measures based off of them are not robust and cannot identify
common types of real-world bias, whilst analogies utilising them are unsuitable
indicators of bias. In particular, the well-known analogy ""man is to
computer-programmer as woman is to homemaker"" is due to word similarity rather
than societal bias. This has important implications for work on measuring bias
in embeddings and related work debiasing embeddings.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 16:09:05 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 21:24:16 GMT'}]",2020-10-29,"[['Zhang', 'Haiyang', ''], ['Sneyd', 'Alison', ''], ['Stevenson', 'Mark', '']]"
1356617,2010.00287,Ehsan Doostmohammadi,"Ehsan Doostmohammadi, Minoo Nassajian, Adel Rahimi","Joint Persian Word Segmentation Correction and Zero-Width Non-Joiner
  Recognition Using BERT",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Words are properly segmented in the Persian writing system; in practice,
however, these writing rules are often neglected, resulting in single words
being written disjointedly and multiple words written without any white spaces
between them. This paper addresses the problems of word segmentation and
zero-width non-joiner (ZWNJ) recognition in Persian, which we approach jointly
as a sequence labeling problem. We achieved a macro-averaged F1-score of 92.40%
on a carefully collected corpus of 500 sentences with a high level of
difficulty.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 10:32:17 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 09:40:18 GMT'}]",2020-10-29,"[['Doostmohammadi', 'Ehsan', ''], ['Nassajian', 'Minoo', ''], ['Rahimi', 'Adel', '']]"
1280566,2005.00771,Xiang Li,"Michael Boratko, Xiang Lorraine Li, Rajarshi Das, Tim O'Gorman, Dan
  Le, Andrew McCallum","ProtoQA: A Question Answering Dataset for Prototypical Common-Sense
  Reasoning",First four authors contribute equally,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Given questions regarding some prototypical situation such as Name something
that people usually do before they leave the house for work? a human can easily
answer them via acquired experiences. There can be multiple right answers for
such questions, with some more common for a situation than others. This paper
introduces a new question answering dataset for training and evaluating common
sense reasoning capabilities of artificial intelligence systems in such
prototypical situations. The training set is gathered from an existing set of
questions played in a long-running international game show FAMILY- FEUD. The
hidden evaluation set is created by gathering answers for each question from
100 crowd-workers. We also propose a generative evaluation task where a model
has to output a ranked list of answers, ideally covering all prototypical
answers for a question. After presenting multiple competitive baseline models,
we find that human performance still exceeds model scores on all evaluation
metrics with a meaningful gap, supporting the challenging nature of the task.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 09:40:05 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 05:35:05 GMT'}, {'version': 'v3', 'created': 'Tue, 27 Oct 2020 21:23:03 GMT'}]",2020-10-29,"[['Boratko', 'Michael', ''], ['Li', 'Xiang Lorraine', ''], ['Das', 'Rajarshi', ''], [""O'Gorman"", 'Tim', ''], ['Le', 'Dan', ''], ['McCallum', 'Andrew', '']]"
1279954,2005.00159,Pratyush Maini,"Pratyush Maini, Keshav Kolluru, Danish Pruthi, Mausam","Why and when should you pool? Analyzing Pooling in Recurrent
  Architectures","Accepted to Findings of EMNLP 2020, to be presented at BlackBoxNLP.
  Updated Version",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pooling-based recurrent neural architectures consistently outperform their
counterparts without pooling. However, the reasons for their enhanced
performance are largely unexamined. In this work, we examine three commonly
used pooling techniques (mean-pooling, max-pooling, and attention), and propose
max-attention, a novel variant that effectively captures interactions among
predictive tokens in a sentence. We find that pooling-based architectures
substantially differ from their non-pooling equivalents in their learning
ability and positional biases--which elucidate their performance benefits. By
analyzing the gradient propagation, we discover that pooling facilitates better
gradient flow compared to BiLSTMs. Further, we expose how BiLSTMs are
positionally biased towards tokens in the beginning and the end of a sequence.
Pooling alleviates such biases. Consequently, we identify settings where
pooling offers large benefits: (i) in low resource scenarios, and (ii) when
important words lie towards the middle of the sentence. Among the pooling
techniques studied, max-attention is the most effective, resulting in
significant performance gains on several text classification tasks.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 00:47:37 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 02:11:02 GMT'}]",2020-10-29,"[['Maini', 'Pratyush', ''], ['Kolluru', 'Keshav', ''], ['Pruthi', 'Danish', ''], ['Mausam', '', '']]"
1332657,2008.04935,Brian Thompson,Brian Thompson and Matt Post,"Paraphrase Generation as Zero-Shot Multilingual Translation:
  Disentangling Semantic Similarity from Lexical and Syntactic Diversity",WMT2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has shown that a multilingual neural machine translation (NMT)
model can be used to judge how well a sentence paraphrases another sentence in
the same language (Thompson and Post, 2020); however, attempting to generate
paraphrases from such a model using standard beam search produces trivial
copies or near copies. We introduce a simple paraphrase generation algorithm
which discourages the production of n-grams that are present in the input. Our
approach enables paraphrase generation in many languages from a single
multilingual NMT model. Furthermore, the amount of lexical diversity between
the input and output can be controlled at generation time. We conduct a human
evaluation to compare our method to a paraphraser trained on the large English
synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our
method produces paraphrases that better preserve meaning and are more
gramatical, for the same level of lexical diversity. Additional smaller human
assessments demonstrate our approach also works in two non-English languages.
","[{'version': 'v1', 'created': 'Tue, 11 Aug 2020 18:05:34 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 02:54:13 GMT'}]",2020-10-29,"[['Thompson', 'Brian', ''], ['Post', 'Matt', '']]"
1349467,2009.07964,Zhijing Jin,"Xiaoyu Xing, Zhijing Jin, Di Jin, Bingning Wang, Qi Zhang, and
  Xuanjing Huang","Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based
  Sentiment Analysis","EMNLP 2020, long paper",,,,cs.CL cs.IR cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards
a specific aspect in the text. However, existing ABSA test sets cannot be used
to probe whether a model can distinguish the sentiment of the target aspect
from the non-target aspects. To solve this problem, we develop a simple but
effective approach to enrich ABSA test sets. Specifically, we generate new
examples to disentangle the confounding sentiments of the non-target aspects
from the target aspect's sentiment. Based on the SemEval 2014 dataset, we
construct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the
aspect robustness of ABSA models. Over 92% data of ARTS show high fluency and
desired sentiment on all aspects by human evaluation. Using ARTS, we analyze
the robustness of nine ABSA models, and observe, surprisingly, that their
accuracy drops by up to 69.73%. We explore several ways to improve aspect
robustness, and find that adversarial training can improve models' performance
on ARTS by up to 32.85%. Our code and new test set are available at
https://github.com/zhijing-jin/ARTS_TestSet
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 22:38:18 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 05:36:10 GMT'}, {'version': 'v3', 'created': 'Sun, 4 Oct 2020 15:35:36 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Oct 2020 08:19:36 GMT'}]",2020-10-29,"[['Xing', 'Xiaoyu', ''], ['Jin', 'Zhijing', ''], ['Jin', 'Di', ''], ['Wang', 'Bingning', ''], ['Zhang', 'Qi', ''], ['Huang', 'Xuanjing', '']]"
1303623,2006.09109,Steffen Eger,Steffen Eger and Johannes Daxenberger and Iryna Gurevych,"How to Probe Sentence Embeddings in Low-Resource Languages: On
  Structural Design Choices for Probing Task Evaluation",Accepted for Publication at CONLL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentence encoders map sentences to real valued vectors for use in downstream
applications. To peek into these representations - e.g., to increase
interpretability of their results - probing tasks have been designed which
query them for linguistic knowledge. However, designing probing tasks for
lesser-resourced languages is tricky, because these often lack large-scale
annotated data or (high-quality) dependency parsers as a prerequisite of
probing task design in English. To investigate how to probe sentence embeddings
in such cases, we investigate sensitivity of probing task results to structural
design choices, conducting the first such large scale study. We show that
design choices like size of the annotated probing dataset and type of
classifier used for evaluation do (sometimes substantially) influence probing
outcomes. We then probe embeddings in a multilingual setup with design choices
that lie in a 'stable region', as we identify for English, and find that
results on English do not transfer to other languages. Fairer and more
comprehensive sentence-level probing evaluation should thus be carried out on
multiple languages in the future.
","[{'version': 'v1', 'created': 'Tue, 16 Jun 2020 12:37:50 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 12:38:37 GMT'}]",2020-10-29,"[['Eger', 'Steffen', ''], ['Daxenberger', 'Johannes', ''], ['Gurevych', 'Iryna', '']]"
1139420,1906.07234,Siyuan Feng,"Siyuan Feng, Tan Lee, Zhiyuan Peng","Combining Adversarial Training and Disentangled Speech Representation
  for Robust Zero-Resource Subword Modeling","5 pages, 3 figures, accepted for publication in INTERSPEECH 2019,
  Graz, Austria",,10.21437/Interspeech.2019-1337,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This study addresses the problem of unsupervised subword unit discovery from
untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech
2019, building text-to-speech systems without text labels. In this work, unit
discovery is formulated as a pipeline of phonetically discriminative feature
learning and unit inference. One major difficulty in robust unsupervised
feature learning is dealing with speaker variation. Here the robustness towards
speaker variation is achieved by applying adversarial training and FHVAE based
disentangled speech representation learning. A comparison of the two approaches
as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF)
architecture. Experiments are conducted on ZeroSpeech 2019 and 2017.
Experimental results on ZeroSpeech 2017 show that both approaches are effective
while the latter is more prominent, and that their combination brings further
marginal improvement in across-speaker condition. Results on ZeroSpeech 2019
show that in the ABX discriminability task, our approaches significantly
outperform the official baseline, and are competitive to or even outperform the
official topline. The proposed unit sequence smoothing algorithm improves
synthesis quality, at a cost of slight decrease in ABX discriminability.
","[{'version': 'v1', 'created': 'Mon, 17 Jun 2019 19:40:46 GMT'}, {'version': 'v2', 'created': 'Wed, 3 Jul 2019 12:22:28 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Aug 2019 15:55:00 GMT'}]",2020-10-29,"[['Feng', 'Siyuan', ''], ['Lee', 'Tan', ''], ['Peng', 'Zhiyuan', '']]"
1371089,2010.14759,Yufang Hou,Yufang Hou,"Fine-grained Information Status Classification Using Discourse
  Context-Aware BERT","accepted at COLING2020. arXiv admin note: substantial text overlap
  with arXiv:1908.04755",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous work on bridging anaphora recognition (Hou et al., 2013a) casts the
problem as a subtask of learning fine-grained information status (IS). However,
these systems heavily depend on many hand-crafted linguistic features. In this
paper, we propose a simple discourse context-aware BERT model for fine-grained
IS classification. On the ISNotes corpus (Markert et al., 2012), our model
achieves new state-of-the-art performance on fine-grained IS classification,
obtaining a 4.8 absolute overall accuracy improvement compared to Hou et al.
(2013a). More importantly, we also show an improvement of 10.5 F1 points for
bridging anaphora recognition without using any complex hand-crafted semantic
features designed for capturing the bridging phenomenon. We further analyze the
trained model and find that the most attended signals for each IS category
correspond well to linguistic notions of information status.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 22:30:17 GMT'}]",2020-10-29,"[['Hou', 'Yufang', '']]"
1371128,2010.14798,Shuai Zhang,"Shuai Zhang, Jiangyan Yi, Zhengkun Tian, Ye Bai, Jianhua Tao, Zhengqi
  wen","Decoupling Pronunciation and Language for End-to-end Code-switching
  Automatic Speech Recognition","5 pages, 1 figures",,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the recent significant advances witnessed in end-to-end (E2E) ASR
system for code-switching, hunger for audio-text paired data limits the further
improvement of the models' performance. In this paper, we propose a decoupled
transformer model to use monolingual paired data and unpaired text data to
alleviate the problem of code-switching data shortage. The model is decoupled
into two parts: audio-to-phoneme (A2P) network and phoneme-to-text (P2T)
network. The A2P network can learn acoustic pattern scenarios using large-scale
monolingual paired data. Meanwhile, it generates multiple phoneme sequence
candidates for single audio data in real-time during the training process. Then
the generated phoneme-text paired data is used to train the P2T network. This
network can be pre-trained with large amounts of external unpaired text data.
By using monolingual data and unpaired text data, the decoupled transformer
model reduces the high dependency on code-switching paired training data of E2E
model to a certain extent. Finally, the two networks are optimized jointly
through attention fusion. We evaluate the proposed method on the public
Mandarin-English code-switching dataset. Compared with our transformer
baseline, the proposed method achieves 18.14% relative mix error rate
reduction.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 07:46:15 GMT'}]",2020-10-29,"[['Zhang', 'Shuai', ''], ['Yi', 'Jiangyan', ''], ['Tian', 'Zhengkun', ''], ['Bai', 'Ye', ''], ['Tao', 'Jianhua', ''], ['wen', 'Zhengqi', '']]"
1371202,2010.14872,Kristian Miok,"Kristian Miok, Gregor Pirs and Marko Robnik-Sikonja",Bayesian Methods for Semi-supervised Text Annotation,"Accepted for COLING 2020, The 14th Linguistic Annotation Workshop",,,,cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human annotations are an important source of information in the development
of natural language understanding approaches. As under the pressure of
productivity annotators can assign different labels to a given text, the
quality of produced annotations frequently varies. This is especially the case
if decisions are difficult, with high cognitive load, requires awareness of
broader context, or careful consideration of background knowledge. To alleviate
the problem, we propose two semi-supervised methods to guide the annotation
process: a Bayesian deep learning model and a Bayesian ensemble method. Using a
Bayesian deep learning method, we can discover annotations that cannot be
trusted and might require reannotation. A recently proposed Bayesian ensemble
method helps us to combine the annotators' labels with predictions of trained
models. According to the results obtained from three hate speech detection
experiments, the proposed Bayesian methods can improve the annotations and
prediction performance of BERT models.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 10:42:04 GMT'}]",2020-10-29,"[['Miok', 'Kristian', ''], ['Pirs', 'Gregor', ''], ['Robnik-Sikonja', 'Marko', '']]"
1371171,2010.14841,Chengyu Wang,"Yiwu Yao, Yuchao Li, Chengyu Wang, Tianhang Yu, Houjiang Chen,
  Xiaotang Jiang, Jun Yang, Jun Huang, Wei Lin, Hui Shu, Chengfei Lv","INT8 Winograd Acceleration for Conv1D Equipped ASR Models Deployed on
  Mobile Devices",,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The intensive computation of Automatic Speech Recognition (ASR) models
obstructs them from being deployed on mobile devices. In this paper, we present
a novel quantized Winograd optimization pipeline, which combines the
quantization and fast convolution to achieve efficient inference acceleration
on mobile devices for ASR models. To avoid the information loss due to the
combination of quantization and Winograd convolution, a Range-Scaled
Quantization (RSQ) training method is proposed to expand the quantized
numerical range and to distill knowledge from high-precision values. Moreover,
an improved Conv1D equipped DFSMN (ConvDFSMN) model is designed for mobile
deployment. We conduct extensive experiments on both ConvDFSMN and Wav2letter
models. Results demonstrate the models can be effectively optimized with the
proposed pipeline. Especially, Wav2letter achieves 1.48* speedup with an
approximate 0.07% WER decrease on ARMv7-based mobile devices.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 09:25:49 GMT'}]",2020-10-29,"[['Yao', 'Yiwu', ''], ['Li', 'Yuchao', ''], ['Wang', 'Chengyu', ''], ['Yu', 'Tianhang', ''], ['Chen', 'Houjiang', ''], ['Jiang', 'Xiaotang', ''], ['Yang', 'Jun', ''], ['Huang', 'Jun', ''], ['Lin', 'Wei', ''], ['Shu', 'Hui', ''], ['Lv', 'Chengfei', '']]"
1371136,2010.14806,Xiao Pan,"Liwei Wu, Xiao Pan, Zehui Lin, Yaoming Zhu, Mingxuan Wang, Lei Li",The Volctrans Machine Translation System for WMT20,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes our VolcTrans system on WMT20 shared news translation
task. We participated in 8 translation directions. Our basic systems are based
on Transformer, with several variants (wider or deeper Transformers, dynamic
convolutions). The final system includes text pre-process, data selection,
synthetic data generation, advanced model ensemble, and multilingual
pre-training.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 08:08:12 GMT'}]",2020-10-29,"[['Wu', 'Liwei', ''], ['Pan', 'Xiao', ''], ['Lin', 'Zehui', ''], ['Zhu', 'Yaoming', ''], ['Wang', 'Mingxuan', ''], ['Li', 'Lei', '']]"
1371134,2010.14804,Benlai Tang,"Zhonghao Li, Benlai Tang, Xiang Yin, Yuan Wan, Ling Xu, Chen Shen,
  Zejun Ma","PPG-based singing voice conversion with adversarial representation
  learning",,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Singing voice conversion (SVC) aims to convert the voice of one singer to
that of other singers while keeping the singing content and melody. On top of
recent voice conversion works, we propose a novel model to steadily convert
songs while keeping their naturalness and intonation. We build an end-to-end
architecture, taking phonetic posteriorgrams (PPGs) as inputs and generating
mel spectrograms. Specifically, we implement two separate encoders: one encodes
PPGs as content, and the other compresses mel spectrograms to supply acoustic
and musical information. To improve the performance on timbre and melody, an
adversarial singer confusion module and a mel-regressive representation
learning module are designed for the model. Objective and subjective
experiments are conducted on our private Chinese singing corpus. Comparing with
the baselines, our methods can significantly improve the conversion performance
in terms of naturalness, melody, and voice similarity. Moreover, our PPG-based
method is proved to be robust for noisy sources.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 08:03:27 GMT'}]",2020-10-29,"[['Li', 'Zhonghao', ''], ['Tang', 'Benlai', ''], ['Yin', 'Xiang', ''], ['Wan', 'Yuan', ''], ['Xu', 'Ling', ''], ['Shen', 'Chen', ''], ['Ma', 'Zejun', '']]"
1371124,2010.14794,Kun Zhou,"Kun Zhou, Berrak Sisman, Rui Liu and Haizhou Li","Seen and Unseen emotional style transfer for voice conversion with a new
  emotional speech dataset",Submitted to ICASSP 2021,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Emotional voice conversion aims to transform emotional prosody in speech
while preserving the linguistic content and speaker identity. Prior studies
show that it is possible to disentangle emotional prosody using an
encoder-decoder network conditioned on discrete representation, such as one-hot
emotion labels. Such networks learn to remember a fixed set of emotional
styles. In this paper, we propose a novel framework based on variational
auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes
use of a pre-trained speech emotion recognition (SER) model to transfer
emotional style during training and at run-time inference. In this way, the
network is able to transfer both seen and unseen emotional style to a new
utterance. We show that the proposed framework achieves remarkable performance
by consistently outperforming the baseline framework. This paper also marks the
release of an emotional speech dataset (ESD) for voice conversion, which has
multiple speakers and languages.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 07:16:18 GMT'}]",2020-10-29,"[['Zhou', 'Kun', ''], ['Sisman', 'Berrak', ''], ['Liu', 'Rui', ''], ['Li', 'Haizhou', '']]"
1371114,2010.14784,Yuanhao Zhuo,Yuanhao Zhuo,"A Chinese Text Classification Method With Low Hardware Requirement Based
  on Improved Model Concatenation","5 pages, 2 figures, 5 tables",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In order to improve the accuracy performance of Chinese text classification
models with low hardware requirements, an improved concatenation-based model is
designed in this paper, which is a concatenation of 5 different sub-models,
including TextCNN, LSTM, and Bi-LSTM. Compared with the existing ensemble
learning method, for a text classification mission, this model's accuracy is 2%
higher. Meanwhile, the hardware requirements of this model are much lower than
the BERT-based model.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 06:32:41 GMT'}]",2020-10-29,"[['Zhuo', 'Yuanhao', '']]"
1371060,2010.14730,Xiaoyu Kou,"Xiaoyu Kou, Yankai Lin, Yuntao Li, Jiahao Xu, Peng Li, Jie Zhou, Yan
  Zhang",DisenE: Disentangling Knowledge Graph Embeddings,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graph embedding (KGE), aiming to embed entities and relations into
low-dimensional vectors, has attracted wide attention recently. However, the
existing research is mainly based on the black-box neural models, which makes
it difficult to interpret the learned representation. In this paper, we
introduce DisenE, an end-to-end framework to learn disentangled knowledge graph
embeddings. Specially, we introduce an attention-based mechanism that enables
the model to explicitly focus on relevant components of entity embeddings
according to a given relation. Furthermore, we introduce two novel regularizers
to encourage each component of the entity representation to independently
reflect an isolated semantic aspect. Experimental results demonstrate that our
proposed DisenE investigates a perspective to address the interpretability of
KGE and is proved to be an effective way to improve the performance of link
prediction tasks. The code and datasets are released on
https://github.com/KXY-PUBLIC/DisenE.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 03:45:19 GMT'}]",2020-10-29,"[['Kou', 'Xiaoyu', ''], ['Lin', 'Yankai', ''], ['Li', 'Yuntao', ''], ['Xu', 'Jiahao', ''], ['Li', 'Peng', ''], ['Zhou', 'Jie', ''], ['Zhang', 'Yan', '']]"
1371055,2010.14725,Ruchao Fan,"Ruchao Fan, Wei Chu, Peng Chang, Jing Xiao","CASS-NAT: CTC Alignment-based Single Step Non-autoregressive Transformer
  for Speech Recognition",Submitted to ICASSP2021,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a CTC alignment-based single step non-autoregressive transformer
(CASS-NAT) for speech recognition. Specifically, the CTC alignment contains the
information of (a) the number of tokens for decoder input, and (b) the time
span of acoustics for each token. The information are used to extract acoustic
representation for each token in parallel, referred to as token-level acoustic
embedding which substitutes the word embedding in autoregressive transformer
(AT) to achieve parallel generation in decoder. During inference, an
error-based alignment sampling method is proposed to be applied to the CTC
output space, reducing the WER and retaining the parallelism as well.
Experimental results show that the proposed method achieves WERs of 3.8%/9.1%
on Librispeech test clean/other dataset without an external LM, and a CER of
5.8% on Aishell1 Mandarin corpus, respectively1. Compared to the AT baseline,
the CASS-NAT has a performance reduction on WER, but is 51.2x faster in terms
of RTF. When decoding with an oracle CTC alignment, the lower bound of WER
without LM reaches 2.3% on the test-clean set, indicating the potential of the
proposed method.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 03:14:05 GMT'}]",2020-10-29,"[['Fan', 'Ruchao', ''], ['Chu', 'Wei', ''], ['Chang', 'Peng', ''], ['Xiao', 'Jing', '']]"
1272816,2004.08046,Dongyu Ru,"Dongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingxuan Wang, Weinan
  Zhang, Yong Yu, Lei Li","Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete
  Space",Accepted to EMNLP 2020 Findings,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Active learning for sentence understanding aims at discovering informative
unlabeled data for annotation and therefore reducing the demand for labeled
data. We argue that the typical uncertainty sampling method for active learning
is time-consuming and can hardly work in real-time, which may lead to
ineffective sample selection. We propose adversarial uncertainty sampling in
discrete space (AUSDS) to retrieve informative unlabeled samples more
efficiently. AUSDS maps sentences into latent space generated by the popular
pre-trained language models, and discover informative unlabeled text samples
for annotation via adversarial attack. The proposed approach is extremely
efficient compared with traditional uncertainty sampling with more than 10x
speedup. Experimental results on five datasets show that AUSDS outperforms
strong baselines on effectiveness.
","[{'version': 'v1', 'created': 'Fri, 17 Apr 2020 03:12:34 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 04:45:49 GMT'}]",2020-10-29,"[['Ru', 'Dongyu', ''], ['Feng', 'Jiangtao', ''], ['Qiu', 'Lin', ''], ['Zhou', 'Hao', ''], ['Wang', 'Mingxuan', ''], ['Zhang', 'Weinan', ''], ['Yu', 'Yong', ''], ['Li', 'Lei', '']]"
1371050,2010.14720,Songlin Yang,"Songlin Yang, Yong Jiang, Wenjuan Han, Kewei Tu",Second-Order Unsupervised Neural Dependency Parsing,COLING 2020 camera ready,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most of the unsupervised dependency parsers are based on first-order
probabilistic generative models that only consider local parent-child
information. Inspired by second-order supervised dependency parsing, we
proposed a second-order extension of unsupervised neural dependency models that
incorporate grandparent-child or sibling information. We also propose a novel
design of the neural parameterization and optimization methods of the
dependency models. In second-order models, the number of grammar rules grows
cubically with the increase of vocabulary size, making it difficult to train
lexicalized models that may contain thousands of words. To circumvent this
problem while still benefiting from both second-order parsing and
lexicalization, we use the agreement-based learning framework to jointly train
a second-order unlexicalized model and a first-order lexicalized model.
Experiments on multiple datasets show the effectiveness of our second-order
models compared with recent state-of-the-art methods. Our joint model achieves
a 10% improvement over the previous state-of-the-art parser on the full WSJ
test set
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 03:01:33 GMT'}]",2020-10-29,"[['Yang', 'Songlin', ''], ['Jiang', 'Yong', ''], ['Han', 'Wenjuan', ''], ['Tu', 'Kewei', '']]"
1371037,2010.14707,Yang Qian,"Yang Qian, Yuanchun Jiang, Yidong Chai, Yezheng Liu, Jiansha Sun",TopicModel4J: A Java Package for Topic Models,,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Topic models provide a flexible and principled framework for exploring hidden
structure in high-dimensional co-occurrence data and are commonly used natural
language processing (NLP) of text. In this paper, we design and implement a
Java package, TopicModel4J, which contains 13 kinds of representative
algorithms for fitting topic models. The TopicModel4J in the Java programming
environment provides an easy-to-use interface for data analysts to run the
algorithms, and allow to easily input and output data. In addition, this
package provides a few unstructured text preprocessing techniques, such as
splitting textual data into words, lowercasing the words, preforming
lemmatization and removing the useless characters, URLs and stop words.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 02:33:41 GMT'}]",2020-10-29,"[['Qian', 'Yang', ''], ['Jiang', 'Yuanchun', ''], ['Chai', 'Yidong', ''], ['Liu', 'Yezheng', ''], ['Sun', 'Jiansha', '']]"
1371031,2010.14701,Samuel McCandlish,"Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse,
  Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris
  Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M.
  Ziegler, John Schulman, Dario Amodei, Sam McCandlish",Scaling Laws for Autoregressive Generative Modeling,"20+15 pages, 30 figures",,,,cs.LG cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We identify empirical scaling laws for the cross-entropy loss in four
domains: generative image modeling, video modeling, multimodal
image$\leftrightarrow$text models, and mathematical problem solving. In all
cases autoregressive Transformers smoothly improve in performance as model size
and compute budgets increase, following a power-law plus constant scaling law.
The optimal model size also depends on the compute budget through a power-law,
with exponents that are nearly universal across all data domains.
  The cross-entropy loss has an information theoretic interpretation as
$S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws
suggest a prediction for both the true data distribution's entropy and the KL
divergence between the true and model distributions. With this interpretation,
billion-parameter Transformers are nearly perfect models of the YFCC100M image
distribution downsampled to an $8\times 8$ resolution, and we can forecast the
model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in
nats/image for other resolutions.
  We find a number of additional scaling laws in specific domains: (a) we
identify a scaling relation for the mutual information between captions and
images in multimodal models, and show how to answer the question ""Is a picture
worth a thousand words?""; (b) in the case of mathematical problem solving, we
identify scaling laws for model performance when extrapolating beyond the
training distribution; (c) we finetune generative image models for ImageNet
classification and find smooth scaling of the classification loss and error
rate, even as the generative loss levels off. Taken together, these results
strengthen the case that scaling laws have important implications for neural
network performance, including on downstream tasks.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 02:17:24 GMT'}]",2020-10-29,"[['Henighan', 'Tom', ''], ['Kaplan', 'Jared', ''], ['Katz', 'Mor', ''], ['Chen', 'Mark', ''], ['Hesse', 'Christopher', ''], ['Jackson', 'Jacob', ''], ['Jun', 'Heewoo', ''], ['Brown', 'Tom B.', ''], ['Dhariwal', 'Prafulla', ''], ['Gray', 'Scott', ''], ['Hallacy', 'Chris', ''], ['Mann', 'Benjamin', ''], ['Radford', 'Alec', ''], ['Ramesh', 'Aditya', ''], ['Ryder', 'Nick', ''], ['Ziegler', 'Daniel M.', ''], ['Schulman', 'John', ''], ['Amodei', 'Dario', ''], ['McCandlish', 'Sam', '']]"
1371027,2010.14697,Claire Bowern,Luke Lindemann and Claire Bowern,"Character Entropy in Modern and Historical Texts: Comparison Metrics for
  an Undeciphered Manuscript",,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper outlines the creation of three corpora for multilingual comparison
and analysis of the Voynich manuscript: a corpus of Voynich texts partitioned
by Currier language, scribal hand, and transcription system, a corpus of 294
language samples compiled from Wikipedia, and a corpus of eighteen transcribed
historical texts in eight languages. These corpora will be utilized in
subsequent work by the Voynich Working Group at Yale University.
  We demonstrate the utility of these corpora for studying characteristics of
the Voynich script and language, with an analysis of conditional character
entropy in Voynichese. We discuss the interaction between character entropy and
language, script size and type, glyph compositionality, scribal conventions and
abbreviations, positional character variants, and bigram frequency.
  This analysis characterizes the interaction between script compositionality,
character size, and predictability. We show that substantial manipulations of
glyph composition are not sufficient to align conditional entropy levels with
natural languages. The unusually predictable nature of the Voynichese script is
not attributable to a particular script or transcription system, underlying
language, or substitution cipher. Voynichese is distinct from every comparison
text in our corpora because character placement is highly constrained within
the word, and this may indicate the loss of phonemic distinctions from the
underlying language.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 01:53:59 GMT'}]",2020-10-29,"[['Lindemann', 'Luke', ''], ['Bowern', 'Claire', '']]"
1371008,2010.14678,Amir Pouran Ben Veyseh,"Amir Pouran Ben Veyseh, Franck Dernoncourt, Quan Hung Tran, Thien Huu
  Nguyen","What Does This Acronym Mean? Introducing a New Dataset for Acronym
  Identification and Disambiguation",accepted at COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Acronyms are the short forms of phrases that facilitate conveying lengthy
sentences in documents and serve as one of the mainstays of writing. Due to
their importance, identifying acronyms and corresponding phrases (i.e., acronym
identification (AI)) and finding the correct meaning of each acronym (i.e.,
acronym disambiguation (AD)) are crucial for text understanding. Despite the
recent progress on this task, there are some limitations in the existing
datasets which hinder further improvement. More specifically, limited size of
manually annotated AI datasets or noises in the automatically created acronym
identification datasets obstruct designing advanced high-performing acronym
identification models. Moreover, the existing datasets are mostly limited to
the medical domain and ignore other domains. In order to address these two
limitations, we first create a manually annotated large AI dataset for
scientific domain. This dataset contains 17,506 sentences which is
substantially larger than previous scientific AI datasets. Next, we prepare an
AD dataset for scientific domain with 62,441 samples which is significantly
larger than the previous scientific AD dataset. Our experiments show that the
existing state-of-the-art models fall far behind human-level performance on
both datasets proposed by this work. In addition, we propose a new deep
learning model that utilizes the syntactical structure of the sentence to
expand an ambiguous acronym in a sentence. The proposed model outperforms the
state-of-the-art models on the new AD dataset, providing a strong baseline for
future research on this dataset.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 00:12:36 GMT'}]",2020-10-29,"[['Veyseh', 'Amir Pouran Ben', ''], ['Dernoncourt', 'Franck', ''], ['Tran', 'Quan Hung', ''], ['Nguyen', 'Thien Huu', '']]"
1370995,2010.14665,Yongqiang Wang,"Yongqiang Wang, Yangyang Shi, Frank Zhang, Chunyang Wu, Julian Chan,
  Ching-Feng Yeh, Alex Xiao","Transformer in action: a comparative study of transformer-based acoustic
  models for large scale speech recognition applications",submitted to ICASSP2021,,,,cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we summarize the application of transformer and its streamable
variant, Emformer based acoustic model for large scale speech recognition
applications. We compare the transformer based acoustic models with their LSTM
counterparts on industrial scale tasks. Specifically, we compare Emformer with
latency-controlled BLSTM (LCBLSTM) on medium latency tasks and LSTM on low
latency tasks. On a low latency voice assistant task, Emformer gets 24% to 26%
relative word error rate reductions (WERRs). For medium latency scenarios,
comparing with LCBLSTM with similar model size and latency, Emformer gets
significant WERR across four languages in video captioning datasets with 2-3
times inference real-time factors reduction.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 23:04:21 GMT'}]",2020-10-29,"[['Wang', 'Yongqiang', ''], ['Shi', 'Yangyang', ''], ['Zhang', 'Frank', ''], ['Wu', 'Chunyang', ''], ['Chan', 'Julian', ''], ['Yeh', 'Ching-Feng', ''], ['Xiao', 'Alex', '']]"
1370990,2010.14660,Pierre Dognin,"Pierre L. Dognin, Igor Melnyk, Inkit Padhi, Cicero Nogueira dos
  Santos, Payel Das",DualTKB: A Dual Learning Bridge between Text and Knowledge Base,"Equal Contributions of Authors Pierre L. Dognin, Igor Melnyk, and
  Inkit Padhi. Accepted at EMNLP'20",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we present a dual learning approach for unsupervised text to
path and path to text transfers in Commonsense Knowledge Bases (KBs). We
investigate the impact of weak supervision by creating a weakly supervised
dataset and show that even a slight amount of supervision can significantly
improve the model performance and enable better-quality transfers. We examine
different model architectures, and evaluation metrics, proposing a novel
Commonsense KB completion metric tailored for generative models. Extensive
experimental results show that the proposed method compares very favorably to
the existing baselines. This approach is a viable step towards a more advanced
system for automatic KB construction/expansion and the reverse operation of KB
conversion to coherent textual descriptions.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 22:56:18 GMT'}]",2020-10-29,"[['Dognin', 'Pierre L.', ''], ['Melnyk', 'Igor', ''], ['Padhi', 'Inkit', ''], ['Santos', 'Cicero Nogueira dos', ''], ['Das', 'Payel', '']]"
1370979,2010.14649,Takashi Wada,"Takashi Wada, Tomoharu Iwata, Yuji Matsumoto, Timothy Baldwin, Jey Han
  Lau","Learning Contextualised Cross-lingual Word Embeddings for Extremely
  Low-Resource Languages Using Parallel Corpora",9 pages,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a new approach for learning contextualised cross-lingual word
embeddings based only on a small parallel corpus (e.g. a few hundred sentence
pairs). Our method obtains word embeddings via an LSTM-based encoder-decoder
model that performs bidirectional translation and reconstruction of the input
sentence. Through sharing model parameters among different languages, our model
jointly trains the word embeddings in a common multilingual space. We also
propose a simple method to combine word and subword embeddings to make use of
orthographic similarities across different languages. We base our experiments
on real-world data from endangered languages, namely Yongning Na,
Shipibo-Konibo and Griko. Our experiments on bilingual lexicon induction and
word alignment tasks show that our model outperforms existing methods by a
large margin for most language pairs. These results demonstrate that, contrary
to common belief, an encoder-decoder translation model is beneficial for
learning cross-lingual representations, even in extremely low-resource
scenarios.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 22:24:01 GMT'}]",2020-10-29,"[['Wada', 'Takashi', ''], ['Iwata', 'Tomoharu', ''], ['Matsumoto', 'Yuji', ''], ['Baldwin', 'Timothy', ''], ['Lau', 'Jey Han', '']]"
1370936,2010.14606,Arun Narayanan,"Arun Narayanan, Tara N. Sainath, Ruoming Pang, Jiahui Yu, Chung-Cheng
  Chiu, Rohit Prabhavalkar, Ehsan Variani, Trevor Strohman",Cascaded encoders for unifying streaming and non-streaming ASR,,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end (E2E) automatic speech recognition (ASR) models, by now, have
shown competitive performance on several benchmarks. These models are
structured to either operate in streaming or non-streaming mode. This work
presents cascaded encoders for building a single E2E ASR model that can operate
in both these modes simultaneously. The proposed model consists of streaming
and non-streaming encoders. Input features are first processed by the streaming
encoder; the non-streaming encoder operates exclusively on the output of the
streaming encoder. A single decoder then learns to decode either using the
output of the streaming or the non-streaming encoder. Results show that this
model achieves similar word error rates (WER) as a standalone streaming model
when operating in streaming mode, and obtains 10% -- 27% relative improvement
when operating in non-streaming mode. Our results also show that the proposed
approach outperforms existing E2E two-pass models, especially on long-form
speech.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 20:59:50 GMT'}]",2020-10-29,"[['Narayanan', 'Arun', ''], ['Sainath', 'Tara N.', ''], ['Pang', 'Ruoming', ''], ['Yu', 'Jiahui', ''], ['Chiu', 'Chung-Cheng', ''], ['Prabhavalkar', 'Rohit', ''], ['Variani', 'Ehsan', ''], ['Strohman', 'Trevor', '']]"
1370918,2010.14588,Robert Leaman,Robert Leaman and Zhiyong Lu,"A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and
  SARS-CoV-2",Accepted EMNLP NLP-COVID Workshop,,,,cs.DL cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The number of unique terms in the scientific literature used to refer to
either SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase
rapidly despite well-established standardized terms. This high degree of term
variation makes high recall identification of these important entities
difficult. In this manuscript we present an extensive dictionary of terms used
in the literature to refer to SARS-CoV-2 and COVID-19. We use a rule-based
approach to iteratively generate new term variants, then locate these variants
in a large text corpus. We compare our dictionary to an extensive collection of
terminological resources, demonstrating that our resource provides a
substantial number of additional terms. We use our dictionary to analyze the
usage of SARS-CoV-2 and COVID-19 terms over time and show that the number of
unique terms continues to grow rapidly. Our dictionary is freely available at
https://github.com/ncbi-nlp/CovidTermVar.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:51:53 GMT'}]",2020-10-29,"[['Leaman', 'Robert', ''], ['Lu', 'Zhiyong', '']]"
1370917,2010.14587,Jean-Baptiste Lamare,"Jean-Baptiste Lamare, Tobi Olatunji, Li Yao",On the diminishing return of labeling clinical reports,"Accepted at the EMNLP 2020 Clinical NLP workshop, 9 pages + 2 for
  references, 7 figures, 4 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ample evidence suggests that better machine learning models may be steadily
obtained by training on increasingly larger datasets on natural language
processing (NLP) problems from non-medical domains. Whether the same holds true
for medical NLP has by far not been thoroughly investigated. This work shows
that this is indeed not always the case. We reveal the somehow
counter-intuitive observation that performant medical NLP models may be
obtained with small amount of labeled data, quite the opposite to the common
belief, most likely due to the domain specificity of the problem. We show
quantitatively the effect of training data size on a fixed test set composed of
two of the largest public chest x-ray radiology report datasets on the task of
abnormality classification. The trained models not only make use of the
training data efficiently, but also outperform the current state-of-the-art
rule-based systems by a significant margin.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:51:04 GMT'}]",2020-10-29,"[['Lamare', 'Jean-Baptiste', ''], ['Olatunji', 'Tobi', ''], ['Yao', 'Li', '']]"
1370906,2010.14576,Jeniya Tabassum,"Jeniya Tabassum, Sydney Lee, Wei Xu, Alan Ritter","WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet
  Lab Protocols",to appear in EMNLP 2020 (WNUT),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents the results of the wet lab information extraction task at
WNUT 2020. This task consisted of two sub tasks: (1) a Named Entity Recognition
(NER) task with 13 participants and (2) a Relation Extraction (RE) task with 2
participants. We outline the task, data annotation process, corpus statistics,
and provide a high-level overview of the participating systems for each sub
task.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:34:53 GMT'}]",2020-10-29,"[['Tabassum', 'Jeniya', ''], ['Lee', 'Sydney', ''], ['Xu', 'Wei', ''], ['Ritter', 'Alan', '']]"
1370898,2010.14568,Kaiyu Yang,"Kaiyu Yang, Jia Deng",Strongly Incremental Constituency Parsing with Graph Neural Networks,Accepted to NeurIPS 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Parsing sentences into syntax trees can benefit downstream applications in
NLP. Transition-based parsers build trees by executing actions in a state
transition system. They are computationally efficient, and can leverage machine
learning to predict actions based on partial trees. However, existing
transition-based parsers are predominantly based on the shift-reduce transition
system, which does not align with how humans are known to parse sentences.
Psycholinguistic research suggests that human parsing is strongly incremental:
humans grow a single parse tree by adding exactly one token at each step. In
this paper, we propose a novel transition system called attach-juxtapose. It is
strongly incremental; it represents a partial sentence using a single tree;
each action adds exactly one token into the partial tree. Based on our
transition system, we develop a strongly incremental parser. At each step, it
encodes the partial tree using a graph neural network and predicts an action.
We evaluate our parser on Penn Treebank (PTB) and Chinese Treebank (CTB). On
PTB, it outperforms existing parsers trained with only constituency trees; and
it performs on par with state-of-the-art parsers that use dependency trees as
additional training data. On CTB, our parser establishes a new state of the
art. Code is available at
https://github.com/princeton-vl/attach-juxtapose-parser.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:19:38 GMT'}]",2020-10-29,"[['Yang', 'Kaiyu', ''], ['Deng', 'Jia', '']]"
1370887,2010.14557,Ruizhe Li,"Xiao Li, Guanyi Chen, Chenghua Lin, Ruizhe Li",DGST: a Dual-Generator Network for Text Style Transfer,"Accepted by EMNLP 2020, camera ready version",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose DGST, a novel and simple Dual-Generator network architecture for
text Style Transfer. Our model employs two generators only, and does not rely
on any discriminators or parallel corpus for training. Both quantitative and
qualitative experiments on the Yelp and IMDb datasets show that our model gives
competitive performance compared to several strong baselines with more
complicated architecture designs.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 18:54:51 GMT'}]",2020-10-29,"[['Li', 'Xiao', ''], ['Chen', 'Guanyi', ''], ['Lin', 'Chenghua', ''], ['Li', 'Ruizhe', '']]"
1370864,2010.14534,Marion Bartl,Marion Bartl and Malvina Nissim and Albert Gatt,"Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender
  Bias","10 pages, 4 figures, to appear in Proceedings of the 2nd Workshop on
  Gender Bias in Natural Language Processing at COLING 2020",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Contextualized word embeddings have been replacing standard embeddings as the
representational knowledge source of choice in NLP systems. Since a variety of
biases have previously been found in standard word embeddings, it is crucial to
assess biases encoded in their replacements as well. Focusing on BERT (Devlin
et al., 2018), we measure gender bias by studying associations between
gender-denoting target words and names of professions in English and German,
comparing the findings with real-world workforce statistics. We mitigate bias
by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying
Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that
our method of measuring bias is appropriate for languages such as English, but
not for languages with a rich morphology and gender-marking, such as German.
Our results highlight the importance of investigating bias and mitigation
techniques cross-linguistically, especially in view of the current emphasis on
large-scale, multilingual language models.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 18:06:09 GMT'}]",2020-10-29,"[['Bartl', 'Marion', ''], ['Nissim', 'Malvina', ''], ['Gatt', 'Albert', '']]"
1146834,1907.02298,Zi Lin,"Junjie Cao, Zi Lin, Weiwei Sun, Xiaojun Wan","A Comparative Analysis of Knowledge-Intensive and Data-Intensive
  Semantic Parsers",submitted to the journal Computational Linguistics,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We present a phenomenon-oriented comparative analysis of the two dominant
approaches in task-independent semantic parsing: classic, knowledge-intensive
and neural, data-intensive models. To reflect state-of-the-art neural NLP
technologies, we introduce a new target structure-centric parser that can
produce semantic graphs much more accurately than previous data-driven parsers.
We then show that, in spite of comparable performance overall, knowledge- and
data-intensive models produce different types of errors, in a way that can be
explained by their theoretical properties. This analysis leads to new
directions for parser development.
","[{'version': 'v1', 'created': 'Thu, 4 Jul 2019 09:40:27 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Aug 2019 10:36:59 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Oct 2020 08:39:30 GMT'}]",2020-10-29,"[['Cao', 'Junjie', ''], ['Lin', 'Zi', ''], ['Sun', 'Weiwei', ''], ['Wan', 'Xiaojun', '']]"
1247530,2002.10107,Issa Annamoradnejad,"Issa Annamoradnejad, Mohammadamin Fazli, Jafar Habibi",Predicting Subjective Features of Questions of QA Websites using BERT,"5 pages, 4 figures, 2 tables","2020 6th International Conference on Web Research (ICWR), Tehran,
  Iran, 2020, pp. 240-244",10.1109/ICWR49608.2020.9122318,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Community Question-Answering websites, such as StackOverflow and Quora,
expect users to follow specific guidelines in order to maintain content
quality. These systems mainly rely on community reports for assessing contents,
which has serious problems such as the slow handling of violations, the loss of
normal and experienced users' time, the low quality of some reports, and
discouraging feedback to new users. Therefore, with the overall goal of
providing solutions for automating moderation actions in Q&A websites, we aim
to provide a model to predict 20 quality or subjective aspects of questions in
QA websites. To this end, we used data gathered by the CrowdSource team at
Google Research in 2019 and a fine-tuned pre-trained BERT model on our problem.
Based on the evaluation by Mean-Squared-Error (MSE), the model achieved a value
of 0.046 after 2 epochs of training, which did not improve substantially in the
next ones. Results confirm that by simple fine-tuning, we can achieve accurate
models in little time and on less amount of data.
","[{'version': 'v1', 'created': 'Mon, 24 Feb 2020 07:56:02 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Mar 2020 08:10:16 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Jun 2020 13:22:04 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Oct 2020 14:37:39 GMT'}]",2020-10-29,"[['Annamoradnejad', 'Issa', ''], ['Fazli', 'Mohammadamin', ''], ['Habibi', 'Jafar', '']]"
1369332,2010.13002,Sam Shleifer,Sam Shleifer and Alexander M. Rush,Pre-trained Summarization Distillation,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent state-of-the-art approaches to summarization utilize large pre-trained
Transformer models. Distilling these models to smaller student models has
become critically important for practical use; however there are many different
distillation methods proposed by the NLP literature. Recent work on distilling
BERT for classification and regression tasks shows strong performance using
direct knowledge distillation. Alternatively, machine translation practitioners
distill using pseudo-labeling, where a small model is trained on the
translations of a larger model. A third, simpler approach is to 'shrink and
fine-tune' (SFT), which avoids any explicit distillation by copying parameters
to a smaller student model and then fine-tuning. We compare these three
approaches for distillation of Pegasus and BART, the current and former state
of the art, pre-trained summarization models, and find that SFT outperforms
knowledge distillation and pseudo-labeling on the CNN/DailyMail dataset, but
under-performs pseudo-labeling on the more abstractive XSUM dataset. PyTorch
Code and checkpoints of different sizes are available through Hugging Face
transformers here http://tiny.cc/4iy0tz.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 23:15:43 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 04:47:59 GMT'}]",2020-10-29,"[['Shleifer', 'Sam', ''], ['Rush', 'Alexander M.', '']]"
1371221,2010.14891,Mayuko Kori,"Mayuko Kori, Takeshi Tsukada and Naoki Kobayashi",A Cyclic Proof System for HFLN,27 pages,,,,cs.LO cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A cyclic proof system allows us to perform inductive reasoning without
explicit inductions. We propose a cyclic proof system for HFLN, which is a
higher-order predicate logic with natural numbers and alternating fixed-points.
Ours is the first cyclic proof system for a higher-order logic, to our
knowledge. Due to the presence of higher-order predicates and alternating
fixed-points, our cyclic proof system requires a more delicate global condition
on cyclic proofs than the original system of Brotherston and Simpson. We prove
the decidability of checking the global condition and soundness of this system,
and also prove a restricted form of standard completeness for an infinitary
variant of our cyclic proof system. A potential application of our cyclic proof
system is semi-automated verification of higher-order programs, based on
Kobayashi et al.'s recent work on reductions from program verification to HFLN
validity checking.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 11:19:53 GMT'}]",2020-10-29,"[['Kori', 'Mayuko', ''], ['Tsukada', 'Takeshi', ''], ['Kobayashi', 'Naoki', '']]"
1371250,2010.14920,Yuchen Liu,"Yuchen Liu, Junnan Zhu, Jiajun Zhang, and Chengqing Zong",Bridging the Modality Gap for Speech-to-Text Translation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end speech translation aims to translate speech in one language into
text in another language via an end-to-end way. Most existing methods employ an
encoder-decoder structure with a single encoder to learn acoustic
representation and semantic information simultaneously, which ignores the
speech-and-text modality differences and makes the encoder overloaded, leading
to great difficulty in learning such a model. To address these issues, we
propose a Speech-to-Text Adaptation for Speech Translation (STAST) model which
aims to improve the end-to-end model performance by bridging the modality gap
between speech and text. Specifically, we decouple the speech translation
encoder into three parts and introduce a shrink mechanism to match the length
of speech representation with that of the corresponding text transcription. To
obtain better semantic representation, we completely integrate a text-based
translation model into the STAST so that two tasks can be trained in the same
latent space. Furthermore, we introduce a cross-modal adaptation method to
close the distance between speech and text representation. Experimental results
on English-French and English-German speech translation corpora have shown that
our model significantly outperforms strong baselines, and achieves the new
state-of-the-art performance.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 12:33:04 GMT'}]",2020-10-29,"[['Liu', 'Yuchen', ''], ['Zhu', 'Junnan', ''], ['Zhang', 'Jiajun', ''], ['Zong', 'Chengqing', '']]"
1279293,2004.14523,Brian Thompson,Brian Thompson and Philipp Koehn,Exploiting Sentence Order in Document Alignment,EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a simple document alignment method that incorporates sentence
order information in both candidate generation and candidate re-scoring. Our
method results in 61% relative reduction in error compared to the best
previously published result on the WMT16 document alignment shared task. Our
method improves downstream MT performance on web-scraped Sinhala--English
documents from ParaCrawl, outperforming the document alignment method used in
the most recent ParaCrawl release. It also outperforms a comparable corpora
method which uses the same multilingual embeddings, demonstrating that
exploiting sentence order is beneficial even if the end goal is sentence-level
bitext.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 00:11:34 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 01:23:22 GMT'}]",2020-10-29,"[['Thompson', 'Brian', ''], ['Koehn', 'Philipp', '']]"
1371355,2010.15025,Xingchen Song,"Xingchen Song, Zhiyong Wu, Yiheng Huang, Chao Weng, Dan Su, Helen Meng",Non-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input,submitted to ICASSP 2021,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Non-autoregressive (NAR) transformer models have achieved significantly
inference speedup but at the cost of inferior accuracy compared to
autoregressive (AR) models in automatic speech recognition (ASR). Most of the
NAR transformers take a fixed-length sequence filled with MASK tokens or a
redundant sequence copied from encoder states as decoder input, they cannot
provide efficient target-side information thus leading to accuracy degradation.
To address this problem, we propose a CTC-enhanced NAR transformer, which
generates target sequence by refining predictions of the CTC module.
Experimental results show that our method outperforms all previous NAR
counterparts and achieves 50x faster decoding speed than a strong AR baseline
with only 0.0 ~ 0.3 absolute CER degradation on Aishell-1 and Aishell-2
datasets.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 15:00:09 GMT'}]",2020-10-29,"[['Song', 'Xingchen', ''], ['Wu', 'Zhiyong', ''], ['Huang', 'Yiheng', ''], ['Weng', 'Chao', ''], ['Su', 'Dan', ''], ['Meng', 'Helen', '']]"
1371282,2010.14952,Isar Nejadgholi,Svetlana Kiritchenko and Isar Nejadgholi,Towards Ethics by Design in Online Abusive Content Detection,"14 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To support safety and inclusion in online communications, significant efforts
in NLP research have been put towards addressing the problem of abusive content
detection, commonly defined as a supervised classification task. The research
effort has spread out across several closely related sub-areas, such as
detection of hate speech, toxicity, cyberbullying, etc. There is a pressing
need to consolidate the field under a common framework for task formulation,
dataset design and performance evaluation. Further, despite current
technologies achieving high classification accuracies, several ethical issues
have been revealed. We bring ethical issues to forefront and propose a unified
framework as a two-step process. First, online content is categorized around
personal and identity-related subject matters. Second, severity of abuse is
identified through comparative annotation within each category. The novel
framework is guided by the Ethics by Design principle and is a step towards
building more accurate and trusted models.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 13:10:24 GMT'}]",2020-10-29,"[['Kiritchenko', 'Svetlana', ''], ['Nejadgholi', 'Isar', '']]"
1371444,2010.15114,Kyle Aitken,"Kyle Aitken, Vinay V. Ramasesh, Ankush Garg, Yuan Cao, David Sussillo,
  Niru Maheswaranathan",The geometry of integration in text classification RNNs,"9+19 pages, 30 figures",,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the widespread application of recurrent neural networks (RNNs) across
a variety of tasks, a unified understanding of how RNNs solve these tasks
remains elusive. In particular, it is unclear what dynamical patterns arise in
trained RNNs, and how those patterns depend on the training dataset or task.
This work addresses these questions in the context of a specific natural
language processing task: text classification. Using tools from dynamical
systems analysis, we study recurrent networks trained on a battery of both
natural and synthetic text classification tasks. We find the dynamics of these
trained RNNs to be both interpretable and low-dimensional. Specifically, across
architectures and datasets, RNNs accumulate evidence for each class as they
process the text, using a low-dimensional attractor manifold as the underlying
mechanism. Moreover, the dimensionality and geometry of the attractor manifold
are determined by the structure of the training dataset; in particular, we
describe how simple word-count statistics computed on the training dataset can
be used to predict these properties. Our observations span multiple
architectures and datasets, reflecting a common mechanism RNNs employ to
perform text classification. To the degree that integration of evidence towards
a decision is a common computational primitive, this work lays the foundation
for using dynamical systems techniques to study the inner workings of RNNs.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 17:58:53 GMT'}]",2020-10-29,"[['Aitken', 'Kyle', ''], ['Ramasesh', 'Vinay V.', ''], ['Garg', 'Ankush', ''], ['Cao', 'Yuan', ''], ['Sussillo', 'David', ''], ['Maheswaranathan', 'Niru', '']]"
1371397,2010.15067,Muhammed Tarik Altuncu,"M. Tarik Altuncu, Sophia N. Yaliraki, Mauricio Barahona","Graph-based Topic Extraction from Vector Embeddings of Text Documents:
  Application to a Corpus of News Articles",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Production of news content is growing at an astonishing rate. To help manage
and monitor the sheer amount of text, there is an increasing need to develop
efficient methods that can provide insights into emerging content areas, and
stratify unstructured corpora of text into `topics' that stem intrinsically
from content similarity. Here we present an unsupervised framework that brings
together powerful vector embeddings from natural language processing with tools
from multiscale graph partitioning that can reveal natural partitions at
different resolutions without making a priori assumptions about the number of
clusters in the corpus. We show the advantages of graph-based clustering
through end-to-end comparisons with other popular clustering and topic
modelling methods, and also evaluate different text vector embeddings, from
classic Bag-of-Words to Doc2Vec to the recent transformers based model Bert.
This comparative work is showcased through an analysis of a corpus of US news
coverage during the presidential election year of 2016.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 16:20:05 GMT'}]",2020-10-29,"[['Altuncu', 'M. Tarik', ''], ['Yaliraki', 'Sophia N.', ''], ['Barahona', 'Mauricio', '']]"
1371420,2010.15090,Vishal Sunder,Vishal Sunder and Eric Fosler-Lussier,"Handling Class Imbalance in Low-Resource Dialogue Systems by Combining
  Few-Shot Classification and Interpolation","5 pages, 4 figures, 3 tables",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Utterance classification performance in low-resource dialogue systems is
constrained by an inevitably high degree of data imbalance in class labels. We
present a new end-to-end pairwise learning framework that is designed
specifically to tackle this phenomenon by inducing a few-shot classification
capability in the utterance representations and augmenting data through an
interpolation of utterance representations. Our approach is a general purpose
training methodology, agnostic to the neural architecture used for encoding
utterances. We show significant improvements in macro-F1 score over standard
cross-entropy training for three different neural architectures, demonstrating
improvements on a Virtual Patient dialogue dataset as well as a low-resourced
emulation of the Switchboard dialogue act classification dataset.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 17:05:24 GMT'}]",2020-10-29,"[['Sunder', 'Vishal', ''], ['Fosler-Lussier', 'Eric', '']]"
1371366,2010.15036,Usman Naseem,"Usman Naseem, Imran Razzak, Shah Khalid Khan, Mukesh Prasad","A Comprehensive Survey on Word Representation Models: From Classical to
  State-Of-The-Art Word Representation Language Models",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Word representation has always been an important research area in the history
of natural language processing (NLP). Understanding such complex text data is
imperative, given that it is rich in information and can be used widely across
various applications. In this survey, we explore different word representation
models and its power of expression, from the classical to modern-day
state-of-the-art word representation language models (LMS). We describe a
variety of text representation methods, and model designs have blossomed in the
context of NLP, including SOTA LMs. These models can transform large volumes of
text into effective vector representations capturing the same semantic
information. Further, such representations can be utilized by various machine
learning (ML) algorithms for a variety of NLP related tasks. In the end, this
survey briefly discusses the commonly used ML and DL based classifiers,
evaluation metrics and the applications of these word embeddings in different
NLP tasks.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 15:15:13 GMT'}]",2020-10-29,"[['Naseem', 'Usman', ''], ['Razzak', 'Imran', ''], ['Khan', 'Shah Khalid', ''], ['Prasad', 'Mukesh', '']]"
1371395,2010.15065,Amir Shanehsazzadeh,"Amir Shanehsazzadeh, David Belanger, David Dohan",Fixed-Length Protein Embeddings using Contextual Lenses,,,,,q-bio.BM cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Basic Local Alignment Search Tool (BLAST) is currently the most popular
method for searching databases of biological sequences. BLAST compares
sequences via similarity defined by a weighted edit distance, which results in
it being computationally expensive. As opposed to working with edit distance, a
vector similarity approach can be accelerated substantially using modern
hardware or hashing techniques. Such an approach would require fixed-length
embeddings for biological sequences. There has been recent interest in learning
fixed-length protein embeddings using deep learning models under the hypothesis
that the hidden layers of supervised or semi-supervised models could produce
potentially useful vector embeddings. We consider transformer (BERT) protein
language models that are pretrained on the TrEMBL data set and learn
fixed-length embeddings on top of them with contextual lenses. The embeddings
are trained to predict the family a protein belongs to for sequences in the
Pfam database. We show that for nearest-neighbor family classification,
pretraining offers a noticeable boost in performance and that the corresponding
learned embeddings are competitive with BLAST. Furthermore, we show that the
raw transformer embeddings, obtained via static pooling, do not perform well on
nearest-neighbor family classification, which suggests that learning embeddings
in a supervised manner via contextual lenses may be a compute-efficient
alternative to fine-tuning.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 14:54:55 GMT'}]",2020-10-29,"[['Shanehsazzadeh', 'Amir', ''], ['Belanger', 'David', ''], ['Dohan', 'David', '']]"
1267866,2004.03096,Yiming Cui,"Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, Guoping Hu",Is Graph Structure Necessary for Multi-hop Question Answering?,"6 pages, to appear at EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, attempting to model texts as graph structure and introducing graph
neural networks to deal with it has become a trend in many NLP research areas.
In this paper, we investigate whether the graph structure is necessary for
multi-hop question answering. Our analysis is centered on HotpotQA. We
construct a strong baseline model to establish that, with the proper use of
pre-trained models, graph structure may not be necessary for multi-hop question
answering. We point out that both graph structure and adjacency matrix are
task-related prior knowledge, and graph-attention can be considered as a
special case of self-attention. Experiments and visualized analysis demonstrate
that graph-attention or the entire graph structure can be replaced by
self-attention or Transformers.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 02:59:42 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 09:29:19 GMT'}]",2020-10-30,"[['Shao', 'Nan', ''], ['Cui', 'Yiming', ''], ['Liu', 'Ting', ''], ['Wang', 'Shijin', ''], ['Hu', 'Guoping', '']]"
1356512,2010.00182,Yang Zhang,"Yang Zhang, Qiang Ma",Dual Attention Model for Citation Recommendation,,,,,cs.IR cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Based on an exponentially increasing number of academic articles, discovering
and citing comprehensive and appropriate resources has become a non-trivial
task. Conventional citation recommender methods suffer from severe information
loss. For example, they do not consider the section of the paper that the user
is writing and for which they need to find a citation, the relatedness between
the words in the local context (the text span that describes a citation), or
the importance on each word from the local context. These shortcomings make
such methods insufficient for recommending adequate citations to academic
manuscripts. In this study, we propose a novel embedding-based neural network
called ""dual attention model for citation recommendation (DACR)"" to recommend
citations during manuscript preparation. Our method adapts embedding of three
dimensions of semantic information: words in the local context, structural
contexts, and the section on which a user is working. A neural network is
designed to maximize the similarity between the embedding of the three input
(local context words, section and structural contexts) and the target citation
appearing in the context. The core of the neural network is composed of
self-attention and additive attention, where the former aims to capture the
relatedness between the contextual words and structural context, and the latter
aims to learn the importance of them. The experiments on real-world datasets
demonstrate the effectiveness of the proposed approach.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 02:41:47 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 11:27:20 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Oct 2020 12:57:58 GMT'}, {'version': 'v4', 'created': 'Thu, 29 Oct 2020 12:31:26 GMT'}]",2020-10-30,"[['Zhang', 'Yang', ''], ['Ma', 'Qiang', '']]"
1371865,2010.15535,Craig Stewart,"Ricardo Rei, Craig Stewart, Catarina Farinha, Alon Lavie",Unbabel's Participation in the WMT20 Metrics Shared Task,WMT Metrics Shared Task 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present the contribution of the Unbabel team to the WMT 2020 Shared Task
on Metrics. We intend to participate on the segment-level, document-level and
system-level tracks on all language pairs, as well as the 'QE as a Metric'
track. Accordingly, we illustrate results of our models in these tracks with
reference to test sets from the previous year. Our submissions build upon the
recently proposed COMET framework: We train several estimator models to regress
on different human-generated quality scores and a novel ranking model trained
on relative ranks obtained from Direct Assessments. We also propose a simple
technique for converting segment-level predictions into a document-level score.
Overall, our systems achieve strong results for all language pairs on previous
test sets and in many cases set a new state-of-the-art.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 12:59:44 GMT'}]",2020-10-30,"[['Rei', 'Ricardo', ''], ['Stewart', 'Craig', ''], ['Farinha', 'Catarina', ''], ['Lavie', 'Alon', '']]"
1371796,2010.15466,Yuyang Nie,"Yuyang Nie, Yuanhe Tian, Yan Song, Xiang Ao, and Xiang Wan","Improving Named Entity Recognition with Attentive Ensemble of Syntactic
  Information","Natural Language Processing. 15 pages, 3 figures, Findings of
  EMNLP-2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Named entity recognition (NER) is highly sensitive to sentential syntactic
and semantic properties where entities may be extracted according to how they
are used and placed in the running text. To model such properties, one could
rely on existing resources to providing helpful knowledge to the NER task; some
existing studies proved the effectiveness of doing so, and yet are limited in
appropriately leveraging the knowledge such as distinguishing the important
ones for particular context. In this paper, we improve NER by leveraging
different types of syntactic information through attentive ensemble, which
functionalizes by the proposed key-value memory networks, syntax attention, and
the gate mechanism for encoding, weighting and aggregating such syntactic
information, respectively. Experimental results on six English and Chinese
benchmark datasets suggest the effectiveness of the proposed model and show
that it outperforms previous studies on all experiment datasets.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 10:25:17 GMT'}]",2020-10-30,"[['Nie', 'Yuyang', ''], ['Tian', 'Yuanhe', ''], ['Song', 'Yan', ''], ['Ao', 'Xiang', ''], ['Wan', 'Xiang', '']]"
1371788,2010.15458,Yuyang Nie,"Yuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, and Bo Dai","Named Entity Recognition for Social Media Texts with Semantic
  Augmentation","Natural Language Processing. 9 pages, 3 figures. EMNLP-2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing approaches for named entity recognition suffer from data sparsity
problems when conducted on short and informal texts, especially user-generated
social media content. Semantic augmentation is a potential way to alleviate
this problem. Given that rich semantic information is implicitly preserved in
pre-trained word embeddings, they are potential ideal resources for semantic
augmentation. In this paper, we propose a neural-based approach to NER for
social media texts where both local (from running text) and augmented semantics
are taken into account. In particular, we obtain the augmented semantic
information from a large-scale corpus, and propose an attentive semantic
augmentation module and a gate module to encode and aggregate such information,
respectively. Extensive experiments are performed on three benchmark datasets
collected from English and Chinese social media platforms, where the results
demonstrate the superiority of our approach to previous studies across all
three datasets.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 10:06:46 GMT'}]",2020-10-30,"[['Nie', 'Yuyang', ''], ['Tian', 'Yuanhe', ''], ['Wan', 'Xiang', ''], ['Song', 'Yan', ''], ['Dai', 'Bo', '']]"
1370914,2010.14584,Aleksandra Edwards Mrs,"Aleksandra Edwards, David Rogers, Jose Camacho-Collados, H\'el\`ene de
  Ribaupierre, Alun Preece","Predicting Themes within Complex Unstructured Texts: A Case Study on
  Safeguarding Reports","10 pages, 5 figures, workshop",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task of text and sentence classification is associated with the need for
large amounts of labelled training data. The acquisition of high volumes of
labelled datasets can be expensive or unfeasible, especially for
highly-specialised domains for which documents are hard to obtain. Research on
the application of supervised classification based on small amounts of training
data is limited. In this paper, we address the combination of state-of-the-art
deep learning and classification methods and provide an insight into what
combination of methods fit the needs of small, domain-specific, and
terminologically-rich corpora. We focus on a real-world scenario related to a
collection of safeguarding reports comprising learning experiences and
reflections on tackling serious incidents involving children and vulnerable
adults. The relatively small volume of available reports and their use of
highly domain-specific terminology makes the application of automated
approaches difficult. We focus on the problem of automatically identifying the
main themes in a safeguarding report using supervised classification
approaches. Our results show the potential of deep learning models to simulate
subject-expert behaviour even for complex tasks with limited labelled data.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:48:23 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 09:15:14 GMT'}]",2020-10-30,"[['Edwards', 'Aleksandra', ''], ['Rogers', 'David', ''], ['Camacho-Collados', 'Jose', ''], ['de Ribaupierre', 'Hélène', ''], ['Preece', 'Alun', '']]"
1371767,2010.15437,Mana Ihori,"Mana Ihori, Ryo Masumura, Naoki Makishima, Tomohiro Tanaka, Akihiko
  Takashima, Shota Orihashi","Memory Attentive Fusion: External Language Model Integration for
  Transformer-based Sequence-to-Sequence Model",Accepted as a short paper at INLG 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a novel fusion method for integrating an external
language model (LM) into the Transformer based sequence-to-sequence (seq2seq)
model. While paired data are basically required to train the seq2seq model, the
external LM can be trained with only unpaired data. Thus, it is important to
leverage memorized knowledge in the external LM for building the seq2seq model,
since it is hard to prepare a large amount of paired data. However, the
existing fusion methods assume that the LM is integrated with recurrent neural
network-based seq2seq models instead of the Transformer. Therefore, this paper
proposes a fusion method that can explicitly utilize network structures in the
Transformer. The proposed method, called {\bf memory attentive fusion},
leverages the Transformer-style attention mechanism that repeats source-target
attention in a multi-hop manner for reading the memorized knowledge in the LM.
Our experiments on two text-style conversion tasks demonstrate that the
proposed method performs better than conventional fusion methods.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 09:16:23 GMT'}]",2020-10-30,"[['Ihori', 'Mana', ''], ['Masumura', 'Ryo', ''], ['Makishima', 'Naoki', ''], ['Tanaka', 'Tomohiro', ''], ['Takashima', 'Akihiko', ''], ['Orihashi', 'Shota', '']]"
1370901,2010.14571,Isaac Caswell,"Isaac Caswell, Theresa Breiner, Daan van Esch, Ankur Bapna","Language ID in the Wild: Unexpected Challenges on the Path to a
  Thousand-Language Web Text Corpus",Accepted to COLING 2020. 9 pages with 8 page abstract,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large text corpora are increasingly important for a wide variety of Natural
Language Processing (NLP) tasks, and automatic language identification (LangID)
is a core technology needed to collect such datasets in a multilingual context.
LangID is largely treated as solved in the literature, with models reported
that achieve over 90% average F1 on as many as 1,366 languages. We train LangID
models on up to 1,629 languages with comparable quality on held-out test sets,
but find that human-judged LangID accuracy for web-crawl text corpora created
using these models is only around 5% for many lower-resource languages,
suggesting a need for more robust evaluation. Further analysis revealed a
variety of error modes, arising from domain mismatch, class imbalance, language
similarity, and insufficiently expressive models. We propose two classes of
techniques to mitigate these errors: wordlist-based tunable-precision filters
(for which we release curated lists in about 500 languages) and
transformer-based semi-supervised LangID models, which increase median dataset
precision from 5.5% to 71.2%. These techniques enable us to create an initial
data set covering 100K or more relatively clean sentences in each of 500+
languages, paving the way towards a 1,000-language web text corpus.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:29:17 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 15:18:35 GMT'}]",2020-10-30,"[['Caswell', 'Isaac', ''], ['Breiner', 'Theresa', ''], ['van Esch', 'Daan', ''], ['Bapna', 'Ankur', '']]"
1371753,2010.15423,M\=arcis Pinnis,"Rihards Kri\v{s}lauks, M\=arcis Pinnis",Tilde at WMT 2020: News Task Systems,,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  This paper describes Tilde's submission to the WMT2020 shared task on news
translation for both directions of the English-Polish language pair in both the
constrained and the unconstrained tracks. We follow our submissions from the
previous years and build our baseline systems to be morphologically motivated
sub-word unit-based Transformer base models that we train using the Marian
machine translation toolkit. Additionally, we experiment with different
parallel and monolingual data selection schemes, as well as sampled
back-translation. Our final models are ensembles of Transformer base and
Transformer big models that feature right-to-left re-ranking.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 08:59:37 GMT'}]",2020-10-30,"[['Krišlauks', 'Rihards', ''], ['Pinnis', 'Mārcis', '']]"
1371741,2010.15411,Milan Gritta,"Milan Gritta, Gerasimos Lampouras and Ignacio Iacobacci","Conversation Graph: Data Augmentation, Training and Evaluation for
  Non-Deterministic Dialogue Management","Accepted at Transactions of Association of Computational Linguistics
  (to be presented at ACL 2021)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task-oriented dialogue systems typically rely on large amounts of
high-quality training data or require complex handcrafted rules. However,
existing datasets are often limited in size considering the complexity of the
dialogues. Additionally, conventional training signal inference is not suitable
for non-deterministic agent behaviour, i.e. considering multiple actions as
valid in identical dialogue states. We propose the Conversation Graph
(ConvGraph), a graph-based representation of dialogues that can be exploited
for data augmentation, multi-reference training and evaluation of
non-deterministic agents. ConvGraph generates novel dialogue paths to augment
data volume and diversity. Intrinsic and extrinsic evaluation across three
datasets shows that data augmentation and/or multi-reference training with
ConvGraph can improve dialogue success rates by up to 6.4%.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 08:23:24 GMT'}]",2020-10-30,"[['Gritta', 'Milan', ''], ['Lampouras', 'Gerasimos', ''], ['Iacobacci', 'Ignacio', '']]"
1371928,2010.15598,Micaela Kaplan,Micaela Kaplan,"May I Ask Who's Calling? Named Entity Recognition on Call Center
  Transcripts for Privacy Law Compliance",The 6th Workshop on Noisy User-generated Text (W-NUT) 2020 at EMNLP,"Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop
  on Noisy User-generated Text (2020) 1-6",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate using Named Entity Recognition on a new type of user-generated
text: a call center conversation. These conversations combine problems from
spontaneous speech with problems novel to conversational Automated Speech
Recognition, including incorrect recognition, alongside other common problems
from noisy user-generated text. Using our own corpus with new annotations,
training custom contextual string embeddings, and applying a BiLSTM-CRF, we
match state-of-the-art results on our novel task.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 13:53:42 GMT'}]",2020-10-30,"[['Kaplan', 'Micaela', '']]"
1371696,2010.15366,Sung-Feng Huang,"Sung-Feng Huang, Shun-Po Chuang, Da-Rong Liu, Yi-Chen Chen, Gene-Ping
  Yang, Hung-yi Lee","Self-supervised Pre-training Reduces Label Permutation Instability of
  Speech Separation",submitted to ICASSP2021,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Speech separation has been well-developed while there are still problems
waiting to be solved. The main problem we focus on in this paper is the
frequent label permutation switching of permutation invariant training (PIT).
For N-speaker separation, there would be N! possible label permutations. How to
stably select correct label permutations is a long-standing problem. In this
paper, we utilize self-supervised pre-training to stabilize the label
permutations. Among several types of self-supervised tasks, speech enhancement
based pre-training tasks show significant effectiveness in our experiments.
When using off-the-shelf pre-trained models, training duration could be
shortened to one-third to two-thirds. Furthermore, even taking pre-training
time into account, the entire training process could still be shorter without a
performance drop when using a larger batch size.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 06:07:01 GMT'}]",2020-10-30,"[['Huang', 'Sung-Feng', ''], ['Chuang', 'Shun-Po', ''], ['Liu', 'Da-Rong', ''], ['Chen', 'Yi-Chen', ''], ['Yang', 'Gene-Ping', ''], ['Lee', 'Hung-yi', '']]"
1371690,2010.15360,Shaolei Wang,"Shaolei Wang, Zhongyuan Wang, Wanxiang Che, Ting Liu","Combining Self-Training and Self-Supervised Learning for Unsupervised
  Disfluency Detection",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most existing approaches to disfluency detection heavily rely on
human-annotated corpora, which is expensive to obtain in practice. There have
been several proposals to alleviate this issue with, for instance,
self-supervised learning techniques, but they still require human-annotated
corpora. In this work, we explore the unsupervised learning paradigm which can
potentially work with unlabeled text corpora that are cheaper and easier to
obtain. Our model builds upon the recent work on Noisy Student Training, a
semi-supervised learning approach that extends the idea of self-training.
Experimental results on the commonly used English Switchboard test set show
that our approach achieves competitive performance compared to the previous
state-of-the-art supervised systems using contextualized word embeddings (e.g.
BERT and ELECTRA).
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 05:29:26 GMT'}]",2020-10-30,"[['Wang', 'Shaolei', ''], ['Wang', 'Zhongyuan', ''], ['Che', 'Wanxiang', ''], ['Liu', 'Ting', '']]"
1348756,2009.07253,Alexander Lin,"Alexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei",Autoregressive Knowledge Distillation through Imitation Learning,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The performance of autoregressive models on natural language generation tasks
has dramatically improved due to the adoption of deep, self-attentive
architectures. However, these gains have come at the cost of hindering
inference speed, making state-of-the-art models cumbersome to deploy in
real-world, time-sensitive settings. We develop a compression technique for
autoregressive models that is driven by an imitation learning perspective on
knowledge distillation. The algorithm is designed to address the exposure bias
problem. On prototypical language generation tasks such as translation and
summarization, our method consistently outperforms other distillation
algorithms, such as sequence-level knowledge distillation. Student models
trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those
trained from scratch, while increasing inference speed by up to 14 times in
comparison to the teacher model.
","[{'version': 'v1', 'created': 'Tue, 15 Sep 2020 17:43:02 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 00:40:45 GMT'}]",2020-10-30,"[['Lin', 'Alexander', ''], ['Wohlwend', 'Jeremy', ''], ['Chen', 'Howard', ''], ['Lei', 'Tao', '']]"
1371646,2010.15316,Michal Malyska,"Alister D Costa, Stefan Denkovski, Michal Malyska, Sae Young Moon,
  Brandon Rufino, Zhen Yang, Taylor Killian, Marzyeh Ghassemi",Multiple Sclerosis Severity Classification From Clinical Text,EMNLP 2020 Clinical NLP workshop,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative
neurological disease, which is monitored by a specialist using the Expanded
Disability Status Scale (EDSS) and recorded in unstructured text in the form of
a neurology consult note. An EDSS measurement contains an overall ""EDSS"" score
and several functional subscores. Typically, expert knowledge is required to
interpret consult notes and generate these scores. Previous approaches used
limited context length Word2Vec embeddings and keyword searches to predict
scores given a consult note, but often failed when scores were not explicitly
stated. In this work, we present MS-BERT, the first publicly available
transformer model trained on real clinical data other than MIMIC. Next, we
present MSBC, a classifier that applies MS-BERT to generate embeddings and
predict EDSS and functional subscores. Lastly, we explore combining MSBC with
other models through the use of Snorkel to generate scores for unlabelled
consult notes. MSBC achieves state-of-the-art performance on all metrics and
prediction tasks and outperforms the models generated from the Snorkel
ensemble. We improve Macro-F1 by 0.12 (to 0.88) for predicting EDSS and on
average by 0.29 (to 0.63) for predicting functional subscores over previous
Word2Vec CNN and rule-based approaches.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 02:15:23 GMT'}]",2020-10-30,"[['Costa', 'Alister D', ''], ['Denkovski', 'Stefan', ''], ['Malyska', 'Michal', ''], ['Moon', 'Sae Young', ''], ['Rufino', 'Brandon', ''], ['Yang', 'Zhen', ''], ['Killian', 'Taylor', ''], ['Ghassemi', 'Marzyeh', '']]"
1265269,2004.00499,Shengbin Jia,Shengbin Jia,Unique Chinese Linguistic Phenomena,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Linguistics holds unique characteristics of generality, stability, and
nationality, which will affect the formulation of extraction strategies and
should be incorporated into the relation extraction. Chinese open relation
extraction is not well-established, because of the complexity of Chinese
linguistics makes it harder to operate, and the methods for English are not
compatible with that for Chinese. The diversities between Chinese and English
linguistics are mainly reflected in morphology and syntax.
","[{'version': 'v1', 'created': 'Sun, 23 Feb 2020 12:13:48 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Jul 2020 10:00:08 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 06:07:07 GMT'}]",2020-10-30,"[['Jia', 'Shengbin', '']]"
1150762,1907.06226,Jipeng Qiang,Jipeng Qiang and Yun Li and Yi Zhu and Yunhao Yuan and Xindong Wu,Lexical Simplification with Pretrained Encoders,,,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lexical simplification (LS) aims to replace complex words in a given sentence
with their simpler alternatives of equivalent meaning. Recently unsupervised
lexical simplification approaches only rely on the complex word itself
regardless of the given sentence to generate candidate substitutions, which
will inevitably produce a large number of spurious candidates. We present a
simple LS approach that makes use of the Bidirectional Encoder Representations
from Transformers (BERT) which can consider both the given sentence and the
complex word during generating candidate substitutions for the complex word.
Specifically, we mask the complex word of the original sentence for feeding
into the BERT to predict the masked token. The predicted results will be used
as candidate substitutions. Despite being entirely unsupervised, experimental
results show that our approach obtains obvious improvement compared with these
baselines leveraging linguistic databases and parallel corpus, outperforming
the state-of-the-art by more than 12 Accuracy points on three well-known
benchmarks.
","[{'version': 'v1', 'created': 'Sun, 14 Jul 2019 14:19:22 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Jul 2019 14:36:41 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Jul 2019 03:36:12 GMT'}, {'version': 'v4', 'created': 'Fri, 16 Aug 2019 01:48:46 GMT'}, {'version': 'v5', 'created': 'Thu, 29 Oct 2020 03:21:25 GMT'}]",2020-10-30,"[['Qiang', 'Jipeng', ''], ['Li', 'Yun', ''], ['Zhu', 'Yi', ''], ['Yuan', 'Yunhao', ''], ['Wu', 'Xindong', '']]"
1371643,2010.15313,Keen You,Keen You and Dan Goldwasser,"""where is this relationship going?"": Understanding Relationship
  Trajectories in Narrative Text",Accepted to *Sem 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We examine a new commonsense reasoning task: given a narrative describing a
social interaction that centers on two protagonists, systems make inferences
about the underlying relationship trajectory. Specifically, we propose two
evaluation tasks: Relationship Outlook Prediction MCQ and Resolution Prediction
MCQ. In Relationship Outlook Prediction, a system maps an interaction to a
relationship outlook that captures how the interaction is expected to change
the relationship. In Resolution Prediction, a system attributes a given
relationship outlook to a particular resolution that explains the outcome.
These two tasks parallel two real-life questions that people frequently ponder
upon as they navigate different social situations: ""where is this relationship
going?"" and ""how did we end up here?"". To facilitate the investigation of human
social relationships through these two tasks, we construct a new dataset,
Social Narrative Tree, which consists of 1250 stories documenting a variety of
daily social interactions. The narratives encode a multitude of social elements
that interweave to give rise to rich commonsense knowledge of how relationships
evolve with respect to social interactions. We establish baseline performances
using language models and the accuracies are significantly lower than human
performance. The results demonstrate that models need to look beyond syntactic
and semantic signals to comprehend complex human relationships.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 02:07:05 GMT'}]",2020-10-30,"[['You', 'Keen', ''], ['Goldwasser', 'Dan', '']]"
1371930,2010.15600,Ciro Garcia Mr,Ciro Ivan Garcia Lopez,Three computational models and its equivalence,,,,,cs.LO cs.CC cs.CL cs.GL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The study of computability has its origin in Hilbert's conference of 1900,
where an adjacent question, to the ones he asked, is to give a precise
description of the notion of algorithm. In the search for a good definition
arose three independent theories: Turing and the Turing machines, G\""odel and
the recursive functions, Church and the Lambda Calculus.
  Later there were established by Kleene that the classic models of computation
are equivalent. This fact is widely accepted by many textbooks and the proof is
omitted since the proof is tedious and unreadable. We intend to fill this gap
presenting the proof in a modern way, without forgetting the mathematical
details.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 05:55:19 GMT'}]",2020-10-30,"[['Lopez', 'Ciro Ivan Garcia', '']]"
1371983,2010.15653,Niko Moritz,"Niko Moritz, Takaaki Hori, Jonathan Le Roux","Semi-Supervised Speech Recognition via Graph-based Temporal
  Classification",Submitted to ICASSP 2021,,,,cs.LG cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semi-supervised learning has demonstrated promising results in automatic
speech recognition (ASR) by self-training using a seed ASR model with
pseudo-labels generated for unlabeled data. The effectiveness of this approach
largely relies on the pseudo-label accuracy, for which typically only the
1-best ASR hypothesis is used. However, alternative ASR hypotheses of an N-best
list can provide more accurate labels for an unlabeled speech utterance and
also reflect uncertainties of the seed ASR model. In this paper, we propose a
generalized form of the connectionist temporal classification (CTC) objective
that accepts a graph representation of the training targets. The newly proposed
graph-based temporal classification (GTC) objective is applied for
self-training with WFST-based supervision, which is generated from an N-best
list of pseudo-labels. In this setup, GTC is used to learn not only a temporal
alignment, similarly to CTC, but also a label alignment to obtain the optimal
pseudo-label sequence from the weighted graph. Results show that this approach
can effectively exploit an N-best list of pseudo-labels with associated scores,
outperforming standard pseudo-labeling by a large margin, with ASR results
close to an oracle experiment in which the best hypotheses of the N-best lists
are selected manually.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 14:56:56 GMT'}]",2020-10-30,"[['Moritz', 'Niko', ''], ['Hori', 'Takaaki', ''], ['Roux', 'Jonathan Le', '']]"
1358840,2010.02510,Lily Ou,"Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli, Sharon
  Levy, Diba Mirza, William Yang Wang","Investigating African-American Vernacular English in Transformer-Based
  Text Generation","7 pages, EMNLP 2020",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The growth of social media has encouraged the written use of African American
Vernacular English (AAVE), which has traditionally been used only in oral
contexts. However, NLP models have historically been developed using dominant
English varieties, such as Standard American English (SAE), due to text corpora
availability. We investigate the performance of GPT-2 on AAVE text by creating
a dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating
syntactic structure and AAVE- or SAE-specific language for each pair. We
evaluate each sample and its GPT-2 generated text with pretrained sentiment
classifiers and find that while AAVE text results in more classifications of
negative sentiment than SAE, the use of GPT-2 generally increases occurrences
of positive sentiment for both. Additionally, we conduct human evaluation of
AAVE and SAE text generated with GPT-2 to compare contextual rigor and overall
quality.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 06:27:02 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 04:00:46 GMT'}]",2020-10-30,"[['Groenwold', 'Sophie', ''], ['Ou', 'Lily', ''], ['Parekh', 'Aesha', ''], ['Honnavalli', 'Samhita', ''], ['Levy', 'Sharon', ''], ['Mirza', 'Diba', ''], ['Wang', 'William Yang', '']]"
1367089,2010.10759,Yangyang Shi,"Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian
  Chan, Frank Zhang, Duc Le, Mike Seltzer","Emformer: Efficient Memory Transformer Based Acoustic Model For Low
  Latency Streaming Speech Recognition","5 pages, 2 figures, submitted to ICASSP 2021",,,,cs.SD cs.CL cs.LG eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper proposes an efficient memory transformer Emformer for low latency
streaming speech recognition. In Emformer, the long-range history context is
distilled into an augmented memory bank to reduce self-attention's computation
complexity. A cache mechanism saves the computation for the key and value in
self-attention for the left context. Emformer applies a parallelized block
processing in training to support low latency models. We carry out experiments
on benchmark LibriSpeech data. Under average latency of 960 ms, Emformer gets
WER $2.50\%$ on test-clean and $5.62\%$ on test-other. Comparing with a strong
baseline augmented memory transformer (AM-TRF), Emformer gets $4.6$ folds
training speedup and $18\%$ relative real-time factor (RTF) reduction in
decoding with relative WER reduction $17\%$ on test-clean and $9\%$ on
test-other. For a low latency scenario with an average latency of 80 ms,
Emformer achieves WER $3.01\%$ on test-clean and $7.09\%$ on test-other.
Comparing with the LSTM baseline with the same latency and model size, Emformer
gets relative WER reduction $9\%$ and $16\%$ on test-clean and test-other,
respectively.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 04:38:09 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 19:59:08 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 14:55:59 GMT'}]",2020-10-30,"[['Shi', 'Yangyang', ''], ['Wang', 'Yongqiang', ''], ['Wu', 'Chunyang', ''], ['Yeh', 'Ching-Feng', ''], ['Chan', 'Julian', ''], ['Zhang', 'Frank', ''], ['Le', 'Duc', ''], ['Seltzer', 'Mike', '']]"
1371388,2010.15058,Tomek Korbak,Tomasz Korbak and Julian Zubek and Joanna R\k{a}czaszek-Leonardi,Measuring non-trivial compositionality in emergent communication,"4th Workshop on Emergent Communication, NeurIPS 2020",,,,cs.NE cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Compositionality is an important explanatory target in emergent communication
and language evolution. The vast majority of computational models of
communication account for the emergence of only a very basic form of
compositionality: trivial compositionality. A compositional protocol is
trivially compositional if the meaning of a complex signal (e.g. blue circle)
boils down to the intersection of meanings of its constituents (e.g. the
intersection of the set of blue objects and the set of circles). A protocol is
non-trivially compositional (NTC) if the meaning of a complex signal (e.g.
biggest apple) is a more complex function of the meanings of their
constituents. In this paper, we review several metrics of compositionality used
in emergent communication and experimentally show that most of them fail to
detect NTC - i.e. they treat non-trivial compositionality as a failure of
compositionality. The one exception is tree reconstruction error, a metric
motivated by formal accounts of compositionality. These results emphasise
important limitations of emergent communication research that could hamper
progress on modelling the emergence of NTC.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 16:11:07 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 16:22:44 GMT'}]",2020-10-30,"[['Korbak', 'Tomasz', ''], ['Zubek', 'Julian', ''], ['Rączaszek-Leonardi', 'Joanna', '']]"
1301728,2006.07214,Andre Martins,"Andr\'e F. T. Martins, Ant\'onio Farinhas, Marcos Treviso, Vlad
  Niculae, Pedro M. Q. Aguiar, M\'ario A. T. Figueiredo",Sparse and Continuous Attention Mechanisms,Accepted for spotlight presentation at NeurIPS 2020,,,,cs.LG cs.CL cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Exponential families are widely used in machine learning; they include many
distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet,
Poisson, and categorical distributions via the softmax transformation).
Distributions in each of these families have fixed support. In contrast, for
finite domains, there has been recent work on sparse alternatives to softmax
(e.g. sparsemax and alpha-entmax), which have varying support, being able to
assign zero probability to irrelevant categories. This paper expands that work
in two directions: first, we extend alpha-entmax to continuous domains,
revealing a link with Tsallis statistics and deformed exponential families.
Second, we introduce continuous-domain attention mechanisms, deriving efficient
gradient backpropagation algorithms for alpha in {1,2}. Experiments on
attention-based text classification, machine translation, and visual question
answering illustrate the use of continuous attention in 1D and 2D, showing that
it allows attending to time intervals and compact regions.
","[{'version': 'v1', 'created': 'Fri, 12 Jun 2020 14:16:48 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 22:22:38 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 08:39:54 GMT'}]",2020-10-30,"[['Martins', 'André F. T.', ''], ['Farinhas', 'António', ''], ['Treviso', 'Marcos', ''], ['Niculae', 'Vlad', ''], ['Aguiar', 'Pedro M. Q.', ''], ['Figueiredo', 'Mário A. T.', '']]"
1324516,2007.13002,Siyuan Feng,"Siyuan Feng, Odette Scharenborg","Unsupervised Subword Modeling Using Autoregressive Pretraining and
  Cross-Lingual Phone-Aware Modeling","5 pages, 3 figures. Accepted for publication in INTERSPEECH 2020,
  Shanghai, China",,10.21437/Interspeech.2020-1170,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This study addresses unsupervised subword modeling, i.e., learning feature
representations that can distinguish subword units of a language. The proposed
approach adopts a two-stage bottleneck feature (BNF) learning framework,
consisting of autoregressive predictive coding (APC) as a front-end and a
DNN-BNF model as a back-end. APC pretrained features are set as input features
to a DNN-BNF model. A language-mismatched ASR system is used to provide
cross-lingual phone labels for DNN-BNF model training. Finally, BNFs are
extracted as the subword-discriminative feature representation. A second aim of
this work is to investigate the robustness of our approach's effectiveness to
different amounts of training data. The results on Libri-light and the
ZeroSpeech 2017 databases show that APC is effective in front-end feature
pretraining. Our whole system outperforms the state of the art on both
databases. Cross-lingual phone labels for English data by a Dutch ASR
outperform those by a Mandarin ASR, possibly linked to the larger similarity of
Dutch compared to Mandarin with English. Our system is less sensitive to
training data amount when the training data is over 50 hours. APC pretraining
leads to a reduction of needed training material from over 5,000 hours to
around 200 hours with little performance degradation.
","[{'version': 'v1', 'created': 'Sat, 25 Jul 2020 19:41:41 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Aug 2020 19:15:48 GMT'}]",2020-10-30,"[['Feng', 'Siyuan', ''], ['Scharenborg', 'Odette', '']]"
1371596,2010.15266,Abhinav Singh,"Abhinav Singh, Patrick Xia, Guanghui Qin, Mahsa Yarmohammadi, Benjamin
  Van Durme","CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence
  Models",4th Workshop on Structured Prediction for NLP (EMNLP 2020),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Copy mechanisms are employed in sequence to sequence models (seq2seq) to
generate reproductions of words from the input to the output. These frameworks,
operating at the lexical type level, fail to provide an explicit alignment that
records where each token was copied from. Further, they require contiguous
token sequences from the input (spans) to be copied individually. We present a
model with an explicit token-level copy operation and extend it to copying
entire spans. Our model provides hard alignments between spans in the input and
output, allowing for nontraditional applications of seq2seq, like information
extraction. We demonstrate the approach on Nested Named Entity Recognition,
achieving near state-of-the-art accuracy with an order of magnitude increase in
decoding speed.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 22:45:16 GMT'}]",2020-10-30,"[['Singh', 'Abhinav', ''], ['Xia', 'Patrick', ''], ['Qin', 'Guanghui', ''], ['Yarmohammadi', 'Mahsa', ''], ['Van Durme', 'Benjamin', '']]"
1294236,2005.14441,Xiang Hao,"Xiang Hao, Xiangdong Su, Zhiyu Wang, Qiang Zhang, Huali Xu and
  Guanglai Gao",SNR-Based Teachers-Student Technique for Speech Enhancement,"Published in 2020 IEEE International Conference on Multimedia and
  Expo (ICME 2020)",,10.1109/ICME46284.2020.9102846,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is very challenging for speech enhancement methods to achieves robust
performance under both high signal-to-noise ratio (SNR) and low SNR
simultaneously. In this paper, we propose a method that integrates an SNR-based
teachers-student technique and time-domain U-Net to deal with this problem.
Specifically, this method consists of multiple teacher models and a student
model. We first train the teacher models under multiple small-range SNRs that
do not coincide with each other so that they can perform speech enhancement
well within the specific SNR range. Then, we choose different teacher models to
supervise the training of the student model according to the SNR of the
training data. Eventually, the student model can perform speech enhancement
under both high SNR and low SNR. To evaluate the proposed method, we
constructed a dataset with an SNR ranging from -20dB to 20dB based on the
public dataset. We experimentally analyzed the effectiveness of the SNR-based
teachers-student technique and compared the proposed method with several
state-of-the-art methods.
","[{'version': 'v1', 'created': 'Fri, 29 May 2020 08:13:01 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 12:12:20 GMT'}]",2020-10-30,"[['Hao', 'Xiang', ''], ['Su', 'Xiangdong', ''], ['Wang', 'Zhiyu', ''], ['Zhang', 'Qiang', ''], ['Xu', 'Huali', ''], ['Gao', 'Guanglai', '']]"
1294230,2005.14435,Xiang Hao,"Xiang Hao, Shixue Wen, Xiangdong Su, Yun Liu, Guanglai Gao and Xiaofei
  Li",Sub-Band Knowledge Distillation Framework for Speech Enhancement,Published in Interspeech 2020,,10.21437/Interspeech.2020-1539,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In single-channel speech enhancement, methods based on full-band spectral
features have been widely studied. However, only a few methods pay attention to
non-full-band spectral features. In this paper, we explore a knowledge
distillation framework based on sub-band spectral mapping for single-channel
speech enhancement. Specifically, we divide the full frequency band into
multiple sub-bands and pre-train an elite-level sub-band enhancement model
(teacher model) for each sub-band. These teacher models are dedicated to
processing their own sub-bands. Next, under the teacher models' guidance, we
train a general sub-band enhancement model (student model) that works for all
sub-bands. Without increasing the number of model parameters and computational
complexity, the student model's performance is further improved. To evaluate
our proposed method, we conducted a large number of experiments on an
open-source data set. The final experimental results show that the guidance
from the elite-level teacher models dramatically improves the student model's
performance, which exceeds the full-band model by employing fewer parameters.
","[{'version': 'v1', 'created': 'Fri, 29 May 2020 07:55:12 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 12:14:59 GMT'}]",2020-10-30,"[['Hao', 'Xiang', ''], ['Wen', 'Shixue', ''], ['Su', 'Xiangdong', ''], ['Liu', 'Yun', ''], ['Gao', 'Guanglai', ''], ['Li', 'Xiaofei', '']]"
1292684,2005.12889,Ruixiang Cui,"Ruixiang Cui, Daniel Hershcovich",Refining Implicit Argument Annotation for UCCA,DMR 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Predicate-argument structure analysis is a central component in meaning
representations of text. The fact that some arguments are not explicitly
mentioned in a sentence gives rise to ambiguity in language understanding, and
renders it difficult for machines to interpret text correctly. However, only
few resources represent implicit roles for NLU, and existing studies in NLP
only make coarse distinctions between categories of arguments omitted from
linguistic form. This paper proposes a typology for fine-grained implicit
argument annotation on top of Universal Conceptual Cognitive Annotation's
foundational layer. The proposed implicit argument categorisation is driven by
theories of implicit role interpretation and consists of six types: Deictic,
Generic, Genre-based, Type-identifiable, Non-specific, and Iterated-set. We
exemplify our design by revisiting part of the UCCA EWT corpus, providing a new
dataset annotated with the refinement layer, and making a comparative analysis
with other schemes.
","[{'version': 'v1', 'created': 'Tue, 26 May 2020 17:24:15 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 16:07:33 GMT'}]",2020-10-30,"[['Cui', 'Ruixiang', ''], ['Hershcovich', 'Daniel', '']]"
1358810,2010.02480,Cheng-Han Chiang,"Cheng-Han Chiang, Sung-Feng Huang and Hung-yi Lee",Pretrained Language Model Embryology: The Birth of ALBERT,"Accepted to EMNLP 2020, short paper",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While behaviors of pretrained language models (LMs) have been thoroughly
examined, what happened during pretraining is rarely studied. We thus
investigate the developmental process from a set of randomly initialized
parameters to a totipotent language model, which we refer to as the embryology
of a pretrained language model. Our results show that ALBERT learns to
reconstruct and predict tokens of different parts of speech (POS) in different
learning speeds during pretraining. We also find that linguistic knowledge and
world knowledge do not generally improve as pretraining proceeds, nor do
downstream tasks' performance. These findings suggest that knowledge of a
pretrained model varies during pretraining, and having more pretrain steps does
not necessarily provide a model with more comprehensive knowledge. We will
provide source codes and pretrained models to reproduce our results at
https://github.com/d223302/albert-embryology.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 05:15:39 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 00:07:43 GMT'}]",2020-10-30,"[['Chiang', 'Cheng-Han', ''], ['Huang', 'Sung-Feng', ''], ['Lee', 'Hung-yi', '']]"
1201305,1911.02711,Sen Yang,"Sen Yang, Leyang Cui, Jun Xie and Yue Zhang",Making the Best Use of Review Summary for Sentiment Analysis,To be published in COLING-2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Sentiment analysis provides a useful overview of customer review contents.
Many review websites allow a user to enter a summary in addition to a full
review. Intuitively, summary information may give additional benefit for review
sentiment analysis. In this paper, we conduct a study to exploit methods for
better use of summary information. We start by finding out that the sentimental
signal distribution of a review and that of its corresponding summary are in
fact complementary to each other. We thus explore various architectures to
better guide the interactions between the two and propose a
hierarchically-refined review-centric attention model. Empirical results show
that our review-centric model can make better use of user-written summaries for
review sentiment analysis, and is also more effective compared to existing
methods when the user summary is replaced with summary generated by an
automatic summarization system.
","[{'version': 'v1', 'created': 'Thu, 7 Nov 2019 01:46:54 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 07:15:01 GMT'}]",2020-10-30,"[['Yang', 'Sen', ''], ['Cui', 'Leyang', ''], ['Xie', 'Jun', ''], ['Zhang', 'Yue', '']]"
1202469,1911.03875,Khalil Mrini,"Khalil Mrini, Franck Dernoncourt, Quan Tran, Trung Bui, Walter Chang,
  Ndapa Nakashole",Rethinking Self-Attention: Towards Interpretability in Neural Parsing,EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Attention mechanisms have improved the performance of NLP tasks while
allowing models to remain explainable. Self-attention is currently widely used,
however interpretability is difficult due to the numerous attention
distributions. Recent work has shown that model representations can benefit
from label-specific information, while facilitating interpretation of
predictions. We introduce the Label Attention Layer: a new form of
self-attention where attention heads represent labels. We test our novel layer
by running constituency and dependency parsing experiments and show our new
model obtains new state-of-the-art results for both tasks on both the Penn
Treebank (PTB) and Chinese Treebank. Additionally, our model requires fewer
self-attention layers compared to existing work. Finally, we find that the
Label Attention heads learn relations between syntactic categories and show
pathways to analyze errors.
","[{'version': 'v1', 'created': 'Sun, 10 Nov 2019 08:17:11 GMT'}, {'version': 'v2', 'created': 'Sat, 2 May 2020 04:34:52 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 06:17:11 GMT'}]",2020-10-30,"[['Mrini', 'Khalil', ''], ['Dernoncourt', 'Franck', ''], ['Tran', 'Quan', ''], ['Bui', 'Trung', ''], ['Chang', 'Walter', ''], ['Nakashole', 'Ndapa', '']]"
1217216,1912.05320,Carlos S. Armendariz,"Carlos Santos Armendariz, Matthew Purver, Matej Ul\v{c}ar, Senja
  Pollak, Nikola Ljube\v{s}i\'c, Marko Robnik-\v{S}ikonja, Mark
  Granroth-Wilding, Kristiina Vaik",CoSimLex: A Resource for Evaluating Graded Word Similarity in Context,,"Proceedings of the 12th Language Resources and Evaluation
  Conference (2020) 5878-5886",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State of the art natural language processing tools are built on
context-dependent word embeddings, but no direct method for evaluating these
representations currently exists. Standard tasks and datasets for intrinsic
evaluation of embeddings are based on judgements of similarity, but ignore
context; standard tasks for word sense disambiguation take account of context
but do not provide continuous measures of meaning similarity. This paper
describes an effort to build a new dataset, CoSimLex, intended to fill this
gap. Building on the standard pairwise similarity task of SimLex-999, it
provides context-dependent similarity measures; covers not only discrete
differences in word sense but more subtle, graded changes in meaning; and
covers not only a well-resourced language (English) but a number of
less-resourced languages. We define the task and evaluation metrics, outline
the dataset collection methodology, and describe the status of the dataset so
far.
","[{'version': 'v1', 'created': 'Wed, 11 Dec 2019 14:02:59 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Dec 2019 10:33:05 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 15:22:27 GMT'}]",2020-10-30,"[['Armendariz', 'Carlos Santos', ''], ['Purver', 'Matthew', ''], ['Ulčar', 'Matej', ''], ['Pollak', 'Senja', ''], ['Ljubešić', 'Nikola', ''], ['Robnik-Šikonja', 'Marko', ''], ['Granroth-Wilding', 'Mark', ''], ['Vaik', 'Kristiina', '']]"
1371555,2010.15225,Dylan Ebert,"Dylan Ebert, Ellie Pavlick",A Visuospatial Dataset for Naturalistic Verb Learning,"9 pages, 3 figures, starsem 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a new dataset for training and evaluating grounded language
models. Our data is collected within a virtual reality environment and is
designed to emulate the quality of language data to which a pre-verbal child is
likely to have access: That is, naturalistic, spontaneous speech paired with
richly grounded visuospatial context. We use the collected data to compare
several distributional semantics models for verb learning. We evaluate neural
models based on 2D (pixel) features as well as feature-engineered models based
on 3D (symbolic, spatial) features, and show that neither modeling approach
achieves satisfactory performance. Our results are consistent with evidence
from child language acquisition that emphasizes the difficulty of learning
verbs from naive distributional data. We discuss avenues for future work on
cognitively-inspired grounded language learning, and release our corpus with
the intent of facilitating research on the topic.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 20:47:13 GMT'}]",2020-10-30,"[['Ebert', 'Dylan', ''], ['Pavlick', 'Ellie', '']]"
1362681,2010.06351,Fuli Luo,"Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun","CAPT: Contrastive Pre-Training for Learning Denoised Sequence
  Representations",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained self-supervised models such as BERT have achieved striking
success in learning sequence representations, especially for natural language
processing. These models typically corrupt the given sequences with certain
types of noise, such as masking, shuffling, or substitution, and then try to
recover the original input. However, such pre-training approaches are prone to
learning representations that are covariant with the noise, leading to the
discrepancy between the pre-training and fine-tuning stage. To remedy this, we
present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence
representations. The proposed CAPT encourages the consistency between
representations of the original sequence and its corrupted version via
unsupervised instance-wise training signals. In this way, it not only
alleviates the pretrain-finetune discrepancy induced by the noise of
pre-training, but also aids the pre-trained model in better capturing global
semantics of the input via more effective sentence-level supervision. Different
from most prior work that focuses on a particular modality, comprehensive
empirical evidence on 11 natural language understanding and cross-modal tasks
illustrates that CAPT is applicable for both language and vision-language
tasks, and obtains surprisingly consistent improvement, including 0.6% absolute
gain on GLUE benchmarks and 0.8% absolute increment on NLVR.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 13:08:34 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 09:30:12 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 06:41:07 GMT'}]",2020-10-30,"[['Luo', 'Fuli', ''], ['Yang', 'Pengcheng', ''], ['Li', 'Shicheng', ''], ['Ren', 'Xuancheng', ''], ['Sun', 'Xu', '']]"
1372108,2010.15778,Timo Denk,Timo I. Denk and Ana Peleteiro Ramallo,Contextual BERT: Conditioning the Language Model Using a Global State,"Accepted at the TextGraphs-14 workshop at COLING'2020 - The 28th
  International Conference on Computational Linguistics",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  BERT is a popular language model whose main pre-training task is to fill in
the blank, i.e., predicting a word that was masked out of a sentence, based on
the remaining words. In some applications, however, having an additional
context can help the model make the right prediction, e.g., by taking the
domain or the time of writing into account. This motivates us to advance the
BERT architecture by adding a global state for conditioning on a fixed-sized
context. We present our two novel approaches and apply them to an industry
use-case, where we complete fashion outfits with missing articles, conditioned
on a specific customer. An experimental comparison to other methods from the
literature shows that our methods improve personalization significantly.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 17:25:20 GMT'}]",2020-10-30,"[['Denk', 'Timo I.', ''], ['Ramallo', 'Ana Peleteiro', '']]"
1371581,2010.15251,Marimuthu Kalimuthu,"Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow",Fusion Models for Improved Visual Captioning,"Under review at ""Multi-Modal Deep Learning: Challenges and
  Applications"", ICPR-2020",,,,cs.CV cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Visual captioning aims to generate textual descriptions given images.
Traditionally, the captioning models are trained on human annotated datasets
such as Flickr30k and MS-COCO, which are limited in size and diversity. This
limitation hinders the generalization capabilities of these models while also
rendering them to often make mistakes. Language models can, however, be trained
on vast amounts of freely available unlabelled data and have recently emerged
as successful language encoders and coherent text generators. Meanwhile,
several unimodal and multimodal fusion techniques have been proven to work well
for natural language generation and automatic speech recognition. Building on
these recent developments, and with an aim of improving the quality of
generated captions, the contribution of our work in this paper is two-fold:
First, we propose a generic multimodal model fusion framework for caption
generation as well as emendation where we utilize different fusion strategies
to integrate a pretrained Auxiliary Language Model (AuxLM) within the
traditional encoder-decoder visual captioning frameworks. Next, we employ the
same fusion strategies to integrate a pretrained Masked Language Model (MLM),
namely BERT, with a visual captioning model, viz. Show, Attend, and Tell, for
emending both syntactic and semantic errors in captions. Our caption emendation
experiments on three benchmark image captioning datasets, viz. Flickr8k,
Flickr30k, and MSCOCO, show improvements over the baseline, indicating the
usefulness of our proposed multimodal fusion strategies. Further, we perform a
preliminary qualitative analysis on the emended captions and identify error
categories based on the type of corrections.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 21:55:25 GMT'}]",2020-10-30,"[['Kalimuthu', 'Marimuthu', ''], ['Mogadala', 'Aditya', ''], ['Mosbach', 'Marius', ''], ['Klakow', 'Dietrich', '']]"
1372058,2010.15728,Hang Dong,"Hang Dong, V\'ictor Su\'arez-Paniagua, William Whiteley, Honghan Wu","Explainable Automated Coding of Clinical Notes using Hierarchical
  Label-wise Attention Networks and Label Embedding Initialisation","Structured abstract in full text, 17 pages, 5 figures, 4
  supplementary materials (3 extra pages), submitted to Journal of Biomedical
  Informatics",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diagnostic or procedural coding of clinical notes aims to derive a coded
summary of disease-related information about patients. Such coding is usually
done manually in hospitals but could potentially be automated to improve the
efficiency and accuracy of medical coding. Recent studies on deep learning for
automated medical coding achieved promising performances. However, the
explainability of these models is usually poor, preventing them to be used
confidently in supporting clinical practice. Another limitation is that these
models mostly assume independence among labels, ignoring the complex
correlation among medical codes which can potentially be exploited to improve
the performance. We propose a Hierarchical Label-wise Attention Network (HLAN),
which aimed to interpret the model by quantifying importance (as attention
weights) of words and sentences related to each of the labels. Secondly, we
propose to enhance the major deep learning models with a label embedding (LE)
initialisation approach, which learns a dense, continuous vector representation
and then injects the representation into the final layers and the label-wise
attention layers in the models. We evaluated the methods using three settings
on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS
COVID-19 shielding codes. Experiments were conducted to compare HLAN and LE
initialisation to the state-of-the-art neural network based methods. HLAN
achieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and
comparable results on the NHS COVID-19 shielding code prediction to other
models. By highlighting the most salient words and sentences for each label,
HLAN showed more meaningful and comprehensive model interpretation compared to
its downgraded baselines and the CNN-based models. LE initialisation
consistently boosted most deep learning models for automated medical coding.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 16:21:26 GMT'}]",2020-10-30,"[['Dong', 'Hang', ''], ['Suárez-Paniagua', 'Víctor', ''], ['Whiteley', 'William', ''], ['Wu', 'Honghan', '']]"
1371932,2010.15602,Junhua Liu,Nachamma Sockalingam and Junhua Liu,Designing learning experiences for online teaching and learning,,,,,cs.CY cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Teaching is about constantly innovating strategies, ways and means to engage
diverse students in active and meaningful learning. In line with this, SUTD
adopts various student-centric teaching and learning teaching methods and
approaches. This means that our graduate/undergraduate instructors have to be
ready to teach using these student student-centric teaching and learning
pedagogies. In this article, I share my experiences of redesigning this
teaching course that is typically conducted face-to-face to a synchronous
online course and also invite one of the participant in this course to reflect
on his experience as a student.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 07:03:49 GMT'}]",2020-10-30,"[['Sockalingam', 'Nachamma', ''], ['Liu', 'Junhua', '']]"
1371479,2010.15149,Yiwei Luo,"Yiwei Luo, Dallas Card, Dan Jurafsky",DeSMOG: Detecting Stance in Media On Global Warming,"9 pages, 6 figures (excluding references and appendices). To appear
  in Findings of EMNLP 2020",Findings of EMNLP 2020,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Citing opinions is a powerful yet understudied strategy in argumentation. For
example, an environmental activist might say, ""Leading scientists agree that
global warming is a serious concern,"" framing a clause which affirms their own
stance (""that global warming is serious"") as an opinion endorsed (""[scientists]
agree"") by a reputable source (""leading""). In contrast, a global warming denier
might frame the same clause as the opinion of an untrustworthy source with a
predicate connoting doubt: ""Mistaken scientists claim [...]."" Our work studies
opinion-framing in the global warming (GW) debate, an increasingly partisan
issue that has received little attention in NLP. We introduce DeSMOG, a dataset
of stance-labeled GW sentences, and train a BERT classifier to study novel
aspects of argumentation in how different sides of a debate represent their own
and each other's opinions. From 56K news articles, we find that similar
linguistic devices for self-affirming and opponent-doubting discourse are used
across GW-accepting and skeptic media, though GW-skeptical media shows more
opponent-doubt. We also find that authors often characterize sources as
hypocritical, by ascribing opinions expressing the author's own view to source
entities known to publicly endorse the opposing view. We release our stance
dataset, model, and lexicons of framing devices for future work on
opinion-framing and the automatic detection of GW stance.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 18:01:02 GMT'}]",2020-10-30,"[['Luo', 'Yiwei', ''], ['Card', 'Dallas', ''], ['Jurafsky', 'Dan', '']]"
1371630,2010.15300,Emaad Manzoor,"Emaad Manzoor, Nihar B. Shah",Uncovering Latent Biases in Text: Method and Application to Peer Review,,,,,cs.CL cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Quantifying systematic disparities in numerical quantities such as employment
rates and wages between population subgroups provides compelling evidence for
the existence of societal biases. However, biases in the text written for
members of different subgroups (such as in recommendation letters for male and
non-male candidates), though widely reported anecdotally, remain challenging to
quantify. In this work, we introduce a novel framework to quantify bias in text
caused by the visibility of subgroup membership indicators. We develop a
nonparametric estimation and inference procedure to estimate this bias. We then
formalize an identification strategy to causally link the estimated bias to the
visibility of subgroup membership indicators, provided observations from time
periods both before and after an identity-hiding policy change. We identify an
application wherein ""ground truth"" bias can be inferred to evaluate our
framework, instead of relying on synthetic or secondary data. Specifically, we
apply our framework to quantify biases in the text of peer reviews from a
reputed machine learning conference before and after the conference adopted a
double-blind reviewing policy. We show evidence of biases in the review ratings
that serves as ""ground truth"", and show that our proposed framework accurately
detects these biases from the review text without having access to the review
ratings.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 01:24:19 GMT'}]",2020-10-30,"[['Manzoor', 'Emaad', ''], ['Shah', 'Nihar B.', '']]"
