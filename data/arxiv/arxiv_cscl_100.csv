,id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed
1370321,2010.13991,Wei Zou,"Dongwei Jiang, Wubo Li, Miao Cao, Ruixiong Zhang, Wei Zou, Kun Han,
  Xiangang Li","Speech SIMCLR: Combining Contrastive and Reconstruction Objective for
  Self-supervised Speech Representation Learning",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Self-supervised visual pretraining has shown significant progress recently.
Among those methods, SimCLR greatly advanced the state of the art in
self-supervised and semi-supervised learning on ImageNet. The input feature
representations for speech and visual tasks are both continuous, so it is
natural to consider applying similar objective on speech representation
learning. In this paper, we propose Speech SimCLR, a new self-supervised
objective for speech representation learning. During training, Speech SimCLR
applies augmentation on raw speech and its spectrogram. Its objective is the
combination of contrastive loss that maximizes agreement between differently
augmented samples in the latent space and reconstruction loss of input
representation. The proposed method achieved competitive results on speech
emotion recognition and speech recognition. When used as feature extractor, our
best model achieved 5.89% word error rate on LibriSpeech test-clean set using
LibriSpeech 960 hours as pretraining data and LibriSpeech train-clean-100 set
as fine-tuning data, which is the lowest error rate obtained in this setup to
the best of our knowledge.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 02:09:06 GMT'}]",2020-10-28,"[['Jiang', 'Dongwei', ''], ['Li', 'Wubo', ''], ['Cao', 'Miao', ''], ['Zhang', 'Ruixiong', ''], ['Zou', 'Wei', ''], ['Han', 'Kun', ''], ['Li', 'Xiangang', '']]"
1339591,2008.11869,Xinsong Zhang,Xinsong Zhang and Hang Li,AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained language models such as BERT have exhibited remarkable
performances in many tasks in natural language understanding (NLU). The tokens
in the models are usually fine-grained in the sense that for languages like
English they are words or sub-words and for languages like Chinese they are
characters. In English, for example, there are multi-word expressions which
form natural lexical units and thus the use of coarse-grained tokenization also
appears to be reasonable. In fact, both fine-grained and coarse-grained
tokenizations have advantages and disadvantages for learning of pre-trained
language models. In this paper, we propose a novel pre-trained language model,
referred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained
and coarse-grained tokenizations. For English, AMBERT takes both the sequence
of words (fine-grained tokens) and the sequence of phrases (coarse-grained
tokens) as input after tokenization, employs one encoder for processing the
sequence of words and the other encoder for processing the sequence of the
phrases, utilizes shared parameters between the two encoders, and finally
creates a sequence of contextualized representations of the words and a
sequence of contextualized representations of the phrases. Experiments have
been conducted on benchmark datasets for Chinese and English, including CLUE,
GLUE, SQuAD and RACE. The results show that AMBERT outperforms the existing
best performing models in almost all cases, particularly the improvements are
significant for Chinese.
","[{'version': 'v1', 'created': 'Thu, 27 Aug 2020 00:23:48 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Sep 2020 05:29:27 GMT'}, {'version': 'v3', 'created': 'Tue, 27 Oct 2020 06:53:33 GMT'}]",2020-10-28,"[['Zhang', 'Xinsong', ''], ['Li', 'Hang', '']]"
1279334,2004.14564,Brian Thompson,Brian Thompson and Matt Post,"Automatic Machine Translation Evaluation in Many Languages via Zero-Shot
  Paraphrasing",EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We frame the task of machine translation evaluation as one of scoring machine
translation output with a sequence-to-sequence paraphraser, conditioned on a
human reference. We propose training the paraphraser as a multilingual NMT
system, treating paraphrasing as a zero-shot translation task (e.g., Czech to
Czech). This results in the paraphraser's output mode being centered around a
copy of the input sequence, which represents the best case scenario where the
MT system output matches a human reference. Our method is simple and intuitive,
and does not require human judgements for training. Our single model (trained
in 39 languages) outperforms or statistically ties with all prior metrics on
the WMT 2019 segment-level shared metrics task in all languages (excluding
Gujarati where the model had no training data). We also explore using our model
for the task of quality estimation as a metric--conditioning on the source
instead of the reference--and find that it significantly outperforms every
submission to the WMT 2019 shared task on quality estimation in every language
pair.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 03:32:34 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 23:54:02 GMT'}]",2020-10-29,"[['Thompson', 'Brian', ''], ['Post', 'Matt', '']]"
1187463,1910.03544,Jianguo Zhang,"Jian-Guo Zhang, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wan, Philip S.
  Yu, Richard Socher, Caiming Xiong","Find or Classify? Dual Strategy for Slot-Value Predictions on
  Multi-Domain Dialog State Tracking","14 pages, accepted at the 9th Joint Conference on Lexical and
  Computational Semantics (*SEM 2020). This version fixes small errors",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dialog state tracking (DST) is a core component in task-oriented dialog
systems. Existing approaches for DST mainly fall into one of two categories,
namely, ontology-based and ontology-free methods. An ontology-based method
selects a value from a candidate-value list for each target slot, while an
ontology-free method extracts spans from dialog contexts. Recent work
introduced a BERT-based model to strike a balance between the two methods by
pre-defining categorical and non-categorical slots. However, it is not clear
enough which slots are better handled by either of the two slot types, and the
way to use the pre-trained model has not been well investigated. In this paper,
we propose a simple yet effective dual-strategy model for DST, by adapting a
single BERT-style reading comprehension model to jointly handle both the
categorical and non-categorical slots. Our experiments on the MultiWOZ datasets
show that our method significantly outperforms the BERT-based counterpart,
finding that the key is a deep interaction between the domain-slot and context
information. When evaluated on noisy (MultiWOZ 2.0) and cleaner (MultiWOZ 2.1)
settings, our method performs competitively and robustly across the two
different settings. Our method sets the new state of the art in the noisy
setting, while performing more robustly than the best model in the cleaner
setting. We also conduct a comprehensive error analysis on the dataset,
including the effects of the dual strategy for each slot, to facilitate future
research.
","[{'version': 'v1', 'created': 'Tue, 8 Oct 2019 17:08:39 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Oct 2019 08:04:12 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Sep 2020 08:37:44 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Oct 2020 10:07:01 GMT'}]",2020-10-29,"[['Zhang', 'Jian-Guo', ''], ['Hashimoto', 'Kazuma', ''], ['Wu', 'Chien-Sheng', ''], ['Wan', 'Yao', ''], ['Yu', 'Philip S.', ''], ['Socher', 'Richard', ''], ['Xiong', 'Caiming', '']]"
1342828,2009.01325,Ryan Lowe T.,"Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe,
  Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano",Learning to summarize from human feedback,NeurIPS 2020 camera ready,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As language models become more powerful, training and evaluation are
increasingly bottlenecked by the data and metrics used for a particular task.
For example, summarization models are often trained to predict human reference
summaries and evaluated using ROUGE, but both of these metrics are rough
proxies for what we really care about---summary quality. In this work, we show
that it is possible to significantly improve summary quality by training a
model to optimize for human preferences. We collect a large, high-quality
dataset of human comparisons between summaries, train a model to predict the
human-preferred summary, and use that model as a reward function to fine-tune a
summarization policy using reinforcement learning. We apply our method to a
version of the TL;DR dataset of Reddit posts and find that our models
significantly outperform both human reference summaries and much larger models
fine-tuned with supervised learning alone. Our models also transfer to CNN/DM
news articles, producing summaries nearly as good as the human reference
without any news-specific fine-tuning. We conduct extensive analyses to
understand our human feedback dataset and fine-tuned models We establish that
our reward model generalizes to new datasets, and that optimizing our reward
model results in better summaries than optimizing ROUGE according to humans. We
hope the evidence from our paper motivates machine learning researchers to pay
closer attention to how their training loss affects the model behavior they
actually want.
","[{'version': 'v1', 'created': 'Wed, 2 Sep 2020 19:54:41 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 22:19:53 GMT'}]",2020-10-29,"[['Stiennon', 'Nisan', ''], ['Ouyang', 'Long', ''], ['Wu', 'Jeff', ''], ['Ziegler', 'Daniel M.', ''], ['Lowe', 'Ryan', ''], ['Voss', 'Chelsea', ''], ['Radford', 'Alec', ''], ['Amodei', 'Dario', ''], ['Christiano', 'Paul', '']]"
1201327,1911.02733,Xue Mengge,"Xue Mengge, Yu Bowen, Liu Tingwen, Zhang Yue, Meng Erli, Wang Bin",Porous Lattice-based Transformer Encoder for Chinese NER,"9 pages, 4 figures",COLING 2020,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Incorporating lattices into character-level Chinese named entity recognition
is an effective method to exploit explicit word information. Recent works
extend recurrent and convolutional neural networks to model lattice inputs.
However, due to the DAG structure or the variable-sized potential word set for
lattice inputs, these models prevent the convenient use of batched computation,
resulting in serious inefficient. In this paper, we propose a porous
lattice-based transformer encoder for Chinese named entity recognition, which
is capable to better exploit the GPU parallelism and batch the computation
owing to the mask mechanism in transformer. We first investigate the
lattice-aware self-attention coupled with relative position representations to
explore effective word information in the lattice structure. Besides, to
strengthen the local dependencies among neighboring tokens, we propose a novel
porous structure during self-attentional computation processing, in which every
two non-neighboring tokens are connected through a shared pivot node.
Experimental results on four datasets show that our model performs up to 9.47
times faster than state-of-the-art models, while is roughly on a par with its
performance. The source code of this paper can be obtained from
https://github.com/xxx/xxx.
","[{'version': 'v1', 'created': 'Thu, 7 Nov 2019 02:58:17 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Apr 2020 14:46:51 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Oct 2020 12:52:24 GMT'}]",2020-10-29,"[['Mengge', 'Xue', ''], ['Bowen', 'Yu', ''], ['Tingwen', 'Liu', ''], ['Yue', 'Zhang', ''], ['Erli', 'Meng', ''], ['Bin', 'Wang', '']]"
1290078,2005.10283,Bryan Eikema,Bryan Eikema and Wilker Aziz,"Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural
  Machine Translation",COLING 2020 camera-ready,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recent studies have revealed a number of pathologies of neural machine
translation (NMT) systems. Hypotheses explaining these mostly suggest there is
something fundamentally wrong with NMT as a model or its training algorithm,
maximum likelihood estimation (MLE). Most of this evidence was gathered using
maximum a posteriori (MAP) decoding, a decision rule aimed at identifying the
highest-scoring translation, i.e. the mode. We argue that the evidence
corroborates the inadequacy of MAP decoding more than casts doubt on the model
and its training algorithm. In this work, we show that translation
distributions do reproduce various statistics of the data well, but that beam
search strays from such statistics. We show that some of the known pathologies
and biases of NMT are due to MAP decoding and not to NMT's statistical
assumptions nor MLE. In particular, we show that the most likely translations
under the model accumulate so little probability mass that the mode can be
considered essentially arbitrary. We therefore advocate for the use of decision
rules that take into account the translation distribution holistically. We show
that an approximation to minimum Bayes risk decoding gives competitive results
confirming that NMT models do capture important aspects of translation well in
expectation.
","[{'version': 'v1', 'created': 'Wed, 20 May 2020 18:05:51 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 11:29:52 GMT'}]",2020-10-29,"[['Eikema', 'Bryan', ''], ['Aziz', 'Wilker', '']]"
1303020,2006.08506,Tobias Watzel,"Tobias Watzel, Ludwig K\""urzinger, Lujun Li, Gerhard Rigoll",Regularized Forward-Backward Decoder for Attention Models,,,,,eess.AS cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Nowadays, attention models are one of the popular candidates for speech
recognition. So far, many studies mainly focus on the encoder structure or the
attention module to enhance the performance of these models. However, mostly
ignore the decoder. In this paper, we propose a novel regularization technique
incorporating a second decoder during the training phase. This decoder is
optimized on time-reversed target labels beforehand and supports the standard
decoder during training by adding knowledge from future context. Since it is
only added during training, we are not changing the basic structure of the
network or adding complexity during decoding. We evaluate our approach on the
smaller TEDLIUMv2 and the larger LibriSpeech dataset, achieving consistent
improvements on both of them.
","[{'version': 'v1', 'created': 'Mon, 15 Jun 2020 16:04:16 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 14:00:52 GMT'}]",2020-10-29,"[['Watzel', 'Tobias', ''], ['Kürzinger', 'Ludwig', ''], ['Li', 'Lujun', ''], ['Rigoll', 'Gerhard', '']]"
1295146,2006.00632,Barbara Plank,Alan Ramponi and Barbara Plank,Neural Unsupervised Domain Adaptation in NLP---A Survey,"COLING 2020. Accompanying repository:
  https://github.com/bplank/awesome-neural-adaptation-in-NLP",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks excel at learning from labeled data and achieve
state-of-the-art resultson a wide array of Natural Language Processing tasks.
In contrast, learning from unlabeled data, especially under domain shift,
remains a challenge. Motivated by the latest advances, in this survey we review
neural unsupervised domain adaptation techniques which do not require labeled
target domain data. This is a more challenging yet a more widely applicable
setup. We outline methods, from early traditional non-neural methods to
pre-trained model transfer. We also revisit the notion of domain, and we
uncover a bias in the type of Natural Language Processing tasks which received
most attention. Lastly, we outline future directions, particularly the broader
need for out-of-distribution generalization of future NLP.
","[{'version': 'v1', 'created': 'Sun, 31 May 2020 22:34:14 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 08:24:14 GMT'}]",2020-10-29,"[['Ramponi', 'Alan', ''], ['Plank', 'Barbara', '']]"
1292163,2005.12368,Marija Stepanovi\'c,"Andreas Kirkedal, Marija Stepanovi\'c, Barbara Plank",FT Speech: Danish Parliament Speech Corpus,Accepted at Interspeech 2020,,10.21437/Interspeech.2020-3164,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces FT Speech, a new speech corpus created from the
recorded meetings of the Danish Parliament, otherwise known as the Folketing
(FT). The corpus contains over 1,800 hours of transcribed speech by a total of
434 speakers. It is significantly larger in duration, vocabulary, and amount of
spontaneous speech than the existing public speech corpora for Danish, which
are largely limited to read-aloud and dictation data. We outline design
considerations, including the preprocessing methods and the alignment
procedure. To evaluate the quality of the corpus, we train automatic speech
recognition systems on the new resource and compare them to the systems trained
on the Danish part of Spr\r{a}kbanken, the largest public ASR corpus for Danish
to date. Our baseline results show that we achieve a 14.01 WER on the new
corpus. A combination of FT Speech with in-domain language data provides
comparable results to models trained specifically on Spr\r{a}kbanken, showing
that FT Speech transfers well to this data set. Interestingly, our results
demonstrate that the opposite is not the case. This shows that FT Speech
provides a valuable resource for promoting research on Danish ASR with more
spontaneous speech.
","[{'version': 'v1', 'created': 'Mon, 25 May 2020 19:51:18 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 13:36:44 GMT'}]",2020-10-29,"[['Kirkedal', 'Andreas', ''], ['Stepanović', 'Marija', ''], ['Plank', 'Barbara', '']]"
1364540,2010.08210,Xue Mengge,"Mengge Xue, Bowen Yu, Zhenyu Zhang, Tingwen Liu, Yue Zhang, Bin Wang",Coarse-to-Fine Pre-training for Named Entity Recognition,,EMNLP 2020,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  More recently, Named Entity Recognition hasachieved great advances aided by
pre-trainingapproaches such as BERT. However, currentpre-training techniques
focus on building lan-guage modeling objectives to learn a gen-eral
representation, ignoring the named entity-related knowledge. To this end, we
proposea NER-specific pre-training framework to in-ject coarse-to-fine
automatically mined entityknowledge into pre-trained models. Specifi-cally, we
first warm-up the model via an en-tity span identification task by training it
withWikipedia anchors, which can be deemed asgeneral-typed entities. Then we
leverage thegazetteer-based distant supervision strategy totrain the model
extract coarse-grained typedentities. Finally, we devise a
self-supervisedauxiliary task to mine the fine-grained namedentity knowledge
via clustering.Empiricalstudies on three public NER datasets demon-strate that
our framework achieves significantimprovements against several pre-trained
base-lines, establishing the new state-of-the-art per-formance on three
benchmarks. Besides, weshow that our framework gains promising re-sults without
using human-labeled trainingdata, demonstrating its effectiveness in label-few
and low-resource scenarios
","[{'version': 'v1', 'created': 'Fri, 16 Oct 2020 07:39:20 GMT'}]",2020-10-29,"[['Xue', 'Mengge', ''], ['Yu', 'Bowen', ''], ['Zhang', 'Zhenyu', ''], ['Liu', 'Tingwen', ''], ['Zhang', 'Yue', ''], ['Wang', 'Bin', '']]"
1361978,2010.05648,Steffen Eger,Steffen Eger and Yannik Benz,From Hero to Z\'eroe: A Benchmark of Low-Level Adversarial Attacks,"Authors accidentally in wrong order; cannot be undone due to
  conference constraints. Accepted for publication at AACL 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversarial attacks are label-preserving modifications to inputs of machine
learning classifiers designed to fool machines but not humans. Natural Language
Processing (NLP) has mostly focused on high-level attack scenarios such as
paraphrasing input texts. We argue that these are less realistic in typical
application scenarios such as in social media, and instead focus on low-level
attacks on the character-level. Guided by human cognitive abilities and human
robustness, we propose the first large-scale catalogue and benchmark of
low-level adversarial attacks, which we dub Z\'eroe, encompassing nine
different attack modes including visual and phonetic adversaries. We show that
RoBERTa, NLP's current workhorse, fails on our attacks. Our dataset provides a
benchmark for testing robustness of future more human-like NLP models.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 12:35:36 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 12:53:05 GMT'}]",2020-10-29,"[['Eger', 'Steffen', ''], ['Benz', 'Yannik', '']]"
1359177,2010.02847,Haiyang Zhang,"Haiyang Zhang, Alison Sneyd and Mark Stevenson","Robustness and Reliability of Gender Bias Assessment in Word Embeddings:
  The Role of Base Pairs",Accepted at AACL-IJCNLP 2020,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It has been shown that word embeddings can exhibit gender bias, and various
methods have been proposed to quantify this. However, the extent to which the
methods are capturing social stereotypes inherited from the data has been
debated. Bias is a complex concept and there exist multiple ways to define it.
Previous work has leveraged gender word pairs to measure bias and extract
biased analogies. We show that the reliance on these gendered pairs has strong
limitations: bias measures based off of them are not robust and cannot identify
common types of real-world bias, whilst analogies utilising them are unsuitable
indicators of bias. In particular, the well-known analogy ""man is to
computer-programmer as woman is to homemaker"" is due to word similarity rather
than societal bias. This has important implications for work on measuring bias
in embeddings and related work debiasing embeddings.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 16:09:05 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 21:24:16 GMT'}]",2020-10-29,"[['Zhang', 'Haiyang', ''], ['Sneyd', 'Alison', ''], ['Stevenson', 'Mark', '']]"
1356617,2010.00287,Ehsan Doostmohammadi,"Ehsan Doostmohammadi, Minoo Nassajian, Adel Rahimi","Joint Persian Word Segmentation Correction and Zero-Width Non-Joiner
  Recognition Using BERT",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Words are properly segmented in the Persian writing system; in practice,
however, these writing rules are often neglected, resulting in single words
being written disjointedly and multiple words written without any white spaces
between them. This paper addresses the problems of word segmentation and
zero-width non-joiner (ZWNJ) recognition in Persian, which we approach jointly
as a sequence labeling problem. We achieved a macro-averaged F1-score of 92.40%
on a carefully collected corpus of 500 sentences with a high level of
difficulty.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 10:32:17 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 09:40:18 GMT'}]",2020-10-29,"[['Doostmohammadi', 'Ehsan', ''], ['Nassajian', 'Minoo', ''], ['Rahimi', 'Adel', '']]"
1280566,2005.00771,Xiang Li,"Michael Boratko, Xiang Lorraine Li, Rajarshi Das, Tim O'Gorman, Dan
  Le, Andrew McCallum","ProtoQA: A Question Answering Dataset for Prototypical Common-Sense
  Reasoning",First four authors contribute equally,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Given questions regarding some prototypical situation such as Name something
that people usually do before they leave the house for work? a human can easily
answer them via acquired experiences. There can be multiple right answers for
such questions, with some more common for a situation than others. This paper
introduces a new question answering dataset for training and evaluating common
sense reasoning capabilities of artificial intelligence systems in such
prototypical situations. The training set is gathered from an existing set of
questions played in a long-running international game show FAMILY- FEUD. The
hidden evaluation set is created by gathering answers for each question from
100 crowd-workers. We also propose a generative evaluation task where a model
has to output a ranked list of answers, ideally covering all prototypical
answers for a question. After presenting multiple competitive baseline models,
we find that human performance still exceeds model scores on all evaluation
metrics with a meaningful gap, supporting the challenging nature of the task.
","[{'version': 'v1', 'created': 'Sat, 2 May 2020 09:40:05 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Oct 2020 05:35:05 GMT'}, {'version': 'v3', 'created': 'Tue, 27 Oct 2020 21:23:03 GMT'}]",2020-10-29,"[['Boratko', 'Michael', ''], ['Li', 'Xiang Lorraine', ''], ['Das', 'Rajarshi', ''], [""O'Gorman"", 'Tim', ''], ['Le', 'Dan', ''], ['McCallum', 'Andrew', '']]"
1279954,2005.00159,Pratyush Maini,"Pratyush Maini, Keshav Kolluru, Danish Pruthi, Mausam","Why and when should you pool? Analyzing Pooling in Recurrent
  Architectures","Accepted to Findings of EMNLP 2020, to be presented at BlackBoxNLP.
  Updated Version",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pooling-based recurrent neural architectures consistently outperform their
counterparts without pooling. However, the reasons for their enhanced
performance are largely unexamined. In this work, we examine three commonly
used pooling techniques (mean-pooling, max-pooling, and attention), and propose
max-attention, a novel variant that effectively captures interactions among
predictive tokens in a sentence. We find that pooling-based architectures
substantially differ from their non-pooling equivalents in their learning
ability and positional biases--which elucidate their performance benefits. By
analyzing the gradient propagation, we discover that pooling facilitates better
gradient flow compared to BiLSTMs. Further, we expose how BiLSTMs are
positionally biased towards tokens in the beginning and the end of a sequence.
Pooling alleviates such biases. Consequently, we identify settings where
pooling offers large benefits: (i) in low resource scenarios, and (ii) when
important words lie towards the middle of the sentence. Among the pooling
techniques studied, max-attention is the most effective, resulting in
significant performance gains on several text classification tasks.
","[{'version': 'v1', 'created': 'Fri, 1 May 2020 00:47:37 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 02:11:02 GMT'}]",2020-10-29,"[['Maini', 'Pratyush', ''], ['Kolluru', 'Keshav', ''], ['Pruthi', 'Danish', ''], ['Mausam', '', '']]"
1332657,2008.04935,Brian Thompson,Brian Thompson and Matt Post,"Paraphrase Generation as Zero-Shot Multilingual Translation:
  Disentangling Semantic Similarity from Lexical and Syntactic Diversity",WMT2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has shown that a multilingual neural machine translation (NMT)
model can be used to judge how well a sentence paraphrases another sentence in
the same language (Thompson and Post, 2020); however, attempting to generate
paraphrases from such a model using standard beam search produces trivial
copies or near copies. We introduce a simple paraphrase generation algorithm
which discourages the production of n-grams that are present in the input. Our
approach enables paraphrase generation in many languages from a single
multilingual NMT model. Furthermore, the amount of lexical diversity between
the input and output can be controlled at generation time. We conduct a human
evaluation to compare our method to a paraphraser trained on the large English
synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our
method produces paraphrases that better preserve meaning and are more
gramatical, for the same level of lexical diversity. Additional smaller human
assessments demonstrate our approach also works in two non-English languages.
","[{'version': 'v1', 'created': 'Tue, 11 Aug 2020 18:05:34 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 02:54:13 GMT'}]",2020-10-29,"[['Thompson', 'Brian', ''], ['Post', 'Matt', '']]"
1349467,2009.07964,Zhijing Jin,"Xiaoyu Xing, Zhijing Jin, Di Jin, Bingning Wang, Qi Zhang, and
  Xuanjing Huang","Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based
  Sentiment Analysis","EMNLP 2020, long paper",,,,cs.CL cs.IR cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards
a specific aspect in the text. However, existing ABSA test sets cannot be used
to probe whether a model can distinguish the sentiment of the target aspect
from the non-target aspects. To solve this problem, we develop a simple but
effective approach to enrich ABSA test sets. Specifically, we generate new
examples to disentangle the confounding sentiments of the non-target aspects
from the target aspect's sentiment. Based on the SemEval 2014 dataset, we
construct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the
aspect robustness of ABSA models. Over 92% data of ARTS show high fluency and
desired sentiment on all aspects by human evaluation. Using ARTS, we analyze
the robustness of nine ABSA models, and observe, surprisingly, that their
accuracy drops by up to 69.73%. We explore several ways to improve aspect
robustness, and find that adversarial training can improve models' performance
on ARTS by up to 32.85%. Our code and new test set are available at
https://github.com/zhijing-jin/ARTS_TestSet
","[{'version': 'v1', 'created': 'Wed, 16 Sep 2020 22:38:18 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 05:36:10 GMT'}, {'version': 'v3', 'created': 'Sun, 4 Oct 2020 15:35:36 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Oct 2020 08:19:36 GMT'}]",2020-10-29,"[['Xing', 'Xiaoyu', ''], ['Jin', 'Zhijing', ''], ['Jin', 'Di', ''], ['Wang', 'Bingning', ''], ['Zhang', 'Qi', ''], ['Huang', 'Xuanjing', '']]"
1303623,2006.09109,Steffen Eger,Steffen Eger and Johannes Daxenberger and Iryna Gurevych,"How to Probe Sentence Embeddings in Low-Resource Languages: On
  Structural Design Choices for Probing Task Evaluation",Accepted for Publication at CONLL 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sentence encoders map sentences to real valued vectors for use in downstream
applications. To peek into these representations - e.g., to increase
interpretability of their results - probing tasks have been designed which
query them for linguistic knowledge. However, designing probing tasks for
lesser-resourced languages is tricky, because these often lack large-scale
annotated data or (high-quality) dependency parsers as a prerequisite of
probing task design in English. To investigate how to probe sentence embeddings
in such cases, we investigate sensitivity of probing task results to structural
design choices, conducting the first such large scale study. We show that
design choices like size of the annotated probing dataset and type of
classifier used for evaluation do (sometimes substantially) influence probing
outcomes. We then probe embeddings in a multilingual setup with design choices
that lie in a 'stable region', as we identify for English, and find that
results on English do not transfer to other languages. Fairer and more
comprehensive sentence-level probing evaluation should thus be carried out on
multiple languages in the future.
","[{'version': 'v1', 'created': 'Tue, 16 Jun 2020 12:37:50 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 12:38:37 GMT'}]",2020-10-29,"[['Eger', 'Steffen', ''], ['Daxenberger', 'Johannes', ''], ['Gurevych', 'Iryna', '']]"
1139420,1906.07234,Siyuan Feng,"Siyuan Feng, Tan Lee, Zhiyuan Peng","Combining Adversarial Training and Disentangled Speech Representation
  for Robust Zero-Resource Subword Modeling","5 pages, 3 figures, accepted for publication in INTERSPEECH 2019,
  Graz, Austria",,10.21437/Interspeech.2019-1337,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This study addresses the problem of unsupervised subword unit discovery from
untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech
2019, building text-to-speech systems without text labels. In this work, unit
discovery is formulated as a pipeline of phonetically discriminative feature
learning and unit inference. One major difficulty in robust unsupervised
feature learning is dealing with speaker variation. Here the robustness towards
speaker variation is achieved by applying adversarial training and FHVAE based
disentangled speech representation learning. A comparison of the two approaches
as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF)
architecture. Experiments are conducted on ZeroSpeech 2019 and 2017.
Experimental results on ZeroSpeech 2017 show that both approaches are effective
while the latter is more prominent, and that their combination brings further
marginal improvement in across-speaker condition. Results on ZeroSpeech 2019
show that in the ABX discriminability task, our approaches significantly
outperform the official baseline, and are competitive to or even outperform the
official topline. The proposed unit sequence smoothing algorithm improves
synthesis quality, at a cost of slight decrease in ABX discriminability.
","[{'version': 'v1', 'created': 'Mon, 17 Jun 2019 19:40:46 GMT'}, {'version': 'v2', 'created': 'Wed, 3 Jul 2019 12:22:28 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Aug 2019 15:55:00 GMT'}]",2020-10-29,"[['Feng', 'Siyuan', ''], ['Lee', 'Tan', ''], ['Peng', 'Zhiyuan', '']]"
1371089,2010.14759,Yufang Hou,Yufang Hou,"Fine-grained Information Status Classification Using Discourse
  Context-Aware BERT","accepted at COLING2020. arXiv admin note: substantial text overlap
  with arXiv:1908.04755",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous work on bridging anaphora recognition (Hou et al., 2013a) casts the
problem as a subtask of learning fine-grained information status (IS). However,
these systems heavily depend on many hand-crafted linguistic features. In this
paper, we propose a simple discourse context-aware BERT model for fine-grained
IS classification. On the ISNotes corpus (Markert et al., 2012), our model
achieves new state-of-the-art performance on fine-grained IS classification,
obtaining a 4.8 absolute overall accuracy improvement compared to Hou et al.
(2013a). More importantly, we also show an improvement of 10.5 F1 points for
bridging anaphora recognition without using any complex hand-crafted semantic
features designed for capturing the bridging phenomenon. We further analyze the
trained model and find that the most attended signals for each IS category
correspond well to linguistic notions of information status.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 22:30:17 GMT'}]",2020-10-29,"[['Hou', 'Yufang', '']]"
1371128,2010.14798,Shuai Zhang,"Shuai Zhang, Jiangyan Yi, Zhengkun Tian, Ye Bai, Jianhua Tao, Zhengqi
  wen","Decoupling Pronunciation and Language for End-to-end Code-switching
  Automatic Speech Recognition","5 pages, 1 figures",,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the recent significant advances witnessed in end-to-end (E2E) ASR
system for code-switching, hunger for audio-text paired data limits the further
improvement of the models' performance. In this paper, we propose a decoupled
transformer model to use monolingual paired data and unpaired text data to
alleviate the problem of code-switching data shortage. The model is decoupled
into two parts: audio-to-phoneme (A2P) network and phoneme-to-text (P2T)
network. The A2P network can learn acoustic pattern scenarios using large-scale
monolingual paired data. Meanwhile, it generates multiple phoneme sequence
candidates for single audio data in real-time during the training process. Then
the generated phoneme-text paired data is used to train the P2T network. This
network can be pre-trained with large amounts of external unpaired text data.
By using monolingual data and unpaired text data, the decoupled transformer
model reduces the high dependency on code-switching paired training data of E2E
model to a certain extent. Finally, the two networks are optimized jointly
through attention fusion. We evaluate the proposed method on the public
Mandarin-English code-switching dataset. Compared with our transformer
baseline, the proposed method achieves 18.14% relative mix error rate
reduction.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 07:46:15 GMT'}]",2020-10-29,"[['Zhang', 'Shuai', ''], ['Yi', 'Jiangyan', ''], ['Tian', 'Zhengkun', ''], ['Bai', 'Ye', ''], ['Tao', 'Jianhua', ''], ['wen', 'Zhengqi', '']]"
1371202,2010.14872,Kristian Miok,"Kristian Miok, Gregor Pirs and Marko Robnik-Sikonja",Bayesian Methods for Semi-supervised Text Annotation,"Accepted for COLING 2020, The 14th Linguistic Annotation Workshop",,,,cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human annotations are an important source of information in the development
of natural language understanding approaches. As under the pressure of
productivity annotators can assign different labels to a given text, the
quality of produced annotations frequently varies. This is especially the case
if decisions are difficult, with high cognitive load, requires awareness of
broader context, or careful consideration of background knowledge. To alleviate
the problem, we propose two semi-supervised methods to guide the annotation
process: a Bayesian deep learning model and a Bayesian ensemble method. Using a
Bayesian deep learning method, we can discover annotations that cannot be
trusted and might require reannotation. A recently proposed Bayesian ensemble
method helps us to combine the annotators' labels with predictions of trained
models. According to the results obtained from three hate speech detection
experiments, the proposed Bayesian methods can improve the annotations and
prediction performance of BERT models.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 10:42:04 GMT'}]",2020-10-29,"[['Miok', 'Kristian', ''], ['Pirs', 'Gregor', ''], ['Robnik-Sikonja', 'Marko', '']]"
1371171,2010.14841,Chengyu Wang,"Yiwu Yao, Yuchao Li, Chengyu Wang, Tianhang Yu, Houjiang Chen,
  Xiaotang Jiang, Jun Yang, Jun Huang, Wei Lin, Hui Shu, Chengfei Lv","INT8 Winograd Acceleration for Conv1D Equipped ASR Models Deployed on
  Mobile Devices",,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The intensive computation of Automatic Speech Recognition (ASR) models
obstructs them from being deployed on mobile devices. In this paper, we present
a novel quantized Winograd optimization pipeline, which combines the
quantization and fast convolution to achieve efficient inference acceleration
on mobile devices for ASR models. To avoid the information loss due to the
combination of quantization and Winograd convolution, a Range-Scaled
Quantization (RSQ) training method is proposed to expand the quantized
numerical range and to distill knowledge from high-precision values. Moreover,
an improved Conv1D equipped DFSMN (ConvDFSMN) model is designed for mobile
deployment. We conduct extensive experiments on both ConvDFSMN and Wav2letter
models. Results demonstrate the models can be effectively optimized with the
proposed pipeline. Especially, Wav2letter achieves 1.48* speedup with an
approximate 0.07% WER decrease on ARMv7-based mobile devices.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 09:25:49 GMT'}]",2020-10-29,"[['Yao', 'Yiwu', ''], ['Li', 'Yuchao', ''], ['Wang', 'Chengyu', ''], ['Yu', 'Tianhang', ''], ['Chen', 'Houjiang', ''], ['Jiang', 'Xiaotang', ''], ['Yang', 'Jun', ''], ['Huang', 'Jun', ''], ['Lin', 'Wei', ''], ['Shu', 'Hui', ''], ['Lv', 'Chengfei', '']]"
1371136,2010.14806,Xiao Pan,"Liwei Wu, Xiao Pan, Zehui Lin, Yaoming Zhu, Mingxuan Wang, Lei Li",The Volctrans Machine Translation System for WMT20,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper describes our VolcTrans system on WMT20 shared news translation
task. We participated in 8 translation directions. Our basic systems are based
on Transformer, with several variants (wider or deeper Transformers, dynamic
convolutions). The final system includes text pre-process, data selection,
synthetic data generation, advanced model ensemble, and multilingual
pre-training.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 08:08:12 GMT'}]",2020-10-29,"[['Wu', 'Liwei', ''], ['Pan', 'Xiao', ''], ['Lin', 'Zehui', ''], ['Zhu', 'Yaoming', ''], ['Wang', 'Mingxuan', ''], ['Li', 'Lei', '']]"
1371134,2010.14804,Benlai Tang,"Zhonghao Li, Benlai Tang, Xiang Yin, Yuan Wan, Ling Xu, Chen Shen,
  Zejun Ma","PPG-based singing voice conversion with adversarial representation
  learning",,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Singing voice conversion (SVC) aims to convert the voice of one singer to
that of other singers while keeping the singing content and melody. On top of
recent voice conversion works, we propose a novel model to steadily convert
songs while keeping their naturalness and intonation. We build an end-to-end
architecture, taking phonetic posteriorgrams (PPGs) as inputs and generating
mel spectrograms. Specifically, we implement two separate encoders: one encodes
PPGs as content, and the other compresses mel spectrograms to supply acoustic
and musical information. To improve the performance on timbre and melody, an
adversarial singer confusion module and a mel-regressive representation
learning module are designed for the model. Objective and subjective
experiments are conducted on our private Chinese singing corpus. Comparing with
the baselines, our methods can significantly improve the conversion performance
in terms of naturalness, melody, and voice similarity. Moreover, our PPG-based
method is proved to be robust for noisy sources.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 08:03:27 GMT'}]",2020-10-29,"[['Li', 'Zhonghao', ''], ['Tang', 'Benlai', ''], ['Yin', 'Xiang', ''], ['Wan', 'Yuan', ''], ['Xu', 'Ling', ''], ['Shen', 'Chen', ''], ['Ma', 'Zejun', '']]"
1371124,2010.14794,Kun Zhou,"Kun Zhou, Berrak Sisman, Rui Liu and Haizhou Li","Seen and Unseen emotional style transfer for voice conversion with a new
  emotional speech dataset",Submitted to ICASSP 2021,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Emotional voice conversion aims to transform emotional prosody in speech
while preserving the linguistic content and speaker identity. Prior studies
show that it is possible to disentangle emotional prosody using an
encoder-decoder network conditioned on discrete representation, such as one-hot
emotion labels. Such networks learn to remember a fixed set of emotional
styles. In this paper, we propose a novel framework based on variational
auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes
use of a pre-trained speech emotion recognition (SER) model to transfer
emotional style during training and at run-time inference. In this way, the
network is able to transfer both seen and unseen emotional style to a new
utterance. We show that the proposed framework achieves remarkable performance
by consistently outperforming the baseline framework. This paper also marks the
release of an emotional speech dataset (ESD) for voice conversion, which has
multiple speakers and languages.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 07:16:18 GMT'}]",2020-10-29,"[['Zhou', 'Kun', ''], ['Sisman', 'Berrak', ''], ['Liu', 'Rui', ''], ['Li', 'Haizhou', '']]"
1371114,2010.14784,Yuanhao Zhuo,Yuanhao Zhuo,"A Chinese Text Classification Method With Low Hardware Requirement Based
  on Improved Model Concatenation","5 pages, 2 figures, 5 tables",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In order to improve the accuracy performance of Chinese text classification
models with low hardware requirements, an improved concatenation-based model is
designed in this paper, which is a concatenation of 5 different sub-models,
including TextCNN, LSTM, and Bi-LSTM. Compared with the existing ensemble
learning method, for a text classification mission, this model's accuracy is 2%
higher. Meanwhile, the hardware requirements of this model are much lower than
the BERT-based model.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 06:32:41 GMT'}]",2020-10-29,"[['Zhuo', 'Yuanhao', '']]"
1371060,2010.14730,Xiaoyu Kou,"Xiaoyu Kou, Yankai Lin, Yuntao Li, Jiahao Xu, Peng Li, Jie Zhou, Yan
  Zhang",DisenE: Disentangling Knowledge Graph Embeddings,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge graph embedding (KGE), aiming to embed entities and relations into
low-dimensional vectors, has attracted wide attention recently. However, the
existing research is mainly based on the black-box neural models, which makes
it difficult to interpret the learned representation. In this paper, we
introduce DisenE, an end-to-end framework to learn disentangled knowledge graph
embeddings. Specially, we introduce an attention-based mechanism that enables
the model to explicitly focus on relevant components of entity embeddings
according to a given relation. Furthermore, we introduce two novel regularizers
to encourage each component of the entity representation to independently
reflect an isolated semantic aspect. Experimental results demonstrate that our
proposed DisenE investigates a perspective to address the interpretability of
KGE and is proved to be an effective way to improve the performance of link
prediction tasks. The code and datasets are released on
https://github.com/KXY-PUBLIC/DisenE.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 03:45:19 GMT'}]",2020-10-29,"[['Kou', 'Xiaoyu', ''], ['Lin', 'Yankai', ''], ['Li', 'Yuntao', ''], ['Xu', 'Jiahao', ''], ['Li', 'Peng', ''], ['Zhou', 'Jie', ''], ['Zhang', 'Yan', '']]"
1371055,2010.14725,Ruchao Fan,"Ruchao Fan, Wei Chu, Peng Chang, Jing Xiao","CASS-NAT: CTC Alignment-based Single Step Non-autoregressive Transformer
  for Speech Recognition",Submitted to ICASSP2021,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a CTC alignment-based single step non-autoregressive transformer
(CASS-NAT) for speech recognition. Specifically, the CTC alignment contains the
information of (a) the number of tokens for decoder input, and (b) the time
span of acoustics for each token. The information are used to extract acoustic
representation for each token in parallel, referred to as token-level acoustic
embedding which substitutes the word embedding in autoregressive transformer
(AT) to achieve parallel generation in decoder. During inference, an
error-based alignment sampling method is proposed to be applied to the CTC
output space, reducing the WER and retaining the parallelism as well.
Experimental results show that the proposed method achieves WERs of 3.8%/9.1%
on Librispeech test clean/other dataset without an external LM, and a CER of
5.8% on Aishell1 Mandarin corpus, respectively1. Compared to the AT baseline,
the CASS-NAT has a performance reduction on WER, but is 51.2x faster in terms
of RTF. When decoding with an oracle CTC alignment, the lower bound of WER
without LM reaches 2.3% on the test-clean set, indicating the potential of the
proposed method.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 03:14:05 GMT'}]",2020-10-29,"[['Fan', 'Ruchao', ''], ['Chu', 'Wei', ''], ['Chang', 'Peng', ''], ['Xiao', 'Jing', '']]"
1272816,2004.08046,Dongyu Ru,"Dongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingxuan Wang, Weinan
  Zhang, Yong Yu, Lei Li","Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete
  Space",Accepted to EMNLP 2020 Findings,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Active learning for sentence understanding aims at discovering informative
unlabeled data for annotation and therefore reducing the demand for labeled
data. We argue that the typical uncertainty sampling method for active learning
is time-consuming and can hardly work in real-time, which may lead to
ineffective sample selection. We propose adversarial uncertainty sampling in
discrete space (AUSDS) to retrieve informative unlabeled samples more
efficiently. AUSDS maps sentences into latent space generated by the popular
pre-trained language models, and discover informative unlabeled text samples
for annotation via adversarial attack. The proposed approach is extremely
efficient compared with traditional uncertainty sampling with more than 10x
speedup. Experimental results on five datasets show that AUSDS outperforms
strong baselines on effectiveness.
","[{'version': 'v1', 'created': 'Fri, 17 Apr 2020 03:12:34 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 04:45:49 GMT'}]",2020-10-29,"[['Ru', 'Dongyu', ''], ['Feng', 'Jiangtao', ''], ['Qiu', 'Lin', ''], ['Zhou', 'Hao', ''], ['Wang', 'Mingxuan', ''], ['Zhang', 'Weinan', ''], ['Yu', 'Yong', ''], ['Li', 'Lei', '']]"
1371050,2010.14720,Songlin Yang,"Songlin Yang, Yong Jiang, Wenjuan Han, Kewei Tu",Second-Order Unsupervised Neural Dependency Parsing,COLING 2020 camera ready,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most of the unsupervised dependency parsers are based on first-order
probabilistic generative models that only consider local parent-child
information. Inspired by second-order supervised dependency parsing, we
proposed a second-order extension of unsupervised neural dependency models that
incorporate grandparent-child or sibling information. We also propose a novel
design of the neural parameterization and optimization methods of the
dependency models. In second-order models, the number of grammar rules grows
cubically with the increase of vocabulary size, making it difficult to train
lexicalized models that may contain thousands of words. To circumvent this
problem while still benefiting from both second-order parsing and
lexicalization, we use the agreement-based learning framework to jointly train
a second-order unlexicalized model and a first-order lexicalized model.
Experiments on multiple datasets show the effectiveness of our second-order
models compared with recent state-of-the-art methods. Our joint model achieves
a 10% improvement over the previous state-of-the-art parser on the full WSJ
test set
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 03:01:33 GMT'}]",2020-10-29,"[['Yang', 'Songlin', ''], ['Jiang', 'Yong', ''], ['Han', 'Wenjuan', ''], ['Tu', 'Kewei', '']]"
1371037,2010.14707,Yang Qian,"Yang Qian, Yuanchun Jiang, Yidong Chai, Yezheng Liu, Jiansha Sun",TopicModel4J: A Java Package for Topic Models,,,,,cs.CL cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Topic models provide a flexible and principled framework for exploring hidden
structure in high-dimensional co-occurrence data and are commonly used natural
language processing (NLP) of text. In this paper, we design and implement a
Java package, TopicModel4J, which contains 13 kinds of representative
algorithms for fitting topic models. The TopicModel4J in the Java programming
environment provides an easy-to-use interface for data analysts to run the
algorithms, and allow to easily input and output data. In addition, this
package provides a few unstructured text preprocessing techniques, such as
splitting textual data into words, lowercasing the words, preforming
lemmatization and removing the useless characters, URLs and stop words.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 02:33:41 GMT'}]",2020-10-29,"[['Qian', 'Yang', ''], ['Jiang', 'Yuanchun', ''], ['Chai', 'Yidong', ''], ['Liu', 'Yezheng', ''], ['Sun', 'Jiansha', '']]"
1371031,2010.14701,Samuel McCandlish,"Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse,
  Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris
  Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M.
  Ziegler, John Schulman, Dario Amodei, Sam McCandlish",Scaling Laws for Autoregressive Generative Modeling,"20+15 pages, 30 figures",,,,cs.LG cs.CL cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We identify empirical scaling laws for the cross-entropy loss in four
domains: generative image modeling, video modeling, multimodal
image$\leftrightarrow$text models, and mathematical problem solving. In all
cases autoregressive Transformers smoothly improve in performance as model size
and compute budgets increase, following a power-law plus constant scaling law.
The optimal model size also depends on the compute budget through a power-law,
with exponents that are nearly universal across all data domains.
  The cross-entropy loss has an information theoretic interpretation as
$S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws
suggest a prediction for both the true data distribution's entropy and the KL
divergence between the true and model distributions. With this interpretation,
billion-parameter Transformers are nearly perfect models of the YFCC100M image
distribution downsampled to an $8\times 8$ resolution, and we can forecast the
model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in
nats/image for other resolutions.
  We find a number of additional scaling laws in specific domains: (a) we
identify a scaling relation for the mutual information between captions and
images in multimodal models, and show how to answer the question ""Is a picture
worth a thousand words?""; (b) in the case of mathematical problem solving, we
identify scaling laws for model performance when extrapolating beyond the
training distribution; (c) we finetune generative image models for ImageNet
classification and find smooth scaling of the classification loss and error
rate, even as the generative loss levels off. Taken together, these results
strengthen the case that scaling laws have important implications for neural
network performance, including on downstream tasks.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 02:17:24 GMT'}]",2020-10-29,"[['Henighan', 'Tom', ''], ['Kaplan', 'Jared', ''], ['Katz', 'Mor', ''], ['Chen', 'Mark', ''], ['Hesse', 'Christopher', ''], ['Jackson', 'Jacob', ''], ['Jun', 'Heewoo', ''], ['Brown', 'Tom B.', ''], ['Dhariwal', 'Prafulla', ''], ['Gray', 'Scott', ''], ['Hallacy', 'Chris', ''], ['Mann', 'Benjamin', ''], ['Radford', 'Alec', ''], ['Ramesh', 'Aditya', ''], ['Ryder', 'Nick', ''], ['Ziegler', 'Daniel M.', ''], ['Schulman', 'John', ''], ['Amodei', 'Dario', ''], ['McCandlish', 'Sam', '']]"
1371027,2010.14697,Claire Bowern,Luke Lindemann and Claire Bowern,"Character Entropy in Modern and Historical Texts: Comparison Metrics for
  an Undeciphered Manuscript",,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This paper outlines the creation of three corpora for multilingual comparison
and analysis of the Voynich manuscript: a corpus of Voynich texts partitioned
by Currier language, scribal hand, and transcription system, a corpus of 294
language samples compiled from Wikipedia, and a corpus of eighteen transcribed
historical texts in eight languages. These corpora will be utilized in
subsequent work by the Voynich Working Group at Yale University.
  We demonstrate the utility of these corpora for studying characteristics of
the Voynich script and language, with an analysis of conditional character
entropy in Voynichese. We discuss the interaction between character entropy and
language, script size and type, glyph compositionality, scribal conventions and
abbreviations, positional character variants, and bigram frequency.
  This analysis characterizes the interaction between script compositionality,
character size, and predictability. We show that substantial manipulations of
glyph composition are not sufficient to align conditional entropy levels with
natural languages. The unusually predictable nature of the Voynichese script is
not attributable to a particular script or transcription system, underlying
language, or substitution cipher. Voynichese is distinct from every comparison
text in our corpora because character placement is highly constrained within
the word, and this may indicate the loss of phonemic distinctions from the
underlying language.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 01:53:59 GMT'}]",2020-10-29,"[['Lindemann', 'Luke', ''], ['Bowern', 'Claire', '']]"
1371008,2010.14678,Amir Pouran Ben Veyseh,"Amir Pouran Ben Veyseh, Franck Dernoncourt, Quan Hung Tran, Thien Huu
  Nguyen","What Does This Acronym Mean? Introducing a New Dataset for Acronym
  Identification and Disambiguation",accepted at COLING 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Acronyms are the short forms of phrases that facilitate conveying lengthy
sentences in documents and serve as one of the mainstays of writing. Due to
their importance, identifying acronyms and corresponding phrases (i.e., acronym
identification (AI)) and finding the correct meaning of each acronym (i.e.,
acronym disambiguation (AD)) are crucial for text understanding. Despite the
recent progress on this task, there are some limitations in the existing
datasets which hinder further improvement. More specifically, limited size of
manually annotated AI datasets or noises in the automatically created acronym
identification datasets obstruct designing advanced high-performing acronym
identification models. Moreover, the existing datasets are mostly limited to
the medical domain and ignore other domains. In order to address these two
limitations, we first create a manually annotated large AI dataset for
scientific domain. This dataset contains 17,506 sentences which is
substantially larger than previous scientific AI datasets. Next, we prepare an
AD dataset for scientific domain with 62,441 samples which is significantly
larger than the previous scientific AD dataset. Our experiments show that the
existing state-of-the-art models fall far behind human-level performance on
both datasets proposed by this work. In addition, we propose a new deep
learning model that utilizes the syntactical structure of the sentence to
expand an ambiguous acronym in a sentence. The proposed model outperforms the
state-of-the-art models on the new AD dataset, providing a strong baseline for
future research on this dataset.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 00:12:36 GMT'}]",2020-10-29,"[['Veyseh', 'Amir Pouran Ben', ''], ['Dernoncourt', 'Franck', ''], ['Tran', 'Quan Hung', ''], ['Nguyen', 'Thien Huu', '']]"
1370995,2010.14665,Yongqiang Wang,"Yongqiang Wang, Yangyang Shi, Frank Zhang, Chunyang Wu, Julian Chan,
  Ching-Feng Yeh, Alex Xiao","Transformer in action: a comparative study of transformer-based acoustic
  models for large scale speech recognition applications",submitted to ICASSP2021,,,,cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we summarize the application of transformer and its streamable
variant, Emformer based acoustic model for large scale speech recognition
applications. We compare the transformer based acoustic models with their LSTM
counterparts on industrial scale tasks. Specifically, we compare Emformer with
latency-controlled BLSTM (LCBLSTM) on medium latency tasks and LSTM on low
latency tasks. On a low latency voice assistant task, Emformer gets 24% to 26%
relative word error rate reductions (WERRs). For medium latency scenarios,
comparing with LCBLSTM with similar model size and latency, Emformer gets
significant WERR across four languages in video captioning datasets with 2-3
times inference real-time factors reduction.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 23:04:21 GMT'}]",2020-10-29,"[['Wang', 'Yongqiang', ''], ['Shi', 'Yangyang', ''], ['Zhang', 'Frank', ''], ['Wu', 'Chunyang', ''], ['Chan', 'Julian', ''], ['Yeh', 'Ching-Feng', ''], ['Xiao', 'Alex', '']]"
1370990,2010.14660,Pierre Dognin,"Pierre L. Dognin, Igor Melnyk, Inkit Padhi, Cicero Nogueira dos
  Santos, Payel Das",DualTKB: A Dual Learning Bridge between Text and Knowledge Base,"Equal Contributions of Authors Pierre L. Dognin, Igor Melnyk, and
  Inkit Padhi. Accepted at EMNLP'20",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we present a dual learning approach for unsupervised text to
path and path to text transfers in Commonsense Knowledge Bases (KBs). We
investigate the impact of weak supervision by creating a weakly supervised
dataset and show that even a slight amount of supervision can significantly
improve the model performance and enable better-quality transfers. We examine
different model architectures, and evaluation metrics, proposing a novel
Commonsense KB completion metric tailored for generative models. Extensive
experimental results show that the proposed method compares very favorably to
the existing baselines. This approach is a viable step towards a more advanced
system for automatic KB construction/expansion and the reverse operation of KB
conversion to coherent textual descriptions.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 22:56:18 GMT'}]",2020-10-29,"[['Dognin', 'Pierre L.', ''], ['Melnyk', 'Igor', ''], ['Padhi', 'Inkit', ''], ['Santos', 'Cicero Nogueira dos', ''], ['Das', 'Payel', '']]"
1370979,2010.14649,Takashi Wada,"Takashi Wada, Tomoharu Iwata, Yuji Matsumoto, Timothy Baldwin, Jey Han
  Lau","Learning Contextualised Cross-lingual Word Embeddings for Extremely
  Low-Resource Languages Using Parallel Corpora",9 pages,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a new approach for learning contextualised cross-lingual word
embeddings based only on a small parallel corpus (e.g. a few hundred sentence
pairs). Our method obtains word embeddings via an LSTM-based encoder-decoder
model that performs bidirectional translation and reconstruction of the input
sentence. Through sharing model parameters among different languages, our model
jointly trains the word embeddings in a common multilingual space. We also
propose a simple method to combine word and subword embeddings to make use of
orthographic similarities across different languages. We base our experiments
on real-world data from endangered languages, namely Yongning Na,
Shipibo-Konibo and Griko. Our experiments on bilingual lexicon induction and
word alignment tasks show that our model outperforms existing methods by a
large margin for most language pairs. These results demonstrate that, contrary
to common belief, an encoder-decoder translation model is beneficial for
learning cross-lingual representations, even in extremely low-resource
scenarios.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 22:24:01 GMT'}]",2020-10-29,"[['Wada', 'Takashi', ''], ['Iwata', 'Tomoharu', ''], ['Matsumoto', 'Yuji', ''], ['Baldwin', 'Timothy', ''], ['Lau', 'Jey Han', '']]"
1370936,2010.14606,Arun Narayanan,"Arun Narayanan, Tara N. Sainath, Ruoming Pang, Jiahui Yu, Chung-Cheng
  Chiu, Rohit Prabhavalkar, Ehsan Variani, Trevor Strohman",Cascaded encoders for unifying streaming and non-streaming ASR,,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end (E2E) automatic speech recognition (ASR) models, by now, have
shown competitive performance on several benchmarks. These models are
structured to either operate in streaming or non-streaming mode. This work
presents cascaded encoders for building a single E2E ASR model that can operate
in both these modes simultaneously. The proposed model consists of streaming
and non-streaming encoders. Input features are first processed by the streaming
encoder; the non-streaming encoder operates exclusively on the output of the
streaming encoder. A single decoder then learns to decode either using the
output of the streaming or the non-streaming encoder. Results show that this
model achieves similar word error rates (WER) as a standalone streaming model
when operating in streaming mode, and obtains 10% -- 27% relative improvement
when operating in non-streaming mode. Our results also show that the proposed
approach outperforms existing E2E two-pass models, especially on long-form
speech.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 20:59:50 GMT'}]",2020-10-29,"[['Narayanan', 'Arun', ''], ['Sainath', 'Tara N.', ''], ['Pang', 'Ruoming', ''], ['Yu', 'Jiahui', ''], ['Chiu', 'Chung-Cheng', ''], ['Prabhavalkar', 'Rohit', ''], ['Variani', 'Ehsan', ''], ['Strohman', 'Trevor', '']]"
1370918,2010.14588,Robert Leaman,Robert Leaman and Zhiyong Lu,"A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and
  SARS-CoV-2",Accepted EMNLP NLP-COVID Workshop,,,,cs.DL cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The number of unique terms in the scientific literature used to refer to
either SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase
rapidly despite well-established standardized terms. This high degree of term
variation makes high recall identification of these important entities
difficult. In this manuscript we present an extensive dictionary of terms used
in the literature to refer to SARS-CoV-2 and COVID-19. We use a rule-based
approach to iteratively generate new term variants, then locate these variants
in a large text corpus. We compare our dictionary to an extensive collection of
terminological resources, demonstrating that our resource provides a
substantial number of additional terms. We use our dictionary to analyze the
usage of SARS-CoV-2 and COVID-19 terms over time and show that the number of
unique terms continues to grow rapidly. Our dictionary is freely available at
https://github.com/ncbi-nlp/CovidTermVar.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:51:53 GMT'}]",2020-10-29,"[['Leaman', 'Robert', ''], ['Lu', 'Zhiyong', '']]"
1370917,2010.14587,Jean-Baptiste Lamare,"Jean-Baptiste Lamare, Tobi Olatunji, Li Yao",On the diminishing return of labeling clinical reports,"Accepted at the EMNLP 2020 Clinical NLP workshop, 9 pages + 2 for
  references, 7 figures, 4 tables",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ample evidence suggests that better machine learning models may be steadily
obtained by training on increasingly larger datasets on natural language
processing (NLP) problems from non-medical domains. Whether the same holds true
for medical NLP has by far not been thoroughly investigated. This work shows
that this is indeed not always the case. We reveal the somehow
counter-intuitive observation that performant medical NLP models may be
obtained with small amount of labeled data, quite the opposite to the common
belief, most likely due to the domain specificity of the problem. We show
quantitatively the effect of training data size on a fixed test set composed of
two of the largest public chest x-ray radiology report datasets on the task of
abnormality classification. The trained models not only make use of the
training data efficiently, but also outperform the current state-of-the-art
rule-based systems by a significant margin.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:51:04 GMT'}]",2020-10-29,"[['Lamare', 'Jean-Baptiste', ''], ['Olatunji', 'Tobi', ''], ['Yao', 'Li', '']]"
1370906,2010.14576,Jeniya Tabassum,"Jeniya Tabassum, Sydney Lee, Wei Xu, Alan Ritter","WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet
  Lab Protocols",to appear in EMNLP 2020 (WNUT),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents the results of the wet lab information extraction task at
WNUT 2020. This task consisted of two sub tasks: (1) a Named Entity Recognition
(NER) task with 13 participants and (2) a Relation Extraction (RE) task with 2
participants. We outline the task, data annotation process, corpus statistics,
and provide a high-level overview of the participating systems for each sub
task.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:34:53 GMT'}]",2020-10-29,"[['Tabassum', 'Jeniya', ''], ['Lee', 'Sydney', ''], ['Xu', 'Wei', ''], ['Ritter', 'Alan', '']]"
1370898,2010.14568,Kaiyu Yang,"Kaiyu Yang, Jia Deng",Strongly Incremental Constituency Parsing with Graph Neural Networks,Accepted to NeurIPS 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Parsing sentences into syntax trees can benefit downstream applications in
NLP. Transition-based parsers build trees by executing actions in a state
transition system. They are computationally efficient, and can leverage machine
learning to predict actions based on partial trees. However, existing
transition-based parsers are predominantly based on the shift-reduce transition
system, which does not align with how humans are known to parse sentences.
Psycholinguistic research suggests that human parsing is strongly incremental:
humans grow a single parse tree by adding exactly one token at each step. In
this paper, we propose a novel transition system called attach-juxtapose. It is
strongly incremental; it represents a partial sentence using a single tree;
each action adds exactly one token into the partial tree. Based on our
transition system, we develop a strongly incremental parser. At each step, it
encodes the partial tree using a graph neural network and predicts an action.
We evaluate our parser on Penn Treebank (PTB) and Chinese Treebank (CTB). On
PTB, it outperforms existing parsers trained with only constituency trees; and
it performs on par with state-of-the-art parsers that use dependency trees as
additional training data. On CTB, our parser establishes a new state of the
art. Code is available at
https://github.com/princeton-vl/attach-juxtapose-parser.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:19:38 GMT'}]",2020-10-29,"[['Yang', 'Kaiyu', ''], ['Deng', 'Jia', '']]"
1370887,2010.14557,Ruizhe Li,"Xiao Li, Guanyi Chen, Chenghua Lin, Ruizhe Li",DGST: a Dual-Generator Network for Text Style Transfer,"Accepted by EMNLP 2020, camera ready version",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose DGST, a novel and simple Dual-Generator network architecture for
text Style Transfer. Our model employs two generators only, and does not rely
on any discriminators or parallel corpus for training. Both quantitative and
qualitative experiments on the Yelp and IMDb datasets show that our model gives
competitive performance compared to several strong baselines with more
complicated architecture designs.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 18:54:51 GMT'}]",2020-10-29,"[['Li', 'Xiao', ''], ['Chen', 'Guanyi', ''], ['Lin', 'Chenghua', ''], ['Li', 'Ruizhe', '']]"
1370864,2010.14534,Marion Bartl,Marion Bartl and Malvina Nissim and Albert Gatt,"Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender
  Bias","10 pages, 4 figures, to appear in Proceedings of the 2nd Workshop on
  Gender Bias in Natural Language Processing at COLING 2020",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Contextualized word embeddings have been replacing standard embeddings as the
representational knowledge source of choice in NLP systems. Since a variety of
biases have previously been found in standard word embeddings, it is crucial to
assess biases encoded in their replacements as well. Focusing on BERT (Devlin
et al., 2018), we measure gender bias by studying associations between
gender-denoting target words and names of professions in English and German,
comparing the findings with real-world workforce statistics. We mitigate bias
by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying
Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that
our method of measuring bias is appropriate for languages such as English, but
not for languages with a rich morphology and gender-marking, such as German.
Our results highlight the importance of investigating bias and mitigation
techniques cross-linguistically, especially in view of the current emphasis on
large-scale, multilingual language models.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 18:06:09 GMT'}]",2020-10-29,"[['Bartl', 'Marion', ''], ['Nissim', 'Malvina', ''], ['Gatt', 'Albert', '']]"
1146834,1907.02298,Zi Lin,"Junjie Cao, Zi Lin, Weiwei Sun, Xiaojun Wan","A Comparative Analysis of Knowledge-Intensive and Data-Intensive
  Semantic Parsers",submitted to the journal Computational Linguistics,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We present a phenomenon-oriented comparative analysis of the two dominant
approaches in task-independent semantic parsing: classic, knowledge-intensive
and neural, data-intensive models. To reflect state-of-the-art neural NLP
technologies, we introduce a new target structure-centric parser that can
produce semantic graphs much more accurately than previous data-driven parsers.
We then show that, in spite of comparable performance overall, knowledge- and
data-intensive models produce different types of errors, in a way that can be
explained by their theoretical properties. This analysis leads to new
directions for parser development.
","[{'version': 'v1', 'created': 'Thu, 4 Jul 2019 09:40:27 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Aug 2019 10:36:59 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Oct 2020 08:39:30 GMT'}]",2020-10-29,"[['Cao', 'Junjie', ''], ['Lin', 'Zi', ''], ['Sun', 'Weiwei', ''], ['Wan', 'Xiaojun', '']]"
1247530,2002.10107,Issa Annamoradnejad,"Issa Annamoradnejad, Mohammadamin Fazli, Jafar Habibi",Predicting Subjective Features of Questions of QA Websites using BERT,"5 pages, 4 figures, 2 tables","2020 6th International Conference on Web Research (ICWR), Tehran,
  Iran, 2020, pp. 240-244",10.1109/ICWR49608.2020.9122318,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Community Question-Answering websites, such as StackOverflow and Quora,
expect users to follow specific guidelines in order to maintain content
quality. These systems mainly rely on community reports for assessing contents,
which has serious problems such as the slow handling of violations, the loss of
normal and experienced users' time, the low quality of some reports, and
discouraging feedback to new users. Therefore, with the overall goal of
providing solutions for automating moderation actions in Q&A websites, we aim
to provide a model to predict 20 quality or subjective aspects of questions in
QA websites. To this end, we used data gathered by the CrowdSource team at
Google Research in 2019 and a fine-tuned pre-trained BERT model on our problem.
Based on the evaluation by Mean-Squared-Error (MSE), the model achieved a value
of 0.046 after 2 epochs of training, which did not improve substantially in the
next ones. Results confirm that by simple fine-tuning, we can achieve accurate
models in little time and on less amount of data.
","[{'version': 'v1', 'created': 'Mon, 24 Feb 2020 07:56:02 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Mar 2020 08:10:16 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Jun 2020 13:22:04 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Oct 2020 14:37:39 GMT'}]",2020-10-29,"[['Annamoradnejad', 'Issa', ''], ['Fazli', 'Mohammadamin', ''], ['Habibi', 'Jafar', '']]"
1369332,2010.13002,Sam Shleifer,Sam Shleifer and Alexander M. Rush,Pre-trained Summarization Distillation,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent state-of-the-art approaches to summarization utilize large pre-trained
Transformer models. Distilling these models to smaller student models has
become critically important for practical use; however there are many different
distillation methods proposed by the NLP literature. Recent work on distilling
BERT for classification and regression tasks shows strong performance using
direct knowledge distillation. Alternatively, machine translation practitioners
distill using pseudo-labeling, where a small model is trained on the
translations of a larger model. A third, simpler approach is to 'shrink and
fine-tune' (SFT), which avoids any explicit distillation by copying parameters
to a smaller student model and then fine-tuning. We compare these three
approaches for distillation of Pegasus and BART, the current and former state
of the art, pre-trained summarization models, and find that SFT outperforms
knowledge distillation and pseudo-labeling on the CNN/DailyMail dataset, but
under-performs pseudo-labeling on the more abstractive XSUM dataset. PyTorch
Code and checkpoints of different sizes are available through Hugging Face
transformers here http://tiny.cc/4iy0tz.
","[{'version': 'v1', 'created': 'Sat, 24 Oct 2020 23:15:43 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 04:47:59 GMT'}]",2020-10-29,"[['Shleifer', 'Sam', ''], ['Rush', 'Alexander M.', '']]"
1371221,2010.14891,Mayuko Kori,"Mayuko Kori, Takeshi Tsukada and Naoki Kobayashi",A Cyclic Proof System for HFLN,27 pages,,,,cs.LO cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A cyclic proof system allows us to perform inductive reasoning without
explicit inductions. We propose a cyclic proof system for HFLN, which is a
higher-order predicate logic with natural numbers and alternating fixed-points.
Ours is the first cyclic proof system for a higher-order logic, to our
knowledge. Due to the presence of higher-order predicates and alternating
fixed-points, our cyclic proof system requires a more delicate global condition
on cyclic proofs than the original system of Brotherston and Simpson. We prove
the decidability of checking the global condition and soundness of this system,
and also prove a restricted form of standard completeness for an infinitary
variant of our cyclic proof system. A potential application of our cyclic proof
system is semi-automated verification of higher-order programs, based on
Kobayashi et al.'s recent work on reductions from program verification to HFLN
validity checking.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 11:19:53 GMT'}]",2020-10-29,"[['Kori', 'Mayuko', ''], ['Tsukada', 'Takeshi', ''], ['Kobayashi', 'Naoki', '']]"
1371250,2010.14920,Yuchen Liu,"Yuchen Liu, Junnan Zhu, Jiajun Zhang, and Chengqing Zong",Bridging the Modality Gap for Speech-to-Text Translation,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  End-to-end speech translation aims to translate speech in one language into
text in another language via an end-to-end way. Most existing methods employ an
encoder-decoder structure with a single encoder to learn acoustic
representation and semantic information simultaneously, which ignores the
speech-and-text modality differences and makes the encoder overloaded, leading
to great difficulty in learning such a model. To address these issues, we
propose a Speech-to-Text Adaptation for Speech Translation (STAST) model which
aims to improve the end-to-end model performance by bridging the modality gap
between speech and text. Specifically, we decouple the speech translation
encoder into three parts and introduce a shrink mechanism to match the length
of speech representation with that of the corresponding text transcription. To
obtain better semantic representation, we completely integrate a text-based
translation model into the STAST so that two tasks can be trained in the same
latent space. Furthermore, we introduce a cross-modal adaptation method to
close the distance between speech and text representation. Experimental results
on English-French and English-German speech translation corpora have shown that
our model significantly outperforms strong baselines, and achieves the new
state-of-the-art performance.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 12:33:04 GMT'}]",2020-10-29,"[['Liu', 'Yuchen', ''], ['Zhu', 'Junnan', ''], ['Zhang', 'Jiajun', ''], ['Zong', 'Chengqing', '']]"
1279293,2004.14523,Brian Thompson,Brian Thompson and Philipp Koehn,Exploiting Sentence Order in Document Alignment,EMNLP2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a simple document alignment method that incorporates sentence
order information in both candidate generation and candidate re-scoring. Our
method results in 61% relative reduction in error compared to the best
previously published result on the WMT16 document alignment shared task. Our
method improves downstream MT performance on web-scraped Sinhala--English
documents from ParaCrawl, outperforming the document alignment method used in
the most recent ParaCrawl release. It also outperforms a comparable corpora
method which uses the same multilingual embeddings, demonstrating that
exploiting sentence order is beneficial even if the end goal is sentence-level
bitext.
","[{'version': 'v1', 'created': 'Thu, 30 Apr 2020 00:11:34 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 01:23:22 GMT'}]",2020-10-29,"[['Thompson', 'Brian', ''], ['Koehn', 'Philipp', '']]"
1371355,2010.15025,Xingchen Song,"Xingchen Song, Zhiyong Wu, Yiheng Huang, Chao Weng, Dan Su, Helen Meng",Non-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input,submitted to ICASSP 2021,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Non-autoregressive (NAR) transformer models have achieved significantly
inference speedup but at the cost of inferior accuracy compared to
autoregressive (AR) models in automatic speech recognition (ASR). Most of the
NAR transformers take a fixed-length sequence filled with MASK tokens or a
redundant sequence copied from encoder states as decoder input, they cannot
provide efficient target-side information thus leading to accuracy degradation.
To address this problem, we propose a CTC-enhanced NAR transformer, which
generates target sequence by refining predictions of the CTC module.
Experimental results show that our method outperforms all previous NAR
counterparts and achieves 50x faster decoding speed than a strong AR baseline
with only 0.0 ~ 0.3 absolute CER degradation on Aishell-1 and Aishell-2
datasets.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 15:00:09 GMT'}]",2020-10-29,"[['Song', 'Xingchen', ''], ['Wu', 'Zhiyong', ''], ['Huang', 'Yiheng', ''], ['Weng', 'Chao', ''], ['Su', 'Dan', ''], ['Meng', 'Helen', '']]"
1371282,2010.14952,Isar Nejadgholi,Svetlana Kiritchenko and Isar Nejadgholi,Towards Ethics by Design in Online Abusive Content Detection,"14 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To support safety and inclusion in online communications, significant efforts
in NLP research have been put towards addressing the problem of abusive content
detection, commonly defined as a supervised classification task. The research
effort has spread out across several closely related sub-areas, such as
detection of hate speech, toxicity, cyberbullying, etc. There is a pressing
need to consolidate the field under a common framework for task formulation,
dataset design and performance evaluation. Further, despite current
technologies achieving high classification accuracies, several ethical issues
have been revealed. We bring ethical issues to forefront and propose a unified
framework as a two-step process. First, online content is categorized around
personal and identity-related subject matters. Second, severity of abuse is
identified through comparative annotation within each category. The novel
framework is guided by the Ethics by Design principle and is a step towards
building more accurate and trusted models.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 13:10:24 GMT'}]",2020-10-29,"[['Kiritchenko', 'Svetlana', ''], ['Nejadgholi', 'Isar', '']]"
1371444,2010.15114,Kyle Aitken,"Kyle Aitken, Vinay V. Ramasesh, Ankush Garg, Yuan Cao, David Sussillo,
  Niru Maheswaranathan",The geometry of integration in text classification RNNs,"9+19 pages, 30 figures",,,,cs.LG cs.CL stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the widespread application of recurrent neural networks (RNNs) across
a variety of tasks, a unified understanding of how RNNs solve these tasks
remains elusive. In particular, it is unclear what dynamical patterns arise in
trained RNNs, and how those patterns depend on the training dataset or task.
This work addresses these questions in the context of a specific natural
language processing task: text classification. Using tools from dynamical
systems analysis, we study recurrent networks trained on a battery of both
natural and synthetic text classification tasks. We find the dynamics of these
trained RNNs to be both interpretable and low-dimensional. Specifically, across
architectures and datasets, RNNs accumulate evidence for each class as they
process the text, using a low-dimensional attractor manifold as the underlying
mechanism. Moreover, the dimensionality and geometry of the attractor manifold
are determined by the structure of the training dataset; in particular, we
describe how simple word-count statistics computed on the training dataset can
be used to predict these properties. Our observations span multiple
architectures and datasets, reflecting a common mechanism RNNs employ to
perform text classification. To the degree that integration of evidence towards
a decision is a common computational primitive, this work lays the foundation
for using dynamical systems techniques to study the inner workings of RNNs.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 17:58:53 GMT'}]",2020-10-29,"[['Aitken', 'Kyle', ''], ['Ramasesh', 'Vinay V.', ''], ['Garg', 'Ankush', ''], ['Cao', 'Yuan', ''], ['Sussillo', 'David', ''], ['Maheswaranathan', 'Niru', '']]"
1371397,2010.15067,Muhammed Tarik Altuncu,"M. Tarik Altuncu, Sophia N. Yaliraki, Mauricio Barahona","Graph-based Topic Extraction from Vector Embeddings of Text Documents:
  Application to a Corpus of News Articles",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Production of news content is growing at an astonishing rate. To help manage
and monitor the sheer amount of text, there is an increasing need to develop
efficient methods that can provide insights into emerging content areas, and
stratify unstructured corpora of text into `topics' that stem intrinsically
from content similarity. Here we present an unsupervised framework that brings
together powerful vector embeddings from natural language processing with tools
from multiscale graph partitioning that can reveal natural partitions at
different resolutions without making a priori assumptions about the number of
clusters in the corpus. We show the advantages of graph-based clustering
through end-to-end comparisons with other popular clustering and topic
modelling methods, and also evaluate different text vector embeddings, from
classic Bag-of-Words to Doc2Vec to the recent transformers based model Bert.
This comparative work is showcased through an analysis of a corpus of US news
coverage during the presidential election year of 2016.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 16:20:05 GMT'}]",2020-10-29,"[['Altuncu', 'M. Tarik', ''], ['Yaliraki', 'Sophia N.', ''], ['Barahona', 'Mauricio', '']]"
1371420,2010.15090,Vishal Sunder,Vishal Sunder and Eric Fosler-Lussier,"Handling Class Imbalance in Low-Resource Dialogue Systems by Combining
  Few-Shot Classification and Interpolation","5 pages, 4 figures, 3 tables",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Utterance classification performance in low-resource dialogue systems is
constrained by an inevitably high degree of data imbalance in class labels. We
present a new end-to-end pairwise learning framework that is designed
specifically to tackle this phenomenon by inducing a few-shot classification
capability in the utterance representations and augmenting data through an
interpolation of utterance representations. Our approach is a general purpose
training methodology, agnostic to the neural architecture used for encoding
utterances. We show significant improvements in macro-F1 score over standard
cross-entropy training for three different neural architectures, demonstrating
improvements on a Virtual Patient dialogue dataset as well as a low-resourced
emulation of the Switchboard dialogue act classification dataset.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 17:05:24 GMT'}]",2020-10-29,"[['Sunder', 'Vishal', ''], ['Fosler-Lussier', 'Eric', '']]"
1371366,2010.15036,Usman Naseem,"Usman Naseem, Imran Razzak, Shah Khalid Khan, Mukesh Prasad","A Comprehensive Survey on Word Representation Models: From Classical to
  State-Of-The-Art Word Representation Language Models",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Word representation has always been an important research area in the history
of natural language processing (NLP). Understanding such complex text data is
imperative, given that it is rich in information and can be used widely across
various applications. In this survey, we explore different word representation
models and its power of expression, from the classical to modern-day
state-of-the-art word representation language models (LMS). We describe a
variety of text representation methods, and model designs have blossomed in the
context of NLP, including SOTA LMs. These models can transform large volumes of
text into effective vector representations capturing the same semantic
information. Further, such representations can be utilized by various machine
learning (ML) algorithms for a variety of NLP related tasks. In the end, this
survey briefly discusses the commonly used ML and DL based classifiers,
evaluation metrics and the applications of these word embeddings in different
NLP tasks.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 15:15:13 GMT'}]",2020-10-29,"[['Naseem', 'Usman', ''], ['Razzak', 'Imran', ''], ['Khan', 'Shah Khalid', ''], ['Prasad', 'Mukesh', '']]"
1371395,2010.15065,Amir Shanehsazzadeh,"Amir Shanehsazzadeh, David Belanger, David Dohan",Fixed-Length Protein Embeddings using Contextual Lenses,,,,,q-bio.BM cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Basic Local Alignment Search Tool (BLAST) is currently the most popular
method for searching databases of biological sequences. BLAST compares
sequences via similarity defined by a weighted edit distance, which results in
it being computationally expensive. As opposed to working with edit distance, a
vector similarity approach can be accelerated substantially using modern
hardware or hashing techniques. Such an approach would require fixed-length
embeddings for biological sequences. There has been recent interest in learning
fixed-length protein embeddings using deep learning models under the hypothesis
that the hidden layers of supervised or semi-supervised models could produce
potentially useful vector embeddings. We consider transformer (BERT) protein
language models that are pretrained on the TrEMBL data set and learn
fixed-length embeddings on top of them with contextual lenses. The embeddings
are trained to predict the family a protein belongs to for sequences in the
Pfam database. We show that for nearest-neighbor family classification,
pretraining offers a noticeable boost in performance and that the corresponding
learned embeddings are competitive with BLAST. Furthermore, we show that the
raw transformer embeddings, obtained via static pooling, do not perform well on
nearest-neighbor family classification, which suggests that learning embeddings
in a supervised manner via contextual lenses may be a compute-efficient
alternative to fine-tuning.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 14:54:55 GMT'}]",2020-10-29,"[['Shanehsazzadeh', 'Amir', ''], ['Belanger', 'David', ''], ['Dohan', 'David', '']]"
1267866,2004.03096,Yiming Cui,"Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, Guoping Hu",Is Graph Structure Necessary for Multi-hop Question Answering?,"6 pages, to appear at EMNLP 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, attempting to model texts as graph structure and introducing graph
neural networks to deal with it has become a trend in many NLP research areas.
In this paper, we investigate whether the graph structure is necessary for
multi-hop question answering. Our analysis is centered on HotpotQA. We
construct a strong baseline model to establish that, with the proper use of
pre-trained models, graph structure may not be necessary for multi-hop question
answering. We point out that both graph structure and adjacency matrix are
task-related prior knowledge, and graph-attention can be considered as a
special case of self-attention. Experiments and visualized analysis demonstrate
that graph-attention or the entire graph structure can be replaced by
self-attention or Transformers.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 02:59:42 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 09:29:19 GMT'}]",2020-10-30,"[['Shao', 'Nan', ''], ['Cui', 'Yiming', ''], ['Liu', 'Ting', ''], ['Wang', 'Shijin', ''], ['Hu', 'Guoping', '']]"
1356512,2010.00182,Yang Zhang,"Yang Zhang, Qiang Ma",Dual Attention Model for Citation Recommendation,,,,,cs.IR cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Based on an exponentially increasing number of academic articles, discovering
and citing comprehensive and appropriate resources has become a non-trivial
task. Conventional citation recommender methods suffer from severe information
loss. For example, they do not consider the section of the paper that the user
is writing and for which they need to find a citation, the relatedness between
the words in the local context (the text span that describes a citation), or
the importance on each word from the local context. These shortcomings make
such methods insufficient for recommending adequate citations to academic
manuscripts. In this study, we propose a novel embedding-based neural network
called ""dual attention model for citation recommendation (DACR)"" to recommend
citations during manuscript preparation. Our method adapts embedding of three
dimensions of semantic information: words in the local context, structural
contexts, and the section on which a user is working. A neural network is
designed to maximize the similarity between the embedding of the three input
(local context words, section and structural contexts) and the target citation
appearing in the context. The core of the neural network is composed of
self-attention and additive attention, where the former aims to capture the
relatedness between the contextual words and structural context, and the latter
aims to learn the importance of them. The experiments on real-world datasets
demonstrate the effectiveness of the proposed approach.
","[{'version': 'v1', 'created': 'Thu, 1 Oct 2020 02:41:47 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 11:27:20 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Oct 2020 12:57:58 GMT'}, {'version': 'v4', 'created': 'Thu, 29 Oct 2020 12:31:26 GMT'}]",2020-10-30,"[['Zhang', 'Yang', ''], ['Ma', 'Qiang', '']]"
1371865,2010.15535,Craig Stewart,"Ricardo Rei, Craig Stewart, Catarina Farinha, Alon Lavie",Unbabel's Participation in the WMT20 Metrics Shared Task,WMT Metrics Shared Task 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present the contribution of the Unbabel team to the WMT 2020 Shared Task
on Metrics. We intend to participate on the segment-level, document-level and
system-level tracks on all language pairs, as well as the 'QE as a Metric'
track. Accordingly, we illustrate results of our models in these tracks with
reference to test sets from the previous year. Our submissions build upon the
recently proposed COMET framework: We train several estimator models to regress
on different human-generated quality scores and a novel ranking model trained
on relative ranks obtained from Direct Assessments. We also propose a simple
technique for converting segment-level predictions into a document-level score.
Overall, our systems achieve strong results for all language pairs on previous
test sets and in many cases set a new state-of-the-art.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 12:59:44 GMT'}]",2020-10-30,"[['Rei', 'Ricardo', ''], ['Stewart', 'Craig', ''], ['Farinha', 'Catarina', ''], ['Lavie', 'Alon', '']]"
1371796,2010.15466,Yuyang Nie,"Yuyang Nie, Yuanhe Tian, Yan Song, Xiang Ao, and Xiang Wan","Improving Named Entity Recognition with Attentive Ensemble of Syntactic
  Information","Natural Language Processing. 15 pages, 3 figures, Findings of
  EMNLP-2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Named entity recognition (NER) is highly sensitive to sentential syntactic
and semantic properties where entities may be extracted according to how they
are used and placed in the running text. To model such properties, one could
rely on existing resources to providing helpful knowledge to the NER task; some
existing studies proved the effectiveness of doing so, and yet are limited in
appropriately leveraging the knowledge such as distinguishing the important
ones for particular context. In this paper, we improve NER by leveraging
different types of syntactic information through attentive ensemble, which
functionalizes by the proposed key-value memory networks, syntax attention, and
the gate mechanism for encoding, weighting and aggregating such syntactic
information, respectively. Experimental results on six English and Chinese
benchmark datasets suggest the effectiveness of the proposed model and show
that it outperforms previous studies on all experiment datasets.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 10:25:17 GMT'}]",2020-10-30,"[['Nie', 'Yuyang', ''], ['Tian', 'Yuanhe', ''], ['Song', 'Yan', ''], ['Ao', 'Xiang', ''], ['Wan', 'Xiang', '']]"
1371788,2010.15458,Yuyang Nie,"Yuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, and Bo Dai","Named Entity Recognition for Social Media Texts with Semantic
  Augmentation","Natural Language Processing. 9 pages, 3 figures. EMNLP-2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing approaches for named entity recognition suffer from data sparsity
problems when conducted on short and informal texts, especially user-generated
social media content. Semantic augmentation is a potential way to alleviate
this problem. Given that rich semantic information is implicitly preserved in
pre-trained word embeddings, they are potential ideal resources for semantic
augmentation. In this paper, we propose a neural-based approach to NER for
social media texts where both local (from running text) and augmented semantics
are taken into account. In particular, we obtain the augmented semantic
information from a large-scale corpus, and propose an attentive semantic
augmentation module and a gate module to encode and aggregate such information,
respectively. Extensive experiments are performed on three benchmark datasets
collected from English and Chinese social media platforms, where the results
demonstrate the superiority of our approach to previous studies across all
three datasets.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 10:06:46 GMT'}]",2020-10-30,"[['Nie', 'Yuyang', ''], ['Tian', 'Yuanhe', ''], ['Wan', 'Xiang', ''], ['Song', 'Yan', ''], ['Dai', 'Bo', '']]"
1370914,2010.14584,Aleksandra Edwards Mrs,"Aleksandra Edwards, David Rogers, Jose Camacho-Collados, H\'el\`ene de
  Ribaupierre, Alun Preece","Predicting Themes within Complex Unstructured Texts: A Case Study on
  Safeguarding Reports","10 pages, 5 figures, workshop",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The task of text and sentence classification is associated with the need for
large amounts of labelled training data. The acquisition of high volumes of
labelled datasets can be expensive or unfeasible, especially for
highly-specialised domains for which documents are hard to obtain. Research on
the application of supervised classification based on small amounts of training
data is limited. In this paper, we address the combination of state-of-the-art
deep learning and classification methods and provide an insight into what
combination of methods fit the needs of small, domain-specific, and
terminologically-rich corpora. We focus on a real-world scenario related to a
collection of safeguarding reports comprising learning experiences and
reflections on tackling serious incidents involving children and vulnerable
adults. The relatively small volume of available reports and their use of
highly domain-specific terminology makes the application of automated
approaches difficult. We focus on the problem of automatically identifying the
main themes in a safeguarding report using supervised classification
approaches. Our results show the potential of deep learning models to simulate
subject-expert behaviour even for complex tasks with limited labelled data.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:48:23 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 09:15:14 GMT'}]",2020-10-30,"[['Edwards', 'Aleksandra', ''], ['Rogers', 'David', ''], ['Camacho-Collados', 'Jose', ''], ['de Ribaupierre', 'Hélène', ''], ['Preece', 'Alun', '']]"
1371767,2010.15437,Mana Ihori,"Mana Ihori, Ryo Masumura, Naoki Makishima, Tomohiro Tanaka, Akihiko
  Takashima, Shota Orihashi","Memory Attentive Fusion: External Language Model Integration for
  Transformer-based Sequence-to-Sequence Model",Accepted as a short paper at INLG 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a novel fusion method for integrating an external
language model (LM) into the Transformer based sequence-to-sequence (seq2seq)
model. While paired data are basically required to train the seq2seq model, the
external LM can be trained with only unpaired data. Thus, it is important to
leverage memorized knowledge in the external LM for building the seq2seq model,
since it is hard to prepare a large amount of paired data. However, the
existing fusion methods assume that the LM is integrated with recurrent neural
network-based seq2seq models instead of the Transformer. Therefore, this paper
proposes a fusion method that can explicitly utilize network structures in the
Transformer. The proposed method, called {\bf memory attentive fusion},
leverages the Transformer-style attention mechanism that repeats source-target
attention in a multi-hop manner for reading the memorized knowledge in the LM.
Our experiments on two text-style conversion tasks demonstrate that the
proposed method performs better than conventional fusion methods.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 09:16:23 GMT'}]",2020-10-30,"[['Ihori', 'Mana', ''], ['Masumura', 'Ryo', ''], ['Makishima', 'Naoki', ''], ['Tanaka', 'Tomohiro', ''], ['Takashima', 'Akihiko', ''], ['Orihashi', 'Shota', '']]"
1370901,2010.14571,Isaac Caswell,"Isaac Caswell, Theresa Breiner, Daan van Esch, Ankur Bapna","Language ID in the Wild: Unexpected Challenges on the Path to a
  Thousand-Language Web Text Corpus",Accepted to COLING 2020. 9 pages with 8 page abstract,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large text corpora are increasingly important for a wide variety of Natural
Language Processing (NLP) tasks, and automatic language identification (LangID)
is a core technology needed to collect such datasets in a multilingual context.
LangID is largely treated as solved in the literature, with models reported
that achieve over 90% average F1 on as many as 1,366 languages. We train LangID
models on up to 1,629 languages with comparable quality on held-out test sets,
but find that human-judged LangID accuracy for web-crawl text corpora created
using these models is only around 5% for many lower-resource languages,
suggesting a need for more robust evaluation. Further analysis revealed a
variety of error modes, arising from domain mismatch, class imbalance, language
similarity, and insufficiently expressive models. We propose two classes of
techniques to mitigate these errors: wordlist-based tunable-precision filters
(for which we release curated lists in about 500 languages) and
transformer-based semi-supervised LangID models, which increase median dataset
precision from 5.5% to 71.2%. These techniques enable us to create an initial
data set covering 100K or more relatively clean sentences in each of 500+
languages, paving the way towards a 1,000-language web text corpus.
","[{'version': 'v1', 'created': 'Tue, 27 Oct 2020 19:29:17 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 15:18:35 GMT'}]",2020-10-30,"[['Caswell', 'Isaac', ''], ['Breiner', 'Theresa', ''], ['van Esch', 'Daan', ''], ['Bapna', 'Ankur', '']]"
1371753,2010.15423,M\=arcis Pinnis,"Rihards Kri\v{s}lauks, M\=arcis Pinnis",Tilde at WMT 2020: News Task Systems,,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  This paper describes Tilde's submission to the WMT2020 shared task on news
translation for both directions of the English-Polish language pair in both the
constrained and the unconstrained tracks. We follow our submissions from the
previous years and build our baseline systems to be morphologically motivated
sub-word unit-based Transformer base models that we train using the Marian
machine translation toolkit. Additionally, we experiment with different
parallel and monolingual data selection schemes, as well as sampled
back-translation. Our final models are ensembles of Transformer base and
Transformer big models that feature right-to-left re-ranking.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 08:59:37 GMT'}]",2020-10-30,"[['Krišlauks', 'Rihards', ''], ['Pinnis', 'Mārcis', '']]"
1371741,2010.15411,Milan Gritta,"Milan Gritta, Gerasimos Lampouras and Ignacio Iacobacci","Conversation Graph: Data Augmentation, Training and Evaluation for
  Non-Deterministic Dialogue Management","Accepted at Transactions of Association of Computational Linguistics
  (to be presented at ACL 2021)",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Task-oriented dialogue systems typically rely on large amounts of
high-quality training data or require complex handcrafted rules. However,
existing datasets are often limited in size considering the complexity of the
dialogues. Additionally, conventional training signal inference is not suitable
for non-deterministic agent behaviour, i.e. considering multiple actions as
valid in identical dialogue states. We propose the Conversation Graph
(ConvGraph), a graph-based representation of dialogues that can be exploited
for data augmentation, multi-reference training and evaluation of
non-deterministic agents. ConvGraph generates novel dialogue paths to augment
data volume and diversity. Intrinsic and extrinsic evaluation across three
datasets shows that data augmentation and/or multi-reference training with
ConvGraph can improve dialogue success rates by up to 6.4%.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 08:23:24 GMT'}]",2020-10-30,"[['Gritta', 'Milan', ''], ['Lampouras', 'Gerasimos', ''], ['Iacobacci', 'Ignacio', '']]"
1371928,2010.15598,Micaela Kaplan,Micaela Kaplan,"May I Ask Who's Calling? Named Entity Recognition on Call Center
  Transcripts for Privacy Law Compliance",The 6th Workshop on Noisy User-generated Text (W-NUT) 2020 at EMNLP,"Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop
  on Noisy User-generated Text (2020) 1-6",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate using Named Entity Recognition on a new type of user-generated
text: a call center conversation. These conversations combine problems from
spontaneous speech with problems novel to conversational Automated Speech
Recognition, including incorrect recognition, alongside other common problems
from noisy user-generated text. Using our own corpus with new annotations,
training custom contextual string embeddings, and applying a BiLSTM-CRF, we
match state-of-the-art results on our novel task.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 13:53:42 GMT'}]",2020-10-30,"[['Kaplan', 'Micaela', '']]"
1371696,2010.15366,Sung-Feng Huang,"Sung-Feng Huang, Shun-Po Chuang, Da-Rong Liu, Yi-Chen Chen, Gene-Ping
  Yang, Hung-yi Lee","Self-supervised Pre-training Reduces Label Permutation Instability of
  Speech Separation",submitted to ICASSP2021,,,,cs.SD cs.CL eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Speech separation has been well-developed while there are still problems
waiting to be solved. The main problem we focus on in this paper is the
frequent label permutation switching of permutation invariant training (PIT).
For N-speaker separation, there would be N! possible label permutations. How to
stably select correct label permutations is a long-standing problem. In this
paper, we utilize self-supervised pre-training to stabilize the label
permutations. Among several types of self-supervised tasks, speech enhancement
based pre-training tasks show significant effectiveness in our experiments.
When using off-the-shelf pre-trained models, training duration could be
shortened to one-third to two-thirds. Furthermore, even taking pre-training
time into account, the entire training process could still be shorter without a
performance drop when using a larger batch size.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 06:07:01 GMT'}]",2020-10-30,"[['Huang', 'Sung-Feng', ''], ['Chuang', 'Shun-Po', ''], ['Liu', 'Da-Rong', ''], ['Chen', 'Yi-Chen', ''], ['Yang', 'Gene-Ping', ''], ['Lee', 'Hung-yi', '']]"
1371690,2010.15360,Shaolei Wang,"Shaolei Wang, Zhongyuan Wang, Wanxiang Che, Ting Liu","Combining Self-Training and Self-Supervised Learning for Unsupervised
  Disfluency Detection",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most existing approaches to disfluency detection heavily rely on
human-annotated corpora, which is expensive to obtain in practice. There have
been several proposals to alleviate this issue with, for instance,
self-supervised learning techniques, but they still require human-annotated
corpora. In this work, we explore the unsupervised learning paradigm which can
potentially work with unlabeled text corpora that are cheaper and easier to
obtain. Our model builds upon the recent work on Noisy Student Training, a
semi-supervised learning approach that extends the idea of self-training.
Experimental results on the commonly used English Switchboard test set show
that our approach achieves competitive performance compared to the previous
state-of-the-art supervised systems using contextualized word embeddings (e.g.
BERT and ELECTRA).
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 05:29:26 GMT'}]",2020-10-30,"[['Wang', 'Shaolei', ''], ['Wang', 'Zhongyuan', ''], ['Che', 'Wanxiang', ''], ['Liu', 'Ting', '']]"
1348756,2009.07253,Alexander Lin,"Alexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei",Autoregressive Knowledge Distillation through Imitation Learning,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The performance of autoregressive models on natural language generation tasks
has dramatically improved due to the adoption of deep, self-attentive
architectures. However, these gains have come at the cost of hindering
inference speed, making state-of-the-art models cumbersome to deploy in
real-world, time-sensitive settings. We develop a compression technique for
autoregressive models that is driven by an imitation learning perspective on
knowledge distillation. The algorithm is designed to address the exposure bias
problem. On prototypical language generation tasks such as translation and
summarization, our method consistently outperforms other distillation
algorithms, such as sequence-level knowledge distillation. Student models
trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those
trained from scratch, while increasing inference speed by up to 14 times in
comparison to the teacher model.
","[{'version': 'v1', 'created': 'Tue, 15 Sep 2020 17:43:02 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 00:40:45 GMT'}]",2020-10-30,"[['Lin', 'Alexander', ''], ['Wohlwend', 'Jeremy', ''], ['Chen', 'Howard', ''], ['Lei', 'Tao', '']]"
1371646,2010.15316,Michal Malyska,"Alister D Costa, Stefan Denkovski, Michal Malyska, Sae Young Moon,
  Brandon Rufino, Zhen Yang, Taylor Killian, Marzyeh Ghassemi",Multiple Sclerosis Severity Classification From Clinical Text,EMNLP 2020 Clinical NLP workshop,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative
neurological disease, which is monitored by a specialist using the Expanded
Disability Status Scale (EDSS) and recorded in unstructured text in the form of
a neurology consult note. An EDSS measurement contains an overall ""EDSS"" score
and several functional subscores. Typically, expert knowledge is required to
interpret consult notes and generate these scores. Previous approaches used
limited context length Word2Vec embeddings and keyword searches to predict
scores given a consult note, but often failed when scores were not explicitly
stated. In this work, we present MS-BERT, the first publicly available
transformer model trained on real clinical data other than MIMIC. Next, we
present MSBC, a classifier that applies MS-BERT to generate embeddings and
predict EDSS and functional subscores. Lastly, we explore combining MSBC with
other models through the use of Snorkel to generate scores for unlabelled
consult notes. MSBC achieves state-of-the-art performance on all metrics and
prediction tasks and outperforms the models generated from the Snorkel
ensemble. We improve Macro-F1 by 0.12 (to 0.88) for predicting EDSS and on
average by 0.29 (to 0.63) for predicting functional subscores over previous
Word2Vec CNN and rule-based approaches.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 02:15:23 GMT'}]",2020-10-30,"[['Costa', 'Alister D', ''], ['Denkovski', 'Stefan', ''], ['Malyska', 'Michal', ''], ['Moon', 'Sae Young', ''], ['Rufino', 'Brandon', ''], ['Yang', 'Zhen', ''], ['Killian', 'Taylor', ''], ['Ghassemi', 'Marzyeh', '']]"
1265269,2004.00499,Shengbin Jia,Shengbin Jia,Unique Chinese Linguistic Phenomena,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Linguistics holds unique characteristics of generality, stability, and
nationality, which will affect the formulation of extraction strategies and
should be incorporated into the relation extraction. Chinese open relation
extraction is not well-established, because of the complexity of Chinese
linguistics makes it harder to operate, and the methods for English are not
compatible with that for Chinese. The diversities between Chinese and English
linguistics are mainly reflected in morphology and syntax.
","[{'version': 'v1', 'created': 'Sun, 23 Feb 2020 12:13:48 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Jul 2020 10:00:08 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 06:07:07 GMT'}]",2020-10-30,"[['Jia', 'Shengbin', '']]"
1150762,1907.06226,Jipeng Qiang,Jipeng Qiang and Yun Li and Yi Zhu and Yunhao Yuan and Xindong Wu,Lexical Simplification with Pretrained Encoders,,,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lexical simplification (LS) aims to replace complex words in a given sentence
with their simpler alternatives of equivalent meaning. Recently unsupervised
lexical simplification approaches only rely on the complex word itself
regardless of the given sentence to generate candidate substitutions, which
will inevitably produce a large number of spurious candidates. We present a
simple LS approach that makes use of the Bidirectional Encoder Representations
from Transformers (BERT) which can consider both the given sentence and the
complex word during generating candidate substitutions for the complex word.
Specifically, we mask the complex word of the original sentence for feeding
into the BERT to predict the masked token. The predicted results will be used
as candidate substitutions. Despite being entirely unsupervised, experimental
results show that our approach obtains obvious improvement compared with these
baselines leveraging linguistic databases and parallel corpus, outperforming
the state-of-the-art by more than 12 Accuracy points on three well-known
benchmarks.
","[{'version': 'v1', 'created': 'Sun, 14 Jul 2019 14:19:22 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Jul 2019 14:36:41 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Jul 2019 03:36:12 GMT'}, {'version': 'v4', 'created': 'Fri, 16 Aug 2019 01:48:46 GMT'}, {'version': 'v5', 'created': 'Thu, 29 Oct 2020 03:21:25 GMT'}]",2020-10-30,"[['Qiang', 'Jipeng', ''], ['Li', 'Yun', ''], ['Zhu', 'Yi', ''], ['Yuan', 'Yunhao', ''], ['Wu', 'Xindong', '']]"
1371643,2010.15313,Keen You,Keen You and Dan Goldwasser,"""where is this relationship going?"": Understanding Relationship
  Trajectories in Narrative Text",Accepted to *Sem 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We examine a new commonsense reasoning task: given a narrative describing a
social interaction that centers on two protagonists, systems make inferences
about the underlying relationship trajectory. Specifically, we propose two
evaluation tasks: Relationship Outlook Prediction MCQ and Resolution Prediction
MCQ. In Relationship Outlook Prediction, a system maps an interaction to a
relationship outlook that captures how the interaction is expected to change
the relationship. In Resolution Prediction, a system attributes a given
relationship outlook to a particular resolution that explains the outcome.
These two tasks parallel two real-life questions that people frequently ponder
upon as they navigate different social situations: ""where is this relationship
going?"" and ""how did we end up here?"". To facilitate the investigation of human
social relationships through these two tasks, we construct a new dataset,
Social Narrative Tree, which consists of 1250 stories documenting a variety of
daily social interactions. The narratives encode a multitude of social elements
that interweave to give rise to rich commonsense knowledge of how relationships
evolve with respect to social interactions. We establish baseline performances
using language models and the accuracies are significantly lower than human
performance. The results demonstrate that models need to look beyond syntactic
and semantic signals to comprehend complex human relationships.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 02:07:05 GMT'}]",2020-10-30,"[['You', 'Keen', ''], ['Goldwasser', 'Dan', '']]"
1371930,2010.15600,Ciro Garcia Mr,Ciro Ivan Garcia Lopez,Three computational models and its equivalence,,,,,cs.LO cs.CC cs.CL cs.GL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The study of computability has its origin in Hilbert's conference of 1900,
where an adjacent question, to the ones he asked, is to give a precise
description of the notion of algorithm. In the search for a good definition
arose three independent theories: Turing and the Turing machines, G\""odel and
the recursive functions, Church and the Lambda Calculus.
  Later there were established by Kleene that the classic models of computation
are equivalent. This fact is widely accepted by many textbooks and the proof is
omitted since the proof is tedious and unreadable. We intend to fill this gap
presenting the proof in a modern way, without forgetting the mathematical
details.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 05:55:19 GMT'}]",2020-10-30,"[['Lopez', 'Ciro Ivan Garcia', '']]"
1371983,2010.15653,Niko Moritz,"Niko Moritz, Takaaki Hori, Jonathan Le Roux","Semi-Supervised Speech Recognition via Graph-based Temporal
  Classification",Submitted to ICASSP 2021,,,,cs.LG cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semi-supervised learning has demonstrated promising results in automatic
speech recognition (ASR) by self-training using a seed ASR model with
pseudo-labels generated for unlabeled data. The effectiveness of this approach
largely relies on the pseudo-label accuracy, for which typically only the
1-best ASR hypothesis is used. However, alternative ASR hypotheses of an N-best
list can provide more accurate labels for an unlabeled speech utterance and
also reflect uncertainties of the seed ASR model. In this paper, we propose a
generalized form of the connectionist temporal classification (CTC) objective
that accepts a graph representation of the training targets. The newly proposed
graph-based temporal classification (GTC) objective is applied for
self-training with WFST-based supervision, which is generated from an N-best
list of pseudo-labels. In this setup, GTC is used to learn not only a temporal
alignment, similarly to CTC, but also a label alignment to obtain the optimal
pseudo-label sequence from the weighted graph. Results show that this approach
can effectively exploit an N-best list of pseudo-labels with associated scores,
outperforming standard pseudo-labeling by a large margin, with ASR results
close to an oracle experiment in which the best hypotheses of the N-best lists
are selected manually.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 14:56:56 GMT'}]",2020-10-30,"[['Moritz', 'Niko', ''], ['Hori', 'Takaaki', ''], ['Roux', 'Jonathan Le', '']]"
1358840,2010.02510,Lily Ou,"Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli, Sharon
  Levy, Diba Mirza, William Yang Wang","Investigating African-American Vernacular English in Transformer-Based
  Text Generation","7 pages, EMNLP 2020",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The growth of social media has encouraged the written use of African American
Vernacular English (AAVE), which has traditionally been used only in oral
contexts. However, NLP models have historically been developed using dominant
English varieties, such as Standard American English (SAE), due to text corpora
availability. We investigate the performance of GPT-2 on AAVE text by creating
a dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating
syntactic structure and AAVE- or SAE-specific language for each pair. We
evaluate each sample and its GPT-2 generated text with pretrained sentiment
classifiers and find that while AAVE text results in more classifications of
negative sentiment than SAE, the use of GPT-2 generally increases occurrences
of positive sentiment for both. Additionally, we conduct human evaluation of
AAVE and SAE text generated with GPT-2 to compare contextual rigor and overall
quality.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 06:27:02 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 04:00:46 GMT'}]",2020-10-30,"[['Groenwold', 'Sophie', ''], ['Ou', 'Lily', ''], ['Parekh', 'Aesha', ''], ['Honnavalli', 'Samhita', ''], ['Levy', 'Sharon', ''], ['Mirza', 'Diba', ''], ['Wang', 'William Yang', '']]"
1367089,2010.10759,Yangyang Shi,"Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian
  Chan, Frank Zhang, Duc Le, Mike Seltzer","Emformer: Efficient Memory Transformer Based Acoustic Model For Low
  Latency Streaming Speech Recognition","5 pages, 2 figures, submitted to ICASSP 2021",,,,cs.SD cs.CL cs.LG eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper proposes an efficient memory transformer Emformer for low latency
streaming speech recognition. In Emformer, the long-range history context is
distilled into an augmented memory bank to reduce self-attention's computation
complexity. A cache mechanism saves the computation for the key and value in
self-attention for the left context. Emformer applies a parallelized block
processing in training to support low latency models. We carry out experiments
on benchmark LibriSpeech data. Under average latency of 960 ms, Emformer gets
WER $2.50\%$ on test-clean and $5.62\%$ on test-other. Comparing with a strong
baseline augmented memory transformer (AM-TRF), Emformer gets $4.6$ folds
training speedup and $18\%$ relative real-time factor (RTF) reduction in
decoding with relative WER reduction $17\%$ on test-clean and $9\%$ on
test-other. For a low latency scenario with an average latency of 80 ms,
Emformer achieves WER $3.01\%$ on test-clean and $7.09\%$ on test-other.
Comparing with the LSTM baseline with the same latency and model size, Emformer
gets relative WER reduction $9\%$ and $16\%$ on test-clean and test-other,
respectively.
","[{'version': 'v1', 'created': 'Wed, 21 Oct 2020 04:38:09 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Oct 2020 19:59:08 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 14:55:59 GMT'}]",2020-10-30,"[['Shi', 'Yangyang', ''], ['Wang', 'Yongqiang', ''], ['Wu', 'Chunyang', ''], ['Yeh', 'Ching-Feng', ''], ['Chan', 'Julian', ''], ['Zhang', 'Frank', ''], ['Le', 'Duc', ''], ['Seltzer', 'Mike', '']]"
1371388,2010.15058,Tomek Korbak,Tomasz Korbak and Julian Zubek and Joanna R\k{a}czaszek-Leonardi,Measuring non-trivial compositionality in emergent communication,"4th Workshop on Emergent Communication, NeurIPS 2020",,,,cs.NE cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Compositionality is an important explanatory target in emergent communication
and language evolution. The vast majority of computational models of
communication account for the emergence of only a very basic form of
compositionality: trivial compositionality. A compositional protocol is
trivially compositional if the meaning of a complex signal (e.g. blue circle)
boils down to the intersection of meanings of its constituents (e.g. the
intersection of the set of blue objects and the set of circles). A protocol is
non-trivially compositional (NTC) if the meaning of a complex signal (e.g.
biggest apple) is a more complex function of the meanings of their
constituents. In this paper, we review several metrics of compositionality used
in emergent communication and experimentally show that most of them fail to
detect NTC - i.e. they treat non-trivial compositionality as a failure of
compositionality. The one exception is tree reconstruction error, a metric
motivated by formal accounts of compositionality. These results emphasise
important limitations of emergent communication research that could hamper
progress on modelling the emergence of NTC.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 16:11:07 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 16:22:44 GMT'}]",2020-10-30,"[['Korbak', 'Tomasz', ''], ['Zubek', 'Julian', ''], ['Rączaszek-Leonardi', 'Joanna', '']]"
1301728,2006.07214,Andre Martins,"Andr\'e F. T. Martins, Ant\'onio Farinhas, Marcos Treviso, Vlad
  Niculae, Pedro M. Q. Aguiar, M\'ario A. T. Figueiredo",Sparse and Continuous Attention Mechanisms,Accepted for spotlight presentation at NeurIPS 2020,,,,cs.LG cs.CL cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Exponential families are widely used in machine learning; they include many
distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet,
Poisson, and categorical distributions via the softmax transformation).
Distributions in each of these families have fixed support. In contrast, for
finite domains, there has been recent work on sparse alternatives to softmax
(e.g. sparsemax and alpha-entmax), which have varying support, being able to
assign zero probability to irrelevant categories. This paper expands that work
in two directions: first, we extend alpha-entmax to continuous domains,
revealing a link with Tsallis statistics and deformed exponential families.
Second, we introduce continuous-domain attention mechanisms, deriving efficient
gradient backpropagation algorithms for alpha in {1,2}. Experiments on
attention-based text classification, machine translation, and visual question
answering illustrate the use of continuous attention in 1D and 2D, showing that
it allows attending to time intervals and compact regions.
","[{'version': 'v1', 'created': 'Fri, 12 Jun 2020 14:16:48 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Oct 2020 22:22:38 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 08:39:54 GMT'}]",2020-10-30,"[['Martins', 'André F. T.', ''], ['Farinhas', 'António', ''], ['Treviso', 'Marcos', ''], ['Niculae', 'Vlad', ''], ['Aguiar', 'Pedro M. Q.', ''], ['Figueiredo', 'Mário A. T.', '']]"
1324516,2007.13002,Siyuan Feng,"Siyuan Feng, Odette Scharenborg","Unsupervised Subword Modeling Using Autoregressive Pretraining and
  Cross-Lingual Phone-Aware Modeling","5 pages, 3 figures. Accepted for publication in INTERSPEECH 2020,
  Shanghai, China",,10.21437/Interspeech.2020-1170,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This study addresses unsupervised subword modeling, i.e., learning feature
representations that can distinguish subword units of a language. The proposed
approach adopts a two-stage bottleneck feature (BNF) learning framework,
consisting of autoregressive predictive coding (APC) as a front-end and a
DNN-BNF model as a back-end. APC pretrained features are set as input features
to a DNN-BNF model. A language-mismatched ASR system is used to provide
cross-lingual phone labels for DNN-BNF model training. Finally, BNFs are
extracted as the subword-discriminative feature representation. A second aim of
this work is to investigate the robustness of our approach's effectiveness to
different amounts of training data. The results on Libri-light and the
ZeroSpeech 2017 databases show that APC is effective in front-end feature
pretraining. Our whole system outperforms the state of the art on both
databases. Cross-lingual phone labels for English data by a Dutch ASR
outperform those by a Mandarin ASR, possibly linked to the larger similarity of
Dutch compared to Mandarin with English. Our system is less sensitive to
training data amount when the training data is over 50 hours. APC pretraining
leads to a reduction of needed training material from over 5,000 hours to
around 200 hours with little performance degradation.
","[{'version': 'v1', 'created': 'Sat, 25 Jul 2020 19:41:41 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Aug 2020 19:15:48 GMT'}]",2020-10-30,"[['Feng', 'Siyuan', ''], ['Scharenborg', 'Odette', '']]"
1371596,2010.15266,Abhinav Singh,"Abhinav Singh, Patrick Xia, Guanghui Qin, Mahsa Yarmohammadi, Benjamin
  Van Durme","CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence
  Models",4th Workshop on Structured Prediction for NLP (EMNLP 2020),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Copy mechanisms are employed in sequence to sequence models (seq2seq) to
generate reproductions of words from the input to the output. These frameworks,
operating at the lexical type level, fail to provide an explicit alignment that
records where each token was copied from. Further, they require contiguous
token sequences from the input (spans) to be copied individually. We present a
model with an explicit token-level copy operation and extend it to copying
entire spans. Our model provides hard alignments between spans in the input and
output, allowing for nontraditional applications of seq2seq, like information
extraction. We demonstrate the approach on Nested Named Entity Recognition,
achieving near state-of-the-art accuracy with an order of magnitude increase in
decoding speed.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 22:45:16 GMT'}]",2020-10-30,"[['Singh', 'Abhinav', ''], ['Xia', 'Patrick', ''], ['Qin', 'Guanghui', ''], ['Yarmohammadi', 'Mahsa', ''], ['Van Durme', 'Benjamin', '']]"
1294236,2005.14441,Xiang Hao,"Xiang Hao, Xiangdong Su, Zhiyu Wang, Qiang Zhang, Huali Xu and
  Guanglai Gao",SNR-Based Teachers-Student Technique for Speech Enhancement,"Published in 2020 IEEE International Conference on Multimedia and
  Expo (ICME 2020)",,10.1109/ICME46284.2020.9102846,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is very challenging for speech enhancement methods to achieves robust
performance under both high signal-to-noise ratio (SNR) and low SNR
simultaneously. In this paper, we propose a method that integrates an SNR-based
teachers-student technique and time-domain U-Net to deal with this problem.
Specifically, this method consists of multiple teacher models and a student
model. We first train the teacher models under multiple small-range SNRs that
do not coincide with each other so that they can perform speech enhancement
well within the specific SNR range. Then, we choose different teacher models to
supervise the training of the student model according to the SNR of the
training data. Eventually, the student model can perform speech enhancement
under both high SNR and low SNR. To evaluate the proposed method, we
constructed a dataset with an SNR ranging from -20dB to 20dB based on the
public dataset. We experimentally analyzed the effectiveness of the SNR-based
teachers-student technique and compared the proposed method with several
state-of-the-art methods.
","[{'version': 'v1', 'created': 'Fri, 29 May 2020 08:13:01 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 12:12:20 GMT'}]",2020-10-30,"[['Hao', 'Xiang', ''], ['Su', 'Xiangdong', ''], ['Wang', 'Zhiyu', ''], ['Zhang', 'Qiang', ''], ['Xu', 'Huali', ''], ['Gao', 'Guanglai', '']]"
1294230,2005.14435,Xiang Hao,"Xiang Hao, Shixue Wen, Xiangdong Su, Yun Liu, Guanglai Gao and Xiaofei
  Li",Sub-Band Knowledge Distillation Framework for Speech Enhancement,Published in Interspeech 2020,,10.21437/Interspeech.2020-1539,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In single-channel speech enhancement, methods based on full-band spectral
features have been widely studied. However, only a few methods pay attention to
non-full-band spectral features. In this paper, we explore a knowledge
distillation framework based on sub-band spectral mapping for single-channel
speech enhancement. Specifically, we divide the full frequency band into
multiple sub-bands and pre-train an elite-level sub-band enhancement model
(teacher model) for each sub-band. These teacher models are dedicated to
processing their own sub-bands. Next, under the teacher models' guidance, we
train a general sub-band enhancement model (student model) that works for all
sub-bands. Without increasing the number of model parameters and computational
complexity, the student model's performance is further improved. To evaluate
our proposed method, we conducted a large number of experiments on an
open-source data set. The final experimental results show that the guidance
from the elite-level teacher models dramatically improves the student model's
performance, which exceeds the full-band model by employing fewer parameters.
","[{'version': 'v1', 'created': 'Fri, 29 May 2020 07:55:12 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 12:14:59 GMT'}]",2020-10-30,"[['Hao', 'Xiang', ''], ['Wen', 'Shixue', ''], ['Su', 'Xiangdong', ''], ['Liu', 'Yun', ''], ['Gao', 'Guanglai', ''], ['Li', 'Xiaofei', '']]"
1292684,2005.12889,Ruixiang Cui,"Ruixiang Cui, Daniel Hershcovich",Refining Implicit Argument Annotation for UCCA,DMR 2020,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Predicate-argument structure analysis is a central component in meaning
representations of text. The fact that some arguments are not explicitly
mentioned in a sentence gives rise to ambiguity in language understanding, and
renders it difficult for machines to interpret text correctly. However, only
few resources represent implicit roles for NLU, and existing studies in NLP
only make coarse distinctions between categories of arguments omitted from
linguistic form. This paper proposes a typology for fine-grained implicit
argument annotation on top of Universal Conceptual Cognitive Annotation's
foundational layer. The proposed implicit argument categorisation is driven by
theories of implicit role interpretation and consists of six types: Deictic,
Generic, Genre-based, Type-identifiable, Non-specific, and Iterated-set. We
exemplify our design by revisiting part of the UCCA EWT corpus, providing a new
dataset annotated with the refinement layer, and making a comparative analysis
with other schemes.
","[{'version': 'v1', 'created': 'Tue, 26 May 2020 17:24:15 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 16:07:33 GMT'}]",2020-10-30,"[['Cui', 'Ruixiang', ''], ['Hershcovich', 'Daniel', '']]"
1358810,2010.02480,Cheng-Han Chiang,"Cheng-Han Chiang, Sung-Feng Huang and Hung-yi Lee",Pretrained Language Model Embryology: The Birth of ALBERT,"Accepted to EMNLP 2020, short paper",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While behaviors of pretrained language models (LMs) have been thoroughly
examined, what happened during pretraining is rarely studied. We thus
investigate the developmental process from a set of randomly initialized
parameters to a totipotent language model, which we refer to as the embryology
of a pretrained language model. Our results show that ALBERT learns to
reconstruct and predict tokens of different parts of speech (POS) in different
learning speeds during pretraining. We also find that linguistic knowledge and
world knowledge do not generally improve as pretraining proceeds, nor do
downstream tasks' performance. These findings suggest that knowledge of a
pretrained model varies during pretraining, and having more pretrain steps does
not necessarily provide a model with more comprehensive knowledge. We will
provide source codes and pretrained models to reproduce our results at
https://github.com/d223302/albert-embryology.
","[{'version': 'v1', 'created': 'Tue, 6 Oct 2020 05:15:39 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 00:07:43 GMT'}]",2020-10-30,"[['Chiang', 'Cheng-Han', ''], ['Huang', 'Sung-Feng', ''], ['Lee', 'Hung-yi', '']]"
1201305,1911.02711,Sen Yang,"Sen Yang, Leyang Cui, Jun Xie and Yue Zhang",Making the Best Use of Review Summary for Sentiment Analysis,To be published in COLING-2020,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Sentiment analysis provides a useful overview of customer review contents.
Many review websites allow a user to enter a summary in addition to a full
review. Intuitively, summary information may give additional benefit for review
sentiment analysis. In this paper, we conduct a study to exploit methods for
better use of summary information. We start by finding out that the sentimental
signal distribution of a review and that of its corresponding summary are in
fact complementary to each other. We thus explore various architectures to
better guide the interactions between the two and propose a
hierarchically-refined review-centric attention model. Empirical results show
that our review-centric model can make better use of user-written summaries for
review sentiment analysis, and is also more effective compared to existing
methods when the user summary is replaced with summary generated by an
automatic summarization system.
","[{'version': 'v1', 'created': 'Thu, 7 Nov 2019 01:46:54 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Oct 2020 07:15:01 GMT'}]",2020-10-30,"[['Yang', 'Sen', ''], ['Cui', 'Leyang', ''], ['Xie', 'Jun', ''], ['Zhang', 'Yue', '']]"
1202469,1911.03875,Khalil Mrini,"Khalil Mrini, Franck Dernoncourt, Quan Tran, Trung Bui, Walter Chang,
  Ndapa Nakashole",Rethinking Self-Attention: Towards Interpretability in Neural Parsing,EMNLP 2020,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Attention mechanisms have improved the performance of NLP tasks while
allowing models to remain explainable. Self-attention is currently widely used,
however interpretability is difficult due to the numerous attention
distributions. Recent work has shown that model representations can benefit
from label-specific information, while facilitating interpretation of
predictions. We introduce the Label Attention Layer: a new form of
self-attention where attention heads represent labels. We test our novel layer
by running constituency and dependency parsing experiments and show our new
model obtains new state-of-the-art results for both tasks on both the Penn
Treebank (PTB) and Chinese Treebank. Additionally, our model requires fewer
self-attention layers compared to existing work. Finally, we find that the
Label Attention heads learn relations between syntactic categories and show
pathways to analyze errors.
","[{'version': 'v1', 'created': 'Sun, 10 Nov 2019 08:17:11 GMT'}, {'version': 'v2', 'created': 'Sat, 2 May 2020 04:34:52 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 06:17:11 GMT'}]",2020-10-30,"[['Mrini', 'Khalil', ''], ['Dernoncourt', 'Franck', ''], ['Tran', 'Quan', ''], ['Bui', 'Trung', ''], ['Chang', 'Walter', ''], ['Nakashole', 'Ndapa', '']]"
1217216,1912.05320,Carlos S. Armendariz,"Carlos Santos Armendariz, Matthew Purver, Matej Ul\v{c}ar, Senja
  Pollak, Nikola Ljube\v{s}i\'c, Marko Robnik-\v{S}ikonja, Mark
  Granroth-Wilding, Kristiina Vaik",CoSimLex: A Resource for Evaluating Graded Word Similarity in Context,,"Proceedings of the 12th Language Resources and Evaluation
  Conference (2020) 5878-5886",,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  State of the art natural language processing tools are built on
context-dependent word embeddings, but no direct method for evaluating these
representations currently exists. Standard tasks and datasets for intrinsic
evaluation of embeddings are based on judgements of similarity, but ignore
context; standard tasks for word sense disambiguation take account of context
but do not provide continuous measures of meaning similarity. This paper
describes an effort to build a new dataset, CoSimLex, intended to fill this
gap. Building on the standard pairwise similarity task of SimLex-999, it
provides context-dependent similarity measures; covers not only discrete
differences in word sense but more subtle, graded changes in meaning; and
covers not only a well-resourced language (English) but a number of
less-resourced languages. We define the task and evaluation metrics, outline
the dataset collection methodology, and describe the status of the dataset so
far.
","[{'version': 'v1', 'created': 'Wed, 11 Dec 2019 14:02:59 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Dec 2019 10:33:05 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 15:22:27 GMT'}]",2020-10-30,"[['Armendariz', 'Carlos Santos', ''], ['Purver', 'Matthew', ''], ['Ulčar', 'Matej', ''], ['Pollak', 'Senja', ''], ['Ljubešić', 'Nikola', ''], ['Robnik-Šikonja', 'Marko', ''], ['Granroth-Wilding', 'Mark', ''], ['Vaik', 'Kristiina', '']]"
1371555,2010.15225,Dylan Ebert,"Dylan Ebert, Ellie Pavlick",A Visuospatial Dataset for Naturalistic Verb Learning,"9 pages, 3 figures, starsem 2020",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a new dataset for training and evaluating grounded language
models. Our data is collected within a virtual reality environment and is
designed to emulate the quality of language data to which a pre-verbal child is
likely to have access: That is, naturalistic, spontaneous speech paired with
richly grounded visuospatial context. We use the collected data to compare
several distributional semantics models for verb learning. We evaluate neural
models based on 2D (pixel) features as well as feature-engineered models based
on 3D (symbolic, spatial) features, and show that neither modeling approach
achieves satisfactory performance. Our results are consistent with evidence
from child language acquisition that emphasizes the difficulty of learning
verbs from naive distributional data. We discuss avenues for future work on
cognitively-inspired grounded language learning, and release our corpus with
the intent of facilitating research on the topic.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 20:47:13 GMT'}]",2020-10-30,"[['Ebert', 'Dylan', ''], ['Pavlick', 'Ellie', '']]"
1362681,2010.06351,Fuli Luo,"Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun","CAPT: Contrastive Pre-Training for Learning Denoised Sequence
  Representations",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pre-trained self-supervised models such as BERT have achieved striking
success in learning sequence representations, especially for natural language
processing. These models typically corrupt the given sequences with certain
types of noise, such as masking, shuffling, or substitution, and then try to
recover the original input. However, such pre-training approaches are prone to
learning representations that are covariant with the noise, leading to the
discrepancy between the pre-training and fine-tuning stage. To remedy this, we
present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence
representations. The proposed CAPT encourages the consistency between
representations of the original sequence and its corrupted version via
unsupervised instance-wise training signals. In this way, it not only
alleviates the pretrain-finetune discrepancy induced by the noise of
pre-training, but also aids the pre-trained model in better capturing global
semantics of the input via more effective sentence-level supervision. Different
from most prior work that focuses on a particular modality, comprehensive
empirical evidence on 11 natural language understanding and cross-modal tasks
illustrates that CAPT is applicable for both language and vision-language
tasks, and obtains surprisingly consistent improvement, including 0.6% absolute
gain on GLUE benchmarks and 0.8% absolute increment on NLVR.
","[{'version': 'v1', 'created': 'Tue, 13 Oct 2020 13:08:34 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Oct 2020 09:30:12 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Oct 2020 06:41:07 GMT'}]",2020-10-30,"[['Luo', 'Fuli', ''], ['Yang', 'Pengcheng', ''], ['Li', 'Shicheng', ''], ['Ren', 'Xuancheng', ''], ['Sun', 'Xu', '']]"
1372108,2010.15778,Timo Denk,Timo I. Denk and Ana Peleteiro Ramallo,Contextual BERT: Conditioning the Language Model Using a Global State,"Accepted at the TextGraphs-14 workshop at COLING'2020 - The 28th
  International Conference on Computational Linguistics",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  BERT is a popular language model whose main pre-training task is to fill in
the blank, i.e., predicting a word that was masked out of a sentence, based on
the remaining words. In some applications, however, having an additional
context can help the model make the right prediction, e.g., by taking the
domain or the time of writing into account. This motivates us to advance the
BERT architecture by adding a global state for conditioning on a fixed-sized
context. We present our two novel approaches and apply them to an industry
use-case, where we complete fashion outfits with missing articles, conditioned
on a specific customer. An experimental comparison to other methods from the
literature shows that our methods improve personalization significantly.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 17:25:20 GMT'}]",2020-10-30,"[['Denk', 'Timo I.', ''], ['Ramallo', 'Ana Peleteiro', '']]"
1371581,2010.15251,Marimuthu Kalimuthu,"Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow",Fusion Models for Improved Visual Captioning,"Under review at ""Multi-Modal Deep Learning: Challenges and
  Applications"", ICPR-2020",,,,cs.CV cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Visual captioning aims to generate textual descriptions given images.
Traditionally, the captioning models are trained on human annotated datasets
such as Flickr30k and MS-COCO, which are limited in size and diversity. This
limitation hinders the generalization capabilities of these models while also
rendering them to often make mistakes. Language models can, however, be trained
on vast amounts of freely available unlabelled data and have recently emerged
as successful language encoders and coherent text generators. Meanwhile,
several unimodal and multimodal fusion techniques have been proven to work well
for natural language generation and automatic speech recognition. Building on
these recent developments, and with an aim of improving the quality of
generated captions, the contribution of our work in this paper is two-fold:
First, we propose a generic multimodal model fusion framework for caption
generation as well as emendation where we utilize different fusion strategies
to integrate a pretrained Auxiliary Language Model (AuxLM) within the
traditional encoder-decoder visual captioning frameworks. Next, we employ the
same fusion strategies to integrate a pretrained Masked Language Model (MLM),
namely BERT, with a visual captioning model, viz. Show, Attend, and Tell, for
emending both syntactic and semantic errors in captions. Our caption emendation
experiments on three benchmark image captioning datasets, viz. Flickr8k,
Flickr30k, and MSCOCO, show improvements over the baseline, indicating the
usefulness of our proposed multimodal fusion strategies. Further, we perform a
preliminary qualitative analysis on the emended captions and identify error
categories based on the type of corrections.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 21:55:25 GMT'}]",2020-10-30,"[['Kalimuthu', 'Marimuthu', ''], ['Mogadala', 'Aditya', ''], ['Mosbach', 'Marius', ''], ['Klakow', 'Dietrich', '']]"
1372058,2010.15728,Hang Dong,"Hang Dong, V\'ictor Su\'arez-Paniagua, William Whiteley, Honghan Wu","Explainable Automated Coding of Clinical Notes using Hierarchical
  Label-wise Attention Networks and Label Embedding Initialisation","Structured abstract in full text, 17 pages, 5 figures, 4
  supplementary materials (3 extra pages), submitted to Journal of Biomedical
  Informatics",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diagnostic or procedural coding of clinical notes aims to derive a coded
summary of disease-related information about patients. Such coding is usually
done manually in hospitals but could potentially be automated to improve the
efficiency and accuracy of medical coding. Recent studies on deep learning for
automated medical coding achieved promising performances. However, the
explainability of these models is usually poor, preventing them to be used
confidently in supporting clinical practice. Another limitation is that these
models mostly assume independence among labels, ignoring the complex
correlation among medical codes which can potentially be exploited to improve
the performance. We propose a Hierarchical Label-wise Attention Network (HLAN),
which aimed to interpret the model by quantifying importance (as attention
weights) of words and sentences related to each of the labels. Secondly, we
propose to enhance the major deep learning models with a label embedding (LE)
initialisation approach, which learns a dense, continuous vector representation
and then injects the representation into the final layers and the label-wise
attention layers in the models. We evaluated the methods using three settings
on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS
COVID-19 shielding codes. Experiments were conducted to compare HLAN and LE
initialisation to the state-of-the-art neural network based methods. HLAN
achieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and
comparable results on the NHS COVID-19 shielding code prediction to other
models. By highlighting the most salient words and sentences for each label,
HLAN showed more meaningful and comprehensive model interpretation compared to
its downgraded baselines and the CNN-based models. LE initialisation
consistently boosted most deep learning models for automated medical coding.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 16:21:26 GMT'}]",2020-10-30,"[['Dong', 'Hang', ''], ['Suárez-Paniagua', 'Víctor', ''], ['Whiteley', 'William', ''], ['Wu', 'Honghan', '']]"
1371932,2010.15602,Junhua Liu,Nachamma Sockalingam and Junhua Liu,Designing learning experiences for online teaching and learning,,,,,cs.CY cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Teaching is about constantly innovating strategies, ways and means to engage
diverse students in active and meaningful learning. In line with this, SUTD
adopts various student-centric teaching and learning teaching methods and
approaches. This means that our graduate/undergraduate instructors have to be
ready to teach using these student student-centric teaching and learning
pedagogies. In this article, I share my experiences of redesigning this
teaching course that is typically conducted face-to-face to a synchronous
online course and also invite one of the participant in this course to reflect
on his experience as a student.
","[{'version': 'v1', 'created': 'Mon, 26 Oct 2020 07:03:49 GMT'}]",2020-10-30,"[['Sockalingam', 'Nachamma', ''], ['Liu', 'Junhua', '']]"
1371479,2010.15149,Yiwei Luo,"Yiwei Luo, Dallas Card, Dan Jurafsky",DeSMOG: Detecting Stance in Media On Global Warming,"9 pages, 6 figures (excluding references and appendices). To appear
  in Findings of EMNLP 2020",Findings of EMNLP 2020,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Citing opinions is a powerful yet understudied strategy in argumentation. For
example, an environmental activist might say, ""Leading scientists agree that
global warming is a serious concern,"" framing a clause which affirms their own
stance (""that global warming is serious"") as an opinion endorsed (""[scientists]
agree"") by a reputable source (""leading""). In contrast, a global warming denier
might frame the same clause as the opinion of an untrustworthy source with a
predicate connoting doubt: ""Mistaken scientists claim [...]."" Our work studies
opinion-framing in the global warming (GW) debate, an increasingly partisan
issue that has received little attention in NLP. We introduce DeSMOG, a dataset
of stance-labeled GW sentences, and train a BERT classifier to study novel
aspects of argumentation in how different sides of a debate represent their own
and each other's opinions. From 56K news articles, we find that similar
linguistic devices for self-affirming and opponent-doubting discourse are used
across GW-accepting and skeptic media, though GW-skeptical media shows more
opponent-doubt. We also find that authors often characterize sources as
hypocritical, by ascribing opinions expressing the author's own view to source
entities known to publicly endorse the opposing view. We release our stance
dataset, model, and lexicons of framing devices for future work on
opinion-framing and the automatic detection of GW stance.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 18:01:02 GMT'}]",2020-10-30,"[['Luo', 'Yiwei', ''], ['Card', 'Dallas', ''], ['Jurafsky', 'Dan', '']]"
1371630,2010.15300,Emaad Manzoor,"Emaad Manzoor, Nihar B. Shah",Uncovering Latent Biases in Text: Method and Application to Peer Review,,,,,cs.CL cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Quantifying systematic disparities in numerical quantities such as employment
rates and wages between population subgroups provides compelling evidence for
the existence of societal biases. However, biases in the text written for
members of different subgroups (such as in recommendation letters for male and
non-male candidates), though widely reported anecdotally, remain challenging to
quantify. In this work, we introduce a novel framework to quantify bias in text
caused by the visibility of subgroup membership indicators. We develop a
nonparametric estimation and inference procedure to estimate this bias. We then
formalize an identification strategy to causally link the estimated bias to the
visibility of subgroup membership indicators, provided observations from time
periods both before and after an identity-hiding policy change. We identify an
application wherein ""ground truth"" bias can be inferred to evaluate our
framework, instead of relying on synthetic or secondary data. Specifically, we
apply our framework to quantify biases in the text of peer reviews from a
reputed machine learning conference before and after the conference adopted a
double-blind reviewing policy. We show evidence of biases in the review ratings
that serves as ""ground truth"", and show that our proposed framework accurately
detects these biases from the review text without having access to the review
ratings.
","[{'version': 'v1', 'created': 'Thu, 29 Oct 2020 01:24:19 GMT'}]",2020-10-30,"[['Manzoor', 'Emaad', ''], ['Shah', 'Nihar B.', '']]"
